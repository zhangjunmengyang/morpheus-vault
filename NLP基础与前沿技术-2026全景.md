---
title: "NLP基础与前沿技术-2026全景"
date: 2026-02-21
tags: [NLP, Transformer, LLM, RAG, 面试]
---

# NLP 基础与前沿技术——2026 技术全景

> 面试深度参考。覆盖从经典 NLP 到 LLM 时代的完整技术栈。

---

## 一、分词与词表示

### 1.1 子词分词算法

| 算法 | 核心思想 | 代表模型 |
|------|----------|----------|
| **BPE** | 统计字节对频率，贪心合并最高频 pair | GPT-2/3/4、LLaMA |
| **WordPiece** | 最大化语言模型似然来选合并 | BERT、DistilBERT |
| **Unigram LM** | 初始化大词表→逐步剪枝使 loss 最小 | T5、ALBERT |
| **SentencePiece** | 与语言无关的分词框架（BPE/Unigram 均可），直接处理 raw text | LLaMA、mT5 |

**关键区别**：BPE 自底向上合并，Unigram 自顶向下剪枝。SentencePiece 不依赖预分词（whitespace），对中文/日文天然友好。

**Byte-level BPE**（GPT-2+）：以 256 字节为基础词表，确保 100% 覆盖任意输入，无 `[UNK]`。

### 1.2 静态词向量

| 方法 | 训练目标 | 优势 | 局限 |
|------|----------|------|------|
| **Word2Vec (CBOW/SG)** | 局部窗口预测 | 训练快、语义类比 | 静态、OOV |
| **GloVe** | 全局共现矩阵分解 | 捕捉全局统计 | 静态、OOV |
| **FastText** | 子词 n-gram 之和 | 可处理 OOV | 仍然静态 |

**Word2Vec 数学**：Skip-gram 目标 = $\max \sum_{(w,c)} \log \sigma(v_c \cdot v_w) + k \cdot \mathbb{E}_{c' \sim P_n} [\log \sigma(-v_{c'} \cdot v_w)]$（负采样）

### 1.3 上下文化表示（Contextual Embeddings）

- **ELMo**（2018）：双向 LSTM 的各层隐状态加权和，同一词在不同句子中表示不同
- **BERT**（2018）：Transformer Encoder，通过 MLM 预训练获得深层双向表示
- **GPT 系列**：Transformer Decoder，单向上下文，但 LLM 规模下涌现强大能力

**面试高频问题**：
1. BPE 和 WordPiece 的核心区别？（合并依据：频率 vs 似然增益）
2. 为什么 LLaMA 用 SentencePiece 而非 WordPiece？（语言无关、不需要预分词、字节级无 UNK）
3. Word2Vec 的负采样在做什么？（近似 softmax，让正例得分高于随机负例）
4. ELMo 和 BERT 的上下文建模有何本质不同？（浅层双向拼接 vs 深层双向 self-attention）

**答题要点**：
- 词表大小通常 32K-128K，太小切碎太多（序列变长），太大 embedding 矩阵爆炸
- BPE 训练 = O(N × V) 迭代，生产中通常用 SentencePiece 的 BPE 实现
- 静态词向量仍有工程价值：轻量检索、冷启动 embedding 初始化

---

## 二、语言模型演进

### 2.1 演进路线

```
N-gram (统计) → RNN/LSTM (序列) → Transformer (并行) → BERT/GPT (预训练) → LLM (规模涌现)
```

### 2.2 关键里程碑

| 阶段 | 代表 | 参数量级 | 核心突破 |
|------|------|----------|----------|
| 统计 LM | Kneser-Ney N-gram | — | 平滑、回退 |
| 神经 LM | RNNLM / LSTM-LM | ~10M | 连续空间、长距依赖 |
| Attention | Seq2Seq + Attention | ~100M | 动态对齐 |
| Transformer | "Attention Is All You Need" | 65M-213M | 并行化、self-attention |
| 预训练 | BERT / GPT-2 | 110M-1.5B | 迁移学习革命 |
| 大模型 | GPT-3 / PaLM / LLaMA | 7B-540B | Few-shot、涌现能力 |
| LLM 时代 | GPT-4 / Claude / Gemini | 100B+ (MoE 万亿) | 多模态、推理、Agent |

### 2.3 LSTM vs Transformer

| 维度 | LSTM | Transformer |
|------|------|-------------|
| 序列建模 | 顺序递归 O(n) | 并行自注意力 O(1) depth |
| 长距离依赖 | 梯度衰减（实际 ~200 tokens） | 任意距离直接连接 |
| 训练并行度 | 低（时间步依赖） | 高（token 级并行） |
| 推理效率 | O(1) 增量（隐状态固定） | O(n) 增量（KV Cache 线性增长） |

**面试高频问题**：
1. 为什么 Transformer 取代了 LSTM？（并行训练效率 + 长距离建模 + scaling 性能更好）
2. LSTM 有什么 Transformer 不具备的优势？（固定大小隐状态，推理 memory O(1)，无限长度理论上可处理）
3. GPT 和 BERT 的预训练目标有什么本质区别？（CLM 自回归 vs MLM 去噪，单向 vs 双向）

**答题要点**：
- N-gram 的稀疏性问题是神经 LM 的直接动机
- LSTM 的遗忘门机制：$f_t = \sigma(W_f [h_{t-1}, x_t] + b_f)$
- Transformer 的 scaling 表现远优于 RNN 系，这是 LLM 时代的基础

---

## 三、Transformer 深度解析

### 3.1 Self-Attention 数学推导

**Scaled Dot-Product Attention**：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$$

- $Q = XW^Q$, $K = XW^K$, $V = XW^V$，其中 $W \in \mathbb{R}^{d_{model} \times d_k}$
- 除以 $\sqrt{d_k}$ 的原因：假设 $q, k$ 各分量独立均值 0 方差 1，则 $q \cdot k$ 的方差 = $d_k$，除以 $\sqrt{d_k}$ 使方差稳定为 1，防止 softmax 进入饱和区（梯度消失）

**Multi-Head Attention**：

$$\text{MHA}(Q,K,V) = \text{Concat}(\text{head}_1, ..., \text{head}_h) W^O$$
$$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$

- 每个 head 的维度 $d_k = d_{model} / h$，总计算量不变
- 不同 head 学习不同的注意力模式（位置关系、语法依赖、语义关联等）

### 3.2 Position Encoding

| 方案 | 原理 | 外推性 | 代表模型 |
|------|------|--------|----------|
| **绝对正弦** | $\sin/\cos$ 不同频率 | 差 | 原始 Transformer |
| **可学习绝对** | 每个位置一个可学习向量 | 差（超训练长度崩溃） | BERT、GPT-2 |
| **RoPE** | 将位置信息编码为旋转矩阵，作用于 Q/K | 好（NTK-aware 可扩展） | LLaMA、Qwen、Mistral |
| **ALiBi** | 不编码位置，直接在 attention score 上加线性 bias | 极好 | BLOOM、MPT |
| **YaRN** | RoPE 外推优化，NTK + 高频内插 | 极好 | LLaMA 长上下文微调 |

**RoPE 核心思想**：$f(q, m) = q e^{im\theta}$（二维旋转），使 $\langle f(q,m), f(k,n) \rangle$ 仅依赖于相对位置 $m-n$。

### 3.3 KV Cache

- **问题**：自回归生成时，每生成一个 token 需要重新计算所有前缀的 K/V → O(n²) 浪费
- **解法**：缓存历史 token 的 K/V，每步只计算新 token 的 Q/K/V → O(n) per step
- **内存开销**：$2 \times L \times 2 \times d_{model} \times n \times \text{bytes}$（L 层，KV 各一个，n 为序列长度）
- **70B FP16 + 4K context**：KV Cache ≈ 2 × 80 × 2 × 8192 × 4096 × 2B ≈ 20GB

**KV Cache 优化**：
- **PagedAttention**（vLLM）：分页管理消除内存碎片
- **GQA**（Grouped Query Attention）：多 Q head 共享 KV head，减少 KV Cache 大小
- **MQA**（Multi-Query Attention）：所有 Q head 共享 1 组 KV
- **Sliding Window**（Mistral）：只缓存固定窗口长度的 KV

### 3.4 FFN 与架构变体

- 原始：$\text{FFN}(x) = \text{ReLU}(xW_1 + b_1)W_2 + b_2$，隐层 4x
- **SwiGLU**（LLaMA）：$\text{SwiGLU}(x) = \text{Swish}(xW_1) \otimes (xW_3)$，Gate 机制，实验效果更好
- **Pre-Norm vs Post-Norm**：Pre-Norm（LayerNorm 在 attention 前）训练更稳定，现代 LLM 标配
- **RMSNorm**：去掉均值中心化，只做缩放，计算更快

**面试高频问题**：
1. 为什么除以 $\sqrt{d_k}$？（数学推导：方差稳定化，防 softmax 饱和）
2. Multi-Head 的意义？为什么不用一个大 head？（子空间多样性，不同 head 捕捉不同模式）
3. RoPE 和 ALiBi 的本质区别？（RoPE 编码在 embedding 中旋转 Q/K，ALiBi 直接在 score 上加 bias）
4. KV Cache 的内存瓶颈如何缓解？（GQA/MQA、量化 KV、PagedAttention、Sliding Window）
5. 为什么现代 LLM 都用 Pre-Norm + RMSNorm？（训练稳定性 + 计算效率）

**答题要点**：
- Attention 复杂度 O(n²d)，Memory O(n²)——这是长上下文的核心挑战
- GQA 是 MHA 和 MQA 的折中，LLaMA 2 70B 用 8 组 KV head（32 Q head）
- FlashAttention 不改变数学，只优化 IO：tiling + recomputation，避免写 O(n²) attention matrix

---

## 四、预训练范式

### 4.1 三大预训练目标

| 范式 | 目标 | 代表 | 适合任务 |
|------|------|------|----------|
| **CLM**（Causal LM） | 预测下一个 token：$P(x_t \| x_{<t})$ | GPT 系列 | 生成、对话 |
| **MLM**（Masked LM） | 预测被 mask 的 token：$P(x_i \| x_{\setminus i})$ | BERT | 理解、分类、NER |
| **Seq2Seq** | Encoder 读入，Decoder 生成 | T5、BART | 翻译、摘要、QA |

**T5 的 Span Corruption**：随机 mask 连续 span（平均长度 3），Decoder 生成 `<extra_id_0>` 对应的 span。

### 4.2 Scaling Laws

**Kaplan (OpenAI 2020)**：$L(N, D) \approx A / N^{\alpha_N} + B / D^{\alpha_D} + E$
- Loss 与模型参数 N、数据量 D 呈幂律下降
- 给定固定 compute，应优先增大模型（模型参数比数据量更重要）

**Chinchilla (DeepMind 2022)**：最优配比 $D \approx 20 \times N$
- 推翻 Kaplan 结论：之前的大模型普遍 **under-trained**
- 70B + 1.4T tokens（Chinchilla）≈ 280B + 300B tokens（Gopher）

**Chinchilla 之后的实践**（2024-2026）：
- LLaMA 3 405B 训练 15T tokens（远超 Chinchilla optimal）
- 原因：推理成本主要取决于模型大小，训练多投数据降低推理成本
- **Inference-Optimal Scaling**：考虑推理部署成本的新 trade-off

### 4.3 MoE（Mixture of Experts）

- **核心思想**：每个 token 只激活 Top-k 个 expert（通常 k=2），总参数量大但激活参数量小
- **Router**：$G(x) = \text{TopK}(\text{softmax}(xW_g))$，路由网络决定激活哪些 expert
- **Load Balancing Loss**：防止所有 token 只走少数 expert → 加辅助 loss 均衡负载
- **代表**：Mixtral 8×7B（激活 2/8，总 47B 激活 ~13B）、DeepSeek-V3（256 expert，激活 8）
- **优势**：同等激活参数下性能更好 / 推理 FLOPs 可控
- **挑战**：通信开销（expert 在不同 GPU）、负载不均、训练不稳定

**面试高频问题**：
1. Scaling Law 的实际工程意义？（训练前可预测 loss，做 compute-optimal 决策）
2. Chinchilla 结论在 2026 还适用吗？（基准适用，但实际会 over-train 以优化推理成本）
3. MoE 的路由坍塌问题如何解决？（Load balancing loss + expert dropout + capacity factor）
4. MLM 和 CLM 哪个更好？（任务依赖：理解用 MLM，生成用 CLM，LLM 时代 CLM 主导）

**答题要点**：
- 预训练数据质量 > 数量（dedup、filtering、quality scoring 极其关键）
- 数据配比（code:web:book:wiki）对模型能力影响巨大
- MoE 的 Expert Parallelism 需要 All-to-All 通信，是分布式训练的额外挑战

---

## 五、微调技术

### 5.1 Full Fine-tuning

- 更新模型所有参数，最直接但成本最高
- 需要完整模型副本的优化器状态（Adam: 参数量 ×12 bytes for FP32）
- 7B 模型 full FT：至少 2×A100 80GB（模型 14GB + 优化器 84GB + 梯度 14GB）

### 5.2 Parameter-Efficient Fine-Tuning（PEFT）

| 方法 | 原理 | 可训练参数 | 优势 | 局限 |
|------|------|------------|------|------|
| **LoRA** | 在 W 旁加低秩矩阵 $BA$，$\Delta W = BA$ | 0.1-1% | 推理无额外延迟（可合并） | rank 选择需调参 |
| **QLoRA** | 4-bit 量化基模型 + LoRA | 0.1-1% | 单卡 24GB 微调 65B | 量化误差、训练稍慢 |
| **Adapter** | 在 FFN 后插入小 bottleneck 层 | 1-5% | 模块化 | 推理有额外延迟 |
| **Prefix-Tuning** | 在 KV 前拼接可学习虚拟 token | <0.1% | 极少参数 | 占用上下文窗口 |
| **P-Tuning v2** | 每层加可学习 prefix | 0.1-1% | 效果接近 full FT | 实现复杂 |
| **IA3** | 学习向量缩放 K/V/FFN | <0.01% | 极少参数 | 表达力有限 |

### 5.3 LoRA 深度解析

**数学**：$h = W_0 x + \Delta W x = W_0 x + BAx$，其中 $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times d}$, $r \ll d$

- 初始化：$A$ 随机高斯，$B = 0$ → 训练开始时 $\Delta W = 0$
- 缩放因子：$\alpha / r$，通常 $\alpha = 2r$
- **应用位置**：通常加在 Q/V 投影上，实验表明同时加 Q/K/V/O + FFN 效果最好
- **Rank 选择**：r=8-64 常见，任务越复杂需要越大 rank
- **DoRA**（2024）：将权重分解为方向和大小，分别学习，优于 LoRA

**QLoRA 关键技术**：
- **NF4 量化**：4-bit NormalFloat，对正态分布权重最优的 4-bit 数据类型
- **双重量化**：对量化常数本身再量化，节省 ~0.4 bit/param
- **Paged Optimizers**：利用 CPU RAM 处理 GPU OOM 的 spike

### 5.4 实战选型

```
数据量 < 100 条 → Prompt Engineering / In-context Learning
100-10K 条 → LoRA (r=16-64) / QLoRA
10K-100K 条 → LoRA (大 rank) 或 Full FT (小模型)
> 100K 条 + 任务复杂 → Full Fine-tuning
```

**面试高频问题**：
1. LoRA 的数学原理？为什么有效？（预训练权重的任务适应具有低秩结构，Aghajanyan et al. 2020 证明了 intrinsic dimensionality 很低）
2. QLoRA 如何实现单卡微调 70B？（NF4 量化 70B→35GB + LoRA 可训练参数少 + paged optimizer）
3. LoRA 和 Adapter 推理时的区别？（LoRA 可合并到原权重无额外延迟，Adapter 有串行 bottleneck）
4. 微调 vs RAG 怎么选？（知识更新用 RAG，风格/格式调整用微调，通常组合使用）

**答题要点**：
- LoRA 的 rank 不是越大越好，过大会过拟合
- 微调数据质量 >> 数量，1K 高质量数据 > 100K 低质量数据
- 多任务微调时，不同 LoRA adapter 可以热切换（LoRA serving）

---

## 六、对齐技术

### 6.1 RLHF Pipeline

```
预训练模型 → [SFT] → SFT 模型 → [RM Training] → Reward Model → [PPO] → 对齐模型
```

**Stage 1: SFT（Supervised Fine-Tuning）**
- 高质量 instruction-response 数据（通常 10K-100K 条）
- 让模型学会遵循指令的格式和风格

**Stage 2: Reward Modeling**
- 人工标注偏好对 $(x, y_w, y_l)$：$y_w$ 优于 $y_l$
- Bradley-Terry 模型：$P(y_w \succ y_l) = \sigma(r(x, y_w) - r(x, y_l))$
- RM 的质量是 RLHF 的瓶颈（标注一致性仅 65-75%）

**Stage 3: PPO 优化**
- 最大化 reward 同时约束与 SFT 模型的 KL 散度：$\max \mathbb{E}[r(x,y)] - \beta \text{KL}(\pi_\theta \| \pi_{\text{ref}})$
- 需要 4 个模型：policy、reference、reward、value（内存和计算成本极高）

### 6.2 DPO 及其变体

| 方法 | 核心创新 | 优势 | 局限 |
|------|----------|------|------|
| **DPO** | RL 有解析解，绕过 RM+PPO | 简单稳定，2 模型 | offline，无探索 |
| **ORPO** | 不需 reference model，将 odds ratio 作为对比信号 | 1 模型更简单 | 效果略逊 DPO |
| **KTO** | 只需 binary 信号（好/坏），不需成对 | 数据要求最低 | 理论较新 |
| **SimPO** | 用序列平均 log prob 替代 reference，length-normalized | 去掉 ref model | 长度归一化可能丢信息 |

**DPO 损失函数**：$\mathcal{L}_{\text{DPO}} = -\log \sigma\left(\beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)$

### 6.3 Constitutional AI & RLAIF

- **Constitutional AI**（Anthropic）：用 AI 自己评判输出是否符合一组「宪法原则」，替代人工标注
- **RLAIF**：AI 生成偏好标签训练 RM → 成本降低 10-100x，质量接近 RLHF
- **Self-play**（如 SPIN）：模型自己生成→自己判断→迭代提升

### 6.4 2026 前沿对齐

- **GRPO**（DeepSeek）：Group Relative Policy Optimization，去掉 value model，用 group 内相对 reward
- **RLVR**（Reinforcement Learning with Verifiable Rewards）：代码/数学有确定答案可自动验证
- **Process RM**：过程监督 vs 结果监督，每步打分而非只看最终答案

**面试高频问题**：
1. DPO 的数学推导？（从 RLHF 的 KL-constrained 优化有解析解 → 反解 reward → 代入 BT 模型 → partition function 相消）
2. RLHF 的 reward hacking 怎么发生？（模型找到 RM 的漏洞获得高分但实际质量差，如变长/重复/迎合）
3. DPO 和 PPO 的核心 trade-off？（DPO 简单稳定但 offline 无探索；PPO 在线探索但训练复杂不稳定）
4. 为什么 DeepSeek 选择 GRPO 而非 PPO？（省掉 value model 减少内存；group 内对比天然归一化）

**答题要点**：
- 对齐是安全问题不只是性能问题，过度优化 reward 反而有害（Goodhart's Law）
- 数据质量 > 算法选择：好的偏好数据 + 简单 DPO > 差数据 + 复杂 PPO
- Process RM 是推理模型（o1 类）的关键技术

---

## 七、RAG（检索增强生成）

### 7.1 标准 RAG 架构

```
Query → [Embedding] → [向量检索] → [Rerank] → Top-K Chunks → [LLM 生成]
                            ↑
              文档 → [Chunking] → [Embedding] → Vector DB
```

### 7.2 Chunking 策略

| 策略 | 方法 | 适用场景 |
|------|------|----------|
| **固定大小** | 每 512 tokens 切分，overlap 50-100 | 简单快速，通用 |
| **语义分割** | 用 embedding 相似度检测语义断点 | 学术论文、复杂文档 |
| **递归字符** | 按段落→句子→字符逐级切分 | LangChain 默认 |
| **文档结构** | 按标题/章节/表格切分 | 结构化文档（PDF、HTML） |
| **Parent-Child** | 小 chunk 检索，返回 parent chunk 上下文 | 精确检索 + 完整上下文 |

### 7.3 Embedding 模型选型（2026）

| 模型 | 维度 | MTEB 排名 | 特点 |
|------|------|-----------|------|
| **BGE-M3** | 1024 | Top-5 | 多语言、多粒度、多功能 |
| **E5-Mistral** | 4096 | Top-3 | LLM-based，精度高但大 |
| **Cohere embed-v4** | 1024 | Top-3 | 商业方案，multi-modal |
| **GTE-Qwen2** | 1536 | Top-5 | 中文强 |
| **Nomic-embed-text** | 768 | Top-10 | 开源、轻量 |

### 7.4 Reranker

- **Cross-Encoder**：将 query-document pair 拼接输入 BERT，输出相关度分数
- **ColBERT**：late interaction，token 级向量匹配后取 MaxSim，兼顾精度和速度
- **LLM Reranker**：用 LLM 判断相关性（RankGPT），最高精度但最慢
- **典型 pipeline**：向量检索 Top-100 → Reranker → Top-5 → LLM

### 7.5 Advanced RAG

- **Multi-hop RAG**：多步检索，第一步结果指导第二步 query → 解决多跳推理
- **Graph RAG**：构建知识图谱 + 社区摘要，适合全局性问题（"整体趋势是什么"）
- **Hybrid Search**：BM25（关键词） + Dense Retrieval（语义）融合，RRF 排序
- **Query Transformation**：HyDE（假设性回答）、Multi-query（多角度重写）、Step-back（抽象化）

### 7.6 评估框架（RAGAS）

| 指标 | 衡量什么 | 计算方式 |
|------|----------|----------|
| **Faithfulness** | 生成内容是否忠于检索结果 | 逐句验证是否有 context 支持 |
| **Answer Relevancy** | 回答是否切题 | 反向生成问题→与原问题比较 |
| **Context Precision** | 检索结果中相关文档的排序质量 | 相关 chunk 的排名越前越好 |
| **Context Recall** | 是否检索到了所有需要的信息 | 与 ground truth 比较覆盖率 |

**面试高频问题**：
1. RAG 的核心挑战是什么？（检索质量决定天花板，chunk 策略是关键，长上下文 vs RAG 的 trade-off）
2. 长上下文 LLM 能替代 RAG 吗？（不能：成本高、Lost-in-the-Middle、无法实时更新 10M+ 文档）
3. 如何评估 RAG 系统？（Faithfulness + Relevancy + Precision + Recall，RAGAS 框架）
4. Graph RAG 和传统 RAG 的区别？（传统 RAG 适合局部事实查询，Graph RAG 适合全局性总结性问题）

**答题要点**：
- RAG 不是"检索+拼接"这么简单，query understanding → retrieval → reranking → generation 每一步都可优化
- Embedding 模型要与 chunking 策略匹配（chunk 太大 embedding 质量下降）
- Hybrid search (BM25 + Dense) 几乎总是优于纯 dense retrieval

---

## 八、文本分类与信息抽取

### 8.1 方法演进

| 阶段 | 方法 | 准确率（典型） | 成本 |
|------|------|----------------|------|
| 传统 ML | TF-IDF + SVM/LR | 85-90% | 低 |
| 深度学习 | TextCNN / BiLSTM | 88-92% | 中 |
| 预训练微调 | BERT + [CLS] head | 92-96% | 高 |
| LLM ICL | GPT-4 + few-shot | 88-94% | 极高 per query |

### 8.2 BERT 文本分类

```
[CLS] token1 token2 ... [SEP] → BERT → [CLS] hidden → Linear → Softmax → Class
```

- [CLS] 的最终隐状态作为整句表示
- 微调时通常 lr=2e-5，epochs=3-5，warmup=10%

### 8.3 信息抽取

- **关系抽取**：给定 entity pair，判断关系类型
  - Pipeline：NER → 关系分类
  - Joint：同时抽取实体和关系（TPLinker、CasRel）
- **事件抽取**：识别事件触发词 + 论元角色
- **LLM 方案**：结构化 prompt + JSON 输出，零样本能力强但需要 structured generation 保证格式

### 8.4 TF-IDF 本质

$\text{TF-IDF}(t, d) = \text{TF}(t, d) \times \log \frac{N}{\text{DF}(t)}$

- TF：词频（局部信息），IDF：逆文档频率（全局稀缺性）
- 本质是稀疏表示，高维、可解释，但忽略语义和词序

**面试高频问题**：
1. BERT 分类为什么用 [CLS]？能用其他方式吗？（[CLS] 在预训练 NSP 中已学会聚合句级信息；也可用 mean pooling / attention pooling）
2. TF-IDF + SVM 在什么场景下还有优势？（数据少、需要可解释、低延迟、领域词汇明确）
3. LLM 做分类的优劣？（零样本强但成本高、延迟高、标签空间大时不稳定）

**答题要点**：
- 生产中经常用 LLM 标注数据 → 训练小模型蒸馏部署（成本降 100x）
- 多标签分类用 sigmoid 替代 softmax，threshold 需在验证集调
- 长文本分类：hierarchical attention 或 sliding window + pooling

---

## 九、序列标注与 NER

### 9.1 经典架构：BiLSTM-CRF

```
Input → Embedding → BiLSTM → Emission Scores → CRF → Tag Sequence
```

- **BiLSTM**：提取上下文特征，输出每个 token 的各标签得分（emission score）
- **CRF 层的作用**：建模标签间的转移概率，确保输出合法（如 I-PER 不能跟在 B-LOC 后面）
- **Viterbi 解码**：CRF 推理用动态规划找全局最优标签序列

### 9.2 BERT-CRF

- 用 BERT 替代 BiLSTM 作为 encoder，效果大幅提升
- 实践中很多人发现 BERT + softmax 已经很强，CRF 的增益在 BERT 时代变小
- 原因：BERT 的 self-attention 已经捕捉了足够的上下文和标签依赖

### 9.3 Prompt-based NER

- 将 NER 转化为生成/QA 任务：「文本中有哪些人名？」→ LLM 直接输出
- **UIE**（Universal Information Extraction）：统一信息抽取为 Seq2Seq
- **优势**：零样本/少样本能力、新实体类型无需重训练
- **劣势**：速度慢、输出不保证格式正确、长文本处理困难

### 9.4 Few-shot NER

- **Meta-learning**：学习如何从少量样本快速适应新实体类型（ProtoNet、MAML）
- **In-context learning**：给 LLM 几个标注示例 → 直接推理
- **数据增强**：LLM 生成训练数据 → 微调小模型

**BIO 标注方案**：
- B-XXX：实体开始，I-XXX：实体内部，O：非实体
- 变体：BIOES（加 E-end 和 S-single），解决嵌套边界更明确

**面试高频问题**：
1. CRF 层在 NER 中的作用？（建模标签转移约束，保证输出合法序列）
2. BERT 时代还需要 CRF 吗？（小数据集/标签约束严格时有帮助，大数据集增益不大）
3. 嵌套 NER 怎么做？（Span-based：枚举所有 span 分类 / Biaffine 模型 / MRC-based NER）
4. Few-shot NER 的主流方案？（ProtoNet metric learning / LLM in-context / LLM 生成数据+蒸馏）

**答题要点**：
- CRF 的训练用前向后向算法，推理用 Viterbi，复杂度 O(T × K²)（T 序列长，K 标签数）
- 中文 NER 特殊性：没有天然分词边界，通常字级别标注
- 工业级 NER 通常 BERT-base + CRF 即可，不需要 LLM

---

## 十、机器翻译与多语言

### 10.1 Encoder-Decoder 架构

- **经典 Seq2Seq + Attention**：Encoder 编码源语言，Decoder 逐 token 生成目标语言
- **Transformer NMT**：完全基于 attention，并行训练效率高
- **关键技术**：BPE 共享词表（源+目标语言）、label smoothing、beam search

### 10.2 多语言预训练模型

| 模型 | 参数 | 语言数 | 特点 |
|------|------|--------|------|
| **mBERT** | 110M | 104 | 简单扩展 BERT 到多语言 |
| **XLM-R** | 270M-3.5B | 100 | CC-100 数据，跨语言迁移强 |
| **NLLB** | 600M-54B | 200+ | Meta 专注低资源语言翻译 |
| **mT5** | 300M-13B | 101 | T5 多语言版 |

**跨语言迁移**：用英文标注数据微调 XLM-R → 直接在中文/法语等 zero-shot 推理，效果可达 fine-tune 的 70-90%。

### 10.3 翻译质量评估

| 指标 | 类型 | 原理 | 局限 |
|------|------|------|------|
| **BLEU** | N-gram 匹配 | 精确率 + brevity penalty | 不考虑语义，同义词得 0 分 |
| **ROUGE** | N-gram 匹配 | 召回率导向 | 同上 |
| **BERTScore** | 语义匹配 | 用 BERT embedding 计算 token 相似度 | 需要模型 |
| **COMET** | 学习型 | 训练模型预测人类评分 | 与人类评判相关性最高 |

**BLEU 公式**：$\text{BLEU} = \text{BP} \cdot \exp\left(\sum_{n=1}^{4} \frac{1}{4} \log p_n\right)$，BP 是短句惩罚。

**面试高频问题**：
1. BLEU 的主要缺陷？（不考虑语义、不考虑流畅度、与人类判断相关性下降）
2. 多语言模型为什么能跨语言迁移？（共享 subword 词表 + 深层表示空间对齐 + 语言无关的语义结构）
3. 低资源语言翻译的主流方案？（NLLB、回译增强、pivot 语言中转、多语言预训练）

**答题要点**：
- 2026 年 LLM（GPT-4、Claude）翻译质量已超越传统 NMT（尤其是文学/口语/上下文翻译）
- 但专业领域（法律/医学）仍需 domain-specific 微调
- Evaluation 趋势：COMET > BERTScore > BLEU，但 BLEU 仍是 baseline

---

## 十一、文本生成控制

### 11.1 Decoding 策略

| 策略 | 原理 | 适用场景 |
|------|------|----------|
| **Greedy** | 每步选概率最高的 token | 确定性任务（分类标签） |
| **Beam Search** | 维护 B 条候选，选总概率最高 | 翻译、摘要 |
| **Top-k** | 只从概率前 k 个 token 采样 | 创意生成 |
| **Top-p (Nucleus)** | 从概率累积超过 p 的最小集合采样 | 通用生成（推荐） |
| **Temperature** | $p_i = \frac{e^{z_i/T}}{\sum e^{z_j/T}}$，T<1 更确定，T>1 更随机 | 控制多样性 |

**Top-p 的优势**：自适应候选集大小——高信心时集中，低信心时展开。通常 p=0.9-0.95。

### 11.2 Repetition Penalty

- **Frequency Penalty**：出现次数越多，logit 减越多
- **Presence Penalty**：只要出现过就减固定值，鼓励新 topic
- **No-repeat N-gram**：硬性禁止重复 n-gram

### 11.3 Structured Generation（2024-2026 热点）

- **问题**：LLM 输出不保证符合 JSON Schema / 正则 / CFG
- **Outlines**：编译 JSON Schema 为有限状态机（FSM），每步 mask 非法 token
- **Guidance**（微软）：模板引导生成，交替固定文本和 LLM 自由生成
- **SGLang**：将 constrained decoding 集成到 serving 引擎，与 RadixAttention 结合
- **原理**：在每步 logit 上 apply mask，只允许合法的 next token → 保证格式正确

**面试高频问题**：
1. Temperature 和 Top-p 的区别？（Temperature 改变分布形状，Top-p 截断分布尾部）
2. 为什么 Beam Search 生成有时很无聊？（倾向高频通用表达，缺乏多样性——"the the the" 问题）
3. Structured Generation 的性能开销？（FSM 编译一次，之后每步只需 O(V) mask 操作，开销很小）
4. Temperature=0 等于 Greedy 吗？（数学上 T→0 等价于 argmax，实际实现中 T=0 通常就是 greedy）

**答题要点**：
- 生产中通常组合使用：Temperature + Top-p + Frequency penalty
- Structured Generation 是 LLM Agent 的关键基础设施（tool call 参数必须合法 JSON）
- Beam Search 在 LLM 时代用得少了（主要用采样），在 NMT 中仍是标配

---

## 十二、评估与 Benchmark

### 12.1 语言模型基础指标

- **Perplexity**：$\text{PPL} = \exp\left(-\frac{1}{N}\sum \log P(x_i | x_{<i})\right)$
- 越低越好，但只衡量"预测下一个 token 的能力"，不等于"有用性"
- 不同 tokenizer 的 PPL 不可直接比较（token 粒度不同）

### 12.2 主流 Benchmark（2026）

| Benchmark | 评估维度 | 特点 |
|-----------|----------|------|
| **MMLU** | 知识广度（57 学科） | 多选题，被刷分严重 |
| **MMLU-Pro** | 更难的知识题（10 选项+推理） | MMLU 的升级版 |
| **HellaSwag** | 常识推理 | 对抗过滤，人 95% / GPT-4 ~97% |
| **HumanEval / MBPP** | 代码生成 | Pass@k 指标 |
| **GSM8K / MATH** | 数学推理 | 小学到竞赛级 |
| **ARC** | 科学推理 | Challenge set 较难 |
| **TruthfulQA** | 真实性 | 测试是否产生常见误解 |
| **IFEval** | 指令跟随 | 严格格式约束下的服从度 |

### 12.3 LLM-as-Judge

- **核心思想**：用强 LLM（GPT-4/Claude）评判输出质量，替代人工评估
- **MT-Bench**：80 道多轮对话，GPT-4 打 1-10 分
- **Arena Elo**：Chatbot Arena 盲测排名，用户投票 → Elo Rating
- **偏差**：位置偏差（先出现的得高分）、verbosity bias（长回答得高分）、self-bias（GPT-4 偏好 GPT-4 风格）

### 12.4 评估方法论

- **静态 benchmark 的问题**：数据泄漏、过拟合、不代表真实场景
- **动态评估**：LiveBench（定期更新）、Arena（持续对战）
- **领域评估**：法律（LegalBench）、医学（MedQA）、金融（FinBench）
- **安全评估**：HarmBench、DecodingTrust

**面试高频问题**：
1. 为什么不能只看 MMLU 分数？（数据泄漏、多选题不代表生成能力、被刷分优化）
2. LLM-as-Judge 的主要偏差？（位置、长度、自我偏好、不擅长评估数学/代码正确性）
3. Perplexity 低 = 模型好？（不一定：PPL 只衡量语言建模，不衡量指令跟随/推理/安全）
4. 怎样全面评估一个 LLM？（多维度：知识 + 推理 + 代码 + 安全 + 指令跟随 + 人类偏好）

**答题要点**：
- Arena Elo 目前是最接近"真实用户偏好"的排名
- Pass@k 计算：生成 n 个 sample，Pass@k = $1 - \binom{n-c}{k} / \binom{n}{k}$（c 为正确数）
- Benchmark 饱和后会不断出更难版本（MMLU→MMLU-Pro，GSM8K→MATH→AIME）

---

## 十三、2026 前沿趋势

### 13.1 Test-time Compute（推理时计算）

- **核心思想**：在推理阶段投入更多计算换取更好结果
- **OpenAI o1/o3**：内置 chain-of-thought，"思考"时间换准确率
- **DeepSeek R1**：开源推理模型，用 RL 训练"思考"能力
- **Scaling 曲线**：推理 compute 投入与准确率也呈 scaling law 关系
- **实现方式**：extended CoT、tree search（MCTS）、self-verification、multi-sampling + voting

### 13.2 长上下文（1M+ tokens）

- **技术栈**：RoPE 外推（YaRN/NTK-aware）+ Ring Attention + Sparse Attention
- **Gemini 1.5**：原生 1M context，实测能"大海捞针"
- **Ring Attention**：分布式 attention，序列分块在多 GPU 上环形传递 KV
- **挑战**：Lost-in-the-Middle（中间位置信息易丢失）、成本 O(n²)、延迟

### 13.3 多模态 NLP

- **Vision-Language**：LLaVA、GPT-4V、Gemini → 图文理解
- **Audio-Language**：Whisper + LLM、Gemini → 语音理解
- **统一架构趋势**：不再区分"NLP 模型"和"Vision 模型"，一个模型处理所有模态
- **Native Multimodal**：从预训练开始就是多模态（Gemini），而非后期 adapter（LLaVA）

### 13.4 Reasoning Models

- **o1/o3 系列**：闭源，推理任务（AIME/Codeforces）大幅超越 GPT-4
- **DeepSeek R1**：开源，GRPO+RLVR 训练，671B MoE
- **关键发现**：推理能力可以通过 RL 在大模型中"涌现"，不需要显式教 CoT
- **蒸馏**：R1 的能力可蒸馏到小模型（DeepSeek-R1-Distill-Qwen-7B），效果惊人

### 13.5 其他趋势

- **Agentic NLP**：LLM 作为 Agent 的语言核心，tool use + planning + memory
- **Synthetic Data**：高质量合成数据成为预训练/微调的重要来源
- **Efficiency**：KV Cache 压缩、Speculative Decoding、模型蒸馏、1-bit LLM（BitNet）
- **安全与治理**：对齐税（alignment tax）优化、jailbreak 防御、watermarking

**面试高频问题**：
1. Test-time Compute 的本质是什么？（用推理时的算力换取任务准确率，打破训练时 scaling 的瓶颈）
2. 长上下文和 RAG 怎么选？（互补：长上下文适合完整文档、RAG 适合大规模知识库、成本差 100x）
3. 多模态对 NLP 的影响？（NLP 边界模糊化，纯文本任务被多模态模型"免费"解决）
4. DeepSeek R1 的训练范式有什么启发？（RL + verifiable reward 可以涌现推理能力，不需要人工标注 CoT）

**答题要点**：
- 2026 的 NLP 不再是独立方向，而是 LLM 能力的一个维度
- 面试中展现对前沿的了解（test-time compute、reasoning models）是加分项
- 但基础功底（Transformer、attention、tokenization）永远是核心考点

---

## 附录 A：核心公式速查

| 公式 | 表达式 |
|------|--------|
| Self-Attention | $\text{softmax}(QK^T / \sqrt{d_k}) V$ |
| Softmax | $p_i = e^{z_i} / \sum_j e^{z_j}$ |
| Cross-Entropy | $\mathcal{L} = -\sum y_i \log p_i$ |
| Perplexity | $\exp(-\frac{1}{N} \sum \log P(x_i \| x_{<i}))$ |
| BT Model | $P(y_w \succ y_l) = \sigma(r_w - r_l)$ |
| DPO Loss | $-\log \sigma(\beta (\log \frac{\pi_\theta(y_w)}{\pi_{ref}(y_w)} - \log \frac{\pi_\theta(y_l)}{\pi_{ref}(y_l)}))$ |
| BLEU | $BP \cdot \exp(\sum_{n=1}^{4} \frac{1}{4} \log p_n)$ |
| TF-IDF | $\text{TF}(t,d) \times \log(N / \text{DF}(t))$ |
| LoRA | $h = W_0 x + BAx$, $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times d}$ |
| RoPE | $f(q,m) = q \cdot e^{im\theta}$ |

## 附录 B：面试速答对照表

| 问题 | 一句话答案 |
|------|-----------|
| 为什么除以 √d_k | softmax 输入方差稳定为 1，防止梯度消失 |
| GQA vs MQA | GQA 是折中——多组 KV head 共享，MQA 只有 1 组 |
| LoRA 为什么有效 | 任务适应的 intrinsic dimensionality 很低 |
| DPO vs PPO | DPO 简单稳定但 offline；PPO 在线探索但复杂 |
| RAG vs 长上下文 | 互补：RAG 省钱扩展性好，长上下文完整性好 |
| BLEU 的缺陷 | 不考虑语义，同义词得 0 分 |
| Scaling Law 的意义 | 训练前可预测性能，做 compute-optimal 决策 |
| CRF 的作用 | 建模标签转移约束，保证输出合法 |
| Temperature 的作用 | 控制 softmax 分布的锐度/平坦度 |
| MoE 的核心优势 | 总参数大但每个 token 只激活一小部分 |

---

> **最后提醒**：NLP 面试的底层能力是 **Transformer + Attention + Tokenization**，这三个理解透了，其他都是上层应用。2026 年的面试会越来越偏向 LLM 工程化（部署/RAG/Agent/对齐），但底层原理永远是区分度最高的考点。
