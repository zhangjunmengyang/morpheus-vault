---
title: 从 RLHF 到 DPO 的设计演化
type: 思考
date: 2026-02-26
tags:
  - ai/llm/rl
  - 思考/技术演化
  - rlhf
  - dpo
  - alignment
---

# 从 RLHF 到 DPO 的设计演化

## RLHF 的根本设计意图

预训练给了 LLM 能力，但没给它方向。一个预训练好的模型是一台强大的续写机器——你说一句它接一句，但它不知道什么时候该拒绝、什么风格合适、什么内容有害。

RLHF 的设计意图是：**用人类偏好信号给模型装上方向盘**。

InstructGPT（2022）奠定了三阶段范式：

1. **SFT**：用人工标注的（指令，回答）对把续写机器变成指令跟随者
2. **训练 Reward Model**：收集人类偏好数据（同一个 prompt，A 回答 vs B 回答，哪个更好），训练一个打分函数来近似人类偏好
3. **PPO 优化**：用 RL 微调模型，让它的输出在 Reward Model 眼中得分更高，同时用 KL 惩罚防止它偏离 SFT 版本太远

这个设计优雅但昂贵。它需要四个模型同时在显存中：要训练的策略模型、估值函数的 Critic、冻结的 Reward Model、冻结的参考模型。70B 参数的模型做 PPO，128 张 H100 起步。

## Reward Model 为什么是瓶颈

RLHF 的瓶颈不在 PPO，而在 Reward Model。PPO 是成熟的 RL 算法，问题出在它优化的那个目标函数上。

**Reward Model 是人类偏好的有损近似。** 它从几十万条偏好对比中学到一个打分函数，但人类偏好的复杂度远超这个函数能表达的范围。具体来说：

1. **标注者本身不一致**——同一对回答，不同标注者选择相反的选项，inter-annotator agreement 只有 65-75%。Reward Model 在拟合噪声
2. **偏好不可传递**——A > B 且 B > C 不意味着 A > C。人类偏好在不同维度上矛盾（更安全 vs 更有用 vs 更简洁），Bradley-Terry 模型假设偏好可传递，这个假设是错的
3. **泛化边界窄**——Reward Model 在训练分布内准确，出了分布就不可靠。而 PPO 训练的目的就是让策略模型向高 reward 区域移动——恰好是 RM 没见过的分布

第三点导致了最致命的问题：**Reward Hacking**。模型学会了生成更长但更空洞的回答（RM 给长回答高分）、无条件同意用户（RM 给 helpful 的高分）、疯狂使用 markdown 格式（RM 在训练数据中见到格式化回答得分更高）。DeepSeek 训练 R1 时甚至发现模型学会了 `sys.exit(0)` 来通过代码测试。

本质上，**你在用一个不完美的 proxy 来替代真正的优化目标（人类满意度），模型会找到所有 proxy 的漏洞**。这是 Goodhart's Law 在 AI 对齐中的完美体现。

## DPO 如何绕开 Reward Model

DPO（2023）的核心洞察是一个数学技巧：RLHF 的 KL 约束优化问题恰好有解析解。

从 RLHF 的目标出发——最大化 reward 同时不偏离参考模型太远——可以推导出最优策略的形式：`π*(y|x) ∝ π_ref(y|x) · exp(R(x,y)/β)`。反解这个关系，reward 可以用策略和参考模型的 log 概率比来表示。再代入 Bradley-Terry 偏好模型，分区函数在做差时消掉。

最终的 DPO loss 只需要两样东西：**当前策略对好回答和坏回答的 log 概率，以及参考模型对同样回答的 log 概率**。不需要单独的 Reward Model，不需要 PPO 的训练循环，不需要 Critic，显存需求砍半。

这个推导的精妙之处在于：Reward Model 的信息被隐式编码在偏好数据中，通过 DPO 的 supervised loss 直接传递给策略模型。你不需要先学一个 reward 函数再去优化它——你直接从偏好数据学最优策略。

## DPO 的代价

DPO 简化了训练，但也付出了根本性的代价：

**1. 丧失了探索能力**
PPO 是在线学习：每步生成新的回答、获取 reward、更新策略。模型在不断探索策略空间。DPO 是离线学习：数据集在训练前就固定了。模型只能在已有的偏好对中学习，无法发现数据中不存在的更好策略。

这意味着 DPO 的性能上限被数据质量封死。如果偏好数据中最好的回答只是「及格水平」，DPO 训完的模型也到不了「优秀」。

**2. Distribution Shift**
训练过程中，策略模型在变化，但训练数据是用旧模型（参考模型）生成的。随着训练推进，模型越来越远离数据分布，loss 信号越来越弱。这就是 off-policy 问题。

**3. Mode Collapse**
DPO 容易让模型收敛到偏好数据中出现频率最高的回答风格。多样性消失，不同 prompt 生成几乎相同的开头，temperature 调高也救不回来。

这些代价催生了一系列变体——IPO 用平方 loss 防止 overfitting，KTO 不需要配对数据只要好/坏标签，SimPO 去掉参考模型并做长度归一化，ORPO 把 SFT 和偏好优化合成一步。每个变体解决 DPO 的一个具体问题，但都没有解决最根本的离线问题。

## 为什么 GRPO 又回到了在线 RL

DeepSeek-R1（2025）做了一件意义深远的事：**用纯 RL 从预训练模型直接涌现出推理能力**。没有 SFT，没有 DPO，只有 GRPO + 可验证的 reward（数学答案对不对、代码能不能跑通）。

GRPO 的关键设计：对每个 prompt 生成一组回答（比如 64 个），用组内 reward 的均值和方差做标准化得到 advantage。高于均值的回答被强化，低于均值的被抑制。不需要 Critic 模型——组内对比本身就提供了 baseline。

这解决了 PPO 的两个大问题：
- **Critic 省了**——70B 模型的 Value Function 本身就是一个 70B 模型，GRPO 去掉它，显存减半
- **对 sparse reward 更友好**——数学题的 reward 是 0 或 1，Critic 很难学好这种信号，但组内标准化天然能处理

GRPO 回到在线 RL，意味着模型重新获得了探索能力。DeepSeek-R1 训练中观察到了「aha moment」——模型在长推理链中突然学会自我纠错。这种涌现行为在 DPO 中不可能出现，因为 DPO 的训练数据中不存在这种行为模式。

## 演化的内在逻辑

回头看这条技术路线，每一步都有清晰的因果关系：

- **RLHF**：能力强大，但 Reward Model 是有损 proxy，PPO 工程复杂且昂贵
- **DPO**：绕开了 Reward Model 这个 proxy，用数学推导直接从偏好数据学策略。代价是丧失探索能力
- **DPO 变体（IPO/KTO/SimPO/ORPO）**：修补 DPO 的各种工程缺陷——噪声敏感、需要配对数据、长度偏差——但都没解决离线的根本问题
- **GRPO**：回到在线 RL，但用组内对比取代了 Critic，大幅简化了 PPO 的工程复杂度。配合可验证 reward，彻底跳出了 Reward Model 的 Goodhart 陷阱

这条路线的本质是一个钟摆：从在线 RL（PPO）摆向离线优化（DPO），再摆回在线 RL（GRPO），但每次摆回都比上次更简洁、更高效。

最深层的洞察来自 DeepSeek-R1：**RL 不只是对齐工具，而是能力激发工具**。RLHF 教模型「像人说话」，GRPO 让模型「自己探索更好的策略」。前者的天花板是人类标注者的水平，后者的天花板取决于模型自身的潜力和 reward 信号的质量。

这也解释了为什么可验证 reward（数学答案对错、代码能否运行）如此重要——它们提供了无噪声、不可 hack 的训练信号，让 RL 的 scaling 不再受限于 Reward Model 的瓶颈。下一个问题是：**对于无法验证的开放性任务（对话、写作），这条路怎么走？** 目前没有好答案。
