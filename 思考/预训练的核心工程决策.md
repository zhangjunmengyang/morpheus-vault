---
title: 预训练的核心工程决策
type: 思考
date: 2026-02-26
tags:
  - ai/llm
  - topic/pretraining
  - topic/distributed-training
  - type/思考
---

# 预训练的核心工程决策

> 预训练不是炼丹。每一个决策背后都有清晰的工程逻辑。这篇文章不讲"是什么"，讲"怎么选"和"为什么"。

---

## 一、Chinchilla 的真正含义：不是"要更多算力"

Chinchilla Scaling Law（arXiv:2203.15556）被广泛误读为"模型越大越好"。它说的是：

**给定固定算力 C，存在最优的模型大小 N 和数据量 D 的配比，且两者应等比例增长。**

$$N \propto C^{0.5}, \quad D \propto C^{0.5}$$

这意味着：70B 模型的 Chinchilla-optimal 数据量约 1.4T tokens。用 300B tokens 训练 70B 模型是浪费参数，用 3T tokens 训练 7B 模型是浪费数据。

**但实际工程中几乎所有人都"违反"Chinchilla。为什么？**

因为 Chinchilla 只优化训练成本，没算推理。模型训一次，但要服务数百万用户。LLaMA-7B 用了 ~1T tokens（Chinchilla-optimal 的 ~10 倍），Meta 多花训练算力，换来更小的部署模型。这是 **inference-aware scaling**——优化的是 total cost = 训练 + 推理 × 预期调用量。

| 策略 | 训练成本 | 推理成本 | 适用场景 |
|------|---------|---------|---------|
| Chinchilla-optimal | 最低 | 高（模型大） | 训一次、不部署 |
| 过度训练小模型 | 稍高 | 低（模型小） | 大规模部署 |
| 过度训练大模型 | 很高 | 很高 | 追求极限性能 |

**工程师的判断框架：** 先算推理预算，再倒推训练策略。如果模型要跑在端侧，宁可训练时多烧 10 倍 token，也要把模型缩到 3B。

---

## 二、分布式训练五大范式：每种解决一个问题

分布式训练不是"把模型放到更多卡上"这么简单。五种范式各有明确的适用场景，核心区别在通信模式：

| 范式 | 切什么 | 通信模式 | 通信频率 | 放哪里 |
|------|--------|---------|---------|--------|
| **DP** 数据并行 | 数据 | AllReduce 梯度 | 每 step 1 次 | 跨机（频率低可 overlap） |
| **TP** 张量并行 | 层内矩阵 | AllReduce 激活值 | 每层 2 次 | 机内 NVLink（频繁！） |
| **PP** 流水线并行 | 层间 | 点对点传 activation | 每 micro-batch | 跨机（数据量小） |
| **SP** 序列并行 | 序列维度 | AllGather + ReduceScatter | 每层 | 配合 TP 使用 |
| **EP** 专家并行 | Expert | All-to-All | 每 MoE 层 | MoE 专用 |

**组合原则只有一条：把高频通信放在高带宽链路上。**

- TP 每层通信多次 → 必须在 NVLink 内（机内 8 卡）
- PP 只传 activation，数据量小 → 可跨机
- DP 梯度大但只同步一次 → 跨机，和计算 overlap

**128 卡训 70B 的典型设计：** TP=8（机内）× PP=2（跨 2 机）× DP=8（跨 8 组），再叠 ZeRO-2 省显存。这不是经验公式，是通信拓扑决定的。

---

## 三、ZeRO 三阶段的本质：把模型状态拆给所有人

一个 7B 模型用 Adam + FP16 训练，单卡需要：

| 组成 | 大小 | 说明 |
|------|------|------|
| 参数 | 2Φ = 14GB | FP16 |
| 梯度 | 2Φ = 14GB | FP16 |
| 优化器状态 | 12Φ = 84GB | Adam 的 FP32 参数副本 + momentum + variance |
| **合计** | **16Φ = 112GB** | 超过 A100 80GB |

注意：**优化器状态占了 75%**。这才是显存的大头。

ZeRO 的三个阶段，本质是逐步把这三部分从"每卡完整复制"变为"N 卡各存 1/N"：

| 阶段 | 切什么 | 每卡显存 | 额外通信 |
|------|--------|---------|---------|
| ZeRO-1 | 优化器状态 | 4Φ + 12Φ/N | 无（和 DDP 一样） |
| ZeRO-2 | + 梯度 | 2Φ + 14Φ/N | 无（ReduceScatter 替代 AllReduce） |
| ZeRO-3 | + 参数 | 16Φ/N | +50%（forward 要 AllGather 参数） |

**选型判断：** 显存够就用 ZeRO-2（省通信），显存紧张才用 ZeRO-3（牺牲 50% 通信换全切分）。不是"越高越好"。

---

## 四、数据配比：不是越多越好，domain mix 才是关键

预训练数据不是一锅乱炖。配比决定模型的能力分布：

| 数据源 | 典型比例 | 影响什么 |
|--------|---------|---------|
| Web 文本 | ~60-67% | 语言流畅度、常识 |
| 代码 | 8-15% | **推理能力**（结构化思维正迁移） |
| 百科/学术 | 8-10% | 事实知识 |
| 书籍 | 5-8% | 长文本理解、叙事能力 |
| 数学 | 2-4% | 数值推理 |
| 多语言 | 5-10% | 跨语言能力 |

**三个关键判断：**

1. **代码比例是最被低估的杠杆。** Phi 系列证明，高质量代码数据对推理能力有显著正迁移。代码的 if-else、函数调用、类型系统本质上是结构化推理的训练数据。

2. **高质量数据可以重复采样 2-4 次，但不能更多。** Muennighoff et al. 发现重复超过 4 epochs 后效果急剧衰减。数据不够时，合成数据是更好的选择。

3. **退火阶段（annealing）的配比应该不同。** MiniCPM 的实践：最后 10% steps 切换到高质量子集 + cosine decay 到 0，对 benchmark 有确定性提升。这是最低成本的数据课程学习。

---

## 五、训练崩了怎么办：Loss Spike 处置手册

Loss spike 是大模型训练最常见的事故。Google 训练 PaLM 遇到约 20 次，每次都靠手动回滚。

**排查顺序（从便宜到贵）：**

```
1. 看 gradient norm → spike 前是否暴增？
2. 回溯 data batch → 是否有脏数据？
3. 检查硬件日志 → GPU 错误？NCCL 超时？
4. 检查 NaN/Inf → 哪一层先出现？
5. 对照 LR schedule → 是否在变化点附近？
```

**恢复策略（按优先级）：**

| 动作 | 什么时候用 |
|------|-----------|
| 回滚 checkpoint + 跳过问题数据 | 能定位到具体 batch |
| 回滚 + 降 LR 50% 再逐步回升 | 怀疑是 LR 问题 |
| 加强 gradient clipping（1.0→0.5） | grad norm 持续偏高 |
| 切换 BF16（如果还在用 FP16） | 频繁出现数值溢出 |

**预防措施清单：**

- 每 500 steps 存 checkpoint（不是 1000，不是 2000）
- 用 BF16 不用 FP16（BF16 范围 = FP32，不需要 loss scaling）
- Gradient clipping = 1.0（默认值，不要省）
- 监控三件事：loss 曲线、gradient norm、activation 统计量
- 设置自动检测：loss > mean + 2σ 时暂停更新并告警

**关于 BF16 vs FP16 的一句话结论：** BF16 牺牲精度换稳定性。FP16 的 65504 上限让 gradient overflow 变成常态，而 BF16 和 FP32 有一样的数值范围。2024 年之后，没有理由再用 FP16 训练大模型。

---

## 六、决策总结

面对一个预训练项目，工程师需要依次回答这些问题：

```
1. 部署场景是什么？ → 决定模型大小（inference-aware scaling）
2. 有多少算力和数据？ → 决定是否过度训练
3. 有多少卡、什么互联？ → 决定并行策略组合
4. 单卡能放下吗？ → 决定 ZeRO 阶段
5. 数据有什么、质量如何？ → 决定配比和清洗投入
6. 训练崩了怎么恢复？ → checkpoint 频率和监控策略
```

**不是知识越多决策越好。是框架清晰了，遇到任何规模的项目都能用同一套逻辑推导出答案。**

---

> 🔗 相关笔记：
> - [[AI/3-LLM/Architecture/MoE 深度解析]] — EP 和负载均衡的深度展开
> - Transformer架构深度解析-2026技术全景 — 注意力机制和 RoPE
> - [[AI/3-LLM/SFT/SFT 原理]] — 预训练之后的第一步对齐
