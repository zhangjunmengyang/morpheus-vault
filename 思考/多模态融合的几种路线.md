---
title: "多模态融合的几种路线"
brief: "多模态融合不是'加个视觉编码器'那么简单——从 CLIP 对比学习到 Any-to-Any Omni 模型，本质是在解决同一个问题：如何让不同模态的信息在共享语义空间里相互理解。四条主路线各有适用场景和天花板。"
type: thought
domain: ai/mllm
created: 2026-02-27
updated: 2026-02-27
tags: [multimodal, vlm, omni, thought]
---

# 多模态融合的几种路线

多模态大模型研究了这么多年，一直在回答同一个问题：**怎么让模型同时理解图像和语言？**

但"同时理解"这件事，不同的人有不同的理解，导致了完全不同的技术路线。

---

## 路线一：对比学习（CLIP 范式）

**核心思想**：图像和文本不需要共享同一个模型，只需要在同一个向量空间里对齐。

CLIP 的做法：把图像编码成向量，把文本编码成向量，拉近匹配对、推远不匹配对。训练完之后，"一只猫坐在沙发上"这句话和对应图片的向量距离很小，和一张汽车图片的向量距离很大。

**优势**：不需要生成，推理极快，适合大规模检索。SigLIP 是改进版，换 sigmoid loss，大 batch 更稳定。

**天花板**：只能做理解，不能生成。对比学习学到的是"哪张图和哪段文字匹配"，不是"图片里有什么"——细粒度图像推理很弱。

---

## 路线二：Projector 桥接（LLaVA 范式）

**核心思想**：视觉编码器提取视觉特征，通过投影层（Projector）转换成 LLM 能理解的 token，和文字 token 一起输入 LLM。

这是目前最主流的 VLM 范式，LLaVA/Qwen-VL/InternVL 都走这条路。Projector 有层次之分：

- **Linear/MLP**：最简单，参数少，表达能力有限
- **Q-Former（BLIP-2）**：可学习 query token 从视觉特征里注意力过滤，压缩视觉信息
- **Perceiver Resampler**：和 Q-Former 类似但更灵活

关键张力：视觉 token 越多推理越慢。InternVL3 用 pixel shuffle 把 4 个视觉 token 压成 1 个，是效率和信息量的折中。

**天花板**：视觉编码器预训练固定，融合深度有限。视觉特征被压成 token 后，LLM 层看到的是"被翻译成语言的视觉"，两种模态没有真正在底层融合。

---

## 路线三：原生多模态预训练

**核心思想**：不用外挂视觉编码器，从预训练阶段就让模型同时消化图像 patch 和文字 token。

Fuyu 是早期尝试，直接把图像切成 patch 喂 transformer。更新方向如 Seed1.5-VL：视觉理解和语言生成在同一统一框架，结构层面没有"视觉分支"和"语言分支"之分。

**优势**：两种模态在更深层次互相影响，跨模态推理更自然。

**代价**：预训练数据量和计算量大得多，迭代周期长。目前主要是大厂在走。

---

## 路线四：Any-to-Any（Omni 模型）

**核心思想**：输入输出均可为文本/图像/音频/视频任意组合。

Ming-Flash-Omni 2.0、GPT-4o 走这条路。核心工程挑战：
- 音频/视频时序建模
- 跨模态统一表示空间（单纯对比学习不够）
- 输出端独立解码器（图像生成 decoder、TTS 等）

目前真正做到"听说读写"四合一的系统，推理延迟、质量一致性、多模态协调仍是公开难题。

---

## 判断框架

四条路线不是"谁取代谁"，而是不同场景的最优解：

| 场景 | 推荐路线 |
|------|---------|
| 大规模图文检索 | CLIP 对比学习（速度最快） |
| 图文理解/VQA/OCR | Projector 桥接（工程最成熟） |
| 深度跨模态推理 | 原生多模态预训练 |
| 语音+视觉+文本交互 | Any-to-Any Omni |

**2026 年关键趋势**：Projector 桥接路线正向原生多模态靠拢——越来越多模型在预训练阶段就引入视觉数据，路线二和路线三的边界正在模糊。

另一个趋势：**RL 正在进入多模态**。Perception-R1 和 LLaVA-Critic-R1 说明视觉推理任务开始用 GRPO 类方法做 post-training，多模态感知成为可被强化学习优化的目标。多模态融合的训练范式，将与 LLM post-training 路线逐步合并。

---

## See Also

- [[AI/3-LLM/MLLM/MLLM 概述|MLLM 概述]] — 架构全景（模态编码器/Projector/LLM骨干/训练三阶段）
- [[AI/3-LLM/MLLM/多模态 LLM 架构|多模态 LLM 架构]] — VLM 三大设计选择深度对比
- [[AI/3-LLM/MLLM/非文本的模态对齐|非文本的模态对齐]] — 跨模态对齐机制（CLIP/Q-Former/视觉指令微调）
- [[AI/3-LLM/MLLM/Multimodal-Perception-RL-综合分析|多模态感知RL综合分析]] — RL 进入多模态的最新进展
