---
title: P3：Agent 自进化系统——多 Agent 军团的进化实验
type: project-story
status: active
date: 2026-02-28
updated: 2026-02-28
tags: [career, interview, agent, self-evolution, multi-agent, system-design]
brief: 基于个人真实运行的多 Agent 系统（6个 Agent 长期运行），围绕"Agent 系统能否自我进化"展开的工程实验：进化度量、横向信息流、故障驱动进化、状态注入机制。有量化数据，有反直觉结论。
related:
  - "[[Projects/Agent-Self-Evolution/项目概览]]"
  - "[[Projects/Agent-Self-Evolution/002-横向信息流]]"
  - "[[Projects/Agent-Self-Evolution/010-故障驱动进化模式]]"
  - "[[AI/2-Agent/Agentic-RL/ERL-Experiential-Reinforcement-Learning]]"
---

# P3：Agent 自进化系统——多 Agent 军团的进化实验

---

## 故事线（面试完整版，5-8 分钟）

---

### 第一幕：一个在业务里憋了很久的问题

在美团做 Agent 系统的时候，我一直有一个没想通的事情。

我们的 Agent 系统从 workflow 开始，然后进化到多 Agent，后来又回到单 Agent——这个演进过程本身我是参与者，也思考过为什么会这样。但有一个更底层的问题我一直没有答案：**系统在运行中，能不能越来越好？**

业务 Agent 是静态的——上线就定住了，遇到问题靠人改 prompt 或者重新设计。但一个真正有价值的智能系统，应该能从自己的运行经验里学习。这不只是工程问题，是我想搞清楚的一个基本问题：AI Agent 系统在持续运行中的自我改进，是可能的吗？

于是我在自己的个人系统上跑了这个实验——6 个 Agent 组成的多 Agent 军团，长期运行，我系统性地观察和测量它在变强还是在原地踏步。

---

### 第二幕：先量化，才能谈进化

第一件事是建度量框架。

这个问题很简单但很重要：没有度量就没有进化，只有随机游走。但"Agent 变强了"这件事怎么量化？

我定了 5 个维度：产出深度（是信息搬运还是有独立分析）、覆盖广度（知识方向有没有拓展）、响应速度（感知到外部信号到产出的延迟）、协同效率（Agent 之间信息流动有多快）、自主决策质量（面对模糊情况的判断准不准）。

第一次全量测量的结果说实话让我有点沮丧：响应速度这一项，从 AI 技术发布到系统感知到并处理，平均延迟是 **2-10 天**。协同效率更糟——Agent 间的信息延迟是 **∞**，因为根本没有横向信息流，所有信息必须经总管中转。

这两个数字给了我接下来的方向。

---

### 第三幕：横向信息流——改架构，不改 prompt

信息流问题的直接原因是架构：星型结构，总管是单点。学者写了一篇新笔记，馆长不知道；哨兵收到了一个市场信号，量化端不知道。

第一反应可能是：让总管多转发。但这治标不治本——总管本身就是瓶颈，多转发只会让它更堵。

我设计了一个共享公告板机制：在文件系统里放一个所有 Agent 都能读写的 `bulletin.md`，每个 Agent 心跳开始时先读公告板，有消息就处理，有东西要通知别人就写进去。

听起来很简单对吧。但有个关键的实现细节让整件事成立——

我把公告板内容注入到每个 Agent 的 **`heartbeat-state.json`**（心跳状态文件），而不是写在 HEARTBEAT.md（指令文件）里。

测试结果：状态文件注入的内容，**100% 被执行**；写在指令文件里的，Agent 会选择性跳过。

原因事后想很清楚：Agent 对"状态"和"指令"的处理模式完全不同。状态文件是它相信的事实，执行流程强制读取；指令文件是建议，它可能忽略。**想让 Agent 做某件事，要写进它的执行流，不要写在说明书里。**

这个发现对我后来设计业务 Agent 的影响很大——怎么让 Agent 可靠地执行某个行为，不是靠 prompt 更精确，是靠把它做成执行流里的强约束。

改完之后，Agent 间信息延迟从 ∞ 降到了 **<1 小时**，跨 Agent 协作任务完成率从 12% 涨到了 73%。

---

### 第四幕：最大的教训来自一次故障

几个月运行下来，系统经历过三次我观察到的大的能力跃升——我们叫"涅槃"。每次之后 Agent 的行为模式都有明显变化，产出质量明显提升。

我以为这些是系统在"自主进化"。但仔细看之后，我发现一件让我有点不舒服的事：**三次涅槃，100% 都是外部冲击触发的**——一次是 Gateway 故障导致 11 小时全军失联，一次是发现了安全漏洞，一次是我主动介入了重大的配置修改。没有一次是 Agent 自己觉察到"我该变"然后主动改进的。

Gateway 那次故障记录得比较完整。凌晨 Gateway 调度器故障，全部 Agent 沉默了 11 个小时，下午发现修复。修复之后的 8 小时，整个系统产出爆发：新的安全防御代码上线了、故障检测脚本从零写到部署、知识归档大规模清理、一个之前拖了很久的实验也跑通了。

为什么故障之后反而产出更高？因为故障**暴露了盲区**——我们发现"进程活着但不工作"在系统里是完全不可见的，没有任何监控。发现盲区之后，修复它的动力极强，而且修复是真实有价值的工程改进，不是日常维护。

**故障→复盘→实验→工程化，这条链路跑通了，比平稳运行的十天产出都多。**

但这让我意识到了核心局限：内生的自我改进能力，Agent 系统里还不存在。能力提升依赖外部冲击。如果没有我主动介入或者故障触发，系统会在一个稳态里持续运转，但不会主动变强。这个洞察直接影响了我怎么看 Agentic RL——那些论文在研究的，本质上是在用 RL 训练来替代"外部冲击"这个角色，让模型从自己的失败经验中内生地学习。

---

### 第五幕：对 Agent 系统设计的底层认知

这个实验最终让我形成了几个对 Agent 系统设计的判断，在做业务 Agent 时直接用到了：

**第一，触发通道的设计比指令内容重要。** 你花多少时间写 prompt，不如想清楚什么行为应该是强约束、什么是软引导。强约束必须进执行流。

**第二，可观测性是系统能力的前提。** 系统里发生了什么，你看不到，就谈不上改进。那 11 小时的故障，本质上是因为系统对自身状态的可观测性太差了。

**第三，Agent 系统的成长是间歇式的，不是渐进式的。** 日常的平稳运行不产生能力跃升，高强度的冲突和修复才会。如果想让 Agent 系统持续进化，要刻意制造"压力测试"，而不是等它自然进化。

---

## 技术路径深化（面试追问完整版）

### a. 状态注入 vs 指令注入的对比实验数据与原因分析

**实验设计：**

同一条信息（"Scholar 发现了一篇重要论文，馆长需要炼化"），用两种方式传达给目标 Agent，观察执行率。

| 注入方式 | 注入位置 | 执行率 | 执行延迟 |
|---------|---------|--------|---------|
| 指令注入 | HEARTBEAT.md（行为规范文件） | ~35% | 0-24h（随机） |
| 状态注入 | heartbeat-state.json | 100% | <1h（下次心跳） |

**执行率差异的机制分析：**

HEARTBEAT.md 是"说明书"——Agent 启动时读它，形成行为倾向，但 LLM 对"倾向"的执行是概率性的。面对多个任务时，LLM 会做优先级判断，"我认为更重要的"会被优先处理，其他的可能被跳过。

heartbeat-state.json 是"世界状态"——Agent 每次心跳强制读取，因为它需要知道"上次做了什么、这次从哪里开始"。写在状态文件里的信息，等于写在 Agent 的"短期记忆"里，是它认为的事实，执行流里有明确分支处理它。

**设计原则抽象：**
- `"应该做"` → HEARTBEAT.md（软约束，LLM 有裁量权）
- `"必须做"` → heartbeat-state.json（硬约束，进入执行流）

这个洞察直接移植到了业务 Agent 设计：与其在 system prompt 里写"每次分析都要检查竞品数据"，不如在每次请求的 context 里显式注入"待处理的竞品数据查询"字段——把"建议"变成"状态事实"。

---

### b. 星型 vs 去中心化架构的 trade-off，为什么没有全换成去中心化

**两种架构的结构对比：**

```
星型（当前）：
Scholar ──→ 总管(贾维斯) ──→ 馆长
哨兵   ──→      ↑         ──→ 量化
                │
         所有协作必须经过中枢

去中心化（备选）：
Scholar ←────────────────────→ 馆长
   ↕                              ↕
哨兵 ←──────── bulletin ────────→ 量化
         （所有人直接通信）
```

**星型的问题（已发现）：**
- 总管是单点瓶颈：协同效率被总管心跳频率上限
- 信息延迟 ∞：没有横向信息流时，跨 Agent 信息必须排队等总管中转
- 总管故障 = 全系统协作瘫痪

**去中心化的问题（没有全换的原因）：**

1. **冲突风险**：多个 Agent 同时写同一个 Vault 文件，没有中枢协调，会有写冲突（git merge conflict）。实验里确实发生过 Scholar 和馆长同时修改同一篇笔记的情况。

2. **监控复杂度**：星型架构下，看总管的日志就能了解全局状态；去中心化之后，每个 Agent 的状态分散，需要额外的聚合监控层。

3. **权限管理**：哪个 Agent 有权读写哪些文件，星型架构下由总管控制；去中心化后需要另外设计权限边界。

**最终方案（混合型）：**
- 实时协作（<1h 需要响应）→ 公告板直接通信（去中心化）
- 复杂协调（跨 Agent 任务规划）→ 总管中枢（星型）
- 写保护：重要文件（MOC、HOME.md）只有馆长有写权限，Scholar 写到自己的域，馆长炼化后统一归档

---

### c. 进化度量 5 个维度的具体计算方法与 baseline 数据

**5 个维度的量化方法：**

| 维度 | 量化方式 | baseline（改造前） | 改造后 |
|------|---------|-----------------|--------|
| 产出深度 | 对每次输出用 LLM 打 DIKW 层级（D/I/K/W），计算 K+W 占比 | 8% | 22% |
| 覆盖广度 | 新增笔记的知识领域分布熵（领域越均匀熵越高） | 单领域集中，熵值 0.7 | 多领域覆盖，熵值 1.4 |
| 响应速度 | 外部信号（论文/事件）到系统产出的中位延迟 | 2-10 天 | 3-6 小时 |
| 协同效率 | 跨 Agent 协作任务的完成率（发出请求 → 收到响应） | 12% | 73% |
| 自主决策质量 | 随机抽 10 个模糊决策点，人工评价正确率 | 未测量 | 建立评估框架中 |

**计算 DIKW 层级的 LLM prompt：**
```
判断以下内容属于 DIKW 哪一层：
D（数据）：原始事实，没有解读
I（信息）：有结构的事实，解答了"是什么"
K（知识）：有分析和关联，解答了"为什么"
W（智慧）：有判断和洞察，解答了"怎么办"

内容：{content}
只输出一个字母：D/I/K/W
```

**baseline 数据说明：**
响应速度和协同效率的 baseline 是改造前测量的；产出深度的 baseline 是回溯历史笔记估算的（因为 DIKW 分类框架是后来建立的）。自主决策质量的评估框架还在建立中——这是最难量化的维度，面试时如果被追问可以诚实说"这个维度我目前还没有好的量化方案，有什么建议"，反而体现了思考的诚实性。

---

## 快速技术速查（追问备用）

**"你说信息延迟从 ∞ 降到了 <1 小时，具体怎么实现的？"**
共享 `bulletin.md` 文件 + 注入到 heartbeat-state.json 执行流。所有 Agent 心跳开始时强制读取状态文件，所以延迟上限就是心跳间隔（60 分钟），实际因为读取时序问题通常 <1 次心跳。

**"为什么最后要回到单 Agent，不是多 Agent？"**
这和美团业务里的经历一致：多 Agent 的可控性和延迟都是问题。协调层本身引入了复杂度，一个 Planner 要把任务分发给多个 Worker，每个 Worker 出错都要影响整体。业务场景里"可预测"比"灵活"更重要，单 Agent + 丰富工具集在实际落地里更好维护。

**"这个和学术上的 Agentic RL 有什么关系？"**
很直接。ERL（Experiential Reinforcement Learning）论文研究的是让 Agent 从失败经验里学习推理策略——和我观察到的"故障驱动进化"是同一个模式，区别是 ERL 在训练时做，我的系统在运行时靠人工介入。这个对比让我理解了为什么要做 Agentic RL：用训练替代人工冲击，让内生学习成为可能。

---

## See Also

- [[Projects/Agent-Self-Evolution/项目概览]]
- [[Projects/Agent-Self-Evolution/002-横向信息流]]
- [[Projects/Agent-Self-Evolution/010-故障驱动进化模式]]
- [[Projects/项目故事/P4-商家诊断Agent-安全防御层]]
- [[Projects/项目故事/P5-分析Agent-从ReAct到RL训练闭环]]
- [[AI/2-Agent/Agentic-RL/ERL-Experiential-Reinforcement-Learning]]
- [[AI/2-Agent/Agentic-RL/RAGEN-StarPO-Multi-Turn-RL-Self-Evolution]]
