---
title: LLM 基础数学组件手撕
brief: LLM中核心数学组件完整实现：各类Attention（MHA/MQA/GQA）、归一化（LayerNorm/RMSNorm/BatchNorm）、激活函数（GELU/SwiGLU/ReLU）、正则化（Dropout/LoRA）、损失函数（CE/Label-Smoothing/KL）PyTorch代码，来源 MA-RLHF 教学项目。
date: 2026-02-25
type: code-practice
source: MA-RLHF (https://github.com/dhcode-cpp/MA-RLHF)
tags:
  - code-practice
  - attention
  - normalization
  - activation
  - pytorch
  - foundations
related:
  - "[[Projects/MA-RLHF/lc2/lc2-01-Transformer-手撕实操|Transformer-手撕实操]]"
  - "[[Projects/MA-RLHF/lc3/lc3-01-GPT2-手撕实操|GPT2-手撕实操]]"
  - "[[AI/3-LLM/SFT/LoRA|LoRA]]"
  - "[[Projects/MA-RLHF/lc4/lc4-01-Llama-手撕实操|Llama-手撕实操]]"
---

# 基础数学组件手撕

> 来源：MA-RLHF (https://github.com/dhcode-cpp/MA-RLHF)
> 入库日期：2026-02-25

---

## 目录

- [1. AdamW 优化器](#1-adamw-优化器)
- [2. Softmax 数值稳定实现](#2-softmax-数值稳定实现)
- [3. Attention 为何除以 √d](#3-attention-为何除以-d)
- [4. PyTorch Autograd 手撕](#4-pytorch-autograd-手撕)
- [5. MLP 梯度计算](#5-mlp-梯度计算)
- [6. LayerNorm 实现与可视化](#6-layernorm-实现与可视化)
- [7. LayerNorm vs RMSNorm 对比分析](#7-layernorm-vs-rmsnorm-对比分析)
- [8. Beam Search 实现](#8-beam-search-实现)
- [9. FP4 量化原理与实现](#9-fp4-量化原理与实现)

---

## 1. AdamW 优化器

### 核心区别：Adam vs AdamW

- **Adam**：weight decay 被耦合在梯度更新中 → `w = w - lr * m_hat / (sqrt(v_hat) + eps)`
- **AdamW**：weight decay 独立于自适应学习率 → `w = w - lr * (m_hat / (sqrt(v_hat) + eps) + λ * w)`

> **关键洞察**：AdamW 的 weight decay 直接作用在参数上（decoupled），不受 Adam 自适应学习率的缩放影响，正则化效果更稳定。

### 完整实现（非自动求导版本）

```python
import torch

class Adam:
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8):
        self.w = params
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.eps = eps
        self.m = torch.zeros_like(params)  # 一阶矩估计
        self.v = torch.zeros_like(params)  # 二阶矩估计
        self.t = 0

    def step(self, w, grad, weight_decay=1e-2):
        self.t += 1
        # 一阶矩（动量）
        self.m = self.beta1 * self.m + (1 - self.beta1) * grad
        # 二阶矩（自适应学习率）
        self.v = self.beta2 * self.v + (1 - self.beta2) * grad.pow(2)
        # 偏差修正
        m_hat = self.m / (1 - self.beta1 ** self.t)
        v_hat = self.v / (1 - self.beta2 ** self.t)

        if weight_decay is not None:  # AdamW
            return w - self.lr * (m_hat / (v_hat.sqrt() + self.eps) + weight_decay * w)
        # 标准 Adam
        return w - self.lr * m_hat / (v_hat.sqrt() + self.eps)
```

### 训练示例

```python
w = torch.randn(10, 1)
optimizer = Adam(w)
input_data = torch.randn(8, 10)
target = torch.randn(8, 1)

for epoch in range(1000):
    output = input_data @ w
    grad = input_data.t() @ (output - target)  # 手动求梯度
    w = optimizer.step(w, grad, weight_decay=1e-2)  # AdamW
    # w = optimizer.step(w, grad, weight_decay=None)  # Adam
```

---

## 2. Softmax 数值稳定实现

### 2.1 Safe Softmax（减 max 技巧）

> **关键洞察**：直接 `exp(x)` 容易上溢（x 很大）或下溢（x 很小）。减去 `max(x)` 后，最大值变成 0，`exp(0)=1`，数值稳定。

$$
\text{softmax}(x_i) = \frac{e^{x_i - c}}{\sum_{j=1}^{d} e^{x_j - c}}, \quad c = \max_j x_j
$$

```python
def SoftMax(logits):
    """Safe Softmax: 减去 max 防溢出"""
    logits_max, _ = logits.max(dim=-1)
    logits = logits - logits_max.unsqueeze(1)
    logits = logits.exp()
    logits_sum = logits.sum(-1, keepdim=True)
    return (logits / logits_sum).abs()
```

### 2.2 LogSoftmax（避免 log(softmax) 产生 -inf）

> **关键洞察**：`log(softmax(x))` 中，如果 softmax 输出接近 0，`log` 会产生 `-inf`。直接推导 LogSoftmax 的闭式解可以绕过这个问题。

$$
\log\text{softmax}(x_i) = x_i - c - \log\sum_{j=1}^{d} e^{x_j - c}
$$

```python
def LogSoftMax(logits, recover_prob=True):
    logits_max, _ = logits.max(dim=-1)
    safe_logits = logits - logits_max.unsqueeze(1)
    safe_logits_exp = safe_logits.exp()
    safe_logits_sum = safe_logits_exp.sum(-1, keepdim=True)
    log_logits_sum = safe_logits_sum.log()
    log_probs = logits - logits_max.unsqueeze(1) - log_logits_sum
    if recover_prob:
        probs = log_probs.exp()
        probs = probs / probs.sum(-1, keepdim=True)
    return probs, log_probs
```

### 对比验证

```python
logits = torch.tensor(10, 2, 10000, 4)

# Softmax → log → -inf ❌
softmax_probs = SoftMax(logits)
print(softmax_probs.log())  # tensor(-inf, -inf, 0., -inf)

# LogSoftmax → 数值稳定 ✅
_, log_probs = LogSoftMax(logits)
print(log_probs)  # tensor(  -9990., -9998.,   0., -9996.)
```

---

## 3. Attention 为何除以 √d

### 问题

给定 d 维向量 q, k ~ N(0, 1)，它们的内积 `qk = Σ q_i * k_i` 的分布是什么？

### 方差推导

$$
\mathbb{E}[q_i k_i] = 0, \quad \text{Var}(q_i k_i) = 1
$$

$$
\text{Var}(qk) = \text{Var}\left(\sum_{i=1}^{d} q_i k_i\right) = d
$$

**内积的方差随维度 d 线性增长！** 当 d 很大时，注意力分数值很大，Softmax 趋于 one-hot，梯度消失。

### 为什么是 √d 而不是 d？

$$
\text{Var}(qk / \sqrt{d}) = \frac{1}{d} \cdot d = 1 \quad \checkmark
$$

$$
\text{Var}(qk / d) = \frac{1}{d^2} \cdot d = \frac{1}{d} \quad \text{（方差太小，分布太平）}
$$

> **关键洞察**：除以 √d 使 QK 内积恢复标准正态分布（方差=1），Softmax 输出保持合理分布。除以 d 会使分布过于均匀，丢失区分度。

### 实验验证

```python
d = 1024
q = torch.randn(1, d)
K = torch.randn(100, d)

score = q @ K.t()
score_sqrt_d = q @ K.t() / math.sqrt(d)
score_d = q @ K.t() / d

print(score.var())        # ≈ 1024（方差 = d）
print(score_sqrt_d.var()) # ≈ 1（标准正态）
print(score_d.var())      # ≈ 0.001（太小）
```

### 梯度验证

```python
# 不 scale：Softmax 梯度消失
s = q @ K.t() / math.sqrt(1)
s = F.softmax(s, dim=1)
ds = torch.diag(s[0]) - s.t() @ s  # 梯度 ≈ 0

# scale √d：梯度正常
s = q @ K.t() / math.sqrt(d)
s = F.softmax(s, dim=1)
ds = torch.diag(s[0]) - s.t() @ s  # 梯度正常
```

---

## 4. PyTorch Autograd 手撕

基于 Karpathy 的 micrograd，手撕一个支持自动求导的计算图。

### 核心：Value 节点

> **关键洞察**：每个运算（`+`, `*`, `tanh`）创建新节点时，注册 `_backward` 闭包。反向传播时按拓扑排序逆序调用所有 `_backward`。

```python
class Value:
    def __init__(self, data, _children=(), _op='', label=''):
        self.data = data
        self.grad = 0.0
        self._backward = lambda: None
        self._prev = set(_children)
        self._op = _op
        self.label = label

    def __add__(self, other):
        out = Value(self.data + other.data, (self, other), '+')
        def _backward():
            self.grad += 1.0 * out.grad    # ∂(a+b)/∂a = 1
            other.grad += 1.0 * out.grad   # ∂(a+b)/∂b = 1
        out._backward = _backward
        return out

    def __mul__(self, other):
        out = Value(self.data * other.data, (self, other), '*')
        def _backward():
            self.grad += other.data * out.grad  # ∂(ab)/∂a = b
            other.grad += self.data * out.grad  # ∂(ab)/∂b = a
        out._backward = _backward
        return out

    def tanh(self):
        x = self.data
        t = (math.exp(2*x) - 1) / (math.exp(2*x) + 1)
        out = Value(t, (self,), 'tanh')
        def _backward():
            self.grad += (1 - t**2) * out.grad  # tanh' = 1 - tanh²
        out._backward = _backward
        return out

    def backward(self):
        # 拓扑排序
        topo = []
        visited = set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for child in v._prev:
                    build_topo(child)
                topo.append(v)
        build_topo(self)
        # 反向传播
        self.grad = 1.0
        for node in reversed(topo):
            node._backward()
```

### 使用示例

```python
# 前向：L = (a*b + c) * f
a = Value(2.0, label='a')
b = Value(-3.0, label='b')
c = Value(10.0, label='c')
f = Value(-2.0, label='f')
e = a * b          # e = -6
d = e + c          # d = 4
L = d * f          # L = -8

# 反向
L.backward()
# a.grad = f * b = -2 * -3 = 6? 不对... 让我重算
# L = (a*b + c) * f
# ∂L/∂a = b * f = -3 * -2 = 6
# ∂L/∂b = a * f = 2 * -2 = -4
```

---

## 5. MLP 梯度计算

### 前向

```python
x = torch.randn(bs, dim)
w1 = torch.randn(dim, out_dim, requires_grad=True)
w2 = torch.randn(out_dim, head)

y1 = x @ w1
y2 = y1 @ w2
loss = ((y_label - y2) ** 2).mean()
```

### 手动反向

```python
# 从 loss 到 y2
e = 2 * (y2 - y_label) / (bs * head)

# Layer 2: y2 = y1 @ w2
w2_grad = y1.t() @ e           # dL/dw2
y1_grad = e @ w2.t()           # dL/dy1

# Layer 1: y1 = x @ w1
w1_grad = x.t() @ y1_grad     # dL/dw1
x_grad = y1_grad @ w1.t()     # dL/dx
```

> **关键洞察**：反向传播的计算量是前向的 **2 倍**（对于多层网络）。单层时，对 x 的梯度不需要计算（它不是参数），所以计算量与前向相同。

---

## 6. LayerNorm 实现与可视化

### 公式

$$
\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$

- 在**最后一维（特征维度）**计算均值和方差
- `γ`（scale）和 `β`（shift）是**逐特征**的可学习参数

### 实现

```python
class LayerNorm(nn.Module):
    def __init__(self, d_model, eps=1e-12):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.zeros(d_model))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        var = x.var(-1, unbiased=False, keepdim=True)
        out = (x - mean) / torch.sqrt(var + self.eps)
        out = self.gamma * out + self.beta
        return out
```

### Debug 示例

```python
dummy = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)
# Token-level mean: [2.0], [5.0]
# Token-level var:  [0.667], [0.667]
# 每个 token 独立归一化
```

> **关键洞察**：LayerNorm 作用在 token 级别（最后一维），每个 token 的特征独立归一化到 N(0,1)，然后通过可学习的 γ、β 恢复表达能力。训练后 γ、β 各不相同，说明不同特征维度需要不同的缩放和偏移。

---

## 7. LayerNorm vs RMSNorm 对比分析

### RMSNorm 实现

$$
\text{RMSNorm}(x) = \gamma \cdot \frac{x}{\sqrt{\frac{1}{d}\sum_{i=1}^{d} x_i^2 + \epsilon}}
$$

```python
class RMSNorm(nn.Module):
    def __init__(self, d_model, eps=1e-12):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.eps = eps

    def forward(self, x):
        rms = (x ** 2).mean(-1, keepdim=True)
        out = x / torch.sqrt(rms + self.eps)
        out = self.gamma * out
        return out
```

### 对比

| 特性 | LayerNorm | RMSNorm |
|------|-----------|---------|
| 均值中心化 | ✅ 减均值 | ❌ 不减均值 |
| 可学习参数 | γ + β | γ only |
| 计算量 | 较多（mean + var） | 较少（只算 RMS） |
| 尺度不变性 | ✅ | ✅ `RMS(αx) = αRMS(x)` |
| 反向梯度不变性 | - | ✅ 梯度与输入尺度成反比 |

### 手动梯度计算（LayerNorm）

```python
# 雅可比矩阵 dy/dx（d×d）
I_diag = torch.diag(torch.ones(d_model))
I_ones = torch.ones(d_model, d_model)
left = I_diag - 1/d_model * I_ones

for i in range(batch_size):
    for j in range(seq_len):
        d_ln = (left / std[i,j] - x_centered[i,j].outer(x_centered[i,j]) / (std[i,j]**3 * d_model)) * gamma
        grad[i,j] = de_dy[i,j] @ d_ln
```

### 手动梯度计算（RMSNorm）

```python
for i in range(batch_size):
    for j in range(seq_len):
        d_rms = I_diag / rms[i,j] - x[i,j].outer(x[i,j]) / (rms[i,j]**3 * d_model) * gamma
        grad[i,j] = de_dy[i,j] @ d_rms
```

> **关键洞察**：
> - RMSNorm 去掉了 center 操作（减均值），T5 论文认为 center 存储的是预训练任务的先验分布，去掉反而提高迁移能力。
> - RMSNorm 的梯度与输入尺度成反比（`grad(10x) / grad(x) = 1/10`），有自动调节效果。

---

## 8. Beam Search 实现

### 核心思想

- 每步维护 `beam_width` 条最优路径
- 每条路径扩展 `beam_width` 个候选 → 产生 `beam_width²` 条候选
- 按累积 **log 概率和** 排序，保留 top `beam_width` 条
- 遇到 EOS 的路径移入完成列表

> **关键洞察**：使用 log 概率和而非概率乘积，避免浮点下溢。`max(p(x₁)·p(x₂|x₁)·...) = max(Σ log p(xᵢ|x<ᵢ))`。

### 实现

```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer

model = GPT2LMHeadModel.from_pretrained("gpt2")
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
input_ids = tokenizer.encode("Once upon a time", return_tensors="pt")

max_length = 8
beam_width = 3
beams = [(input_ids, 0.0)]
completed_beams = []

for step in range(max_length):
    new_beams = []
    for beam_input_ids, beam_score in beams:
        with torch.no_grad():
            outputs = model(beam_input_ids)
            next_token_logits = outputs.logits[:, -1, :]

        # log 概率
        next_token_scores = F.log_softmax(next_token_logits, dim=-1)
        top_k_scores, top_k_tokens = torch.topk(next_token_scores, beam_width, dim=-1)

        for i in range(beam_width):
            next_token = top_k_tokens[0, i].unsqueeze(0).unsqueeze(0)
            new_input_ids = torch.cat([beam_input_ids, next_token], dim=-1)
            new_score = beam_score + top_k_scores[0, i].item()
            new_beams.append((new_input_ids, new_score))

    # 检查 EOS
    remaining_beams = []
    for ids, score in new_beams:
        if ids[0, -1].item() == tokenizer.eos_token_id:
            completed_beams.append((ids, score))
        else:
            remaining_beams.append((ids, score))

    # 保留 top beam_width
    beams = sorted(remaining_beams, key=lambda x: x[1], reverse=True)[:beam_width]

    if len(completed_beams) >= beam_width:
        break

# 合并未完成路径
completed_beams.extend(beams)
```

### 思考题

1. **时间复杂度**：O(beam_width × vocab_size × max_length)，比 greedy 慢 beam_width 倍
2. **全局最优？**：不是，beam search 是近似搜索
3. **候选路径数**：`beam_width²`（每条路径扩展 beam_width 个）
4. **提前 EOS**：该路径停止扩展，但 beam_width 不变，其他路径继续
5. **长度归一化**：纯概率和对长序列不公平（越长分数越低），实践中常做 length penalty

---

## 9. FP4 量化原理与实现

### 核心概念

FP4 使用 4 bit 表示浮点数：1 位符号 + 2 位指数 + 1 位尾数，共 16 个可表示值。

### 编码规则

- **正常数（normal）**：`value = (1 + mantissa) × 2^(exponent - bias)`
- **次正规数（subnormal, exponent=0）**：`value = mantissa × 2^(-bias)`
- **bias** = `2^(exponent_bits - 1)` = 2

### 实现（来自 bitsandbytes）

```python
import itertools

def create_fp8_map(signed=True, exponent_bits=2, precision_bits=1, total_bits=4):
    e = exponent_bits
    p = precision_bits
    has_sign = 1 if signed else 0
    assert e + p == total_bits - has_sign
    bias = 2 ** (exponent_bits - 1)

    values = []
    lst = list(itertools.product([0, 1], repeat=precision_bits))

    for evalue in range(2 ** exponent_bits):
        for bit_pattern in lst:
            value = 1 if evalue != 0 else 0  # 正常数有隐含的 1
            for i, pval in enumerate(bit_pattern):
                value += pval * (2 ** -(i + 1))

            if evalue == 0:
                # 次正规数
                value = value * 2 ** (-bias)
            else:
                # 正常数
                value = value * 2 ** -(evalue - bias - 1)

            values.append(value)
            if signed:
                values.append(-value)

    values.sort()
    # 填充到 256 长度（bitsandbytes 内部需要）
    if total_bits < 8:
        values.extend([0] * (256 - len(values)))
    values.sort()
    code = torch.Tensor(values)
    code /= code.max()  # 归一化到 [-1, 1]
    return code
```

### FP4 码表（16 个值）

```python
# signed=True, exponent_bits=2, precision_bits=1, total_bits=4
fp4 = create_fp8_map(signed=True, exponent_bits=2, precision_bits=1, total_bits=4)
# 16 个值（归一化后）:
# [-1.0, -0.5, -0.333, -0.25, -0.167, -0.125, -0.0625, 0,
#   0,  0.0625, 0.125, 0.167, 0.25, 0.333, 0.5, 1.0]
```

> **关键洞察**：FP4 只有 16 个可表示值，量化时需要找到最接近的码字。指数位控制数值范围，尾数位控制精度。次正规数（exponent=0）扩展了接近零的表示范围，但精度更低。

---

## 总结表

| 组件 | 核心要点 |
|------|---------|
| **AdamW** | weight decay 解耦，不受自适应学习率缩放 |
| **Safe Softmax** | 减 max 防溢出 |
| **LogSoftmax** | 直接计算 log(softmax) 闭式解，避免 -inf |
| **Scale √d** | 使 QK 内积方差 = 1，防止 Softmax 梯度消失 |
| **Autograd** | 每个 op 注册 _backward 闭包，拓扑排序反向传播 |
| **MLP Grad** | 反向计算量 ≈ 前向 × 2 |
| **LayerNorm** | token 级归一化 + 逐特征 γβ |
| **RMSNorm** | 去 center，只用 RMS 归一化，计算量更少 |
| **Beam Search** | log 概率和 + topk 剪枝，近似最优 |
| **FP4** | 4bit: 1sign + 2exp + 1mantissa，16 个码字 |
