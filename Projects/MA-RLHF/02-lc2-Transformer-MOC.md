---
title: "lc2 Â· Transformer ä¸“é¢˜åœ°å›¾"
type: moc
date: 2026-02-25
source: "https://github.com/dhcode-cpp/MA-RLHF/tree/main/lecture/lc2_transformer"
tags: [moc, ma-rlhf, transformer, attention, encoder-decoder, lc2]
---

# lc2 Â· Transformer ä¸“é¢˜åœ°å›¾

> **ç›®æ ‡**ï¼šä»é›¶æ‰‹å†™å®Œæ•´ Transformer å¹¶è·‘é€šè®­ç»ƒæ¨ç†ï¼ŒæŒæ¡ Encoder-Decoder æ¶æ„çš„æ¯ä¸€ä¸ªç»†èŠ‚ã€‚  
> **æ ¸å¿ƒæŒ‘æˆ˜**ï¼šä¸æ˜¯ã€Œè¯»æ‡‚ã€Transformerï¼Œè€Œæ˜¯èƒ½ç‹¬ç«‹å†™å‡º `model.py` + `train.py` + `inference.py` å¹¶è·‘é€šä¸€ä¸ªä¸­è‹±ç¿»è¯‘ä»»åŠ¡ã€‚

---

## å¸¦ç€è¿™ä¸‰ä¸ªé—®é¢˜å­¦

1. **Attention ä¸ºä»€ä¹ˆè¦é™¤ä»¥ âˆšdï¼Ÿ** ä¸é™¤ä¼šæ€æ ·ï¼Ÿè·Ÿ softmax çš„æ¢¯åº¦æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ
2. **Encoder å’Œ Decoder çš„ Attention mask æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ** ä¸ºä»€ä¹ˆ Decoder éœ€è¦ causal maskï¼Ÿ
3. **Teacher Forcing è®­ç»ƒå’Œè‡ªå›å½’æ¨ç†çš„å·®å¼‚åœ¨å“ªï¼Ÿ** è®­ç»ƒæ—¶ Decoder çœ‹åˆ°äº†ä»€ä¹ˆï¼Ÿæ¨ç†æ—¶å‘¢ï¼Ÿ

---

## å­¦ä¹ é¡ºåº

```
Step 1  Attention æœºåˆ¶            â† Q/K/V è®¡ç®—ï¼ŒScaled Dot-Product
   â†“
Step 2  ä½ç½®ç¼–ç å®ç°               â† Sinusoidal PE ä»£ç ä¸å¯è§†åŒ–
   â†“
Step 3  LayerNorm                 â† ç®—æ³•åŸç† + åå‘ä¼ æ’­
   â†“
Step 4  å®Œæ•´ Transformer æ¨¡å‹      â† Encoder + Decoder + Cross-Attention
   â†“
Step 5  æ•°æ®é›† & Tokenizer         â† WMT19 æ•°æ®åŠ è½½ï¼Œä¸­è‹±åˆ†è¯å™¨è®­ç»ƒ
   â†“
Step 6  è®­ç»ƒæµç¨‹                   â† Teacher Forcingï¼Œäº¤å‰ç†µ Lossï¼Œæ–­ç‚¹æ¢å¤
   â†“
Step 7  æ¨ç†æµç¨‹                   â† è‡ªå›å½’ç”Ÿæˆï¼ŒGreedy/Beam Search
```

---

## ç¬”è®°æ¸…å•

### Step 1ï¼šAttention æœºåˆ¶

**[[AI/3-LLM/Architecture/Transformer-æ‰‹æ’•å®æ“|Transformer æ‰‹æ’•å®æ“]]**ï¼ˆAttention éƒ¨åˆ†ï¼‰

- **Scaled Dot-Product Attention**ï¼š`Attn(Q,K,V) = softmax(QK^T / âˆšd_k) Â· V`
- **ä¸ºä»€ä¹ˆ /âˆšd_k**ï¼šQÂ·K çš„å†…ç§¯æ–¹å·®éš d_k çº¿æ€§å¢é•¿ï¼Œå€¼è¿‡å¤§ â†’ softmax è¿›å…¥é¥±å’ŒåŒº â†’ æ¢¯åº¦æ¶ˆå¤±ã€‚ç¼©æ”¾åæ–¹å·®ä¸º 1ï¼Œsoftmax è¾“å‡ºæ›´å¹³æ»‘
- **Multi-Head Attention**ï¼šå°† d_model æ‹†æˆ h ä¸ªå¤´ï¼Œæ¯ä¸ªå¤´ç‹¬ç«‹åš Attention â†’ æ•è·ä¸åŒå­ç©ºé—´çš„æ¨¡å¼
- **Self-Attention vs Cross-Attention**ï¼šSelf-Attn çš„ Q/K/V æ¥è‡ªåŒä¸€åºåˆ—ï¼›Cross-Attn çš„ Q æ¥è‡ª Decoderï¼ŒK/V æ¥è‡ª Encoder è¾“å‡º

è¯¾ç¨‹ä»£ç ï¼š`Transformer_Attention.ipynb` â€” æ³¨æ„åŠ›å‰å‘ + æ‰‹æ’•åå‘ä¼ æ’­ï¼ˆğŸŒŸ å¿…é¡»èƒ½å†™ï¼‰

---

### Step 2-3ï¼šä½ç½®ç¼–ç  & LayerNorm

**[[AI/3-LLM/Architecture/åŸºç¡€æ•°å­¦ç»„ä»¶æ‰‹æ’•|åŸºç¡€æ•°å­¦ç»„ä»¶æ‰‹æ’•]]**

- **Sinusoidal PE**ï¼š`PE(pos,2i) = sin(pos/10000^{2i/d})`ï¼Œä¸åŒé¢‘ç‡ç¼–ç ä¸åŒç»´åº¦
- **LayerNorm**ï¼šå¯¹æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦åšå½’ä¸€åŒ– `y = (x-Î¼)/Ïƒ * Î³ + Î²`
- **Pre-LN vs Post-LN**ï¼šPost-LNï¼ˆåŸç‰ˆï¼‰æ¢¯åº¦åœ¨æµ…å±‚çˆ†ç‚¸ï¼Œæ·±å±‚æ¶ˆå¤±ï¼›Pre-LN æŠŠ LN æ”¾åœ¨ Attention/FFN ä¹‹å‰ â†’ æ¢¯åº¦è·¯å¾„æ›´å¹³æ»‘ â†’ è®­ç»ƒæ›´ç¨³å®šï¼Œç°ä»£æ¨¡å‹å‡ ä¹éƒ½ç”¨ Pre-LN

è¯¾ç¨‹ä»£ç ï¼š`Position_Encoding.ipynb`ï¼ˆå®ç° + å¯è§†åŒ–ï¼‰ Â· `LayerNorm.ipynb`ï¼ˆåŸç† + åå‘æ¨å¯¼ï¼‰

---

### Step 4ï¼šå®Œæ•´ Transformer æ¨¡å‹

**[[AI/3-LLM/Architecture/Transformer-æ‰‹æ’•å®æ“|Transformer æ‰‹æ’•å®æ“]]**ï¼ˆå®Œæ•´æ¨¡å‹éƒ¨åˆ† ğŸŒŸï¼‰

Encoder-Decoder æ¶æ„ï¼š
```
Encoder:
  Input Embedding + PE
  â†’ N Ã— [Self-Attention â†’ Add&Norm â†’ FFN â†’ Add&Norm]
  â†’ Encoder Output

Decoder:
  Output Embedding + PE
  â†’ N Ã— [Masked Self-Attention â†’ Add&Norm â†’ Cross-Attention(Q=dec, K/V=enc) â†’ Add&Norm â†’ FFN â†’ Add&Norm]
  â†’ Linear â†’ Softmax
```

å…³é”®å®ç°ç»†èŠ‚ï¼š
- Encoder Self-Attentionï¼špadding maskï¼ˆå¿½ç•¥ `<pad>` tokenï¼‰
- Decoder Masked Self-Attentionï¼šcausal mask + padding maskï¼ˆä¸èƒ½çœ‹åˆ°æœªæ¥ tokenï¼‰
- Cross-Attentionï¼šK/V ç”¨ Encoder è¾“å‡ºï¼Œmask æ˜¯ Encoder çš„ padding mask

è¯¾ç¨‹ä»£ç ï¼š`Transformer.ipynb`ï¼ˆğŸŒŸ æ ¸å¿ƒï¼Œå¿…é¡»æ‰‹æ’•ï¼‰ Â· `model.py`ï¼ˆå·¥ç¨‹ç‰ˆï¼‰

---

### Step 5-6ï¼šæ•°æ®é›† & è®­ç»ƒ

â³ å¾…å…¥åº“ï¼š**Transformer è®­ç»ƒå…¨æµç¨‹ç¬”è®°**

- **æ•°æ®é›†**ï¼šWMT19 ä¸­è‹±ç¿»è¯‘ â†’ `data.json`
- **Tokenizer**ï¼šåˆ†åˆ«è®­ç»ƒä¸­/è‹± BPE åˆ†è¯å™¨ï¼Œå­˜å‚¨ merges + vocab
- **Dataset å°è£…**ï¼špadding / mask / label å¤„ç†ï¼ŒDataLoader + DataCollate
- **è®­ç»ƒ**ï¼šTeacher Forcingï¼ˆDecoder è¾“å…¥æ˜¯ ground truth shifted rightï¼‰ï¼ŒCrossEntropy Lossï¼ŒAdamW
- **æ–­ç‚¹æ¢å¤**ï¼šä¿å­˜ model + optimizer çŠ¶æ€ï¼Œcheckpoint æœºåˆ¶

è¯¾ç¨‹ä»£ç ï¼š
- `Load_Dataset.ipynb` / `Dataset.ipynb` â€” æ•°æ®åŠ è½½ä¸å°è£…
- `tokenizer.py` â†’ `train.py --learning_rate 1e-4 --epochs 1` â†’ `inference.py` â€” å®Œæ•´è®­ç»ƒæµæ°´çº¿
- `Model_IO.ipynb` â€” æ¨¡å‹ä¿å­˜/åŠ è½½/æ–­ç‚¹æ¢å¤

---

### Step 7ï¼šæ¨ç†æµç¨‹

â³ å¾…å…¥åº“ï¼š**Transformer æ¨ç†æµç¨‹ç¬”è®°**

- **è‡ªå›å½’æ¨ç†**ï¼šä» `<bos>` å¼€å§‹ï¼Œæ¯æ­¥é¢„æµ‹ä¸€ä¸ª tokenï¼Œå°†é¢„æµ‹ç»“æœæ‹¼æ¥å›è¾“å…¥ï¼Œç›´åˆ° `<eos>`
- **Teacher Forcingï¼ˆè®­ç»ƒï¼‰vs è‡ªå›å½’ï¼ˆæ¨ç†ï¼‰**ï¼šè®­ç»ƒæ—¶ Decoder çœ‹åˆ°å®Œæ•´ ground truthï¼ˆå¹¶è¡Œï¼‰ï¼Œæ¨ç†æ—¶åªèƒ½çœ‹åˆ°è‡ªå·±ä¹‹å‰çš„é¢„æµ‹ï¼ˆä¸²è¡Œï¼‰â†’ Exposure Bias é—®é¢˜
- **Greedy vs Beam Search**ï¼šGreedy æ¯æ­¥å– argmaxï¼›Beam ä¿ç•™ top-k å€™é€‰ï¼Œæœ€ç»ˆé€‰å…¨å±€æœ€ä¼˜åºåˆ—

è¯¾ç¨‹ä»£ç ï¼š`inference.py`ï¼ˆğŸŒŸ æ ¸å¿ƒï¼ŒåŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹åšä¸­è‹±ç¿»è¯‘ï¼‰

---

## é¢è¯•é«˜é¢‘åœºæ™¯é¢˜

**Qï¼šä¸ºä»€ä¹ˆ Attention è¦ç¼©æ”¾ âˆšdï¼Ÿ**  
Aï¼šå‡è®¾ Q å’Œ K çš„æ¯ä¸ªåˆ†é‡ç‹¬ç«‹åŒåˆ†å¸ƒã€å‡å€¼ 0 æ–¹å·® 1ï¼Œåˆ™ QÂ·K çš„å†…ç§¯ï¼ˆd ä¸ªä¹˜ç§¯ä¹‹å’Œï¼‰çš„æ–¹å·®ä¸º dã€‚d è¾ƒå¤§æ—¶å†…ç§¯å€¼æå¤§ â†’ softmax è¾“å‡ºè¶‹è¿‘ one-hot â†’ æ¢¯åº¦æ¥è¿‘é›¶ã€‚é™¤ä»¥ âˆšd ä½¿æ–¹å·®å›åˆ° 1ï¼Œä¿è¯ softmax è¾“å‡ºåˆ†å¸ƒåˆç†ã€æ¢¯åº¦å¯æµåŠ¨ã€‚

**Qï¼šEncoder å’Œ Decoder çš„ Attention mask æœ‰ä½•ä¸åŒï¼Ÿ**  
Aï¼šEncoder Self-Attention åªç”¨ **padding mask**ï¼ˆå¿½ç•¥ `<pad>` tokenï¼Œå…¶ä»–ä½ç½®å…¨å¯è§ï¼‰ã€‚Decoder Masked Self-Attention ä½¿ç”¨ **causal mask + padding mask**ï¼ˆä¸‹ä¸‰è§’çŸ©é˜µï¼Œæ¯ä¸ªä½ç½®åªèƒ½ attend åˆ°è‡ªå·±å’Œä¹‹å‰çš„ä½ç½®ï¼‰ï¼Œé˜²æ­¢ä¿¡æ¯æ³„éœ²æœªæ¥ tokenã€‚

**Qï¼šMulti-Head Attention çš„å¤šå¤´æœ‰ä»€ä¹ˆå¥½å¤„ï¼Ÿ**  
Aï¼šæ¯ä¸ªå¤´åœ¨ d_k = d_model/h çš„ä½ç»´å­ç©ºé—´åš Attentionï¼Œä¸åŒå¤´å¯ä»¥å­¦åˆ°ä¸åŒçš„ attention patternï¼ˆå¦‚ä¸€ä¸ªå¤´å…³æ³¨å±€éƒ¨è¯­æ³•ï¼Œå¦ä¸€ä¸ªå…³æ³¨é•¿è·ç¦»ä¾èµ–ï¼‰ã€‚æ€»å‚æ•°é‡ä¸å•å¤´ç›¸åŒï¼ˆQ/K/V æƒé‡æ‹†åˆ†ï¼‰ï¼Œä½†è¡¨è¾¾åŠ›æ›´å¼ºã€‚

**Qï¼šPre-LN å’Œ Post-LN çš„åŒºåˆ«ï¼Ÿä¸ºä»€ä¹ˆç°ä»£æ¨¡å‹éƒ½ç”¨ Pre-LNï¼Ÿ**  
Aï¼šPost-LN çš„æ®‹å·®è·¯å¾„ `x + Sublayer(LN(x))` åœ¨æ·±å±‚ä¼šå¯¼è‡´æ¢¯åº¦ä¸ç¨³å®šï¼ˆæ®‹å·®è¿æ¥çš„ scale é€å±‚ç´¯ç§¯ï¼‰ã€‚Pre-LN æŠŠ LN æ”¾åˆ° Sublayer å†…éƒ¨ï¼Œæ®‹å·®è·¯å¾„ç›´æ¥ä¼ æ¢¯åº¦ â†’ æ¢¯åº¦èŒƒæ•°æ›´ç¨³å®š â†’ æ·±å±‚ç½‘ç»œï¼ˆ50+ å±‚ï¼‰ä¹Ÿèƒ½ç¨³å®šè®­ç»ƒã€‚ä»£ä»·æ˜¯æœ€ç»ˆè¾“å‡ºå¯èƒ½éœ€è¦é¢å¤– LNã€‚
