---
title: "LLM 应用部署与工程化 — 2026 技术全景与面试深度笔记"
date: 2026-02-21
tags: [面试武器, 部署, serving, 推理工程, vLLM, 量化, 深度笔记]
domain: ai/llm/inference
vault_note_id: 20
series: "面试武器库"
brief: "LLM 应用部署与工程化 2026 面试武器库：vLLM/TRT-LLM/量化/投机解码/在线推理服务化全景；含生产部署坑点与面试高频题。"
status: active
rating: ★★★★★
archived_by: librarian
archived_date: 2026-02-21
---

# LLM 应用部署与工程化 — 2026 技术全景与面试深度笔记

> "模型训好只完成了 20%，上线才是真正的战场。" 面试中被问"你怎么把一个 70B 模型部署上线并服务 1000 QPS"的频率远高于"推导 attention 公式"。

---

## 一、推理引擎全景

### 1.1 主流推理框架对比（2026）

| 框架 | 核心优势 | 适用场景 | 关键特性 |
|------|----------|----------|----------|
| **vLLM** | PagedAttention，throughput 王 | 高吞吐在线服务 | continuous batching, prefix caching, TP/PP |
| **TensorRT-LLM** | NVIDIA 深度优化 | A100/H100 极致性能 | FP8, in-flight batching, 自定义 kernel |
| **llama.cpp/GGUF** | CPU/消费级 GPU | 本地部署/边缘 | GGUF 格式, Q4/Q5/Q8 量化, Metal/CUDA/Vulkan |
| **SGLang** | RadixAttention | 多轮对话/复杂 pipeline | 结构化生成优化, constrained decoding |
| **Ollama** | 一键部署 | 个人/开发者 | 基于 llama.cpp, 模型管理, API 兼容 |
| **DeepSpeed-FastGen** | 动态拆分 | 微软生态 | SplitFuse, 长短 prompt 混合 |
| **TGI** (HuggingFace) | 生产就绪 | HF 模型直接部署 | 水印, 安全, 多模型 |

### 1.2 关键概念

**Continuous Batching（连续批处理）**：
- 传统 static batching：等一批请求到齐再一起推理，短请求等长请求
- Continuous batching：请求完成即退出 batch，新请求随时加入
- 效果：throughput 提升 2-5x，latency 降低 50%+
- vLLM/TGI/TensorRT-LLM 都支持

**PagedAttention（vLLM 核心创新）**：
- 问题：KV-Cache 内存碎片化严重（每个请求独立分配连续内存）
- 解法：借鉴 OS 虚拟内存/分页思想，KV-Cache 按 block 分配
- block 大小通常 16 tokens，block table 做映射
- 内存利用率从 ~50% 提升到 ~95%
- **面试杀手锏**：PagedAttention 不改模型、不改精度，纯工程优化就把吞吐提升 2-4x

**Prefix Caching**：
- 多个请求共享相同的 system prompt → 只计算一次，缓存 KV-Cache
- SGLang 的 RadixAttention 用 Radix Tree 管理前缀共享
- 在 Agent 场景尤其有效（长 system prompt + 短用户输入）

---

## 二、模型量化部署

### 2.1 量化方法分类

| 方法 | 类型 | 精度 | 需要校准？ | 代表 |
|------|------|------|-----------|------|
| **FP16/BF16** | 原始 | 16-bit | 否 | 基准 |
| **INT8** | PTQ/QAT | 8-bit | 是/否 | LLM.int8(), SmoothQuant |
| **INT4/GPTQ** | PTQ | 4-bit | 是（128 样本） | GPTQ, AutoGPTQ |
| **AWQ** | PTQ | 4-bit | 是 | 保护重要权重通道 |
| **GGUF** | PTQ | 2-8bit | 否 | llama.cpp 生态 |
| **FP8** | PTQ | 8-bit | 是 | TensorRT-LLM, H100 原生 |
| **AQLM** | PTQ | 2-bit | 是 | 加法量化 |
| **BitNet** | QAT | 1.58-bit | 训练时 | 微软, 三值权重 |

### 2.2 核心量化技术深度

**GPTQ**（2022）：
- 逐层量化，用 Hessian 矩阵的逆来最小化量化误差
- 每层处理时按列排序，误差大的列优先处理
- 需要 ~128 条校准数据
- 4-bit GPTQ 在 7B 模型上质量损失 <1% perplexity

**AWQ**（Activation-Aware Weight Quantization, MIT 2023）：
- 核心洞察：不是所有权重同等重要，~1% 的权重通道贡献了大部分激活量级
- 保护这些 salient channels（乘以 scale 因子后量化，再除以 scale）
- 比 GPTQ 更快（不需要反量化-重量化迭代）
- 4-bit AWQ ≈ FP16 质量

**SmoothQuant**（MIT 2022）：
- 解决 activation 量化的异常值问题
- 数学上等价的变换：把 activation 的 scale 因子迁移到 weight 上
- W8A8（权重和激活都是 INT8）→ 可用 INT8 矩阵乘法硬件加速

**面试追问**：GPTQ vs AWQ 怎么选？
→ AWQ 更快部署（不需要大量校准数据），GPTQ 在极端压缩（3-bit）下可能更好。AWQ 更适合 vLLM 部署（官方支持更好），GPTQ 更适合 HuggingFace 生态。

### 2.3 GGUF 格式深度

```
GGUF 量化等级（从高到低质量）：
F16   → 无损参考
Q8_0  → 8-bit，几乎无损
Q6_K  → 6-bit 混合，推荐质量上限
Q5_K_M → 5-bit 混合，性价比最优
Q4_K_M → 4-bit 混合，最常用
Q4_0  → 4-bit 均匀，稍差
Q3_K_M → 3-bit，质量开始下降
Q2_K  → 2-bit，仅适合实验
IQ4_XS → imatrix 4-bit，相同 bit 数下质量更好
```

**imatrix 量化**：
- 用校准数据计算每个权重的"重要性矩阵"
- 重要权重分配更多 bit，不重要的压缩更狠
- IQ4_XS 质量优于 Q4_K_M，且更小

**面试追问**：2026 年 ggml.ai 加入 HuggingFace 有什么影响？
→ llama.cpp/GGUF 生态获得 HF 的长期资源支持，本地 AI 部署的标准化和可持续性得到保障。GGUF 作为本地推理的事实标准地位进一步巩固。

---

## 三、KV-Cache 优化

### 3.1 KV-Cache 基础

每次 autoregressive 生成时，之前所有 token 的 Key/Value 需要保留：
- 内存占用：`2 × num_layers × num_heads × head_dim × seq_len × batch_size × dtype_bytes`
- 70B 模型，2048 seq_len，batch 32：~80GB KV-Cache
- 这往往是推理的内存瓶颈，而非模型权重

### 3.2 优化方法

| 方法 | 原理 | 压缩比 |
|------|------|--------|
| **Multi-Query Attention (MQA)** | 所有 head 共享同一组 KV | ~8x |
| **Grouped-Query Attention (GQA)** | 每组 head 共享一组 KV | 2-8x |
| **KV-Cache 量化** | INT4/INT8 量化 KV 值 | 2-4x |
| **Sliding Window** | 只保留最近 N 个 token 的 KV | 窗口外丢弃 |
| **Token Eviction** | 驱逐注意力分数低的 token | 动态 |
| **H2O (Heavy-Hitter Oracle)** | 保留累积注意力最高的 token | 5-10x |
| **PagedAttention** | 消除内存碎片 | ~2x 利用率 |

**GQA 为什么成为标准？**
- Llama 2/3、Mistral、Gemma 全用 GQA
- 比 MHA 省内存，比 MQA 质量好
- 典型配置：32 Q heads / 8 KV heads = 4x 压缩

### 3.3 长上下文推理

**RoPE 外推**：
- 基座模型训练时只用 4k/8k 上下文
- NTK-aware scaling / YaRN：修改 RoPE 频率支持更长上下文
- Dynamic NTK：根据实际输入长度动态调整

**Ring Attention**：
- 将长序列分块分到多个设备
- 每个设备只处理一个 chunk，通过 ring 通信传递 KV
- 理论上支持无限长上下文

---

## 四、Speculative Decoding（推测解码）

### 4.1 核心思想

Autoregressive 生成是串行的（每次只生成一个 token）→ GPU 利用率低。
推测解码：用小模型（draft model）快速生成 K 个候选 token，大模型一次性验证。

```
Draft model (7B):  生成 token 1, 2, 3, 4, 5  ← 快（~5ms/token）
Target model (70B): 并行验证 1✓ 2✓ 3✓ 4✗     ← 一次前向传播
结果：接受 token 1,2,3，从 4 开始重新生成
```

### 4.2 数学保证

- 使用 rejection sampling：接受概率 = min(1, p_target/p_draft)
- 保证最终输出分布与大模型完全一致（数学等价，非近似）
- 加速比取决于 draft model 与 target model 的一致性

### 4.3 变体

- **Self-Speculative Decoding**：用模型自身的早期层做 draft（不需要额外模型）
- **Medusa**：在模型顶部加多个 head，每个预测不同位置的 token
- **Eagle**：基于特征级别而非 token 级别的推测
- **Lookahead Decoding**：利用 Jacobi 迭代并行生成

**面试追问**：推测解码的瓶颈？
→ ① Draft model 质量差 → 接受率低 → 加速比差 ② 需要额外内存加载 draft model ③ 对 batch 场景效果减弱（GPU 已经被批处理利用充分）④ 适合延迟敏感的交互式场景，不适合高吞吐批处理。

---

## 五、模型服务架构

### 5.1 生产级架构

```
                    ┌──────────────┐
                    │   Gateway    │ (rate limit, auth, routing)
                    │  (nginx/Kong)│
                    └──────┬───────┘
                           │
                    ┌──────┴───────┐
                    │  Load Balancer│ (round-robin / least-conn)
                    └──────┬───────┘
                           │
              ┌────────────┼────────────┐
              │            │            │
        ┌─────┴─────┐ ┌───┴────┐ ┌────┴─────┐
        │ vLLM Pod 1│ │ Pod 2  │ │  Pod 3   │ (GPU nodes)
        │ (4×A100)  │ │(4×A100)│ │ (4×A100) │
        └───────────┘ └────────┘ └──────────┘
              │
        ┌─────┴─────┐
        │ Model Store│ (S3/MinIO, GGUF/safetensors)
        └───────────┘
```

### 5.2 关键工程决策

**Tensor Parallelism (TP) vs Pipeline Parallelism (PP)**：
- TP：每层的权重切分到多个 GPU（需要 NVLink/高速互联）
- PP：不同层放在不同 GPU（bubble 问题，但互联要求低）
- 70B 模型：4×A100 用 TP=4 最优。跨机器用 PP
- **面试要点**：TP 需要 all-reduce（通信量 = 2×hidden_size×batch_size），PP 只需要传 activation（通信量小但有 pipeline bubble）

**Batching 策略**：
- max_batch_size：平衡吞吐和延迟
- max_waiting_time：请求等待入 batch 的最大时间
- 动态调整：根据 GPU 利用率和队列深度

**模型热更新**：
- 蓝绿部署：新旧模型并行运行，流量切换
- Canary 发布：5% 流量到新模型，观察指标
- A/B 测试：不同模型在线对比

### 5.3 监控与可观测性

**核心指标**：
- **TTFT** (Time To First Token)：用户体验关键
- **TPS** (Tokens Per Second)：生成速度
- **Throughput** (requests/sec)：系统容量
- **P50/P95/P99 延迟**：尾部延迟
- **GPU 利用率**：<70% 说明 batch 不够，>95% 说明需要扩容
- **KV-Cache 使用率**：接近满说明长请求太多
- **Queue depth**：请求排队长度

**告警规则示例**：
- TTFT P95 > 2s → 扩容或优化
- GPU 利用率 < 30% 持续 10min → 缩容
- 错误率 > 1% → 回滚

---

## 六、成本优化

### 6.1 成本结构

| 组件 | 典型占比 | 优化方向 |
|------|----------|----------|
| GPU 计算 | 60-70% | 量化/蒸馏/batch 优化 |
| GPU 内存 | 包含在上面 | KV-Cache 优化/量化 |
| 存储 | 5-10% | 模型压缩/共享存储 |
| 网络 | 5-10% | 减少跨节点通信 |
| 冗余/冷备 | 10-20% | 自动扩缩容 |

### 6.2 降本策略

1. **模型蒸馏**：70B → 7B，成本降 10x，质量损失 <10%
2. **量化部署**：FP16 → INT4，内存降 4x，吞吐提升 2x
3. **Prompt 压缩**：LLMLingua / 自动摘要，减少输入 token
4. **缓存**：常见问题的回答缓存（语义相似度匹配）
5. **路由**：简单问题用小模型，复杂问题用大模型（Router 模型）
6. **Spot Instance**：非实时任务用 spot GPU，成本降 50-70%

### 6.3 模型选择决策框架

```
需求分析
  ↓
质量要求 ── 高 → 70B+ (4×A100)
  │                └→ 考虑 API (GPT-4/Claude)
  └── 中 → 7-14B (1×A100 / INT4 on 4090)
  │         └→ Llama 3/Qwen 2.5/Mistral
  └── 低 → 1-3B (CPU / 边缘)
            └→ Phi-3/Gemma 2
```

---

## 七、MLOps for LLM

### 7.1 CI/CD Pipeline

```
代码变更 → 单元测试 → 模型评估 → Staging 部署 → 在线评估 → Production
                         ↓
                   质量门禁：
                   - Perplexity 不退步
                   - 核心任务准确率 ≥ baseline
                   - 延迟 ≤ SLA
                   - 安全过滤通过率 ≥ 99.9%
```

### 7.2 模型版本管理

- **模型 Registry**：MLflow / W&B / 自建
- **权重存储**：S3/GCS + safetensors 格式
- **配置即代码**：模型参数、量化方案、serving 配置全部版本化
- **回滚策略**：秒级切换到上一版本

### 7.3 数据飞轮

```
用户请求 → LLM 响应 → 用户反馈（隐式/显式）
    ↓                         ↓
  日志采集                 标注队列
    ↓                         ↓
  分析 & 挖掘             人工标注
    ↓                         ↓
  Prompt 优化 ←←←←←←←← 微调数据集
```

---

## 八、安全与合规

### 8.1 推理安全

- **Prompt Injection 防护**：输入过滤 + 输出审核双层防线
- **输出安全**：内容安全分类器（Llama Guard / OpenAI Moderation）
- **PII 检测**：自动检测和脱敏个人信息
- **速率限制**：防滥用、防 DDoS
- **日志审计**：所有请求/响应可追溯

### 8.2 数据安全

- **数据不出域**：私有部署保证数据不发送到第三方
- **加密传输**：TLS 1.3
- **权限控制**：RBAC，不同用户组访问不同模型
- **合规认证**：SOC 2 / GDPR / 等保

---

## 九、面试高频题精选

### Q1: 如何把 70B 模型部署到 4 张 A100 上服务 1000 QPS？
→ ① TP=4 切分模型 ② vLLM + continuous batching ③ INT4/AWQ 量化降内存 ④ PagedAttention 优化 KV-Cache ⑤ Prefix caching（system prompt 共享）⑥ 水平扩展多副本 + LB。实际需要 benchmark 确认瓶颈是 compute-bound 还是 memory-bound。

### Q2: TTFT（首 token 延迟）太高怎么优化？
→ ① 检查是否 prefill 阶段太慢（长 prompt）→ prompt 压缩或 chunked prefill ② 检查 batch 排队 → 调整 batching 策略或扩容 ③ 检查模型加载 → 预热/常驻 GPU ④ 推测解码（但注意这更影响 decode 速度而非 prefill）。

### Q3: vLLM 和 TensorRT-LLM 怎么选？
→ vLLM：开源生态好、模型支持广、部署简单、社区活跃。TensorRT-LLM：NVIDIA GPU 上性能极致（FP8 支持、自定义 kernel）、但模型支持范围窄、配置复杂。**推荐**：先用 vLLM 快速上线，性能瓶颈时用 TensorRT-LLM 优化关键路径。

### Q4: 在线推理和离线批处理的架构区别？
→ 在线：延迟优先（TTFT < 500ms），auto-scaling，streaming 输出，小 batch。离线：吞吐优先，大 batch（64-256），可用 spot instance，不需要 streaming。两者共用模型权重但 serving 配置完全不同。

### Q5: 量化后模型质量怎么评估？
→ ① Perplexity 对比（最基本）② 下游任务准确率（MMLU/HumanEval）③ 人工评估（盲评 A/B）④ 特定领域测试集 ⑤ 关注长尾 case（量化通常在平均指标上还行，但在边缘 case 上退化更明显）。

### Q6: 如何做模型的灰度发布？
→ ① 流量分桶：5%→20%→50%→100% ② 每阶段监控核心指标（延迟/质量/错误率）③ 自动回滚条件（错误率>1% 或延迟 P99>SLA）④ 影子模式（shadow traffic）：新模型处理真实流量但不返回给用户，只对比指标。

---

_Vault 笔记 #20 | LLM 应用部署与工程化 | 首次创建 2026-02-21 | 面试武器库系列_

---

> ⚠️ **路径偏差警告**：本文由 Scholar 写入根目录，正式版应迁移至 `Career/LLM-应用部署与工程化-2026技术全景.md`（面试武器库系列）。

## See Also

-  — 职业发展知识域索引
- [[AI面试速查手册|AI 面试速查手册]] — 速查层（面试武器系列同伴）
- [[AI/3-LLM/Inference/模型量化综述|模型量化综述]] — 量化技术深度
- [[模型部署实践|模型部署实践]] — 部署技术深度
- [[AI/3-LLM/Inference/LLM推理优化-2026全景|LLM 推理优化 2026 全景]] — 推理系统优化宏观综述

---

## See Also（馆长补充）

-  — 职业发展知识域全索引
- [[AI面试速查手册|AI 面试速查手册]] — 同系列速查版
- [[AI/3-LLM/Inference/模型量化综述|模型量化综述]] — 量化技术深度（vLLM/量化部署基础）
- [[AI/3-LLM/Inference/LLM推理优化-2026全景|LLM 推理优化 2026 全景]] — 推理系统优化宏观综述
-  — 上级知识域
