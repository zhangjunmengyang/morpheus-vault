---
title: æ–‡æ¡£è§£æï¼šPDF/è¡¨æ ¼/å¤šæ¨¡æ€å¤„ç†æŠ€æœ¯
brief: RAG ç³»ç»Ÿä¸­æ–‡æ¡£è§£ææŠ€æœ¯æ ˆçš„å®Œæ•´è¦†ç›–ï¼šPyPDF/PDFPlumber/Unstructured/Docling å››å¤§ PDF è§£ææ–¹æ¡ˆã€Table Transformer/Camelot è¡¨æ ¼è¯†åˆ«ã€LayoutLM/Vision-based å¤šæ¨¡æ€æ–‡æ¡£ç†è§£ã€PaddleOCR/Tesseract OCR æ–¹æ¡ˆï¼Œå«ä»£ç ç¤ºä¾‹å’Œè´¨é‡è¯„ä¼°æ¡†æ¶ã€‚
type: tutorial
domain: ai/rag
tags:
  - ai/rag
  - ai/document-parsing
  - ai/ocr
  - ai/table-extraction
  - type/tutorial
created: 2026-02-14
updated: 2026-02-22
status: complete
sources:
  - Unstructured.io Documentation https://docs.unstructured.io/
  - PyMuPDF Documentation https://pymupdf.readthedocs.io/
  - Marker Project (VikParuchuri) https://github.com/VikParuchuri/marker
  - IBM Docling https://github.com/DS4SD/docling
  - Microsoft Table Transformer https://github.com/microsoft/table-transformer
  - "Huang et al. 'LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking' arXiv:2204.08387"
related:
  - "[[AI/6-åº”ç”¨/RAG/RAG åŸç†ä¸æ¶æ„]]"
  - "[[AI/6-åº”ç”¨/RAG/æ£€ç´¢ç­–ç•¥]]"
  - "[[AI/6-åº”ç”¨/Embedding/Embedding ä¸å‘é‡æ£€ç´¢]]"
  - RAG 2026 å…¨æ™¯
---

# æ–‡æ¡£è§£æï¼šPDF/è¡¨æ ¼/å¤šæ¨¡æ€å¤„ç†æŠ€æœ¯

åœ¨ [[AI/6-åº”ç”¨/RAG/RAG åŸç†ä¸æ¶æ„|]] ä¸­ï¼Œæ–‡æ¡£è§£ææ˜¯æ•´ä¸ªçŸ¥è¯†æ£€ç´¢é“¾è·¯çš„èµ·ç‚¹ï¼Œå…¶è´¨é‡ç›´æ¥å½±å“åç»­çš„å‘é‡åŒ–å’Œæ£€ç´¢æ•ˆæœã€‚æœ¬æ–‡æ·±å…¥åˆ†æç°ä»£æ–‡æ¡£è§£ææŠ€æœ¯æ ˆï¼Œæ¶µç›–PDFå¤„ç†ã€è¡¨æ ¼è¯†åˆ«ã€å¤šæ¨¡æ€ç†è§£å’ŒOCRæ–¹æ¡ˆã€‚

> æ¥æºï¼šUnstructured.io å®˜æ–¹æ–‡æ¡£ã€PyMuPDF æ–‡æ¡£ã€Marker é¡¹ç›®ï¼ˆVikParuchuri/markerï¼‰ã€IBM Docling é¡¹ç›®

## 1. PDFè§£ææŠ€æœ¯æ ˆ

### 1.1 PyPDFç³»åˆ—
PyPDFæ˜¯Pythonç”Ÿæ€ä¸­æœ€åŸºç¡€çš„PDFå¤„ç†åº“ï¼Œé€‚åˆç®€å•æ–‡æœ¬æå–ï¼š

```python
from pypdf import PdfReader

def extract_text_pypdf(pdf_path):
    reader = PdfReader(pdf_path)
    text = ""
    for page in reader.pages:
        text += page.extract_text()
    return text

# ä¼˜ç¼ºç‚¹
ä¼˜ç‚¹ï¼šè½»é‡çº§ï¼Œçº¯æ–‡æœ¬æå–ç¨³å®š
ç¼ºç‚¹ï¼šæ— æ³•å¤„ç†å¤æ‚å¸ƒå±€ï¼Œè¡¨æ ¼è¯†åˆ«èƒ½åŠ›å¼±
```

**é€‚ç”¨åœºæ™¯**ï¼šç®€å•çš„æ–‡æœ¬å‹PDFï¼Œå­¦æœ¯è®ºæ–‡çš„çº¯æ–‡æœ¬éƒ¨åˆ†

### 1.2 PDFPlumber
PDFPlumberä¸“æ³¨äºç»“æ„åŒ–ä¿¡æ¯æå–ï¼Œç‰¹åˆ«æ˜¯è¡¨æ ¼å¤„ç†ï¼š

```python
import pdfplumber

def extract_structured_content(pdf_path):
    with pdfplumber.open(pdf_path) as pdf:
        for page_num, page in enumerate(pdf.pages):
            # æå–æ–‡æœ¬
            text = page.extract_text()
            
            # æå–è¡¨æ ¼
            tables = page.extract_tables()
            
            # æå–å›¾åƒä¿¡æ¯
            images = page.images
            
            # å­—ç¬¦çº§åˆ«ä¿¡æ¯
            chars = page.chars
    
    return {
        'text': text,
        'tables': tables,
        'metadata': {'images': len(images), 'chars': len(chars)}
    }
```

**ç‰¹ç‚¹**ï¼š
- ä¿ç•™å­—ç¬¦çº§åˆ«çš„ä½ç½®ä¿¡æ¯
- è¡¨æ ¼æ£€æµ‹å’Œæå–èƒ½åŠ›å¼º
- æ”¯æŒè‡ªå®šä¹‰è¡¨æ ¼è¯†åˆ«ç­–ç•¥
- å¯è·å–é¡µé¢å¸ƒå±€ç»“æ„

### 1.3 Unstructured.io
Unstructuredæ˜¯ä¸“ä¸šçš„æ–‡æ¡£è§£ææ¡†æ¶ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼ï¼š

```python
from unstructured.partition.pdf import partition_pdf
from unstructured.staging.base import dict_to_elements

def parse_with_unstructured(pdf_path):
    # å¤šç§è§£æç­–ç•¥
    elements = partition_pdf(
        pdf_path,
        strategy="hi_res",          # é«˜ç²¾åº¦æ¨¡å¼
        infer_table_structure=True, # è¡¨æ ¼ç»“æ„æ¨ç†
        model_name="yolox",         # å¸ƒå±€æ£€æµ‹æ¨¡å‹
        chunking_strategy="by_title" # åˆ†å—ç­–ç•¥
    )
    
    # åˆ†ç±»å¤„ç†ä¸åŒå…ƒç´ 
    text_elements = []
    table_elements = []
    
    for element in elements:
        if element.category == "Table":
            table_elements.append(element)
        elif element.category in ["Title", "NarrativeText"]:
            text_elements.append(element)
    
    return text_elements, table_elements
```

**è§£æç­–ç•¥å¯¹æ¯”**ï¼š
- **fast**ï¼šåŸºäºè§„åˆ™ï¼Œé€Ÿåº¦å¿«ï¼Œç²¾åº¦ä¸€èˆ¬
- **ocr_only**ï¼šçº¯OCRæ¨¡å¼ï¼Œé€‚åˆæ‰«æä»¶
- **hi_res**ï¼šæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç²¾åº¦é«˜ä½†é€Ÿåº¦æ…¢

### 1.4 Doclingï¼ˆIBMå¼€æºï¼‰
Doclingæ˜¯IBMæœ€æ–°å¼€æºçš„æ–‡æ¡£ç†è§£æ¡†æ¶ï¼Œç‰¹åˆ«é€‚åˆä¼ä¸šçº§åº”ç”¨ï¼š

```python
from docling.document_converter import DocumentConverter

def parse_with_docling(pdf_path):
    converter = DocumentConverter()
    
    # è½¬æ¢æ–‡æ¡£
    result = converter.convert(pdf_path)
    
    # è·å–ç»“æ„åŒ–å†…å®¹
    document = result.document
    
    # åˆ†åˆ«å¤„ç†æ–‡æœ¬ã€è¡¨æ ¼ã€å›¾åƒ
    text_content = []
    tables = []
    
    for item, level in document.iterate_items():
        if item.item_type == "paragraph":
            text_content.append(item.get_text())
        elif item.item_type == "table":
            tables.append(item.export_to_dataframe())
    
    return {
        'text': text_content,
        'tables': tables,
        'metadata': result.input.file
    }
```

**Doclingä¼˜åŠ¿**ï¼š
- ä¼ä¸šçº§ç¨³å®šæ€§å’Œæ€§èƒ½
- æ”¯æŒå¤§è§„æ¨¡æ‰¹å¤„ç†
- é›†æˆå…ˆè¿›çš„æ·±åº¦å­¦ä¹ æ¨¡å‹
- è‰¯å¥½çš„APIè®¾è®¡å’Œæ–‡æ¡£

## 2. è¡¨æ ¼è¯†åˆ«æŠ€æœ¯

### 2.1 Table Transformer
åŸºäºDETRï¼ˆDEtection TRansformerï¼‰çš„è¡¨æ ¼æ£€æµ‹å’Œç»“æ„è¯†åˆ«ï¼š

```python
from transformers import DetrImageProcessor, TableTransformerForObjectDetection
import torch
from PIL import Image

class TableTransformerParser:
    def __init__(self):
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.processor = DetrImageProcessor.from_pretrained(
            "microsoft/table-transformer-detection"
        )
        self.model = TableTransformerForObjectDetection.from_pretrained(
            "microsoft/table-transformer-detection"
        )
    
    def detect_tables(self, image):
        # é¢„å¤„ç†å›¾åƒ
        inputs = self.processor(images=image, return_tensors="pt")
        
        # æ¨ç†
        outputs = self.model(**inputs)
        
        # åå¤„ç†
        target_sizes = torch.tensor([image.size[::-1]])
        results = self.processor.post_process_object_detection(
            outputs, target_sizes=target_sizes, threshold=0.9
        )[0]
        
        return results['boxes'], results['scores']
    
    def extract_table_structure(self, table_image):
        # ä½¿ç”¨ç»“æ„è¯†åˆ«æ¨¡å‹
        structure_model = TableTransformerForObjectDetection.from_pretrained(
            "microsoft/table-transformer-structure-recognition"
        )
        # ... ç»“æ„è§£æé€»è¾‘
```

**æŠ€æœ¯ç‰¹ç‚¹**ï¼š
- ç«¯åˆ°ç«¯çš„è¡¨æ ¼æ£€æµ‹å’Œç»“æ„è¯†åˆ«
- æ”¯æŒå¤æ‚è¡¨æ ¼å¸ƒå±€ï¼ˆåˆå¹¶å•å…ƒæ ¼ã€å¤šçº§è¡¨å¤´ï¼‰
- åŸºäºTransformeræ¶æ„ï¼Œæ³›åŒ–èƒ½åŠ›å¼º

### 2.2 Camelot
Camelotä¸“æ³¨äºPDFè¡¨æ ¼æå–ï¼Œæä¾›å¤šç§è§£æå¼•æ“ï¼š

```python
import camelot

def extract_tables_camelot(pdf_path, page_num):
    # Latticeæ¨¡å¼ï¼šé€‚åˆæœ‰æ˜æ˜¾è¾¹æ¡†çš„è¡¨æ ¼
    lattice_tables = camelot.read_pdf(
        pdf_path, 
        pages=str(page_num),
        flavor='lattice'
    )
    
    # Streamæ¨¡å¼ï¼šé€‚åˆæ— è¾¹æ¡†è¡¨æ ¼
    stream_tables = camelot.read_pdf(
        pdf_path,
        pages=str(page_num),
        flavor='stream'
    )
    
    # è´¨é‡è¯„ä¼°
    for table in lattice_tables:
        accuracy = table.parsing_report['accuracy']
        whitespace = table.parsing_report['whitespace']
        
        if accuracy > 0.8 and whitespace < 0.3:
            # é«˜è´¨é‡è¡¨æ ¼
            df = table.df
            return df
    
    return None

# é«˜çº§é…ç½®
tables = camelot.read_pdf(
    'document.pdf',
    pages='1-10',
    flavor='lattice',
    table_areas=['72,720,504,200'],  # æŒ‡å®šè¡¨æ ¼åŒºåŸŸ
    columns=['92,180,270,360'],       # æŒ‡å®šåˆ—åˆ†éš”ä½ç½®
    split_text=True                   # å¤„ç†æ¢è¡Œæ–‡æœ¬
)
```

**Camelotç‰¹ç‚¹**ï¼š
- åŒå¼•æ“è®¾è®¡ï¼Œé€‚åº”ä¸åŒè¡¨æ ¼ç±»å‹
- å†…ç½®è´¨é‡è¯„ä¼°æœºåˆ¶
- æ”¯æŒè¡¨æ ¼åŒºåŸŸå’Œåˆ—åˆ†éš”çš„ç²¾ç¡®æ§åˆ¶
- è¾“å‡ºPandas DataFrameï¼Œä¾¿äºåç»­å¤„ç†

## 3. å¤šæ¨¡æ€æ–‡æ¡£ç†è§£

### 3.1 Layout-awareæ–¹æ³•ï¼ˆLayoutLMç³»åˆ—ï¼‰
LayoutLMï¼ˆHuang et al., arXiv:2204.08387ï¼‰å°†æ–‡æœ¬ã€è§†è§‰ã€å¸ƒå±€ä¿¡æ¯èåˆï¼Œå®ç°æ–‡æ¡£ç†è§£ï¼š

```python
from transformers import LayoutLMv3Processor, LayoutLMv3ForTokenClassification
from PIL import Image

class LayoutLMParser:
    def __init__(self):
        self.processor = LayoutLMv3Processor.from_pretrained(
            "microsoft/layoutlmv3-base"
        )
        self.model = LayoutLMv3ForTokenClassification.from_pretrained(
            "microsoft/layoutlmv3-base"
        )
    
    def parse_document(self, image, words, boxes):
        # words: OCRæå–çš„æ–‡æœ¬åˆ—è¡¨
        # boxes: å¯¹åº”çš„è¾¹ç•Œæ¡†åæ ‡
        
        # é¢„å¤„ç†
        encoding = self.processor(
            image,
            words,
            boxes=boxes,
            return_tensors="pt",
            truncation=True,
            padding="max_length"
        )
        
        # æ¨ç†
        outputs = self.model(**encoding)
        predictions = outputs.logits.argmax(-1).squeeze().tolist()
        
        # è§£æç»“æœ
        labels = self.model.config.id2label
        tokens = self.processor.tokenizer.convert_ids_to_tokens(
            encoding["input_ids"].squeeze().tolist()
        )
        
        # é‡æ„æ–‡æ¡£ç»“æ„
        structured_content = self.reconstruct_structure(
            tokens, predictions, labels
        )
        
        return structured_content
```

**LayoutLMä¼˜åŠ¿**ï¼š
- å¤šæ¨¡æ€ä¿¡æ¯èåˆï¼ˆæ–‡æœ¬+è§†è§‰+å¸ƒå±€ï¼‰
- æ”¯æŒæ–‡æ¡£å®ä½“æŠ½å–ã€åˆ†ç±»ç­‰ä¸‹æ¸¸ä»»åŠ¡
- å¯¹å¤æ‚æ–‡æ¡£å¸ƒå±€æœ‰è¾ƒå¼ºç†è§£èƒ½åŠ›

### 3.2 Vision-basedæ–¹æ³•ï¼ˆGPT-4V/GPT-4Oï¼‰
ç›´æ¥ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹ç†è§£æ–‡æ¡£å›¾åƒï¼š

```python
import base64
from openai import OpenAI

class VisionBasedParser:
    def __init__(self, api_key):
        self.client = OpenAI(api_key=api_key)
    
    def parse_document_image(self, image_path):
        # ç¼–ç å›¾åƒ
        with open(image_path, "rb") as image_file:
            base64_image = base64.b64encode(image_file.read()).decode('utf-8')
        
        # æ„é€ Prompt
        prompt = """
        è¯·åˆ†æè¿™ä¸ªæ–‡æ¡£å›¾åƒï¼Œæå–ä»¥ä¸‹ä¿¡æ¯ï¼š
        1. æ–‡æ¡£ç±»å‹å’Œæ ‡é¢˜
        2. ä¸»è¦æ–‡æœ¬å†…å®¹ï¼Œä¿æŒåŸæœ‰ç»“æ„
        3. è¡¨æ ¼å†…å®¹ï¼Œè½¬æ¢ä¸ºmarkdownæ ¼å¼
        4. å›¾è¡¨æè¿°å’Œå…³é”®ä¿¡æ¯
        5. æ–‡æ¡£çš„å±‚æ¬¡ç»“æ„
        
        ä»¥JSONæ ¼å¼è¿”å›ï¼ŒåŒ…å«content_type, title, text, tables, figures, structureå­—æ®µã€‚
        """
        
        response = self.client.chat.completions.create(
            model="gpt-4-vision-preview",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        }
                    ]
                }
            ],
            max_tokens=4000
        )
        
        return response.choices[0].message.content
    
    def batch_parse_pdf(self, pdf_path):
        # å°†PDFè½¬æ¢ä¸ºå›¾åƒåºåˆ—
        images = self.pdf_to_images(pdf_path)
        
        results = []
        for i, image in enumerate(images):
            try:
                parsed_content = self.parse_document_image(image)
                results.append({
                    'page': i + 1,
                    'content': parsed_content
                })
            except Exception as e:
                print(f"Error parsing page {i+1}: {e}")
        
        return results
```

**Vision-basedæ–¹æ³•ç‰¹ç‚¹**ï¼š
- æ— éœ€é¢„å¤„ç†ï¼Œç›´æ¥ç†è§£æ–‡æ¡£å›¾åƒ
- æ”¯æŒå¤æ‚å¸ƒå±€å’Œå¤šè¯­è¨€æ–‡æ¡£
- å¯ä»¥å¤„ç†æ‰‹å†™ã€å›¾è¡¨ã€å…¬å¼ç­‰å¤æ‚å…ƒç´ 
- ä¾èµ–APIæœåŠ¡ï¼Œæˆæœ¬ç›¸å¯¹è¾ƒé«˜

## 4. OCRæŠ€æœ¯æ–¹æ¡ˆ

### 4.1 PaddleOCRï¼ˆç™¾åº¦å¼€æºï¼‰
PaddleOCRæä¾›é«˜ç²¾åº¦çš„ä¸­è‹±æ–‡OCRèƒ½åŠ›ï¼š

```python
from paddleocr import PaddleOCR

class PaddleOCRParser:
    def __init__(self):
        # åˆå§‹åŒ–OCRå¼•æ“
        self.ocr = PaddleOCR(
            use_angle_cls=True,  # å¯ç”¨æ–‡å­—æ–¹å‘åˆ†ç±»
            lang="ch",           # æ”¯æŒä¸­è‹±æ–‡
            use_gpu=True         # ä½¿ç”¨GPUåŠ é€Ÿ
        )
    
    def extract_text_from_image(self, image_path):
        result = self.ocr.ocr(image_path, cls=True)
        
        # è§£æç»“æœ
        texts = []
        for line in result:
            for word in line:
                bbox, (text, confidence) = word
                if confidence > 0.8:  # ç½®ä¿¡åº¦è¿‡æ»¤
                    texts.append({
                        'text': text,
                        'bbox': bbox,
                        'confidence': confidence
                    })
        
        return texts
    
    def ocr_with_layout_analysis(self, image_path):
        # ç»“åˆç‰ˆé¢åˆ†æ
        from paddleocr import PPStructure
        
        table_engine = PPStructure(
            show_log=True,
            image_orientation=True  # å¯ç”¨å›¾åƒæ–¹å‘æ£€æµ‹
        )
        
        result = table_engine(image_path)
        
        # åˆ†ç±»å¤„ç†ä¸åŒå…ƒç´ 
        tables = []
        texts = []
        
        for region in result:
            if region['type'] == 'table':
                # è¡¨æ ¼åŒºåŸŸä½¿ç”¨è¡¨æ ¼è¯†åˆ«
                tables.append(region['res'])
            elif region['type'] == 'text':
                # æ–‡æœ¬åŒºåŸŸä½¿ç”¨OCR
                texts.extend(region['res'])
        
        return {'tables': tables, 'texts': texts}
```

### 4.2 Tesseractï¼ˆGoogleå¼€æºï¼‰
Tesseractæ˜¯ç»å…¸çš„å¼€æºOCRå¼•æ“ï¼Œæ”¯æŒ100+è¯­è¨€ï¼š

```python
import pytesseract
from PIL import Image
import cv2
import numpy as np

class TesseractParser:
    def __init__(self):
        # é…ç½®Tesseractè·¯å¾„ï¼ˆWindowsï¼‰
        pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe'
    
    def preprocess_image(self, image_path):
        # å›¾åƒé¢„å¤„ç†æå‡OCRæ•ˆæœ
        image = cv2.imread(image_path)
        
        # ç°åº¦åŒ–
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        
        # é™å™ª
        denoised = cv2.fastNlMeansDenoising(gray)
        
        # äºŒå€¼åŒ–
        _, binary = cv2.threshold(denoised, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)
        
        return binary
    
    def extract_text_with_confidence(self, image_path):
        # é¢„å¤„ç†å›¾åƒ
        processed_image = self.preprocess_image(image_path)
        
        # è·å–è¯¦ç»†è¯†åˆ«ç»“æœ
        data = pytesseract.image_to_data(
            processed_image,
            output_type=pytesseract.Output.DICT,
            config='--psm 6'  # é¡µé¢åˆ†å‰²æ¨¡å¼
        )
        
        # è¿‡æ»¤ä½ç½®ä¿¡åº¦ç»“æœ
        texts = []
        n_boxes = len(data['level'])
        
        for i in range(n_boxes):
            if int(data['conf'][i]) > 60:  # ç½®ä¿¡åº¦é˜ˆå€¼
                text = data['text'][i].strip()
                if text:
                    texts.append({
                        'text': text,
                        'bbox': (
                            data['left'][i],
                            data['top'][i],
                            data['width'][i],
                            data['height'][i]
                        ),
                        'confidence': data['conf'][i]
                    })
        
        return texts
```

### 4.3 å•†ç”¨OCR APIå¯¹æ¯”

| æœåŠ¡å•† | ç²¾åº¦ | é€Ÿåº¦ | è¯­è¨€æ”¯æŒ | ä»·æ ¼ | ç‰¹è‰²åŠŸèƒ½ |
|--------|------|------|----------|------|----------|
| ç™¾åº¦OCR | 95%+ | å¿« | ä¸­è‹±æ–‡ä¼˜ç§€ | ä½ | è¡¨æ ¼è¯†åˆ«ã€ç‰ˆé¢åˆ†æ |
| è…¾è®¯OCR | 95%+ | å¿« | å¤šè¯­è¨€å‡è¡¡ | ä¸­ | èº«ä»½è¯ã€ç¥¨æ®è¯†åˆ« |
| é˜¿é‡ŒOCR | 94%+ | ä¸­ | ä¸­è‹±æ–‡ | ä¸­ | ç”µå•†åœºæ™¯ä¼˜åŒ– |
| Azure OCR | 96%+ | å¿« | 100+è¯­è¨€ | é«˜ | æ‰‹å†™æ–‡å­—è¯†åˆ« |
| Google OCR | 97%+ | ä¸­ | å…¨çƒè¯­è¨€ | é«˜ | æ–‡æ¡£å¸ƒå±€åˆ†æ |

## 5. è§£æè´¨é‡å¯¹RAGæ•ˆæœçš„å½±å“

### 5.1 å½±å“ç»´åº¦åˆ†æ

```python
# æ–‡æ¡£è§£æè´¨é‡è¯„ä¼°æ¡†æ¶
class DocumentParsingQualityAssessment:
    
    def evaluate_text_extraction(self, original_text, extracted_text):
        """æ–‡æœ¬æå–è´¨é‡è¯„ä¼°"""
        # å­—ç¬¦çº§åˆ«ç›¸ä¼¼åº¦
        char_similarity = self.char_level_similarity(original_text, extracted_text)
        
        # è¯­ä¹‰ç›¸ä¼¼åº¦
        semantic_similarity = self.semantic_similarity(original_text, extracted_text)
        
        # ç»“æ„ä¿æŒåº¦
        structure_preservation = self.structure_preservation_score(original_text, extracted_text)
        
        return {
            'char_similarity': char_similarity,
            'semantic_similarity': semantic_similarity,
            'structure_preservation': structure_preservation,
            'overall_score': (char_similarity + semantic_similarity + structure_preservation) / 3
        }
    
    def evaluate_table_extraction(self, original_table, extracted_table):
        """è¡¨æ ¼æå–è´¨é‡è¯„ä¼°"""
        # å•å…ƒæ ¼å‡†ç¡®ç‡
        cell_accuracy = self.calculate_cell_accuracy(original_table, extracted_table)
        
        # ç»“æ„å®Œæ•´æ€§
        structure_integrity = self.table_structure_score(original_table, extracted_table)
        
        # æ•°æ®ç±»å‹ä¿æŒ
        datatype_preservation = self.datatype_preservation_score(original_table, extracted_table)
        
        return {
            'cell_accuracy': cell_accuracy,
            'structure_integrity': structure_integrity,
            'datatype_preservation': datatype_preservation
        }
    
    def assess_rag_impact(self, parsing_quality, retrieval_results):
        """è¯„ä¼°è§£æè´¨é‡å¯¹RAGçš„å½±å“"""
        # æ£€ç´¢å‡†ç¡®æ€§
        retrieval_accuracy = self.calculate_retrieval_accuracy(retrieval_results)
        
        # å›ç­”è´¨é‡
        answer_quality = self.evaluate_answer_quality(retrieval_results)
        
        # è´¨é‡å…³è”åˆ†æ
        correlation = self.calculate_correlation(parsing_quality, [retrieval_accuracy, answer_quality])
        
        return {
            'retrieval_accuracy': retrieval_accuracy,
            'answer_quality': answer_quality,
            'quality_correlation': correlation
        }
```

### 5.2 ä¼˜åŒ–ç­–ç•¥

1. **å¤šå¼•æ“èåˆ**ï¼š
```python
def hybrid_parsing_strategy(document_path):
    # åŸºç¡€è§£æ
    pypdf_result = parse_with_pypdf(document_path)
    
    # ç»“æ„åŒ–è§£æ
    unstructured_result = parse_with_unstructured(document_path)
    
    # è§†è§‰è§£æï¼ˆå¤æ‚å¸ƒå±€ï¼‰
    vision_result = parse_with_gpt4v(document_path)
    
    # ç»“æœèåˆ
    final_result = merge_parsing_results([
        pypdf_result,
        unstructured_result,
        vision_result
    ])
    
    return final_result
```

2. **è´¨é‡æ£€æµ‹ä¸åå¤„ç†**ï¼š
```python
def quality_control_pipeline(parsed_content):
    # æ–‡æœ¬è´¨é‡æ£€æµ‹
    text_quality = assess_text_quality(parsed_content['text'])
    
    # è¡¨æ ¼è´¨é‡æ£€æµ‹
    table_quality = assess_table_quality(parsed_content['tables'])
    
    # ä½è´¨é‡å†…å®¹é‡æ–°è§£æ
    if text_quality < 0.8:
        parsed_content['text'] = reparse_text_with_ocr(parsed_content)
    
    if table_quality < 0.7:
        parsed_content['tables'] = reparse_tables_with_vision(parsed_content)
    
    return parsed_content
```

## é¢è¯•å¸¸è§é—®é¢˜

### Q1: åœ¨æ„å»ºRAGç³»ç»Ÿæ—¶ï¼Œå¦‚ä½•é€‰æ‹©åˆé€‚çš„PDFè§£æå·¥å…·ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
é€‰æ‹©ç­–ç•¥åº”è¯¥åŸºäºæ–‡æ¡£ç‰¹å¾å’Œåº”ç”¨éœ€æ±‚ï¼š

1. **æ–‡æ¡£ç±»å‹åˆ†æ**ï¼š
   - ç®€å•æ–‡æœ¬PDF â†’ PyPDF
   - å«è¡¨æ ¼çš„ç»“æ„åŒ–æ–‡æ¡£ â†’ PDFPlumberæˆ–Unstructured
   - æ‰«æä»¶/å›¾åƒPDF â†’ OCRæ–¹æ¡ˆ
   - å¤æ‚å¸ƒå±€æ–‡æ¡£ â†’ Unstructuredæˆ–è§†è§‰æ¨¡å‹

2. **æ€§èƒ½è€ƒè™‘**ï¼š
   - å¤„ç†é€Ÿåº¦è¦æ±‚ï¼šPyPDF > PDFPlumber > Unstructured > Vision-based
   - å‡†ç¡®ç‡è¦æ±‚ï¼šVision-based > Unstructured > PDFPlumber > PyPDF
   - æˆæœ¬è€ƒè™‘ï¼šå¼€æºæ–¹æ¡ˆ > å•†ç”¨API

3. **å®é™…ç­–ç•¥**ï¼š
```python
def choose_parsing_strategy(pdf_path):
    # æ–‡æ¡£ç‰¹å¾æ£€æµ‹
    features = analyze_pdf_features(pdf_path)
    
    if features['is_scanned']:
        return 'ocr_based'
    elif features['table_count'] > 5:
        return 'unstructured_hi_res'
    elif features['complex_layout']:
        return 'vision_based'
    else:
        return 'pdfplumber'
```

### Q2: è¡¨æ ¼æå–çš„å‡†ç¡®ç‡å¦‚ä½•è¯„ä¼°ï¼Ÿå¸¸è§é—®é¢˜å¦‚ä½•è§£å†³ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**è¯„ä¼°æŒ‡æ ‡**ï¼š
1. **å•å…ƒæ ¼å‡†ç¡®ç‡**ï¼šæ­£ç¡®è¯†åˆ«çš„å•å…ƒæ ¼æ•°é‡/æ€»å•å…ƒæ ¼æ•°é‡
2. **ç»“æ„å®Œæ•´æ€§**ï¼šè¡Œåˆ—å…³ç³»æ˜¯å¦æ­£ç¡®ä¿æŒ
3. **å†…å®¹å‡†ç¡®æ€§**ï¼šæ–‡æœ¬è¯†åˆ«å‡†ç¡®ç‡
4. **è¾¹æ¡†æ£€æµ‹**ï¼šè¡¨æ ¼è¾¹ç•Œè¯†åˆ«ç²¾åº¦

**å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ**ï¼š

1. **åˆå¹¶å•å…ƒæ ¼å¤„ç†**ï¼š
```python
def handle_merged_cells(table_data):
    # æ£€æµ‹åˆå¹¶å•å…ƒæ ¼æ¨¡å¼
    merged_patterns = detect_merged_cell_patterns(table_data)
    
    # é‡æ„è¡¨æ ¼ç»“æ„
    restructured_table = reconstruct_table_structure(table_data, merged_patterns)
    
    return restructured_table
```

2. **è·¨é¡µè¡¨æ ¼**ï¼š
```python
def merge_cross_page_tables(page_tables):
    # æ£€æµ‹è¡¨æ ¼è¿ç»­æ€§
    continuous_tables = detect_table_continuity(page_tables)
    
    # åˆå¹¶è¿ç»­è¡¨æ ¼
    merged_table = merge_continuous_tables(continuous_tables)
    
    return merged_table
```

3. **æ— è¾¹æ¡†è¡¨æ ¼**ï¼š
   - ä½¿ç”¨åŸºäºç©ºç™½åˆ†å‰²çš„ç®—æ³•
   - é‡‡ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹æ£€æµ‹éšå¼è¾¹ç•Œ
   - ç»“åˆä¸Šä¸‹æ–‡ä¿¡æ¯æ¨æ–­è¡¨æ ¼ç»“æ„

### Q3: OCRåœ¨æ–‡æ¡£è§£æä¸­çš„å±€é™æ€§æœ‰å“ªäº›ï¼Ÿå¦‚ä½•ä¼˜åŒ–ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**ä¸»è¦å±€é™æ€§**ï¼š
1. **å›¾åƒè´¨é‡æ•æ„Ÿ**ï¼šæ¨¡ç³Šã€å€¾æ–œã€å™ªå£°å½±å“è¯†åˆ«ç‡
2. **å¤æ‚å¸ƒå±€å¤„ç†**ï¼šå¤šæ ã€è¡¨æ ¼ã€å…¬å¼ç­‰ç»“æ„è¯†åˆ«å›°éš¾
3. **è¯­è¨€å’Œå­—ä½“é™åˆ¶**ï¼šç‰¹æ®Šå­—ä½“ã€æ‰‹å†™æ–‡å­—è¯†åˆ«å‡†ç¡®ç‡ä½
4. **è¯­ä¹‰ç†è§£ç¼ºå¤±**ï¼šåªèƒ½è¯†åˆ«å­—ç¬¦ï¼Œæ— æ³•ç†è§£è¯­ä¹‰å…³ç³»

**ä¼˜åŒ–ç­–ç•¥**ï¼š

1. **å›¾åƒé¢„å¤„ç†**ï¼š
```python
def advanced_image_preprocessing(image):
    # å€¾æ–œæ ¡æ­£
    corrected_image = deskew_image(image)
    
    # å™ªå£°å»é™¤
    denoised_image = remove_noise(corrected_image)
    
    # å¯¹æ¯”åº¦å¢å¼º
    enhanced_image = enhance_contrast(denoised_image)
    
    # åˆ†è¾¨ç‡ä¼˜åŒ–
    upscaled_image = upscale_resolution(enhanced_image)
    
    return upscaled_image
```

2. **å¤šå¼•æ“èåˆ**ï¼š
```python
def multi_engine_ocr(image):
    # ä½¿ç”¨å¤šä¸ªOCRå¼•æ“
    results = {
        'tesseract': tesseract_ocr(image),
        'paddleocr': paddleocr_ocr(image),
        'azure': azure_ocr_api(image)
    }
    
    # ç»“æœèåˆå’Œç½®ä¿¡åº¦åŠ æƒ
    final_result = weighted_ensemble(results)
    
    return final_result
```

3. **åå¤„ç†ä¼˜åŒ–**ï¼š
   - æ‹¼å†™æ£€æŸ¥å’Œçº é”™
   - ä¸Šä¸‹æ–‡è¯­ä¹‰éªŒè¯
   - é¢†åŸŸè¯å…¸è¾…åŠ©

### Q4: å¤šæ¨¡æ€æ–‡æ¡£ç†è§£ç›¸æ¯”ä¼ ç»Ÿæ–¹æ³•æœ‰å“ªäº›ä¼˜åŠ¿ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**ä¼ ç»Ÿæ–¹æ³•vså¤šæ¨¡æ€æ–¹æ³•**ï¼š

1. **ä¿¡æ¯åˆ©ç”¨ç»´åº¦**ï¼š
   - ä¼ ç»Ÿï¼šä»…æ–‡æœ¬å†…å®¹
   - å¤šæ¨¡æ€ï¼šæ–‡æœ¬+è§†è§‰+å¸ƒå±€+ç»“æ„

2. **å¤„ç†å¤æ‚åº¦**ï¼š
```python
# ä¼ ç»Ÿæ–¹æ³•
def traditional_parsing(pdf):
    text = extract_text(pdf)  # ä»…æ–‡æœ¬
    return clean_text(text)

# å¤šæ¨¡æ€æ–¹æ³•
def multimodal_parsing(pdf):
    # å¤šç»´åº¦ä¿¡æ¯æå–
    text = extract_text(pdf)
    layout = detect_layout(pdf)
    visual_elements = extract_visual_elements(pdf)
    
    # èåˆç†è§£
    structured_content = multimodal_fusion(text, layout, visual_elements)
    
    return structured_content
```

3. **åº”ç”¨ä¼˜åŠ¿**ï¼š
   - **å¸ƒå±€ç†è§£**ï¼šèƒ½è¯†åˆ«æ ‡é¢˜ã€æ®µè½ã€è¡¨æ ¼ç­‰è¯­ä¹‰è§’è‰²
   - **è§†è§‰å…³è”**ï¼šç†è§£å›¾æ–‡å…³ç³»ï¼Œå¤„ç†å›¾è¡¨å’Œå…¬å¼
   - **ç»“æ„æ¨ç†**ï¼šåŸºäºç©ºé—´å…³ç³»æ¨æ–­æ–‡æ¡£é€»è¾‘ç»“æ„
   - **è¯­ä¹‰å¢å¼º**ï¼šç»“åˆè§†è§‰çº¿ç´¢æå‡æ–‡æœ¬ç†è§£å‡†ç¡®æ€§

4. **å±€é™æ€§**ï¼š
   - è®¡ç®—å¤æ‚åº¦é«˜
   - æ¨¡å‹å‚æ•°é‡å¤§ï¼Œéƒ¨ç½²è¦æ±‚é«˜
   - è®­ç»ƒæ•°æ®éœ€æ±‚å¤§
   - æ¨ç†é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢

### Q5: åœ¨ä¼ä¸šçº§RAGç³»ç»Ÿä¸­ï¼Œå¦‚ä½•è®¾è®¡æ–‡æ¡£è§£æçš„è´¨é‡ä¿è¯æœºåˆ¶ï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**è´¨é‡ä¿è¯ä½“ç³»è®¾è®¡**ï¼š

1. **å¤šå±‚æ¬¡éªŒè¯**ï¼š
```python
class DocumentParsingQualityAssurance:
    def __init__(self):
        self.quality_thresholds = {
            'text_accuracy': 0.95,
            'table_accuracy': 0.90,
            'layout_preservation': 0.85
        }
    
    def quality_gate_check(self, parsed_content):
        # è‡ªåŠ¨è´¨é‡æ£€æµ‹
        quality_scores = self.assess_quality(parsed_content)
        
        # è´¨é‡é—¨æ£€æŸ¥
        for metric, score in quality_scores.items():
            if score < self.quality_thresholds[metric]:
                # è§¦å‘é‡æ–°è§£ææˆ–äººå·¥å®¡æ ¸
                return self.trigger_quality_remediation(parsed_content, metric)
        
        return parsed_content
```

2. **ç›‘æ§å’Œå‘Šè­¦**ï¼š
```python
def setup_quality_monitoring():
    # å®æ—¶è´¨é‡ç›‘æ§
    quality_monitor = QualityMonitor()
    
    # è®¾ç½®å‘Šè­¦é˜ˆå€¼
    quality_monitor.set_alerts({
        'parsing_failure_rate': 0.05,      # 5%å¤±è´¥ç‡å‘Šè­¦
        'avg_quality_score': 0.85,         # å¹³å‡è´¨é‡åˆ†å‘Šè­¦
        'processing_time': 30              # 30ç§’å¤„ç†æ—¶é—´å‘Šè­¦
    })
    
    # è´¨é‡è¶‹åŠ¿åˆ†æ
    quality_monitor.enable_trend_analysis()
```

3. **äººå·¥å®¡æ ¸æµç¨‹**ï¼š
   - æŠ½æ ·å®¡æ ¸æœºåˆ¶
   - å¼‚å¸¸æ¡ˆä¾‹æ ‡æ³¨
   - æŒç»­æ”¹è¿›åé¦ˆ
   - è´¨é‡åŸºå‡†æ›´æ–°

4. **é”™è¯¯å¤„ç†ç­–ç•¥**ï¼š
   - è‡ªåŠ¨é‡è¯•æœºåˆ¶
   - é™çº§è§£æç­–ç•¥
   - ç”¨æˆ·åé¦ˆé›†æˆ
   - è´¨é‡æ•°æ®ç§¯ç´¯

---

## ğŸ“š æ¨èé˜…è¯»

### å·¥å…·æ–‡æ¡£
- [Unstructured.io Documentation](https://docs.unstructured.io/) â€” æœ€å…¨é¢çš„å¤šæ ¼å¼æ–‡æ¡£è§£ææ¡†æ¶ â­â­â­â­â­
- [PyMuPDF (fitz) Documentation](https://pymupdf.readthedocs.io/) â€” é«˜æ€§èƒ½ PDF å¤„ç†åº“
- [Marker: Convert PDF to Markdown](https://github.com/VikParuchuri/marker) â€” PDFâ†’Markdown è½¬æ¢ï¼Œä¿ç•™å¸ƒå±€å’Œè¡¨æ ¼ â­â­â­â­

### åŸå§‹è®ºæ–‡
- [LayoutLMv3: Pre-training for Document AI](https://arxiv.org/abs/2204.08387) â€” Microsoft çš„å¤šæ¨¡æ€æ–‡æ¡£ç†è§£æ¨¡å‹
- [Table Transformer: DETR for Table Detection](https://arxiv.org/abs/2110.00061) â€” Microsoft çš„è¡¨æ ¼æ£€æµ‹/ç»“æ„è¯†åˆ«

### å®è·µèµ„æº
- [IBM Docling](https://github.com/DS4SD/docling) â€” ä¼ä¸šçº§æ–‡æ¡£è§£ææ¡†æ¶
- [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) â€” ç™¾åº¦å¼€æº OCRï¼Œä¸­æ–‡è¯†åˆ«æ•ˆæœæœ€å¥½

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **ä¼ä¸šæ–‡æ¡£çŸ¥è¯†åº“**ï¼šå°†å†…éƒ¨ PDF/Word è§£æåå»ºç´¢å¼•ï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€é—®ç­”
- **é‡‘èç ”æŠ¥è§£æ**ï¼šPDF ä¸­çš„è¡¨æ ¼ã€å›¾è¡¨éœ€è¦ Unstructured hi_res æˆ– Vision LLM è§£æ
- **æ‰«æä»¶æ•°å­—åŒ–**ï¼šPaddleOCR + ç‰ˆé¢åˆ†æï¼Œå¤„ç†æ‰‹å†™/å°åˆ·æ··åˆæ–‡æ¡£

### å·¥ç¨‹å®ç°è¦ç‚¹
- **è§£æç­–ç•¥é€‰å‹**ï¼šç®€å•æ–‡æœ¬ PDF â†’ PyPDFï¼›å«è¡¨æ ¼ â†’ PDFPlumber/Unstructuredï¼›æ‰«æä»¶ â†’ OCRï¼›å¤æ‚å¸ƒå±€ â†’ Vision LLM
- **è´¨é‡çº¢çº¿**ï¼šå¯¹é‡è¦æ–‡æ¡£ï¼ˆåˆåŒã€æŠ¥å‘Šï¼‰ç”¨ hi_res æ¨¡å¼ + äººå·¥æŠ½æ£€ï¼›å¤§æ‰¹é‡æ™®é€šæ–‡æ¡£ç”¨ fast æ¨¡å¼
- **è¡¨æ ¼å¤„ç†**ï¼šVision LLM è§£æ â†’ è½¬ Markdown Table â†’ ä½œä¸ºç‹¬ç«‹ chunk å­˜å‚¨ï¼Œæ˜¯å½“å‰æœ€ä¼˜æ–¹æ¡ˆ

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: RAG ä¸­æ–‡æ¡£è§£æçš„æœ€å¤§æŒ‘æˆ˜ï¼Ÿ
  A: PDF è¡¨æ ¼å’Œå¤æ‚å¸ƒå±€â€”â€”OCR ç±»å·¥å…·é€Ÿåº¦å¿«ä½†è´¨é‡å·®ï¼ŒVision LLM è´¨é‡å¥½ä½†æˆæœ¬é«˜ï¼›ç”Ÿäº§ä¸­æŒ‰æ–‡æ¡£é‡è¦æ€§åˆ†çº§å¤„ç†

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- æ–‡æ¡£è§£æè´¨é‡æ˜¯ RAG çš„å¤©èŠ±æ¿â€”â€”æ£€ç´¢å’Œç”Ÿæˆå†å¥½ï¼Œè§£æåƒåœ¾è¿›å»ä¹Ÿæ˜¯åƒåœ¾å‡ºæ¥
- 2025-2026 Vision LLMï¼ˆGPT-4o/Claudeï¼‰æ­£åœ¨é¢ è¦†ä¼ ç»Ÿ OCR æ–¹æ¡ˆï¼Œä½†æˆæœ¬ä»æ˜¯ç“¶é¢ˆ

### æœªè§£é—®é¢˜ä¸å±€é™
- è·¨é¡µè¡¨æ ¼åˆå¹¶ä»æ˜¯éš¾é¢˜ï¼Œå°¤å…¶æ˜¯æ— è¾¹æ¡†è¡¨æ ¼
- Vision LLM è§£ææˆæœ¬é«˜â€”â€”1000 é¡µ PDF ç”¨ GPT-4V å¯èƒ½èŠ±è´¹ $50+
- æ‰‹å†™æ–‡å­— + å¤æ‚å…¬å¼çš„ OCR å‡†ç¡®ç‡ä»ä¸å¤Ÿ

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- ç»“åˆ [[AI/6-åº”ç”¨/RAG/æ£€ç´¢ç­–ç•¥|]] çš„ Semantic Chunkingï¼Œåœ¨è§£æåæŒ‰è¯­ä¹‰åˆ‡åˆ†è€Œéå›ºå®šé•¿åº¦
- å¤šå¼•æ“èåˆï¼ˆPyPDF å¿«é€Ÿè§£æ + Vision LLM å…œåº•å¤æ‚é¡µé¢ï¼‰æ˜¯ç”Ÿäº§ç¯å¢ƒçš„æœ€ä¼˜ç­–ç•¥
- Marker é¡¹ç›®æ­£åœ¨æˆä¸º PDFâ†’Markdown çš„äº‹å®æ ‡å‡†ï¼Œå€¼å¾—æŒç»­å…³æ³¨

> ğŸ”— See also: [[AI/6-åº”ç”¨/RAG/RAG åŸç†ä¸æ¶æ„]] â€” æ–‡æ¡£è§£æåœ¨ RAG ç®¡çº¿ä¸­çš„ä½ç½®
> ğŸ”— See also: [[AI/6-åº”ç”¨/Embedding/Embedding ä¸å‘é‡æ£€ç´¢]] â€” è§£æåçš„ chunk å¦‚ä½•å‘é‡åŒ–å’Œæ£€ç´¢