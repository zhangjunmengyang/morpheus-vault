---
brief: "计算机视觉基础与前沿 2026 技术全景——CNN→ViT→MAE→CLIP 的视觉模型演化；目标检测/分割/生成/多模态的完整 CV 知识图谱；面试标注，视觉方向面试的知识体系速查手册。"
title: "计算机视觉基础与前沿 - 2026 技术全景"
date: 2026-02-21
tags: [面试, CV, 计算机视觉, 深度学习, 多模态, 生成模型]
domain: ai/cv
status: active
rating: ★★★★★
archived_by: librarian
archived_date: 2026-02-21
---

# 计算机视觉基础与前沿 — 2026 技术全景

> 面向 AI 算法工程师面试的深度武器笔记。目标：碾压面试官。
> 覆盖 CV 全栈：经典架构 → 目标检测 → 语义分割 → 视觉 Transformer → 多模态视觉 → 视频理解 → 3D 视觉 → 生成模型 → 自监督学习 → 2026 前沿

---

## 目录

1. [经典 CNN 架构](#一经典-cnn-架构)
2. [目标检测](#二目标检测)
3. [语义分割](#三语义分割)
4. [视觉 Transformer](#四视觉-transformer)
5. [多模态视觉](#五多模态视觉)
6. [视频理解](#六视频理解)
7. [3D 视觉](#七3d-视觉)
8. [生成模型](#八生成模型)
9. [自监督视觉学习](#九自监督视觉学习)
10. [2026 前沿：视觉 Agent 与具身智能](#十2026-前沿视觉-agent-与具身智能)
11. [面试题精选（12+ 道）](#十一面试题精选)
12. [附录：CV 面试必背清单](#附录cv-面试必背清单)
13. [参考文献](#参考文献)

---

## 一、经典 CNN 架构

### 1.1 核心原理

**卷积操作本质**：局部感受野 + 权重共享 + 平移等变性。

标准卷积的输出特征图计算：

$$
y[i,j] = \sum_{m}\sum_{n} x[i+m, j+n] \cdot w[m,n] + b
$$

**输出尺寸公式**：

$$
O = \left\lfloor \frac{I - K + 2P}{S} \right\rfloor + 1
$$

其中 $I$=输入尺寸，$K$=卷积核大小，$P$=padding，$S$=stride。

**参数量与计算量**：
- 参数量：$C_{in} \times C_{out} \times K^2 + C_{out}$（含 bias）
- FLOPs：$2 \times C_{in} \times C_{out} \times K^2 \times H_{out} \times W_{out}$

### 1.2 关键架构演进

| 架构 | 年份 | 核心创新 | ImageNet Top-5 |
|------|------|----------|----------------|
| AlexNet | 2012 | ReLU + Dropout + GPU 训练 | 16.4% |
| VGGNet | 2014 | 3×3 堆叠代替大卷积核 | 7.3% |
| GoogLeNet | 2014 | Inception 多尺度并行 | 6.7% |
| ResNet | 2015 | 残差连接 | 3.57% |
| DenseNet | 2017 | Dense connection + Feature reuse | 3.46% |
| EfficientNet | 2019 | Compound scaling | 2.9% |
| ConvNeXt | 2022 | 用 Transformer 设计理念改造 CNN | 竞争 ViT |

### 1.3 ResNet 深度解析

**残差学习的数学表达**：

$$
\mathbf{y} = \mathcal{F}(\mathbf{x}, \{W_i\}) + \mathbf{x}
$$

**为什么残差连接有效？**

1. **梯度流动**：反向传播时 $\frac{\partial \mathcal{L}}{\partial \mathbf{x}} = \frac{\partial \mathcal{L}}{\partial \mathbf{y}} \cdot (1 + \frac{\partial \mathcal{F}}{\partial \mathbf{x}})$，恒等项 1 保证梯度不消失
2. **Ensemble 视角**：ResNet 隐式地是指数级子网络的集成（Veit et al., 2016）
3. **Loss landscape 平滑化**：残差连接使 loss surface 更加光滑（Li et al., 2018）

**Bottleneck 结构**：1×1（降维）→ 3×3（卷积）→ 1×1（升维），参数量 $\approx \frac{1}{9}$ 直接用 3×3

### 1.4 EfficientNet 核心

**Compound Scaling**：同时缩放深度 $d$、宽度 $w$、分辨率 $r$：

$$
d = \alpha^\phi, \quad w = \beta^\phi, \quad r = \gamma^\phi
$$

约束：$\alpha \cdot \beta^2 \cdot \gamma^2 \approx 2$（FLOPs 约翻倍）

网格搜索找到 $\alpha=1.2, \beta=1.1, \gamma=1.15$。

**MBConv（Mobile Inverted Bottleneck）**：
- Depthwise Separable Convolution：参数量从 $C_{in} \times C_{out} \times K^2$ 降至 $C_{in} \times K^2 + C_{in} \times C_{out}$
- Squeeze-and-Excitation（SE）通道注意力
- Swish 激活函数：$\text{swish}(x) = x \cdot \sigma(x)$

### 1.5 代码：ResNet Bottleneck Block

```python
import torch.nn as nn

class Bottleneck(nn.Module):
    expansion = 4
    
    def __init__(self, in_ch, mid_ch, stride=1, downsample=None):
        super().__init__()
        out_ch = mid_ch * self.expansion
        self.conv1 = nn.Conv2d(in_ch, mid_ch, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_ch)
        self.conv2 = nn.Conv2d(mid_ch, mid_ch, 3, stride=stride, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(mid_ch)
        self.conv3 = nn.Conv2d(mid_ch, out_ch, 1, bias=False)
        self.bn3 = nn.BatchNorm2d(out_ch)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample  # 1x1 conv for dimension matching
    
    def forward(self, x):
        identity = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.relu(self.bn2(self.conv2(out)))
        out = self.bn3(self.conv3(out))
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity  # 残差连接
        return self.relu(out)
```

### 1.6 关键论文

- He et al., "Deep Residual Learning for Image Recognition," CVPR 2016
- Tan & Le, "EfficientNet: Rethinking Model Scaling for CNNs," ICML 2019
- Liu et al., "A ConvNet for the 2020s" (ConvNeXt), CVPR 2022

---

## 二、目标检测

### 2.1 核心概念

**IoU（Intersection over Union）**：

$$
\text{IoU} = \frac{|B_p \cap B_{gt}|}{|B_p \cup B_{gt}|}
$$

**mAP 计算**：
1. 每类按置信度排序，逐阈值计算 Precision-Recall
2. 对 PR 曲线做插值取面积（AP）
3. 所有类 AP 取平均 = mAP

**NMS（Non-Maximum Suppression）**：
1. 按置信度排序
2. 取最高分 box，删除与其 IoU > 阈值的 box
3. 重复直到无剩余

### 2.2 两阶段 vs 单阶段

| 维度 | 两阶段（Faster R-CNN） | 单阶段（YOLO/SSD） |
|------|----------------------|-------------------|
| 流程 | RPN 提 proposal → RoI Pool → 分类回归 | 直接在特征图上密集预测 |
| 精度 | 高（COCO ~42+ mAP） | 中高（追赶中） |
| 速度 | 慢（5-15 FPS） | 快（30-160+ FPS） |
| 适用 | 精度优先场景 | 实时场景 |

### 2.3 YOLO 系列演进

**YOLOv1（2016）**：将检测视为回归问题，输入图像划分为 $S \times S$ 网格，每个网格预测 $B$ 个 bbox + $C$ 类概率。

**YOLO 核心 Loss（v1 简化）**：

$$
\mathcal{L} = \lambda_{coord}\sum_{i}\sum_{j}\mathbb{1}_{ij}^{obj}\left[(x_i-\hat{x}_i)^2+(y_i-\hat{y}_i)^2+(\sqrt{w_i}-\sqrt{\hat{w}_i})^2+(\sqrt{h_i}-\sqrt{\hat{h}_i})^2\right]
$$
$$
+ \sum_{i}\sum_{j}\mathbb{1}_{ij}^{obj}(C_i-\hat{C}_i)^2 + \lambda_{noobj}\sum_{i}\sum_{j}\mathbb{1}_{ij}^{noobj}(C_i-\hat{C}_i)^2 + \sum_{i}\mathbb{1}_{i}^{obj}\sum_{c}(p_i(c)-\hat{p}_i(c))^2
$$

**关键演进线**：

| 版本 | 核心改进 | COCO mAP | 速度 |
|------|----------|----------|------|
| YOLOv3 | Darknet-53 + FPN + 多尺度 | 33.0 | 20ms |
| YOLOv5 | PyTorch 工程化 + Mosaic 增强 | 50.7 | 6.2ms |
| YOLOv8 | Anchor-free + Decoupled head + DFL loss | 53.9 | 3.5ms |
| YOLOv9 | PGI + GELAN 可编程梯度信息 | 55.6 | 5.0ms |
| YOLOv10 | NMS-free + Consistent Dual Assignment | 54.4 | 1.8ms |
| YOLOv11 | C3k2 block + 更轻量 | 54.7 | 2.2ms |
| YOLO-World | Open-vocabulary + CLIP text encoder | Open-set | 52ms |

**YOLOv8 关键改进**：
- **Anchor-free**：直接预测中心点偏移 + 左上右下距离，告别 anchor 超参
- **Decoupled Head**：分类和回归解耦，各自独立优化
- **DFL（Distribution Focal Loss）**：将 bbox 回归建模为离散分布而非 Dirac delta
- **Task-Aligned Assigner**：联合分类+定位质量做正样本分配

### 2.4 DETR：端到端检测

**核心思想**：将检测视为集合预测问题，用 Transformer + 二部图匹配替代 NMS/Anchor。

**架构**：CNN Backbone → Transformer Encoder → Transformer Decoder（N 个 object queries）→ FFN 预测

**二部图匹配（Hungarian Algorithm）**：

$$
\hat{\sigma} = \arg\min_{\sigma \in \mathfrak{S}_N} \sum_{i}^{N} \mathcal{L}_{match}(y_i, \hat{y}_{\sigma(i)})
$$

$$
\mathcal{L}_{match} = -\mathbb{1}_{c_i \neq \varnothing} \hat{p}_{\sigma(i)}(c_i) + \mathbb{1}_{c_i \neq \varnothing} \mathcal{L}_{box}(b_i, \hat{b}_{\sigma(i)})
$$

**DETR 问题与改进**：
- 训练收敛慢（500 epochs vs YOLO ~300）
- 小目标检测弱（缺乏多尺度）
- **Deformable DETR**：可变形注意力，只关注参考点附近 $K$ 个采样点，收敛提速 10×
- **DINO-DETR**（2022）：Denoising + Contrastive Denoising + Mixed Query，COCO 63.3 mAP（SwinL）
- **RT-DETR**（2023）：Real-time DETR，混合编码器 + IoU-aware query selection，T4 上 114 FPS

### 2.5 代码：YOLOv8 推理

```python
from ultralytics import YOLO

model = YOLO("yolov8x.pt")
results = model.predict("image.jpg", conf=0.25, iou=0.45)

for r in results:
    boxes = r.boxes.xyxy.cpu().numpy()    # [N, 4]
    scores = r.boxes.conf.cpu().numpy()   # [N]
    classes = r.boxes.cls.cpu().numpy()    # [N]
    for box, score, cls in zip(boxes, scores, classes):
        print(f"Class {int(cls)}: {score:.2f} @ {box}")
```

### 2.6 关键论文

- Redmon et al., "You Only Look Once," CVPR 2016
- Carion et al., "End-to-End Object Detection with Transformers" (DETR), ECCV 2020
- Zhao et al., "DETRs Beat YOLOs on Real-time Object Detection" (RT-DETR), 2023

---

## 三、语义分割

### 3.1 核心概念

**任务分类**：
- **语义分割**：像素级分类（同类不区分实例）
- **实例分割**：像素级 + 区分实例（Mask R-CNN）
- **全景分割**：语义 + 实例统一（thing + stuff）

**评价指标**：

$$
\text{mIoU} = \frac{1}{C}\sum_{c=1}^{C}\frac{TP_c}{TP_c + FP_c + FN_c}
$$

### 3.2 FCN → UNet → DeepLab

**FCN（2015）**：
- 首次将全连接层替换为 1×1 卷积，实现任意尺寸输入
- 跳跃连接融合多尺度特征

**UNet（2015）**：编码器-解码器 + 对称跳跃连接

```
Encoder:              Decoder:
[572×572] → Conv×2    [388×388] ← UpConv + Concat
[284×284] → Conv×2    [196×196] ← UpConv + Concat
[140×140] → Conv×2    [100×100] ← UpConv + Concat
[68×68]   → Conv×2    [56×56]   ← UpConv + Concat
[32×32]   → Conv×2    → Bottleneck
```

**核心设计**：
- 编码器提取语义特征（下采样路径）
- 解码器恢复空间分辨率（上采样路径）
- Skip connection 保留细节信息，**Concat**（不是 Add）

**DeepLab 系列**：

**ASPP（Atrous Spatial Pyramid Pooling）**：多个空洞卷积并行提取多尺度特征

$$
y[i] = \sum_{k} x[i + r \cdot k] \cdot w[k]
$$

其中 $r$ 为膨胀率。ASPP 用 $r \in \{6, 12, 18\}$ + 1×1 conv + Global Avg Pool。

**DeepLabv3+** = ASPP encoder + 轻量 decoder with low-level feature fusion

### 3.3 SAM（Segment Anything Model）

**2023 年 CV 最重要的工作之一**。

**架构三件套**：
1. **Image Encoder**：MAE 预训练的 ViT-H（仅跑一次）
2. **Prompt Encoder**：处理 point/box/mask/text 提示
3. **Mask Decoder**：轻量 Transformer decoder，生成多个候选 mask + IoU 预测

**关键设计**：
- **Promptable**：点击、框选、文本都能分割
- **Ambiguity-aware**：同时预测 3 个 mask（whole/part/subpart）+ 对应 IoU score
- **SA-1B 数据集**：11M 图像 + 1.1B masks，半自动标注

**SAM 2（2024）**：扩展到视频分割
- 新增 Memory Encoder + Memory Bank + Memory Attention
- Streaming 架构：逐帧处理 + 记忆条件化
- 统一图像和视频分割

**EfficientSAM / MobileSAM / FastSAM**：轻量化变体，将 ViT-H 蒸馏到更小模型

### 3.4 代码：SAM 推理

```python
from segment_anything import sam_model_registry, SamPredictor

sam = sam_model_registry["vit_h"](checkpoint="sam_vit_h.pth")
sam.to("cuda")
predictor = SamPredictor(sam)

predictor.set_image(image)  # 只编码一次
# 点击提示分割
masks, scores, logits = predictor.predict(
    point_coords=np.array([[500, 375]]),
    point_labels=np.array([1]),  # 1=前景, 0=背景
    multimask_output=True,       # 返回 3 个候选
)
best_mask = masks[scores.argmax()]
```

### 3.5 关键论文

- Ronneberger et al., "U-Net: Convolutional Networks for Biomedical Image Segmentation," MICCAI 2015
- Chen et al., "Rethinking Atrous Convolution for Semantic Image Segmentation" (DeepLabv3), 2017
- Kirillov et al., "Segment Anything," ICCV 2023
- Ravi et al., "SAM 2: Segment Anything in Images and Videos," 2024

---

## 四、视觉 Transformer

### 4.1 ViT 核心原理

**核心思想**：将图像切成 patch → 线性 embedding → 标准 Transformer Encoder

**Patch Embedding**：

$$
\mathbf{z}_0 = [\mathbf{x}_{class}; \mathbf{x}_p^1\mathbf{E}; \mathbf{x}_p^2\mathbf{E}; \ldots; \mathbf{x}_p^N\mathbf{E}] + \mathbf{E}_{pos}
$$

- 图像 $224 \times 224$ → $16 \times 16$ patch → $14 \times 14 = 196$ 个 token
- 每个 patch 通过线性投影 $\mathbf{E} \in \mathbb{R}^{(P^2 \cdot C) \times D}$ → $D$ 维 embedding
- [CLS] token 做分类，可学习位置编码

**Self-Attention 计算**：

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V
$$

**ViT 的问题**：
- 需要大数据预训练（JFT-300M），小数据不如 CNN
- 计算量与 token 数平方关系：$O(N^2 \cdot D)$
- 缺乏 CNN 的归纳偏置（locality, translation equivariance）

### 4.2 后续改进架构

| 架构 | 核心改进 | 特点 |
|------|----------|------|
| DeiT | 知识蒸馏 + Distillation token | 仅 ImageNet-1K 训练 |
| Swin Transformer | Shifted Window 局部注意力 | 线性复杂度 $O(N)$，层级特征 |
| PVT | 渐进收缩序列 + SRA | 金字塔结构适配密集预测 |
| CvT | Convolutional Token Embedding | 融合 CNN 局部性 |
| BEiT | BERT-style 视觉预训练 | Masked Image Modeling |
| EVA-02 | MIM + CLIP teacher | 高效大规模视觉预训练 |

**Swin Transformer 关键机制**：

1. **Window Attention**：将特征图分为不重叠窗口（如 7×7），窗口内做 self-attention → $O(W^2 \cdot N)$ vs $O(N^2)$
2. **Shifted Window**：交替使用常规窗口和偏移窗口，实现跨窗口信息交互
3. **Patch Merging**：类似池化，逐层降分辨率，产生层级特征图

### 4.3 DINOv2：视觉通用特征

**DINO（Self-Distillation with No Labels）**：
- Student-Teacher 自蒸馏，**无标签**
- Student 看 local crop，Teacher 看 global crop
- Teacher 用 EMA 更新，centering 防 collapse
- 输出的 attention map 自动发现物体，无需监督

**DINOv2（2023）核心改进**：
- 142M 高质量无标签图像的 curated 数据集（LVD-142M）
- 统一 Self-supervised 目标：iBOT（MIM）+ DINO（self-distillation）+ KoLeo regularizer
- 冻结特征即可在下游任务达到 SOTA（分类/分割/深度估计）
- ViT-g/14：性能超越 CLIP 等有监督预训练

**DINOv2 特征的强大之处**：
- 线性 probe 即可在 ImageNet 达到 86.3%（ViT-g）
- Dense prediction 无需微调：冻结 backbone + 简单 head
- 跨域迁移能力极强

### 4.4 SigLIP

**CLIP 用 Softmax Cross-Entropy Loss**，batch 中构造 $N \times N$ 相似度矩阵做对比学习。

**SigLIP 改进**：用 **Sigmoid Loss** 替代 Softmax：

$$
\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{N}\log\sigma(z_{ij}(-t \cdot (\mathbf{x}_i \cdot \mathbf{y}_j) + b))
$$

其中 $z_{ij} = \begin{cases} 1 & i=j \\ -1 & i \neq j \end{cases}$，$t$ 和 $b$ 是可学习的温度和偏置。

**优势**：
- 去掉全局归一化 → batch 内各 pair 独立 → 支持**超大 batch**分布式训练
- 性能与 CLIP 持平或更优，训练更高效
- 是 PaLI-3、Gemini 等模型的视觉编码器选择

### 4.5 代码：ViT Patch Embedding

```python
import torch
import torch.nn as nn

class PatchEmbed(nn.Module):
    def __init__(self, img_size=224, patch_size=16, in_ch=3, embed_dim=768):
        super().__init__()
        self.num_patches = (img_size // patch_size) ** 2
        # 用 Conv2d 实现 patch 切分 + 线性投影
        self.proj = nn.Conv2d(in_ch, embed_dim, kernel_size=patch_size, stride=patch_size)
    
    def forward(self, x):
        # x: [B, 3, 224, 224] → [B, 768, 14, 14] → [B, 196, 768]
        return self.proj(x).flatten(2).transpose(1, 2)

class ViT(nn.Module):
    def __init__(self, img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12, num_classes=1000):
        super().__init__()
        self.patch_embed = PatchEmbed(img_size, patch_size, 3, embed_dim)
        num_patches = self.patch_embed.num_patches
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=depth)
        self.head = nn.Linear(embed_dim, num_classes)
    
    def forward(self, x):
        B = x.shape[0]
        x = self.patch_embed(x)
        cls = self.cls_token.expand(B, -1, -1)
        x = torch.cat([cls, x], dim=1) + self.pos_embed
        x = self.encoder(x)
        return self.head(x[:, 0])  # CLS token
```

### 4.6 关键论文

- Dosovitskiy et al., "An Image is Worth 16x16 Words" (ViT), ICLR 2021
- Liu et al., "Swin Transformer: Hierarchical Vision Transformer," ICCV 2021
- Oquab et al., "DINOv2: Learning Robust Visual Features without Supervision," 2023
- Zhai et al., "Sigmoid Loss for Language Image Pre-Training" (SigLIP), ICCV 2023

---

## 五、多模态视觉

### 5.1 CLIP：连接视觉与语言

**核心思想**：对比学习对齐图像和文本的 embedding 空间。

**架构**：Image Encoder（ViT/ResNet）+ Text Encoder（Transformer）→ 共享 embedding 空间

**InfoNCE Loss**：

$$
\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N}\left[\log\frac{\exp(\text{sim}(I_i, T_i)/\tau)}{\sum_{j=1}^{N}\exp(\text{sim}(I_i, T_j)/\tau)} + \log\frac{\exp(\text{sim}(T_i, I_i)/\tau)}{\sum_{j=1}^{N}\exp(\text{sim}(T_i, I_j)/\tau)}\right]
$$

**CLIP 的杀手级特性**：
- **Zero-shot 分类**：无需训练数据，用文本 prompt 做分类（"A photo of a {class}"）
- **Open-vocabulary**：识别训练集之外的概念
- **跨模态检索**：图搜文、文搜图
- 在 WIT 400M 图文对上训练

**CLIP 的局限**：
- 粗粒度对齐（图像级 vs 区域级）
- Bag-of-words 问题（"dog chases cat" = "cat chases dog"）
- 组合推理弱（属性绑定、空间关系）
- NegCLIP/SugarCREPE benchmark 暴露了组合理解缺陷

### 5.2 LLaVA：视觉语言大模型

**LLaVA（Large Language and Vision Assistant）架构**：

```
Image → Visual Encoder (CLIP ViT-L/14) → Projection → LLM (Vicuna/LLaMA)
                                          ↑
                                   MLP Projector
                                   (连接视觉和语言空间)
```

**两阶段训练**：
1. **预训练**：冻结 Visual Encoder + LLM，仅训练 Projector，用 595K 图文对
2. **指令微调**：冻结 Visual Encoder，微调 Projector + LLM，用 150K 多模态指令数据

**LLaVA-1.5 改进**：
- MLP projector（2 层）替代线性层
- 更高分辨率 336px
- 添加学术 VQA 数据

**LLaVA-NeXT / LLaVA-OneVision（2024）**：
- 动态高分辨率：AnyRes 策略，将大图切块分别编码
- 多图像 / 视频理解能力
- 统一图像-视频-多图 benchmark

### 5.3 GPT-4V / GPT-4o 视觉能力

**关键能力**：
- 细粒度图像理解（OCR、图表、UI、手写体）
- 空间推理和计数
- 多图比较和推理
- 视觉 grounding（虽然仍有误差）

**与开源模型的差距（截至 2025）**：
- 长文档/复杂图表理解仍领先
- 但 InternVL2.5、Qwen2-VL 在多个 benchmark 追平

### 5.4 2025-2026 多模态前沿

**Qwen2-VL（2024）**：
- Naive Dynamic Resolution：不限制分辨率，直接处理任意尺寸
- M-RoPE（Multi-dimensional Rotary Position Embedding）处理 2D 空间位置
- 统一图像/视频/多图

**InternVL 2.5（2024）**：
- Dynamic High Resolution with thumbnail
- 开源多模态最强之一（MMBench、MathVista SOTA）

**关键趋势**：
- Visual Encoder 从 CLIP → SigLIP → DINOv2+SigLIP 融合
- 分辨率从固定 → 动态 → 任意
- 从图像 → 统一图像+视频+多图
- Mixture of Vision Experts（不同分辨率/任务用不同 encoder）

### 5.5 代码：CLIP Zero-shot 分类

```python
import torch
import clip
from PIL import Image

model, preprocess = clip.load("ViT-B/32", device="cuda")

image = preprocess(Image.open("cat.jpg")).unsqueeze(0).to("cuda")
text = clip.tokenize(["a photo of a cat", "a photo of a dog", "a photo of a car"]).to("cuda")

with torch.no_grad():
    image_features = model.encode_image(image)    # [1, 512]
    text_features = model.encode_text(text)       # [3, 512]
    
    # 归一化 + 余弦相似度
    image_features /= image_features.norm(dim=-1, keepdim=True)
    text_features /= text_features.norm(dim=-1, keepdim=True)
    
    similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
    print(f"Predictions: {similarity[0].cpu().numpy()}")
    # → [0.95, 0.03, 0.02]  cat 最高
```

### 5.6 关键论文

- Radford et al., "Learning Transferable Visual Models from Natural Language Supervision" (CLIP), ICML 2021
- Liu et al., "Visual Instruction Tuning" (LLaVA), NeurIPS 2023
- Wang et al., "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution," 2024

---

## 六、视频理解

### 6.1 核心挑战

视频理解比图像多一个**时间维度**，核心挑战：
- 计算量爆炸：30fps × 10s = 300 帧 → 如何高效处理？
- 时序建模：动作识别需要理解运动模式
- 长视频理解：电影级（>1h）理解

### 6.2 经典方法

**Two-Stream Network**：
- 空间流（RGB 帧）+ 时间流（光流）→ 融合预测
- 光流计算昂贵（TV-L1），但捕获运动信息最直接

**3D CNN（C3D / I3D）**：
- 将 2D 卷积扩展为 3D：$K \times K \times K$ 卷积核
- I3D：用 ImageNet 预训练的 2D 权重 inflate 为 3D（复制 $K$ 次取平均）

**SlowFast Networks**：
- **Slow pathway**：低帧率（如 4fps），大通道数，捕获语义
- **Fast pathway**：高帧率（如 32fps），小通道数（Slow 的 1/8），捕获运动
- 侧向连接融合两路信息

### 6.3 视频 Transformer

**TimeSformer**：分解时空注意力
- Divided Space-Time Attention：先空间 self-attention，再时间 self-attention
- 避免 $O((T \times H \times W)^2)$ 的全时空注意力

**VideoMAE（2022）**：
- Masked Autoencoder 应用于视频
- **极高 mask ratio（90-95%）**：视频时间冗余高，可以 mask 更多
- 数据效率极高，3.5K 视频即可获得有竞争力的结果

**InternVideo2（2024）**：
- 统一视频理解框架
- Progressive Training：从图像 → 短视频 → 长视频
- 融合 Masked Video Modeling + Cross-Modal Contrastive + Next Token Prediction

### 6.4 长视频理解

**关键方法**：
- **帧采样**：均匀/关键帧采样，减少输入量
- **Token 压缩**：时间维度 token 合并/池化
- **Memory Bank**：类似 SAM 2，维护长期记忆
- **LLM 驱动**：视频 → 帧描述 → LLM 推理（Video-ChatGPT / VideoLLaMA）

### 6.5 代码：视频帧采样 + 特征提取

```python
import torch
import decord
from torchvision import transforms

def sample_frames(video_path, num_frames=16):
    """均匀采样 num_frames 帧"""
    vr = decord.VideoReader(video_path)
    total = len(vr)
    indices = torch.linspace(0, total - 1, num_frames).long()
    frames = vr.get_batch(indices.numpy()).asnumpy()  # [T, H, W, C]
    return frames

def extract_video_features(frames, model, transform):
    """逐帧提取特征后时间平均池化"""
    features = []
    for frame in frames:
        img = transform(frame).unsqueeze(0).cuda()
        with torch.no_grad():
            feat = model.encode_image(img)  # [1, D]
        features.append(feat)
    return torch.cat(features, dim=0).mean(dim=0)  # [D] 时间平均
```

### 6.6 关键论文

- Feichtenhofer et al., "SlowFast Networks for Video Recognition," ICCV 2019
- Tong et al., "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training," NeurIPS 2022
- Wang et al., "InternVideo2: Scaling Foundation Models for Multimodal Video Understanding," CVPR 2024

---

## 七、3D 视觉

### 7.1 3D 表示方式

| 表示 | 优势 | 劣势 | 典型方法 |
|------|------|------|----------|
| 点云（Point Cloud） | 原始、轻量 | 无序、稀疏 | PointNet, PointNet++ |
| 体素（Voxel） | 规则结构、可用 3D CNN | 内存 $O(N^3)$、分辨率受限 | VoxelNet, 3D-UNet |
| 网格（Mesh） | 表面精确、渲染高效 | 拓扑固定、难优化 | MeshCNN |
| 隐式表示 | 连续、分辨率无限 | 推理慢 | NeRF, SDF |
| 3D Gaussian | 渲染快、质量高 | 存储大 | 3DGS |

### 7.2 PointNet / PointNet++

**PointNet 核心洞察**：点云是**无序集合**，需要排列不变性。

**解法**：逐点 MLP + 对称函数（Max Pooling）

$$
f(\{x_1, \ldots, x_n\}) = g(\text{MAX}(h(x_1), \ldots, h(x_n)))
$$

- $h$：逐点特征提取（共享 MLP）
- MAX：对称函数，保证排列不变性
- $g$：全局特征 → 分类/分割头

**PointNet++ 改进**：层级结构
- Farthest Point Sampling → Ball Query → PointNet（局部特征提取）
- 多尺度 Grouping（MSG）处理密度不均问题

### 7.3 NeRF（Neural Radiance Fields）

**核心思想**：用 MLP 表示 3D 场景的隐式函数

$$
F_\theta: (\mathbf{x}, \mathbf{d}) \rightarrow (\mathbf{c}, \sigma)
$$

- 输入：3D 位置 $\mathbf{x} = (x,y,z)$ + 观察方向 $\mathbf{d} = (\theta, \phi)$
- 输出：颜色 $\mathbf{c} = (r,g,b)$ + 体密度 $\sigma$

**体渲染（Volume Rendering）**：

$$
C(\mathbf{r}) = \int_{t_n}^{t_f} T(t) \cdot \sigma(\mathbf{r}(t)) \cdot \mathbf{c}(\mathbf{r}(t), \mathbf{d}) \, dt
$$

$$
T(t) = \exp\left(-\int_{t_n}^{t} \sigma(\mathbf{r}(s)) \, ds\right)
$$

其中 $T(t)$ 是累积透射率。

**位置编码（Positional Encoding）**：

$$
\gamma(p) = (\sin(2^0\pi p), \cos(2^0\pi p), \ldots, \sin(2^{L-1}\pi p), \cos(2^{L-1}\pi p))
$$

MLP 学不了高频细节 → 位置编码将低频输入映射到高频空间。

**NeRF 主要问题**：
- 训练慢（数小时/场景）
- 渲染慢（每像素需多次 MLP 前向）
- 单场景一模型，不泛化

### 7.4 3D Gaussian Splatting（3DGS）

**2023 年 3D 重建的突破性工作**。

**核心思想**：用 3D 高斯函数显式表示场景，**可微光栅化**渲染。

每个 Gaussian 由以下参数定义：
- 位置 $\boldsymbol{\mu} \in \mathbb{R}^3$
- 协方差矩阵 $\boldsymbol{\Sigma} \in \mathbb{R}^{3 \times 3}$（通过四元数旋转 $\mathbf{q}$ + 缩放 $\mathbf{s}$ 参数化）
- 不透明度 $\alpha \in [0, 1]$
- 球谐系数（SH coefficients）表示视角相关颜色

**2D 投影**：3D Gaussian 投影到 2D 仍是 Gaussian

$$
\boldsymbol{\Sigma}' = \mathbf{J} \mathbf{W} \boldsymbol{\Sigma} \mathbf{W}^T \mathbf{J}^T
$$

**渲染**：按深度排序后 alpha blending

$$
C = \sum_{i=1}^{N} c_i \alpha_i \prod_{j=1}^{i-1}(1-\alpha_j)
$$

**优势 vs NeRF**：
- 训练 ~30 分钟 vs 数小时
- 实时渲染 (>100 FPS) vs 秒级
- 质量相当甚至更好
- 可编辑（直接操作 Gaussian）

**自适应密度控制（Adaptive Densification）**：
- Clone：梯度大 + Gaussian 小 → 复制
- Split：梯度大 + Gaussian 大 → 分裂
- Prune：$\alpha$ 太低 → 删除

### 7.5 代码：PointNet 核心

```python
import torch
import torch.nn as nn

class PointNet(nn.Module):
    def __init__(self, num_classes=40):
        super().__init__()
        # 逐点 MLP（共享权重）
        self.mlp1 = nn.Sequential(
            nn.Linear(3, 64), nn.ReLU(),
            nn.Linear(64, 128), nn.ReLU(),
            nn.Linear(128, 1024), nn.ReLU(),
        )
        # 分类头
        self.classifier = nn.Sequential(
            nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(512, 256), nn.ReLU(), nn.Dropout(0.3),
            nn.Linear(256, num_classes),
        )
    
    def forward(self, x):
        # x: [B, N, 3] — N 个点，每点 3D 坐标
        point_features = self.mlp1(x)              # [B, N, 1024]
        global_feature = point_features.max(dim=1)[0]  # [B, 1024] 对称函数
        return self.classifier(global_feature)
```

### 7.6 关键论文

- Qi et al., "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation," CVPR 2017
- Mildenhall et al., "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis," ECCV 2020
- Kerbl et al., "3D Gaussian Splatting for Real-Time Radiance Field Rendering," SIGGRAPH 2023

---

## 八、生成模型

### 8.1 Diffusion Model 核心原理

**前向过程（加噪）**：逐步向数据添加高斯噪声

$$
q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1-\beta_t}\mathbf{x}_{t-1}, \beta_t\mathbf{I})
$$

可以一步跳到任意 $t$：

$$
q(\mathbf{x}_t | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_t; \sqrt{\bar{\alpha}_t}\mathbf{x}_0, (1-\bar{\alpha}_t)\mathbf{I})
$$

其中 $\bar{\alpha}_t = \prod_{s=1}^{t}(1-\beta_s)$。

**反向过程（去噪）**：学习预测噪声

$$
p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t) = \mathcal{N}(\mathbf{x}_{t-1}; \boldsymbol{\mu}_\theta(\mathbf{x}_t, t), \sigma_t^2\mathbf{I})
$$

**训练目标**（简化后）：

$$
\mathcal{L}_{simple} = \mathbb{E}_{t, \mathbf{x}_0, \boldsymbol{\epsilon}}\left[\|\boldsymbol{\epsilon} - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)\|^2\right]
$$

模型 $\boldsymbol{\epsilon}_\theta$ 预测添加的噪声，等价于 score matching。

### 8.2 关键技术组件

**Classifier-Free Guidance（CFG）**：

$$
\tilde{\boldsymbol{\epsilon}}_\theta(\mathbf{x}_t, c) = (1+w)\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, c) - w\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, \varnothing)
$$

- 训练时随机 drop 条件（$c \to \varnothing$），推理时用条件与无条件预测的差引导
- $w > 0$ 增强条件一致性但降低多样性
- 典型 $w = 7.5$（SD）

**Latent Diffusion Model（LDM）**：
- 不在像素空间扩散，而在 VAE 的 latent space
- $512 \times 512$ 图像 → $64 \times 64$ latent → 计算量降 48×
- UNet 做噪声预测，Cross-Attention 注入文本条件

### 8.3 Stable Diffusion / SDXL

**SD 架构**：Text Encoder（CLIP）→ UNet（with Cross-Attention）→ VAE Decoder

**SDXL（2023）改进**：
- 更大 UNet（2.6B params vs SD 1.5 的 860M）
- 双文本编码器：CLIP ViT-L + OpenCLIP ViT-bigG
- 条件化扩展：原始分辨率、裁剪参数、目标分辨率 → 消除低分辨率训练的 artifact
- Refiner 模型：两阶段生成，先粗后细
- 微调方法：LoRA、ControlNet、IP-Adapter、T2I-Adapter

### 8.4 FLUX

**Black Forest Labs（SD 原作者团队）的新一代模型**（2024）：

**核心变化：UNet → DiT（Diffusion Transformer）**
- 用 Transformer 替代 UNet 作为去噪网络
- MM-DiT（Multi-Modal DiT）：图像 token 和文本 token 在同一个 Transformer 中交互
- 借鉴 Stable Diffusion 3 的 Rectified Flow 训练

**FLUX 架构**：
- Dual-stream → Single-stream Transformer blocks
- T5-XXL + CLIP 双文本编码器
- Flow Matching 替代 DDPM（更直的轨迹 → 更少采样步数）

**Flow Matching 核心**：

$$
\frac{d\mathbf{x}_t}{dt} = \mathbf{v}_\theta(\mathbf{x}_t, t)
$$

将噪声 $\mathbf{x}_1 \sim \mathcal{N}(0, I)$ 和数据 $\mathbf{x}_0$ 之间的插值路径建模为 ODE：

$$
\mathbf{x}_t = (1-t)\mathbf{x}_0 + t\boldsymbol{\epsilon}, \quad \mathbf{v}_{target} = \boldsymbol{\epsilon} - \mathbf{x}_0
$$

**FLUX 版本**：
- FLUX.1-schnell：蒸馏版，4 步生成
- FLUX.1-dev：引导蒸馏，高质量
- FLUX.1-pro：API-only，商业版

### 8.5 Sora 与视频生成

**Sora（OpenAI, 2024）核心设计（推测）**：
- **Spacetime Patches**：视频切成时空 patch（类似 ViViT）
- **DiT（Diffusion Transformer）**：在 latent space 的时空 patch 上做扩散
- **可变分辨率/时长/宽高比**：Native resolution 训练
- **长时一致性**：通过大规模训练 + 长上下文实现
- 能生成 60s 高质量视频，保持时间一致性

**开源视频生成**：
- **CogVideoX**（智谱）：3D VAE + Expert Transformer，最长 6s
- **Open-Sora**（HPC-AI）：开源复现，STDiT 架构
- **Wan2.1**（阿里）: 14B DiT, T2V 和 I2V, 最长 10s+, 2025年初开源表现优异
- **HunyuanVideo**（腾讯）：13B, 双流 DiT, 统一图像和视频生成
- **Runway Gen-3**：商业视频生成 API

### 8.6 代码：Diffusion 训练循环核心

```python
import torch
import torch.nn.functional as F

def train_step(model, x_0, noise_scheduler, optimizer):
    """简化的 Diffusion 训练步骤"""
    B = x_0.shape[0]
    
    # 1. 随机采样时间步
    t = torch.randint(0, noise_scheduler.num_timesteps, (B,), device=x_0.device)
    
    # 2. 采样噪声
    noise = torch.randn_like(x_0)
    
    # 3. 前向过程：加噪
    alpha_bar = noise_scheduler.alpha_bar[t][:, None, None, None]
    x_t = torch.sqrt(alpha_bar) * x_0 + torch.sqrt(1 - alpha_bar) * noise
    
    # 4. 模型预测噪声
    noise_pred = model(x_t, t)
    
    # 5. 计算 MSE loss
    loss = F.mse_loss(noise_pred, noise)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    return loss.item()

# DDPM 采样
@torch.no_grad()
def sample(model, shape, noise_scheduler, device):
    x = torch.randn(shape, device=device)
    for t in reversed(range(noise_scheduler.num_timesteps)):
        t_batch = torch.full((shape[0],), t, device=device, dtype=torch.long)
        noise_pred = model(x, t_batch)
        x = noise_scheduler.step(noise_pred, t, x)  # 去噪一步
    return x
```

### 8.7 关键论文

- Ho et al., "Denoising Diffusion Probabilistic Models" (DDPM), NeurIPS 2020
- Rombach et al., "High-Resolution Image Synthesis with Latent Diffusion Models," CVPR 2022
- Peebles & Xie, "Scalable Diffusion Models with Transformers" (DiT), ICCV 2023
- Esser et al., "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis" (SD3), ICML 2024
- Brooks et al., "Video Generation Models as World Simulators" (Sora Technical Report), 2024

---

## 九、自监督视觉学习

### 9.1 核心范式

自监督学习（SSL）= 无标签数据学习通用表征。CV 中两大范式：

**1. 对比学习（Contrastive Learning）**

$$
\mathcal{L} = -\log\frac{\exp(\text{sim}(z_i, z_j)/\tau)}{\sum_{k=1}^{2N}\mathbb{1}_{[k \neq i]}\exp(\text{sim}(z_i, z_k)/\tau)}
$$

- 正样本对（同图不同 augmentation）拉近，负样本对推远
- **SimCLR**：大 batch（4096）提供足够负样本
- **MoCo**：动量编码器 + Queue 存储负样本，不需要大 batch

**2. 掩码图像建模（Masked Image Modeling, MIM）**

$$
\mathcal{L}_{MIM} = \mathbb{E}\left[\|f_\theta(\tilde{\mathbf{x}}) - \text{target}(\mathbf{x})\|^2\right]
$$

- **MAE**：随机 mask 75% patch，编码器只处理可见 patch，解码器重建像素
- **BEiT**：用 dVAE token 作为重建目标（离散 token 而非像素）
- **iBOT**：在线 tokenizer（teacher EMA），结合 MIM + self-distillation

### 9.2 关键方法对比

| 方法 | 范式 | 负样本 | 预训练数据 | ImageNet Linear Probe |
|------|------|--------|-----------|----------------------|
| SimCLR v2 | 对比 | 大 batch | IN-1K | 79.8% |
| MoCo v3 | 对比 | 动量队列 | IN-1K | 81.0% |
| BYOL | 自蒸馏 | **不需要** | IN-1K | 80.4% |
| MAE | MIM | 不需要 | IN-1K | 75.8% (ViT-L) |
| DINOv2 | MIM+自蒸馏 | 不需要 | LVD-142M | 86.3% (ViT-g) |

### 9.3 BYOL：无负样本的对比学习

**为什么 BYOL 不需要负样本却不 collapse？**

这是 SSL 最经典面试题。

**架构**：
- Online network：encoder + projector + **predictor**
- Target network：encoder + projector（EMA 更新，无梯度）

**关键**：
1. **Predictor 的非对称性**：只有 online 有 predictor，target 没有
2. **EMA 更新**：target 缓慢变化，提供稳定的学习信号
3. **Batch Normalization 的隐式负样本作用**：BN 引入了 batch 统计量，间接提供了负样本信息（Richemond et al., 2020 去掉 BN 会 collapse）
4. **更本质的解释（Tian et al., 2021）**：BYOL 实际上在优化一种 spectral contrastive loss，EMA + predictor 的组合隐式地避免了 trivial solution

### 9.4 MAE 深度解析

**MAE 的设计哲学**：
- **非对称架构**：大编码器（仅处理 25% 可见 token）+ 小解码器（重建）
- **高 mask ratio（75%）**：图像信息密度低（vs NLP），需要更高 mask 比例才有挑战
- **像素重建**：简单直接，不需要 tokenizer

**MAE 编码器效率**：只处理可见 patch → 训练速度 3× 快于 ViT baseline

**MAE 的局限**：
- Linear probe 性能不如对比学习（75.8% vs 81%+）
- 需要 fine-tuning 才能释放能力
- 语义理解不如对比学习（MAE 更偏底层特征）

### 9.5 代码：MAE 伪代码

```python
# MAE 编码器前向（简化）
def mae_forward(self, images, mask_ratio=0.75):
    # 1. Patch Embedding
    patches = self.patch_embed(images)    # [B, N, D]
    B, N, D = patches.shape
    
    # 2. 随机 mask
    num_keep = int(N * (1 - mask_ratio))
    noise = torch.rand(B, N, device=patches.device)
    ids_shuffle = noise.argsort(dim=1)
    ids_keep = ids_shuffle[:, :num_keep]
    
    # 3. 只保留可见 patch（关键效率技巧）
    visible = torch.gather(patches, 1, ids_keep.unsqueeze(-1).expand(-1, -1, D))
    visible = visible + self.pos_embed_visible  # 位置编码
    
    # 4. Encoder（只处理 25% token！）
    encoded = self.encoder(visible)       # [B, num_keep, D]
    
    # 5. Decoder：补上 mask tokens
    full = torch.cat([encoded, self.mask_token.expand(B, N - num_keep, -1)], dim=1)
    full = self.unshuffle(full, ids_shuffle)  # 恢复原始顺序
    decoded = self.decoder(full + self.decoder_pos_embed)
    
    # 6. 重建 loss（只在 masked patch 上计算）
    target = self.patchify(images)
    loss = F.mse_loss(decoded[:, num_keep:], target_masked, reduction='mean')
    return loss
```

### 9.6 关键论文

- Chen et al., "A Simple Framework for Contrastive Learning of Visual Representations" (SimCLR), ICML 2020
- Grill et al., "Bootstrap Your Own Latent" (BYOL), NeurIPS 2020
- He et al., "Masked Autoencoders Are Scalable Vision Learners" (MAE), CVPR 2022
- Oquab et al., "DINOv2: Learning Robust Visual Features without Supervision," 2023

---

## 十、2026 前沿：视觉 Agent 与具身智能

### 10.1 视觉 Agent

**定义**：能够**看、理解、决策、执行**的 AI 系统，视觉是感知世界的核心通道。

**CUA（Computer-Use Agent）视觉流水线**：
1. 截屏 → 视觉理解（UI 元素检测 + OCR + 布局理解）
2. 状态判断 → 规划下一步操作
3. 执行动作（点击、输入、滚动）
4. 观察反馈 → 循环

**Set-of-Mark（SoM）Prompting**：
- 在截图上叠加数字标记标注可交互元素
- VLM 通过标记 ID 选择操作目标
- 比纯坐标预测更准确

**关键工作**：
- **WebVoyager**：网页导航 Agent，用 GPT-4V 做视觉理解
- **OS-Atlas**：操作系统级 GUI 基础模型，统一 Web/Desktop/Mobile 的 GUI grounding
- **Claude Computer Use**：Anthropic 的 CUA，直接操作桌面
- **UI-TARS**（字节跳动，2025）：端到端 GUI Agent，统一感知-推理-行动，基于 Qwen2-VL，在 OSWorld/AndroidWorld 达到 SOTA
- **SeeClick**：GUI grounding 预训练，元素定位准确率显著提升

### 10.2 具身智能视觉（Embodied Vision）

**核心挑战**：从被动感知 → 主动交互

**视觉在具身中的角色**：
- **场景理解**：3D 重建 + 语义分割 + 物体检测
- **导航**：Visual SLAM + 路径规划 + 避障
- **操作**：6-DoF 抓取 + 物体状态估计 + 手眼协调

**关键范式**：

**1. Vision-Language-Action Models（VLA）**

$$
\pi(a_t | o_{1:t}, l) = \text{VLA}(\text{image}_{1:t}, \text{language\_instruction})
$$

- **RT-2（Robotics Transformer 2）**：将机器人动作表示为文本 token，用 VLM 统一视觉理解和动作输出
- **Octo**：开源通用机器人策略，Transformer-based，支持多传感器输入
- **π0（Physical Intelligence, 2024）**：3B VLA，flow matching 生成动作，zero-shot 在多种机器人上泛化
- **π0.5（2025）**：基于 π0，引入 web-scale 语言预训练 + high-level language reasoning

**2. 3D 基础模型 for Robotics**

- **SPA（3D Spatial Understanding Agent）**：3D 语义理解 + 空间推理
- **LERF / OpenScene**：将 CLIP 特征嵌入 3D 场景（NeRF/3DGS），实现开放词汇 3D 理解

**3. 世界模型（World Models）**

$$
\hat{s}_{t+1} = f_\theta(s_t, a_t) \quad \text{（预测下一状态）}
$$

- 学习环境动力学的内部模型
- 可用于 imagination-based planning
- **Genie 2（DeepMind）**：从单张图像生成可交互的 3D 世界
- **UniSim**：统一的视觉世界模拟器

### 10.3 2026 关键趋势

1. **Vision Foundation Model 统一化**：DINOv2 + SAM + CLIP → 单个模型处理所有视觉任务
2. **视频生成即世界模型**：Sora-like 模型不仅生成视频，还能预测物理规律
3. **Tokenize Everything**：图像/视频/3D/动作统一 token 化，用 Transformer 统一处理
4. **Data Engine 闭环**：SAM/DINO 自动标注 → 训练 → 迭代改进
5. **端侧视觉**：MobileSAM / EfficientViT / TinyViT 等轻量模型 on-device 部署
6. **视觉推理（Visual Reasoning）**：从感知到推理，类似 o1 的视觉版 slow thinking

### 10.4 代码：VLA 伪代码

```python
class VLAPolicy(nn.Module):
    """Vision-Language-Action Model 简化架构"""
    def __init__(self, vision_encoder, llm, action_head):
        super().__init__()
        self.vision_encoder = vision_encoder  # e.g., SigLIP
        self.projector = nn.Linear(vision_dim, llm_dim)
        self.llm = llm                        # e.g., LLaMA
        self.action_head = action_head         # 输出机器人动作
    
    def forward(self, images, instruction_tokens, proprioception=None):
        # 1. 视觉编码
        vis_features = self.vision_encoder(images)       # [B, num_patches, D_v]
        vis_tokens = self.projector(vis_features)         # [B, num_patches, D_llm]
        
        # 2. 拼接 [vision_tokens, instruction_tokens, proprio_tokens]
        input_embeds = torch.cat([vis_tokens, self.llm.embed(instruction_tokens)], dim=1)
        
        # 3. LLM 推理
        hidden = self.llm(inputs_embeds=input_embeds)
        
        # 4. 动作预测（连续动作空间，用 flow matching 或 diffusion）
        action = self.action_head(hidden[:, -1])  # [B, action_dim]
        return action  # e.g., [dx, dy, dz, droll, dpitch, dyaw, gripper]
```

### 10.5 关键论文

- Brohan et al., "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control," 2023
- Black et al., "π0: A Vision-Language-Action Flow Model for General Robot Control," 2024
- Shaw et al., "OS-Atlas: A Foundation Action Model for Generalist GUI Agents," 2024
- Qin et al., "UI-TARS: Pioneering Automated GUI Interaction with Native Agents," 2025

---

## 十一、面试题精选

### 基础题

#### Q1: 为什么 ResNet 能训练 1000+ 层而 VGG 不行？

**深度答案**：

1. **梯度消失/爆炸**：VGG 的梯度需要穿过所有层的连乘，深度增加时梯度指数衰减或膨胀。ResNet 的残差连接提供了 identity shortcut，梯度可以直接回传：

$$
\frac{\partial \mathcal{L}}{\partial \mathbf{x}_l} = \frac{\partial \mathcal{L}}{\partial \mathbf{x}_L}\left(1 + \frac{\partial}{\partial \mathbf{x}_l}\sum_{i=l}^{L-1}\mathcal{F}_i\right)
$$

恒等项 1 保证梯度不为零。

2. **Ensemble 效应**：ResNet 隐式等价于 $2^n$ 个不同深度子网络的集成（Veit et al., 2016），删除单层影响小。

3. **Loss Landscape**：残差连接平滑了 loss surface，使优化更容易（Li et al., 2018 可视化）。

4. **退化问题 vs 过拟合**：VGG 加深后 **训练** loss 变差（不是过拟合），说明是优化问题而非容量问题。ResNet 保证至少能学到 identity mapping。

---

#### Q2: 解释 Batch Normalization 的原理。为什么训练和推理行为不同？

**深度答案**：

$$
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}, \quad y_i = \gamma \hat{x}_i + \beta
$$

**训练时**：$\mu_B, \sigma_B^2$ 从当前 mini-batch 计算，同时用 EMA 维护 running stats。

**推理时**：使用 EMA 累积的 running mean/var（确定性，不依赖 batch）。

**为什么有效（多种假说）**：
- ~~Internal Covariate Shift~~（原论文说法，已被质疑）
- **Loss landscape 平滑化**（Santurkar et al., 2018 更有说服力）
- 允许更大学习率 → 更快收敛
- 隐式正则化效果（batch statistics 引入噪声）

**BN 的坑**：
- Batch size 太小（<16）→ 统计量不稳定 → 用 GroupNorm / LayerNorm
- 分布式训练需要 SyncBN
- 序列模型中不适用（用 LayerNorm）

---

#### Q3: Depthwise Separable Convolution 的计算量节省是多少？

**深度答案**：

标准卷积：$C_{in} \times C_{out} \times K^2 \times H \times W$ FLOPs

Depthwise Separable = Depthwise Conv + Pointwise Conv：
- Depthwise：$C_{in} \times K^2 \times H \times W$（每通道独立卷积）
- Pointwise：$C_{in} \times C_{out} \times H \times W$（1×1 卷积混合通道）

比例：

$$
\frac{C_{in} \times K^2 + C_{in} \times C_{out}}{C_{in} \times C_{out} \times K^2} = \frac{1}{C_{out}} + \frac{1}{K^2}
$$

对于 $K=3, C_{out}=256$：$\frac{1}{256} + \frac{1}{9} \approx \frac{1}{9}$，**约 9 倍计算量节省**。

MobileNet、EfficientNet、MobileViT 都大量使用。

---

### 进阶题

#### Q4: DETR 相比 Faster R-CNN 的核心优势和劣势？

**深度答案**：

**优势**：
1. **端到端**：无 NMS、无 Anchor、无 proposal → 代码简洁，无手工超参
2. **全局推理**：Self-attention 看全图，天然处理遮挡和重复检测
3. **集合预测**：二部图匹配保证一一对应，无需 NMS 后处理
4. **容易扩展**：改 query 数量即可适配不同场景

**劣势**：
1. **收敛慢**：500 epochs vs Faster R-CNN ~12 epochs（Cross-Attention 需要学 spatial prior）
2. **小目标弱**：Self-attention 在低分辨率特征图上丢失细节
3. **计算量**：$O(N^2)$ attention，N=100 queries + ~850 image tokens

**改进路线**：
- Deformable DETR：可变形注意力 → 收敛 10× 加速
- DINO-DETR：Denoising pre-training → COCO 63.3 mAP
- RT-DETR：Hybrid encoder → 实时 DETR

---

#### Q5: Diffusion Model 为什么效果好？和 GAN 比有什么优势？

**深度答案**：

**Diffusion 优势**：
1. **训练稳定**：优化简单的 MSE loss，无对抗训练的不稳定性
2. **模式覆盖好**：不会 mode collapse，因为前向过程覆盖整个数据分布
3. **对数似然可计算**：理论上是似然模型的变分下界
4. **条件生成灵活**：Classifier-Free Guidance 优雅地控制条件强度
5. **可与大规模预训练结合**：CLIP text encoder 提供强语义条件

**GAN 优势**：
1. **速度**：一次前向生成 vs Diffusion 需要 20-50 步迭代
2. **高频细节**：对抗损失天然鼓励高频特征
3. **潜空间可解释**：StyleGAN 的 W 空间有语义分离性

**为什么 Diffusion "赢了"**：
- 可扩展性：Diffusion + Transformer (DiT) 展现了清晰的 scaling law
- GAN 的训练不稳定限制了 scale up
- Diffusion 更容易做条件生成（text/image/layout/...）

---

#### Q6: ViT 和 CNN 的核心区别是什么？各自的归纳偏置？

**深度答案**：

| 维度 | CNN | ViT |
|------|-----|-----|
| 基本操作 | 局部卷积 | 全局 Self-Attention |
| 归纳偏置 | Locality + Translation Equivariance | 几乎无（仅 patch 结构） |
| 感受野 | 逐层增大（layer 1: 3×3 → layer N: 大） | 第一层即全局 |
| 数据需求 | 小数据也能学 | 需要大数据（或强数据增强/蒸馏） |
| 计算复杂度 | $O(N)$ 线性 | $O(N^2)$ 平方 |
| 特征层次 | 天然金字塔 | 单尺度（需要 Swin 等改造） |

**关键洞察**：
- ViT 在大数据下超越 CNN，因为**更少的归纳偏置 = 更大的假设空间**
- CNN 的 locality 是一种先验，小数据下有利（regularization），大数据下是限制
- ConvNeXt 证明：用 ViT 的训练策略（大 kernel、LayerNorm、GELU）改造 CNN，性能可以追平 ViT
- 2025 趋势：Hybrid 架构（CNN 提特征 → ViT 全局推理）

---

#### Q7: CLIP 的 zero-shot 能力来源于什么？局限是什么？

**深度答案**：

**能力来源**：
1. **大规模图文对齐预训练**：WIT 400M 对，覆盖极广的视觉概念
2. **自然语言作为开放标签空间**：任意文本都可以作为分类器
3. **对比学习的 embedding 空间**：图文在共享空间中按语义聚类

**Zero-shot 分类机制**：
```
image_embedding · text_embedding("A photo of a {class}")
→ 余弦相似度 → argmax
```

**局限性**：
1. **粗粒度**：图像级对齐，不理解区域/像素级对应
2. **Bag-of-Words**：对词序不敏感（"horse riding person" ≈ "person riding horse"）
3. **属性绑定弱**：分不清 "red car and blue truck" 中谁红谁蓝
4. **计数/空间关系弱**：数不清有几个物体，判断不了"左边"vs"右边"
5. **分布偏差**：训练数据的 bias 会反映在表征中（如 demographic bias）

**改进**：SigLIP（更好的 loss）、NegCLIP（hard negative）、CLIPA（训练效率）

---

### 前沿题

#### Q8: SAM 的 "Segment Anything" 是怎么做到的？

**深度答案**：

**三大支柱**：

1. **Data Engine（数据引擎）**：
   - 阶段 1：人工标注（assisted-manual）→ 120K 图 4.3M masks
   - 阶段 2：半自动（semi-automatic）→ SAM 预标注 + 人工补充
   - 阶段 3：全自动（fully-automatic）→ 网格点提示 + 置信度过滤 → 11M 图 1.1B masks
   - **关键**：模型和数据互相提升的飞轮效应

2. **Promptable 架构**：
   - 支持点/框/mask/文本多种提示
   - Ambiguity-aware：一个提示返回 3 个候选 mask + IoU score
   - Image Encoder 只需编码一次，Prompt → Mask 是轻量操作

3. **Zero-shot 泛化**：
   - SA-1B 的多样性保证了跨域泛化
   - Promptable 设计使其不依赖类别定义

**与传统分割的本质区别**：SAM 不做语义分类，只做"给定提示，分割出物体"。语义需要结合 CLIP 等模型（如 Grounded-SAM = GroundingDINO + SAM）。

---

#### Q9: 3D Gaussian Splatting vs NeRF，核心 trade-off 是什么？

**深度答案**：

| 维度 | NeRF | 3DGS |
|------|------|------|
| 表示 | 隐式（MLP） | 显式（Gaussian 集合） |
| 渲染方式 | Ray marching（每像素多次查询） | 光栅化（alpha blending） |
| 训练速度 | 数小时 | ~30 分钟 |
| 渲染速度 | 秒级 | 实时（>100 FPS） |
| 质量 | 高 | 同等或更好 |
| 存储 | 小（MLP 参数） | 大（百万 Gaussian 参数） |
| 编辑性 | 难（隐式） | 容易（直接操作 Gaussian） |
| 泛化 | 单场景训练 | 单场景训练 |
| 抗锯齿 | 天然（体渲染）| 需要 Mip-Splatting 等改进 |

**3DGS 的核心挑战**：
- 存储：百万 Gaussian × (位置+SH+协方差+不透明度) = 数百 MB
- 压缩：Compact3D、HAC 等方法压缩到 ~10MB
- 动态场景：4D Gaussian Splatting / Deformable 3DGS
- 大场景：层次化 Gaussian / Octree-GS

---

#### Q10: 解释 Flow Matching，和 DDPM 有什么本质区别？

**深度答案**：

**DDPM**：
- 前向：离散 Markov chain，逐步加噪 $\mathbf{x}_0 \to \mathbf{x}_1 \to \ldots \to \mathbf{x}_T$
- 反向：学习每步的去噪概率 $p_\theta(\mathbf{x}_{t-1}|\mathbf{x}_t)$
- 训练：预测噪声 $\boldsymbol{\epsilon}_\theta(\mathbf{x}_t, t)$
- 采样：需要 $T$ 步迭代（通常 50-1000 步）

**Flow Matching**：
- 将数据分布和噪声分布之间的变换建模为**连续 ODE**：

$$
\frac{d\mathbf{x}_t}{dt} = \mathbf{v}_\theta(\mathbf{x}_t, t), \quad t \in [0, 1]
$$

- **Rectified Flow**：直接用线性插值 $\mathbf{x}_t = (1-t)\mathbf{x}_0 + t\boldsymbol{\epsilon}$
- 训练目标：$\mathcal{L} = \mathbb{E}\|\mathbf{v}_\theta(\mathbf{x}_t, t) - (\boldsymbol{\epsilon} - \mathbf{x}_0)\|^2$

**核心区别**：
1. **路径**：DDPM 曲线路径 vs Flow Matching 直线路径 → 更少采样步数
2. **灵活性**：Flow Matching 可以选择任意概率路径（optimal transport 路径更直）
3. **效率**：Rectified Flow + 蒸馏 → 1-4 步生成
4. **数学框架**：DDPM 基于 SDE，Flow Matching 基于 ODE，更简洁

FLUX、SD3 都采用 Flow Matching / Rectified Flow。

---

#### Q11: DINOv2 为什么能不用标签就学到这么好的特征？

**深度答案**：

**三大关键因素**：

1. **训练目标的互补性**：
   - **Self-distillation（DINO loss）**：Student 看 local crop，Teacher 看 global crop → 学到局部-全局语义一致性
   - **Masked Image Modeling（iBOT loss）**：重建 masked patch → 学到局部结构和纹理
   - **KoLeo regularizer**：特征在球面上均匀分布 → 避免表示坍缩

2. **高质量大规模数据（LVD-142M）**：
   - 从 1.2B 网络图像中 curate 出 142M 高质量子集
   - Retrieval-based deduplication + rebalancing
   - 数据质量 > 数量

3. **训练工程**：
   - ViT-g/14（1.1B 参数）大模型
   - Flash Attention + FSDP + Efficient stochastic depth
   - 从 ViT-L 蒸馏到 ViT-S/B → 小模型也强

**为什么冻结特征也能用**：DINOv2 学到的是 **task-agnostic 的通用视觉特征**，类似"视觉的 word2vec"，简单线性层即可适配下游。

---

#### Q12: 视觉 Agent（如 CUA）面临的核心技术挑战是什么？

**深度答案**：

1. **Grounding 精度**：
   - 需要精确定位 UI 元素（像素级）
   - 当前 VLM 的空间理解仍有误差（点击偏移 10-20px 可能点错按钮）
   - 解法：Set-of-Mark、专用 GUI grounding 模型（OS-Atlas / SeeClick / UI-TARS）

2. **状态理解**：
   - 页面加载中 vs 加载完成？按钮可点 vs 不可点？
   - 需要理解 UI 状态机，不仅仅是视觉外观
   - 动态内容（动画、弹窗）增加复杂度

3. **长链推理**：
   - 完成一个任务可能需要 20-50 步操作
   - 错误累积：每步 95% 准确率 → 20 步后 $0.95^{20} = 36\%$ 成功率
   - 需要错误检测 + 回退机制

4. **泛化性**：
   - 网站/APP 布局千变万化
   - 同一功能在不同平台的 UI 完全不同
   - 需要理解功能语义而非死记布局

5. **安全性**：
   - Agent 操作真实系统 → 不可逆操作（删除、付款）
   - 需要 Human-in-the-loop + 操作沙箱 + 权限控制

6. **速度**：
   - 每步需要截图 + VLM 推理 → 单步 1-5s
   - 对用户体验来说太慢 → 需要端侧轻量模型

---

## 附录：CV 面试必背清单

### A. 主流架构对比表

| 架构 | 参数量 | ImageNet Top-1 | FLOPs | 推理速度 | 适用场景 |
|------|--------|---------------|-------|---------|---------|
| ResNet-50 | 25.6M | 76.1% | 4.1G | 极快 | Baseline/部署 |
| ResNet-152 | 60.2M | 78.3% | 11.6G | 快 | 精度优先 |
| EfficientNet-B7 | 66M | 84.3% | 37G | 中 | 效率-精度平衡 |
| ViT-B/16 | 86M | 81.8% | 17.6G | 中 | 大数据预训练 |
| ViT-L/16 | 304M | 85.2% | 61.6G | 慢 | 研究 |
| Swin-B | 88M | 83.5% | 15.4G | 中 | 密集预测 |
| ConvNeXt-B | 89M | 83.8% | 15.4G | 中 | CNN 的反击 |
| DINOv2 ViT-g | 1.1B | 86.3% (LP) | ~220G | 慢 | 冻结特征 |

### B. 目标检测精度-速度 Trade-off

| 模型 | COCO mAP | FPS (T4) | 参数量 | 特点 |
|------|----------|----------|--------|------|
| YOLOv8-n | 37.3 | 230 | 3.2M | 最轻量 |
| YOLOv8-s | 44.9 | 165 | 11.2M | 平衡 |
| YOLOv8-x | 53.9 | 60 | 68.2M | 最强 YOLO |
| RT-DETR-L | 53.0 | 114 | 32M | 实时 DETR |
| DINO-DETR (SwinL) | 63.3 | 5 | 218M | 精度最高 |
| Co-DETR (SwinL) | 66.0 | 3 | ~230M | COCO SOTA |

### C. 分割模型对比

| 模型 | ADE20K mIoU | 参数量 | 特点 |
|------|-------------|--------|------|
| DeepLabv3+ (R101) | 45.5 | 63M | ASPP + Decoder |
| UPerNet (Swin-L) | 53.5 | 234M | 通用分割头 |
| Mask2Former (SwinL) | 57.8 | 216M | 统一分割 |
| SAM (ViT-H) | Promptable | 632M | 万物分割 |
| SegGPT | In-context | 354M | 上下文学习分割 |

### D. 生成模型对比

| 模型 | FID (COCO) | 分辨率 | 步数 | 架构 | 开源 |
|------|------------|--------|------|------|------|
| SD 1.5 | ~8.5 | 512 | 50 | UNet LDM | ✅ |
| SDXL | ~6.5 | 1024 | 50 | UNet LDM | ✅ |
| DALL-E 3 | — | 1024 | — | UNet + T5 | ❌ |
| SD3 Medium | — | 1024 | 28 | MM-DiT | ✅ |
| FLUX.1-dev | — | 1024 | 50 | MM-DiT | ✅ |
| FLUX.1-schnell | — | 1024 | 4 | MM-DiT蒸馏 | ✅ |

### E. 常见坑

1. **BatchNorm + 小 batch size**：BN 统计量不稳定 → 用 GroupNorm/LayerNorm
2. **数据增强过强**：AutoAugment 对小数据集可能过拟合增强策略
3. **学习率与 batch size**：Linear scaling rule：$lr \propto \text{batch\_size}$，但大 batch 需要 warmup
4. **混合精度训练**：FP16 梯度下溢 → 用 GradScaler + loss scaling
5. **ViT 位置编码插值**：Fine-tune 时分辨率变化 → 双三次插值位置编码
6. **Anchor-based 检测器**：anchor 大小/比例不匹配数据集 → 用 K-Means 聚类调整
7. **分割 class imbalance**：背景像素远多于前景 → Focal Loss / OHEM / class weight
8. **Diffusion 采样**：CFG scale 太大 → 过饱和/artifact，太小 → 不像 prompt
9. **CLIP 温度参数**：$\tau$ 太小 → 训练不稳定，太大 → 判别力弱
10. **NeRF/3DGS 相机标定**：COLMAP 失败 → 特征匹配不足/退化配置 → 用 DUSt3R / MASt3R

### F. 关键 Normalization 对比

| 方法 | 归一化维度 | 典型场景 | 依赖 batch |
|------|-----------|---------|-----------|
| BatchNorm | (N, H, W) per channel | CNN | ✅ |
| LayerNorm | (C, H, W) per sample | Transformer | ❌ |
| GroupNorm | (C/G, H, W) per group | 小 batch CNN | ❌ |
| InstanceNorm | (H, W) per channel per sample | Style Transfer | ❌ |
| RMSNorm | (C) per sample, 无 centering | LLM (LLaMA) | ❌ |

---

## 参考文献

1. He, K., et al. "Deep Residual Learning for Image Recognition." CVPR, 2016.
2. Tan, M., & Le, Q. "EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks." ICML, 2019.
3. Liu, Z., et al. "A ConvNet for the 2020s." (ConvNeXt) CVPR, 2022.
4. Redmon, J., et al. "You Only Look Once: Unified, Real-Time Object Detection." CVPR, 2016.
5. Jocher, G., et al. "Ultralytics YOLOv8." GitHub, 2023.
6. Wang, C.-Y., et al. "YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information." ECCV, 2024.
7. Carion, N., et al. "End-to-End Object Detection with Transformers." (DETR) ECCV, 2020.
8. Zhang, H., et al. "DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection." ICLR, 2023.
9. Zhao, Y., et al. "DETRs Beat YOLOs on Real-time Object Detection." (RT-DETR) CVPR, 2024.
10. Ronneberger, O., et al. "U-Net: Convolutional Networks for Biomedical Image Segmentation." MICCAI, 2015.
11. Chen, L.-C., et al. "Rethinking Atrous Convolution for Semantic Image Segmentation." (DeepLabv3) arXiv, 2017.
12. Kirillov, A., et al. "Segment Anything." ICCV, 2023.
13. Ravi, N., et al. "SAM 2: Segment Anything in Images and Videos." arXiv, 2024.
14. Dosovitskiy, A., et al. "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale." ICLR, 2021.
15. Liu, Z., et al. "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows." ICCV, 2021.
16. Oquab, M., et al. "DINOv2: Learning Robust Visual Features without Supervision." TMLR, 2024.
17. Zhai, X., et al. "Sigmoid Loss for Language Image Pre-Training." (SigLIP) ICCV, 2023.
18. Radford, A., et al. "Learning Transferable Visual Models From Natural Language Supervision." (CLIP) ICML, 2021.
19. Liu, H., et al. "Visual Instruction Tuning." (LLaVA) NeurIPS, 2023.
20. Wang, P., et al. "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution." arXiv, 2024.
21. Feichtenhofer, C., et al. "SlowFast Networks for Video Recognition." ICCV, 2019.
22. Tong, Z., et al. "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training." NeurIPS, 2022.
23. Qi, C. R., et al. "PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation." CVPR, 2017.
24. Mildenhall, B., et al. "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis." ECCV, 2020.
25. Kerbl, B., et al. "3D Gaussian Splatting for Real-Time Radiance Field Rendering." SIGGRAPH, 2023.
26. Ho, J., et al. "Denoising Diffusion Probabilistic Models." NeurIPS, 2020.
27. Rombach, R., et al. "High-Resolution Image Synthesis with Latent Diffusion Models." CVPR, 2022.
28. Peebles, W., & Xie, S. "Scalable Diffusion Models with Transformers." (DiT) ICCV, 2023.
29. Esser, P., et al. "Scaling Rectified Flow Transformers for High-Resolution Image Synthesis." (SD3) ICML, 2024.
30. Brooks, T., et al. "Video Generation Models as World Simulators." (Sora) OpenAI Technical Report, 2024.
31. Chen, T., et al. "A Simple Framework for Contrastive Learning of Visual Representations." (SimCLR) ICML, 2020.
32. Grill, J.-B., et al. "Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning." (BYOL) NeurIPS, 2020.
33. He, K., et al. "Masked Autoencoders Are Scalable Vision Learners." (MAE) CVPR, 2022.
34. Brohan, A., et al. "RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control." arXiv, 2023.
35. Black, K., et al. "π0: A Vision-Language-Action Flow Model for General Robot Control." arXiv, 2024.
36. Shaw, P., et al. "OS-Atlas: A Foundation Action Model for Generalist GUI Agents." arXiv, 2024.
37. Qin, Y., et al. "UI-TARS: Pioneering Automated GUI Interaction with Native Agents." arXiv, 2025.
38. Li, H., et al. "Visualizing the Loss Landscape of Neural Nets." NeurIPS, 2018.
39. Veit, A., et al. "Residual Networks Behave Like Ensembles of Relatively Shallow Networks." NeurIPS, 2016.

---

> **最后更新**: 2026-02-21 | **字数**: ~15000 | **覆盖**: 10 章 + 12 面试题 + 6 张对比表 + 39 篇文献

---

## See Also（馆长补充）

- [[AI/CV/目录|CV MOC]] — 计算机视觉知识域全索引
- [[AI/MLLM/目录|MLLM MOC]] — 视觉-语言多模态（CV 的前沿方向）
- [[AI/MLLM/CLIP|CLIP]] — 视觉-文本对比学习，CV 与 NLP 融合的里程碑
- [[AI/MLLM/InternVL3|InternVL3]] — 2026 最强开源视觉大模型，基于本文 ViT 基础
- [[AI/Foundations/目录|Foundations MOC]] — 数学/DL 基础，CV 模型推导所需
