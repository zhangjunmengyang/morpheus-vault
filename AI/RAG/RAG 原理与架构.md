---
title: "RAG åŸç†ä¸æ¶æ„"
brief: "RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰çš„æ ¸å¿ƒæµç¨‹ã€ä»£ç å®ç°å’Œä¸‰ä»£æ¶æ„æ¼”è¿›ï¼ˆNaiveâ†’Advancedâ†’Modularï¼‰ï¼Œå«æ–‡æ¡£å¤„ç†ã€æ£€ç´¢å¼•æ“ã€ç”Ÿæˆæ¨¡å—çš„å®Œæ•´ä»£ç ç¤ºä¾‹å’Œä¼ä¸š/ä»£ç é—®ç­”åº”ç”¨åœºæ™¯ã€‚"
type: tutorial
domain: ai/rag
tags:
  - ai/rag
  - ai/llm/application
  - ai/retrieval
  - type/tutorial
created: 2026-02-14
updated: "2026-02-22"
status: complete
sources:
  - "Lewis et al. 'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks' arXiv:2005.11401"
  - "Facebook AI Research. 'FAISS: A Library for Efficient Similarity Search' https://github.com/facebookresearch/faiss"
  - "LlamaIndex Documentation https://docs.llamaindex.ai/"
  - "LangChain Documentation https://python.langchain.com/"
related:
  - "[[AI/RAG/RAG-2026-æŠ€æœ¯å…¨æ™¯|RAG 2026 å…¨æ™¯]]"
  - "[[AI/RAG/æ£€ç´¢ç­–ç•¥]]"
  - "[[AI/RAG/æ–‡æ¡£è§£æ]]"
  - "[[AI/LLM/Application/Embedding ä¸å‘é‡æ£€ç´¢]]"
---

# RAG åŸç†ä¸æ¶æ„

RAG (Retrieval-Augmented Generation) é€šè¿‡æ£€ç´¢å¤–éƒ¨çŸ¥è¯†å¢å¼ºå¤§è¯­è¨€æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›ï¼Œæ˜¯è§£å†³ LLM çŸ¥è¯†å±€é™æ€§ã€å¹»è§‰é—®é¢˜çš„æ ¸å¿ƒæŠ€æœ¯æ¶æ„ã€‚è¯¥æ¦‚å¿µç”± Lewis et al. åœ¨ 2020 å¹´æå‡ºï¼ˆarXiv:2005.11401ï¼‰ï¼Œæ ¸å¿ƒæ€è·¯æ˜¯å°†ä¿¡æ¯æ£€ç´¢ä¸ LLM ç”Ÿæˆç›¸ç»“åˆã€‚

## RAG æ ¸å¿ƒæµç¨‹

```mermaid
graph TD
    A[ç”¨æˆ·æŸ¥è¯¢] --> B[æŸ¥è¯¢ç†è§£ä¸é‡å†™]
    B --> C[æ£€ç´¢ç›¸å…³æ–‡æ¡£]
    C --> D[æ–‡æ¡£é‡æ’åº]
    D --> E[ä¸Šä¸‹æ–‡æ„å»º]
    E --> F[LLM ç”Ÿæˆ]
    F --> G[ç­”æ¡ˆè¾“å‡º]
    
    H[çŸ¥è¯†åº“] --> I[æ–‡æ¡£åˆ†å‰²]
    I --> J[å‘é‡åŒ–]
    J --> K[å‘é‡æ•°æ®åº“]
    K --> C
```

### åŸºç¡€å®ç°

```python
class BasicRAG:
    def __init__(self, embedding_model, llm, vector_db):
        self.embedding_model = embedding_model
        self.llm = llm
        self.vector_db = vector_db
    
    def query(self, question: str, top_k: int = 5):
        """åŸºç¡€ RAG æŸ¥è¯¢æµç¨‹"""
        # 1. Retrieveï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£
        query_embedding = self.embedding_model.encode(question)
        retrieved_docs = self.vector_db.search(
            query_embedding, 
            k=top_k
        )
        
        # 2. Augmentï¼šæ„å»ºå¢å¼ºä¸Šä¸‹æ–‡  
        context = "\n".join([doc.content for doc in retrieved_docs])
        
        prompt = f"""æ ¹æ®ä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

èƒŒæ™¯ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}

ç­”æ¡ˆï¼š"""
        
        # 3. Generateï¼šLLM ç”Ÿæˆç­”æ¡ˆ
        response = self.llm.generate(prompt)
        return response, retrieved_docs

# æ„å»ºçŸ¥è¯†åº“
class KnowledgeBase:
    def __init__(self, embedding_model, vector_db):
        self.embedding_model = embedding_model
        self.vector_db = vector_db
    
    def index_documents(self, documents: List[str]):
        """å°†æ–‡æ¡£ç´¢å¼•åˆ°å‘é‡æ•°æ®åº“"""
        for doc in documents:
            # æ–‡æ¡£åˆ†å‰²
            chunks = self.chunk_document(doc, chunk_size=512)
            
            for chunk in chunks:
                # å‘é‡åŒ–
                embedding = self.embedding_model.encode(chunk)
                
                # å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
                self.vector_db.insert(
                    id=generate_id(),
                    vector=embedding,
                    content=chunk,
                    metadata={"source": doc.source}
                )
    
    def chunk_document(self, doc: str, chunk_size: int):
        """æ–‡æ¡£åˆ†å‰²ç­–ç•¥"""
        # ç®€å•æ»‘åŠ¨çª—å£åˆ†å‰²
        chunks = []
        words = doc.split()
        
        for i in range(0, len(words), chunk_size):
            chunk = " ".join(words[i:i + chunk_size])
            chunks.append(chunk)
            
        return chunks
```

## RAG æ¶æ„æ¼”è¿›

### Naive RAG

æœ€åˆçš„ RAG æ¶æ„ï¼Œæµç¨‹ç®€å•ä½†å­˜åœ¨æ˜æ˜¾å±€é™ï¼š

**ä¼˜ç‚¹ï¼š**
- å®ç°ç®€å•ï¼Œæ˜“äºç†è§£
- å¿«é€ŸåŸå‹éªŒè¯

**é—®é¢˜ï¼š**
- æ£€ç´¢ç²¾åº¦ä½ï¼šå•ä¸€å‘é‡ç›¸ä¼¼åº¦åˆ¤æ–­
- ä¸Šä¸‹æ–‡åˆ©ç”¨å·®ï¼šç®€å•æ‹¼æ¥æ–‡æ¡£
- ç¼ºä¹æ¨ç†èƒ½åŠ›ï¼šæ— æ³•å¤„ç†å¤æ‚æŸ¥è¯¢

### Advanced RAG

é’ˆå¯¹ Naive RAG çš„é—®é¢˜ï¼Œå¼•å…¥é¢„å¤„ç†å’Œåå¤„ç†ä¼˜åŒ–ï¼š

```python
class AdvancedRAG:
    def __init__(self, embedding_model, llm, vector_db, reranker):
        self.embedding_model = embedding_model
        self.llm = llm
        self.vector_db = vector_db
        self.reranker = reranker
        
    def query(self, question: str):
        # é¢„æ£€ç´¢ï¼šæŸ¥è¯¢å¢å¼º
        enhanced_queries = self.query_expansion(question)
        
        all_docs = []
        for query in enhanced_queries:
            # æ£€ç´¢
            query_embedding = self.embedding_model.encode(query)
            docs = self.vector_db.search(query_embedding, k=20)
            all_docs.extend(docs)
        
        # å»é‡
        unique_docs = self.deduplicate_documents(all_docs)
        
        # é‡æ’åº
        reranked_docs = self.reranker.rerank(question, unique_docs)
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = self.build_context(reranked_docs[:5])
        
        # ç”Ÿæˆç­”æ¡ˆ
        response = self.generate_with_citation(question, context)
        
        return response
    
    def query_expansion(self, question: str):
        """æŸ¥è¯¢æ‰©å±•"""
        # æ–¹æ³•1: LLM ç”Ÿæˆç›¸å…³é—®é¢˜
        expansion_prompt = f"""
        åŸºäºé—®é¢˜ï¼š{question}
        ç”Ÿæˆ3ä¸ªç›¸å…³çš„æœç´¢æŸ¥è¯¢ï¼Œç”¨äºæ£€ç´¢æ›´å…¨é¢çš„ä¿¡æ¯ï¼š
        """
        expanded = self.llm.generate(expansion_prompt)
        
        # æ–¹æ³•2: åŒä¹‰è¯æ‰©å±•
        synonyms = self.get_synonyms(question)
        
        return [question] + expanded.split('\n') + synonyms
    
    def build_context(self, docs):
        """æ™ºèƒ½ä¸Šä¸‹æ–‡æ„å»º"""
        # æŒ‰ç›¸å…³æ€§æ’åº
        sorted_docs = sorted(docs, key=lambda x: x.score, reverse=True)
        
        # æ§åˆ¶ä¸Šä¸‹æ–‡é•¿åº¦
        context_parts = []
        total_tokens = 0
        max_tokens = 4000
        
        for doc in sorted_docs:
            doc_tokens = len(doc.content.split())
            if total_tokens + doc_tokens > max_tokens:
                break
                
            context_parts.append(f"æ–‡æ¡£{len(context_parts)+1}: {doc.content}")
            total_tokens += doc_tokens
        
        return "\n\n".join(context_parts)
```

### Modular RAG

æ¨¡å—åŒ–æ¶æ„ï¼Œæ”¯æŒçµæ´»çš„ç»„ä»¶ç»„åˆå’Œæµç¨‹å®šåˆ¶ï¼š

```python
class ModularRAG:
    def __init__(self):
        self.modules = {}
        self.pipeline = []
    
    def add_module(self, name: str, module):
        """æ·»åŠ æ¨¡å—"""
        self.modules[name] = module
    
    def set_pipeline(self, pipeline: List[str]):
        """è®¾ç½®å¤„ç†æµæ°´çº¿"""
        self.pipeline = pipeline
    
    async def process(self, query: str):
        """æŒ‰æµæ°´çº¿æ‰§è¡Œæ¨¡å—"""
        context = {"query": query, "documents": [], "response": ""}
        
        for module_name in self.pipeline:
            module = self.modules[module_name]
            context = await module.process(context)
        
        return context["response"]

# ç¤ºä¾‹æ¨¡å—
class QueryRewriteModule:
    def __init__(self, llm):
        self.llm = llm
    
    async def process(self, context):
        original_query = context["query"]
        
        # æŸ¥è¯¢é‡å†™
        rewrite_prompt = f"""
        å°†ä»¥ä¸‹æŸ¥è¯¢é‡å†™ä¸ºæ›´é€‚åˆæ£€ç´¢çš„å½¢å¼ï¼š
        åŸæŸ¥è¯¢ï¼š{original_query}
        é‡å†™æŸ¥è¯¢ï¼š
        """
        
        rewritten = await self.llm.generate(rewrite_prompt)
        context["rewritten_query"] = rewritten
        return context

class HybridRetrievalModule:
    def __init__(self, dense_retriever, sparse_retriever):
        self.dense_retriever = dense_retriever
        self.sparse_retriever = sparse_retriever
    
    async def process(self, context):
        query = context.get("rewritten_query", context["query"])
        
        # ç¨ å¯†æ£€ç´¢
        dense_docs = await self.dense_retriever.search(query, k=10)
        
        # ç¨€ç–æ£€ç´¢ 
        sparse_docs = await self.sparse_retriever.search(query, k=10)
        
        # èåˆç»“æœ
        all_docs = self.fusion_strategy(dense_docs, sparse_docs)
        context["documents"] = all_docs
        
        return context
    
    def fusion_strategy(self, dense_docs, sparse_docs):
        """ç»“æœèåˆç­–ç•¥ - RRF (Reciprocal Rank Fusion)"""
        doc_scores = {}
        
        # ç¨ å¯†æ£€ç´¢ç»“æœ
        for rank, doc in enumerate(dense_docs):
            doc_id = doc.id
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + 1 / (rank + 1)
        
        # ç¨€ç–æ£€ç´¢ç»“æœ  
        for rank, doc in enumerate(sparse_docs):
            doc_id = doc.id
            doc_scores[doc_id] = doc_scores.get(doc_id, 0) + 1 / (rank + 1)
        
        # æŒ‰èåˆåˆ†æ•°æ’åº
        sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
        return [self.get_doc_by_id(doc_id) for doc_id, _ in sorted_docs]

# é…ç½® Modular RAG
rag = ModularRAG()
rag.add_module("query_rewrite", QueryRewriteModule(llm))
rag.add_module("retrieval", HybridRetrievalModule(dense_retriever, sparse_retriever))
rag.add_module("rerank", RerankModule(reranker))
rag.add_module("generation", GenerationModule(llm))

rag.set_pipeline(["query_rewrite", "retrieval", "rerank", "generation"])
```

## å…³é”®ç»„ä»¶æ‹†è§£

### 1. æ–‡æ¡£å¤„ç†ç»„ä»¶

```python
class DocumentProcessor:
    def __init__(self):
        self.chunking_strategies = {
            "fixed": self.fixed_size_chunking,
            "semantic": self.semantic_chunking,
            "recursive": self.recursive_chunking
        }
    
    def semantic_chunking(self, text: str, similarity_threshold: float = 0.5):
        """åŸºäºè¯­ä¹‰çš„æ™ºèƒ½åˆ†å—"""
        sentences = self.split_sentences(text)
        embeddings = self.embedding_model.encode(sentences)
        
        chunks = []
        current_chunk = [sentences[0]]
        
        for i in range(1, len(sentences)):
            similarity = cosine_similarity(embeddings[i-1], embeddings[i])
            
            if similarity < similarity_threshold:
                # è¯­ä¹‰è·³è·ƒï¼Œå¼€å§‹æ–°å—
                chunks.append(" ".join(current_chunk))
                current_chunk = [sentences[i]]
            else:
                current_chunk.append(sentences[i])
        
        chunks.append(" ".join(current_chunk))
        return chunks
    
    def recursive_chunking(self, text: str, max_size: int = 1000):
        """é€’å½’åˆ†å—ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§"""
        if len(text) <= max_size:
            return [text]
        
        # å°è¯•åœ¨æ®µè½è¾¹ç•Œåˆ†å‰²
        paragraphs = text.split('\n\n')
        if len(paragraphs) > 1:
            return self._recursive_split(paragraphs, max_size)
        
        # å°è¯•åœ¨å¥å­è¾¹ç•Œåˆ†å‰²
        sentences = text.split('.')
        if len(sentences) > 1:
            return self._recursive_split(sentences, max_size)
        
        # å¼ºåˆ¶åˆ†å‰²
        return [text[i:i+max_size] for i in range(0, len(text), max_size)]
```

### 2. æ£€ç´¢ç»„ä»¶æ¶æ„

```python
class RetrievalEngine:
    def __init__(self):
        self.retrievers = {}
        self.fusion_method = "rrf"  # reciprocal rank fusion
    
    def add_retriever(self, name: str, retriever):
        self.retrievers[name] = retriever
    
    async def hybrid_search(self, query: str, retrievers: List[str] = None):
        if retrievers is None:
            retrievers = list(self.retrievers.keys())
        
        results = {}
        for retriever_name in retrievers:
            retriever = self.retrievers[retriever_name]
            results[retriever_name] = await retriever.search(query)
        
        return self.fuse_results(results)
    
    def fuse_results(self, results: Dict[str, List]):
        """å¤šè·¯æ£€ç´¢ç»“æœèåˆ"""
        if self.fusion_method == "rrf":
            return self._reciprocal_rank_fusion(results)
        elif self.fusion_method == "weighted":
            return self._weighted_fusion(results)
        else:
            raise ValueError(f"Unknown fusion method: {self.fusion_method}")
```

### 3. ç”Ÿæˆç»„ä»¶ä¼˜åŒ–

```python
class GenerationModule:
    def __init__(self, llm):
        self.llm = llm
        self.prompt_templates = {
            "qa": self._build_qa_prompt,
            "summarization": self._build_summary_prompt,
            "reasoning": self._build_reasoning_prompt
        }
    
    def _build_qa_prompt(self, question: str, context: str):
        return f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºæä¾›çš„èƒŒæ™¯ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

è¦æ±‚ï¼š
1. ç­”æ¡ˆå¿…é¡»åŸºäºèƒŒæ™¯ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ å†…å®¹
2. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè¯·æ˜ç¡®è¯´æ˜  
3. æä¾›å…·ä½“çš„å¼•ç”¨å‡ºå¤„

èƒŒæ™¯ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}

ç­”æ¡ˆï¼š"""
    
    def generate_with_reasoning(self, question: str, context: str):
        """å¸¦æ¨ç†é“¾çš„ç”Ÿæˆ"""
        reasoning_prompt = f"""
        åˆ†æä»¥ä¸‹é—®é¢˜å’ŒèƒŒæ™¯ä¿¡æ¯ï¼ŒæŒ‰æ­¥éª¤æ¨ç†ï¼š

        1. é—®é¢˜åˆ†æï¼šè¿™ä¸ªé—®é¢˜åœ¨é—®ä»€ä¹ˆï¼Ÿ
        2. ä¿¡æ¯æ¢³ç†ï¼šèƒŒæ™¯ä¿¡æ¯ä¸­å“ªäº›éƒ¨åˆ†ç›¸å…³ï¼Ÿ
        3. é€»è¾‘æ¨ç†ï¼šå¦‚ä½•ä»ä¿¡æ¯æ¨å¯¼å‡ºç­”æ¡ˆï¼Ÿ
        4. ç»“è®ºæ€»ç»“ï¼šæœ€ç»ˆç­”æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿ

        èƒŒæ™¯ä¿¡æ¯ï¼š{context}
        é—®é¢˜ï¼š{question}
        
        é€æ­¥æ¨ç†ï¼š
        """
        
        reasoning = self.llm.generate(reasoning_prompt)
        
        # åŸºäºæ¨ç†ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        answer_prompt = f"""
        åŸºäºä»¥ä¸‹æ¨ç†è¿‡ç¨‹ï¼Œæä¾›ç®€æ´æ˜ç¡®çš„æœ€ç»ˆç­”æ¡ˆï¼š
        
        æ¨ç†è¿‡ç¨‹ï¼š{reasoning}
        é—®é¢˜ï¼š{question}
        
        æœ€ç»ˆç­”æ¡ˆï¼š
        """
        
        return self.llm.generate(answer_prompt), reasoning
```

## å…¸å‹åº”ç”¨åœºæ™¯

### 1. ä¼ä¸šçŸ¥è¯†é—®ç­”

```python
class EnterpriseRAG:
    def __init__(self):
        self.document_types = {
            "policy": PolicyProcessor(),
            "technical": TechnicalDocProcessor(),  
            "faq": FAQProcessor()
        }
    
    def build_enterprise_kb(self, documents):
        """æ„å»ºä¼ä¸šçŸ¥è¯†åº“"""
        for doc in documents:
            processor = self.document_types[doc.type]
            processed_chunks = processor.process(doc)
            
            for chunk in processed_chunks:
                # æ·»åŠ ä¼ä¸šç‰¹å®šå…ƒæ•°æ®
                chunk.metadata.update({
                    "department": doc.department,
                    "access_level": doc.access_level,
                    "last_updated": doc.last_updated
                })
                
                self.vector_db.insert(chunk)
```

### 2. ä»£ç é—®ç­”ç³»ç»Ÿ

```python
class CodeRAG:
    def __init__(self):
        self.code_parsers = {
            "python": PythonParser(),
            "javascript": JSParser(),
            "java": JavaParser()
        }
    
    def index_codebase(self, repo_path: str):
        """ç´¢å¼•ä»£ç åº“"""
        for file_path in self.scan_code_files(repo_path):
            language = self.detect_language(file_path)
            parser = self.code_parsers[language]
            
            # è§£æä»£ç ç»“æ„
            functions = parser.extract_functions(file_path)
            classes = parser.extract_classes(file_path)
            
            # ä¸ºæ¯ä¸ªä»£ç å•å…ƒåˆ›å»ºç´¢å¼•
            for func in functions:
                embedding = self.code_embedding_model.encode(
                    func.signature + func.docstring + func.body
                )
                
                self.vector_db.insert(
                    vector=embedding,
                    content=func.body,
                    metadata={
                        "type": "function",
                        "name": func.name,
                        "file": file_path,
                        "language": language,
                        "signature": func.signature
                    }
                )
```

## é¢è¯•å¸¸è§é—®é¢˜

### Q1: RAG ç›¸æ¯”ç›´æ¥ä½¿ç”¨ LLM æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ
**A**: RAG ä¸»è¦è§£å†³äº† LLM çš„å‡ ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š1ï¼‰çŸ¥è¯†æˆªæ­¢ï¼šå¯ä»¥è·å–æœ€æ–°ä¿¡æ¯ï¼›2ï¼‰å¹»è§‰é—®é¢˜ï¼šåŸºäºæ£€ç´¢åˆ°çš„çœŸå®æ–‡æ¡£å›ç­”ï¼›3ï¼‰é¢†åŸŸçŸ¥è¯†ï¼šå¯ä»¥æ³¨å…¥ç§æœ‰æˆ–ä¸“ä¸šçŸ¥è¯†ï¼›4ï¼‰å¯è§£é‡Šæ€§ï¼šæä¾›ä¿¡æ¯æ¥æºå’Œå¼•ç”¨ï¼›5ï¼‰æˆæœ¬æ§åˆ¶ï¼šæ— éœ€é‡æ–°è®­ç»ƒå¤§æ¨¡å‹ã€‚

### Q2: Naive RAGã€Advanced RAG å’Œ Modular RAG çš„ä¸»è¦åŒºåˆ«ï¼Ÿ
**A**: Naive RAG æ˜¯åŸºç¡€æµç¨‹ï¼ˆæ£€ç´¢â†’æ‹¼æ¥â†’ç”Ÿæˆï¼‰ï¼Œå­˜åœ¨æ£€ç´¢ç²¾åº¦ä½ã€ä¸Šä¸‹æ–‡åˆ©ç”¨å·®ç­‰é—®é¢˜ã€‚Advanced RAG å¼•å…¥æŸ¥è¯¢å¢å¼ºã€é‡æ’åºã€ç»“æœèåˆç­‰ä¼˜åŒ–ã€‚Modular RAG é‡‡ç”¨æ¨¡å—åŒ–æ¶æ„ï¼Œæ”¯æŒçµæ´»çš„ç»„ä»¶ç»„åˆå’Œæµç¨‹å®šåˆ¶ï¼Œå¯ä»¥æ ¹æ®å…·ä½“åœºæ™¯é€‰æ‹©æœ€ä¼˜çš„æ¨¡å—ç»„åˆã€‚

### Q3: å¦‚ä½•è¯„ä¼° RAG ç³»ç»Ÿçš„æ€§èƒ½ï¼Ÿ
**A**: RAG è¯„ä¼°åŒ…æ‹¬å¤šä¸ªç»´åº¦ï¼š1ï¼‰æ£€ç´¢è´¨é‡ï¼šRecall@Kã€Precision@Kã€MRRï¼›2ï¼‰ç”Ÿæˆè´¨é‡ï¼šBLEUã€ROUGEã€BERTScoreï¼›3ï¼‰ç«¯åˆ°ç«¯æ€§èƒ½ï¼šAnswer Accuracyã€F1 Scoreï¼›4ï¼‰ç”¨æˆ·ä½“éªŒï¼šå“åº”æ—¶é—´ã€ç›¸å…³æ€§è¯„åˆ†ï¼›5ï¼‰å¯ä¿¡åº¦ï¼šäº‹å®å‡†ç¡®æ€§ã€æ¥æºå¯é æ€§ã€‚éœ€è¦ç»“åˆè‡ªåŠ¨åŒ–æŒ‡æ ‡å’Œäººå·¥è¯„ä¼°ã€‚

### Q4: RAG ä¸­çš„æ£€ç´¢ç­–ç•¥æœ‰å“ªäº›ï¼Ÿ
**A**: ä¸»è¦åŒ…æ‹¬ï¼š1ï¼‰ç¨ å¯†æ£€ç´¢ï¼šåŸºäºå‘é‡ç›¸ä¼¼åº¦ï¼ˆBERTã€Sentence-BERTï¼‰ï¼›2ï¼‰ç¨€ç–æ£€ç´¢ï¼šåŸºäºå…³é”®è¯åŒ¹é…ï¼ˆBM25ã€TF-IDFï¼‰ï¼›3ï¼‰æ··åˆæ£€ç´¢ï¼šç»“åˆç¨ å¯†å’Œç¨€ç–æ–¹æ³•ï¼›4ï¼‰åˆ†é˜¶æ®µæ£€ç´¢ï¼šç²—æ’+ç²¾æ’ï¼›5ï¼‰[[AI/RAG/æ£€ç´¢ç­–ç•¥|æ£€ç´¢ç­–ç•¥]]ï¼šHyDEã€Query Expansionã€Multi-vectorç­‰é«˜çº§ç­–ç•¥ã€‚

### Q5: RAG ç³»ç»Ÿåœ¨ç”Ÿäº§ç¯å¢ƒä¸­çš„ä¸»è¦æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ
**A**: ä¸»è¦æŒ‘æˆ˜åŒ…æ‹¬ï¼š1ï¼‰å»¶è¿Ÿæ§åˆ¶ï¼šæ£€ç´¢å’Œç”Ÿæˆçš„å¹³è¡¡ï¼›2ï¼‰æˆæœ¬ä¼˜åŒ–ï¼šå‘é‡æ•°æ®åº“å’Œ LLM è°ƒç”¨æˆæœ¬ï¼›3ï¼‰æ•°æ®æ›´æ–°ï¼šå®æ—¶æ€§ä¸ä¸€è‡´æ€§ï¼›4ï¼‰è´¨é‡ä¿è¯ï¼šæ£€ç´¢ç²¾åº¦å’Œç”Ÿæˆè´¨é‡ï¼›5ï¼‰è§„æ¨¡æ‰©å±•ï¼šå¤§è§„æ¨¡çŸ¥è¯†åº“çš„ç´¢å¼•å’ŒæŸ¥è¯¢æ•ˆç‡ï¼›6ï¼‰å®‰å…¨éšç§ï¼šæ•æ„Ÿä¿¡æ¯çš„è®¿é—®æ§åˆ¶å’Œæ•°æ®æ³„éœ²é˜²æŠ¤ã€‚

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) â€” Lewis et al. 2020ï¼ŒRAG çš„å¥ åŸºè®ºæ–‡ï¼Œå¿…è¯»
- [Dense Passage Retrieval for Open-Domain QA (DPR)](https://arxiv.org/abs/2004.04906) â€” Karpukhin et al. 2020ï¼Œç¨ å¯†æ£€ç´¢çš„é‡Œç¨‹ç¢‘

### å®è·µèµ„æº
- [FAISS: A Library for Efficient Similarity Search](https://github.com/facebookresearch/faiss) â€” Meta å‡ºå“ï¼Œå‘é‡æ£€ç´¢çš„åŸºç¡€è®¾æ–½
- [LlamaIndex Documentation](https://docs.llamaindex.ai/) â€” RAG ä¸“ç”¨æ¡†æ¶ â­â­â­â­â­
- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/) â€” å…¥é—¨é¦–é€‰

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **ä¼ä¸šçŸ¥è¯†åº“**ï¼šå°†å†…éƒ¨æ–‡æ¡£ï¼ˆPDF/Word/Confluenceï¼‰ç´¢å¼•åæ”¯æŒè‡ªç„¶è¯­è¨€é—®ç­”
- **ä»£ç é—®ç­”**ï¼šå¯¹ä»“åº“å»ºç´¢å¼•ï¼Œæ”¯æŒ"è¿™ä¸ªå‡½æ•°åšä»€ä¹ˆ""å“ªé‡Œå¤„ç†äº†è¶…æ—¶é€»è¾‘"ç­‰æŸ¥è¯¢
- **å®¢æœç³»ç»Ÿ**ï¼šåŸºäº FAQ + äº§å“æ–‡æ¡£çš„è‡ªåŠ¨é—®ç­”ï¼Œå¯é™„å¸¦ citation æå‡å¯ä¿¡åº¦

### å·¥ç¨‹å®ç°è¦ç‚¹
- **å‘é‡æ•°æ®åº“å¿«é€Ÿé€‰å‹**ï¼šåŸå‹ç”¨ Chromaï¼ˆpip install å³ç”¨ï¼‰ï¼›ç”Ÿäº§ç”¨ Qdrant/Milvusï¼›å·²æœ‰ PG ç”¨ pgvector
- **Chunking åŸºçº¿**ï¼šRecursive Splitting + chunk_size=512 + overlap=15%
- **å¿…åŠ  Reranking**ï¼šCross-Encoder Reranker é€šå¸¸æå‡ Hit Rate@5 çº¦ 5-15%

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: RAG ç›¸æ¯”ç›´æ¥ç”¨ LLM çš„ä¼˜åŠ¿ï¼Ÿ
  A: çŸ¥è¯†å®æ—¶æ›´æ–° + å‡å°‘å¹»è§‰ï¼ˆåŸºäºæ£€ç´¢æ–‡æ¡£ï¼‰ + é¢†åŸŸçŸ¥è¯†æ³¨å…¥ + å¯å¼•ç”¨æº¯æº + æ— éœ€é‡è®­æ¨¡å‹

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- RAG æ˜¯"è®© LLM è®¿é—®ç§æœ‰æ•°æ®"çš„æœ€æˆç†Ÿè·¯å¾„ï¼Œå‡ ä¹æ‰€æœ‰ä¼ä¸š AI åº”ç”¨éƒ½éœ€è¦
- ç†è§£ Naiveâ†’Advancedâ†’Modular çš„æ¶æ„æ¼”è¿›ï¼Œèƒ½åœ¨é¢è¯•ä¸­å±•ç¤ºç³»ç»Ÿè®¾è®¡èƒ½åŠ›

### æœªè§£é—®é¢˜ä¸å±€é™
- æ–‡æ¡£è§£æè´¨é‡ä»æ˜¯ç“¶é¢ˆâ€”â€”PDF è¡¨æ ¼ã€æ‰«æä»¶çš„è§£æå‡†ç¡®ç‡å·®è·æ‚¬æ®Š
- è¯„ä¼°ä½“ç³»ä¸å®Œå–„â€”â€”RAGAS çš„ LLM-as-Judge å­˜åœ¨è¯„ä¼°å™¨åå·®

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- ç»“åˆ [[AI/RAG/æ£€ç´¢ç­–ç•¥|æ£€ç´¢ç­–ç•¥]] çš„ HyDE/Query Decomposition å¯æ˜¾è‘—æå‡å¤æ‚æŸ¥è¯¢çš„æ•ˆæœ
- Agentic RAGï¼ˆå‚è§ [[AI/RAG/RAG-2026-æŠ€æœ¯å…¨æ™¯|RAG 2026 å…¨æ™¯]]ï¼‰è®©æ£€ç´¢ä»å›ºå®šç®¡çº¿å˜æˆåŠ¨æ€å†³ç­–

> ğŸ”— See also: [[AI/LLM/Application/Embedding ä¸å‘é‡æ£€ç´¢]] â€” Embedding é€‰å‹å’Œå‘é‡æ•°æ®åº“æ˜¯ RAG æ£€ç´¢è´¨é‡çš„åŸºåº§
> ğŸ”— See also: [[AI/RAG/æ–‡æ¡£è§£æ]] â€” æ–‡æ¡£è§£ææ˜¯ RAG ç®¡çº¿çš„èµ·ç‚¹ï¼Œè´¨é‡ç›´æ¥å†³å®šæ£€ç´¢ä¸Šé™