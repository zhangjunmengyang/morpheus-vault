---
title: "RAG 检索增强生成 2026 技术全景（面试武器版）"
tags: [rag, retrieval, vector-search, embedding, interview-prep]
date: 2026-02-20
---

# RAG 检索增强生成 2026 技术全景（面试武器版）

> 这篇笔记以「面试官会问」为引导线索，覆盖 RAG 从原理到生产的完整知识图谱。
> 适合 30 分钟快速过一遍，也适合逐节深入。

---

## 1. RAG 是什么？为什么长上下文没有杀死 RAG

### 面试官会问

> "现在 Gemini 已经有 2M token context window 了，RAG 还有存在的必要吗？"

### 1.1 RAG 的本质定义

**Retrieval-Augmented Generation（检索增强生成）** 是一种将 **信息检索（IR）** 与 **大语言模型（LLM）** 相结合的架构范式。核心思路：

1. 用户提出 Query
2. 系统从外部知识库中**检索**与 Query 相关的文档/片段
3. 将检索结果作为 Context 注入 Prompt
4. LLM 基于 Context 生成最终回答

这个概念最早由 Meta（Facebook AI Research）在 2020 年的论文 *"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"* 中正式提出。RAG 的核心价值在于：让 LLM **不再只靠参数记忆，而是能实时访问外部知识**。

### 1.2 为什么长上下文没有杀死 RAG

这是 2025-2026 最高频的 debate。答案是多维的：

**成本维度：**
- 将 100 万 token 塞进 context window，每次请求的成本是惊人的。以 GPT-4o 为例，1M input tokens 约 $2.5；而 RAG 只需检索 Top-K（通常 5-20 个 chunk，约 2K-8K tokens），成本降低 100-500 倍。
- 长上下文的推理延迟与 token 数近似线性增长。RAG 的 LLM 侧延迟几乎恒定。

**准确度维度（Lost in the Middle）：**
- 经典研究 *"Lost in the Middle"*（Liu et al., 2023）证明：LLM 对放在 context 中间的信息检索准确率显著下降。即使 context window 够大，也不意味着模型能**有效利用**所有信息。
- 2025 年的后续研究表明，虽然新模型（如 Claude 3.5、GPT-4o）的 "Lost in the Middle" 效应有所减弱，但在 100K+ tokens 时仍然存在显著的注意力衰减。

**实时性维度：**
- 长上下文需要将所有数据预加载。但真实业务中，数据是**持续更新**的（新闻、文档、日志）。RAG 天然支持增量索引，无需重新组装整个 context。

**权限与安全维度：**
- 企业场景中，不同用户有不同的数据访问权限。RAG 可以在检索阶段做 **fine-grained access control**；而长上下文方案难以在 prompt 级别实现行级权限隔离。

**可解释性维度：**
- RAG 天然提供 **citation**：每段生成内容可以追溯到具体的源文档和段落。这对合规性（金融、医疗、法律）至关重要。

### 1.3 面试答题模板

> RAG 和长上下文不是竞争关系，而是互补。长上下文适合处理「已知的、量可控的、不需要频繁更新的」文档（如分析一份合同）；RAG 适合处理「海量的、动态更新的、需要权限控制的」知识库（如企业知识管理）。实际生产中，最优解往往是 **RAG + 适当长度的上下文窗口**——用 RAG 做精准检索，用长上下文做深度理解。

---

## 2. RAG 架构演进

### 面试官会问

> "RAG 从最早的原型到现在经历了哪些架构演进？Agentic RAG 和传统 RAG 有什么本质区别？"

### 2.1 Naive RAG（2020-2022）

最原始的 RAG 架构，也叫 **Vanilla RAG**：

```
Query → Embedding → Vector Search (Top-K) → Concat to Prompt → LLM → Answer
```

**特点：**
- 单次检索，无迭代
- 直接拼接检索结果，无排序/过滤
- Chunk 策略简单（固定长度切分）
- 无 Query 改写

**问题：**
- 检索质量严重依赖 Query 和文档的语义匹配度
- Top-K 中噪声文档会干扰生成质量（"Garbage In, Garbage Out"）
- 无法处理需要多跳推理的复杂问题
- 对 Query 歧义或模糊表述敏感

### 2.2 Advanced RAG（2023-2024）

在 Naive RAG 基础上，**针对检索质量**做了系统性增强：

```
Query → Query Rewrite/Expansion → Hybrid Search → Reranking → Filtering → Prompt Engineering → LLM → Answer
```

**关键改进：**

| 改进点 | 技术 | 解决的问题 |
|--------|------|-----------|
| Pre-Retrieval | Query Rewrite、HyDE、Query Expansion | 弥合 Query 和文档之间的语义鸿沟 |
| Retrieval | Hybrid Search（Dense + Sparse） | 兼顾语义理解和关键词精确匹配 |
| Post-Retrieval | Cross-Encoder Reranker | 精排，过滤噪声文档 |
| Chunking | Semantic Chunking、Recursive Splitting | 保持语义完整性 |
| Prompt | Few-shot、Chain-of-Thought | 引导 LLM 更好地利用 Context |

### 2.3 Modular RAG（2024-2025）

Modular RAG 将 RAG 系统拆解为**可插拔的模块**，每个模块可以独立优化和替换：

```
┌────────────┐    ┌─────────────┐    ┌──────────────┐    ┌───────────┐
│  Indexing   │ →  │  Retrieval   │ →  │ Post-Process │ →  │ Generation│
│  Module     │    │  Module      │    │  Module      │    │  Module   │
└────────────┘    └─────────────┘    └──────────────┘    └───────────┘
     ↑                   ↑                  ↑                  ↑
 多种 Chunker      Dense/Sparse/       Reranker/          多种 LLM
 多种 Embedder     Graph/Table         Compressor/        Prompt Template
                   检索器可替换         Filter 可替换       可替换
```

**核心理念：**
- 每个模块有标准化的 Input/Output 接口
- 可以根据业务需求自由组合（如金融场景加 Compliance Filter，医疗场景加 Evidence Grading）
- 支持 A/B 测试不同模块组合
- 框架代表：LlamaIndex、LangChain、Haystack

### 2.4 Agentic RAG（2025-2026）

**这是当前的前沿方向。** Agentic RAG 将 RAG 从「被动的检索-生成管道」升级为「主动的智能体系统」：

```
User Query
    ↓
┌─────────────────────────────────────────┐
│           RAG Agent (Planner)           │
│                                         │
│  1. 分析 Query 复杂度                    │
│  2. 制定检索策略（单次/多跳/分解）         │
│  3. 选择工具（向量检索/SQL/API/Web）       │
│  4. 执行检索 → 评估结果质量               │
│  5. 不满意 → 自适应调整策略重试            │
│  6. 满意 → 合成最终回答                   │
└─────────────────────────────────────────┘
```

**与传统 RAG 的本质区别：**

| 维度 | 传统 RAG | Agentic RAG |
|------|---------|-------------|
| 检索策略 | 固定 Pipeline | 动态规划 |
| 迭代能力 | 单次检索 | 多轮自适应检索 |
| 工具使用 | 只有向量检索 | 多工具（SQL、API、Web Search、Calculator） |
| 自我评估 | 无 | 检索结果质量自评，决定是否重试 |
| 复杂推理 | 弱 | 支持 Query Decomposition、Multi-hop |
| 代表技术 | — | Self-RAG、CRAG、Adaptive RAG |

**Agentic RAG 的典型代表：**
- **Self-RAG**（Asai et al., 2023）：LLM 自己判断是否需要检索、检索结果是否相关、生成是否 faithful
- **CRAG**（Corrective RAG，Yan et al., 2024）：引入 Knowledge Refinement 步骤，对检索结果做纠正
- **Adaptive RAG**：根据 Query 复杂度动态选择不同的 RAG 策略（No Retrieval / Single-step / Multi-step）

### 2.5 面试答题框架

> 四代演进的核心逻辑是：**从固定管道到动态智能体**。Naive RAG 解决了"LLM 如何访问外部知识"的问题；Advanced RAG 解决了"如何检索得更准"的问题；Modular RAG 解决了"如何让系统更灵活可维护"的问题；Agentic RAG 解决了"如何让系统自主决策检索策略"的问题。不是每个场景都需要最新的架构——简单 FAQ 场景 Naive RAG 足够，复杂知识推理才需要 Agentic RAG。

---

## 3. 检索器设计

### 面试官会问

> "Dense Retrieval 和 Sparse Retrieval 的本质区别是什么？你在生产中怎么选 Embedding 模型？"

### 3.1 Sparse Retrieval

**核心思想：** 基于词频统计的倒排索引检索。

**代表算法：**
- **TF-IDF：** 经典的词频-逆文档频率
- **BM25：** TF-IDF 的改进版，引入文档长度归一化和饱和函数，至今仍是 Sparse Retrieval 的 SOTA
- **SPLADE（2021-2023）：** 学习式稀疏表示，用 BERT 学习每个 token 的 term importance weight，兼具可解释性和语义理解

**优势：**
- 精确关键词匹配能力强（如产品型号 "iPhone 15 Pro Max"、法律条文编号）
- 可解释性好——可以看到匹配了哪些 term
- 零样本泛化能力好，不依赖领域特定训练数据
- 推理速度快，内存开销可控

**劣势：**
- 无法理解同义词和语义关系（"car" vs "automobile"）
- 对 Query 的表述方式敏感

### 3.2 Dense Retrieval

**核心思想：** 将 Query 和 Document 分别编码为稠密向量（Dense Embedding），用向量相似度（通常是 Cosine Similarity 或 Inner Product）衡量相关性。

**经典架构：Bi-Encoder**
```
Query  → Encoder → q_vec ──┐
                            ├→ similarity(q_vec, d_vec)
Doc    → Encoder → d_vec ──┘
```

**优势：**
- 语义理解能力强，能捕捉同义词、上下位关系、隐含语义
- Document Embedding 可以离线预计算，检索时只需计算 Query Embedding + ANN Search

**劣势：**
- 对分布外（OOD）数据泛化能力有限
- 需要大量标注数据 fine-tune 才能在特定领域达到最佳效果
- 精确匹配能力弱（如专有名词、ID）

### 3.3 Hybrid Retrieval（生产首选）

**核心思想：** 融合 Sparse 和 Dense 的优势。

**常见融合策略：**

1. **Score Fusion（分数融合）：**
   ```
   final_score = α * dense_score + (1 - α) * sparse_score
   ```
   - α 通常在 0.5-0.7 之间（偏重语义检索）
   - 需要对两种分数做归一化（Min-Max 或 Z-Score）

2. **Reciprocal Rank Fusion（RRF）：**
   ```
   RRF_score(d) = Σ 1 / (k + rank_i(d))
   ```
   - k 通常取 60（原论文推荐值）
   - 不需要分数归一化，只依赖排名
   - 对 outlier 不敏感，实践中效果稳定
   - **生产中最常用的融合方法**

3. **Learned Fusion：** 用一个小模型学习最优融合权重

### 3.4 Embedding 模型选型指南（2025-2026）

| 模型 | 维度 | 最大 tokens | 多语言 | 许可证 | 适用场景 |
|------|------|------------|--------|--------|---------|
| **BGE-M3** (BAAI) | 1024 | 8192 | ✅ 100+ 语言 | MIT | 中文场景首选，支持 Dense+Sparse+ColBERT 三模态检索 |
| **BGE-large-en-v1.5** | 1024 | 512 | ❌ 英文为主 | MIT | 英文高精度场景 |
| **E5-Mistral-7B** (Microsoft) | 4096 | 32768 | ✅ | MIT | 长文档检索，LLM-based Embedding |
| **Jina-embeddings-v3** | 可变(64-1024) | 8192 | ✅ | Apache 2.0 | Matryoshka Embedding，支持维度自适应 |
| **Cohere embed-v4** | 1024 | 512 | ✅ | 商业 API | 生产级 API 首选，内置 Binary Quantization |
| **text-embedding-3-large** (OpenAI) | 3072 | 8191 | ✅ | 商业 API | OpenAI 生态内首选 |
| **voyage-3** (Voyage AI) | 1024 | 32000 | ✅ | 商业 API | 代码检索和长文档场景 |
| **GTE-Qwen2-7B** (Alibaba) | 可变 | 131072 | ✅ | Apache 2.0 | 超长上下文 Embedding |

**选型决策树：**

```
需要中文？
├── 是 → BGE-M3（开源首选）/ Cohere embed-v4（API 首选）
└── 否 → 需要长文档？
          ├── 是 → E5-Mistral-7B / GTE-Qwen2-7B / voyage-3
          └── 否 → 预算充足？
                    ├── 是 → Cohere embed-v4 / OpenAI text-embedding-3-large
                    └── 否 → BGE-large-en-v1.5 / Jina-embeddings-v3
```

### 3.5 关键趋势：Late Interaction 和 Multi-Vector

**ColBERT / ColBERTv2：**
- 不是单向量表示，而是为每个 token 生成独立向量
- 检索时做 token-level 的 MaxSim 交互
- 精度介于 Bi-Encoder 和 Cross-Encoder 之间，速度接近 Bi-Encoder
- BGE-M3 已经内置了 ColBERT 模式

**Matryoshka Representation Learning（MRL）：**
- 训练时让模型在不同维度截断点都保持高质量表示
- 部署时可以根据存储/速度需求选择维度（如 256d / 512d / 1024d）
- Jina v3 和 OpenAI text-embedding-3 已支持

### 3.6 面试答题要点

> Dense Retrieval 核心优势是语义理解，Sparse 的核心优势是精确匹配和可解释性。生产环境中 Hybrid Retrieval + RRF 是最稳妥的方案。Embedding 模型选型要考虑四个维度：语言覆盖、最大 token 长度、维度/存储成本、开源 vs 商业 API。中文场景 BGE-M3 几乎是默认选择。

---

## 4. 向量数据库对比

### 面试官会问

> "向量数据库这么多，你怎么选型？pgvector 够用吗？什么时候需要专用向量数据库？"

### 4.1 主流向量数据库全景

| 数据库 | 类型 | 语言 | ANN 算法 | 标量过滤 | 分布式 | 许可证 | 亮点 |
|--------|------|------|---------|---------|--------|--------|------|
| **Milvus** | 专用 | Go/C++ | HNSW, IVF, DiskANN, GPU | ✅ 强 | ✅ 原生 | Apache 2.0 | 大规模生产首选，支持 10B+ 向量 |
| **Qdrant** | 专用 | Rust | HNSW | ✅ 强 | ✅ | Apache 2.0 | 性能极致，Payload 过滤优秀 |
| **Weaviate** | 专用 | Go | HNSW | ✅ GraphQL | ✅ | BSD-3 | 内置向量化模块，开箱即用 |
| **Pinecone** | 托管服务 | — | 专有 | ✅ | ✅ 托管 | 商业 | 零运维，Serverless 模式 |
| **Chroma** | 轻量 | Python | HNSW | ✅ 基础 | ❌ | Apache 2.0 | 原型开发首选，嵌入应用 |
| **pgvector** | PG 扩展 | C | IVFFlat, HNSW | ✅ SQL | ✅ (PG集群) | PostgreSQL | 已有 PG 基础设施时首选 |
| **LanceDB** | 嵌入式 | Rust | IVF-PQ, DiskANN | ✅ | ❌ | Apache 2.0 | 零拷贝，多模态原生支持 |

### 4.2 深入对比

**Milvus：**
- 阿里云、腾讯云、字节等大厂大规模使用
- Zilliz Cloud 是其托管版本
- 支持多种索引类型（HNSW / IVF_FLAT / IVF_PQ / DiskANN / GPU_IVF_FLAT）
- 原生支持 Hybrid Search（Dense + Sparse）
- 缺点：部署复杂度较高（依赖 etcd、MinIO、Pulsar），学习曲线陡

**Qdrant：**
- Rust 写的，单机性能极其强悍
- Payload Index 设计优秀，支持复杂的标量过滤条件
- gRPC + REST 双协议
- 支持 Quantization（Scalar / Product / Binary）大幅降低内存
- 缺点：社区规模比 Milvus 小，大规模分布式场景经验较少

**Weaviate：**
- 独特优势：内置 Vectorizer Module，可以自动调用 OpenAI / Cohere / HuggingFace 做向量化
- GraphQL API，对前端开发者友好
- 支持 Generative Module，检索+生成一体化
- 缺点：性能在极大规模下不如 Milvus/Qdrant

**Pinecone：**
- 纯托管，零运维负担
- Serverless 模式按查询计费，冷启动场景成本极低
- 缺点：厂商锁定、数据主权受限、大规模时成本不可控

**pgvector：**
- 对已有 PostgreSQL 技术栈的团队**极其友好**
- HNSW 索引（pgvector 0.5.0+）性能已经非常可用
- 可以直接和业务数据 JOIN，实现检索+过滤的一体化 SQL
- 缺点：超过 1000 万向量时性能下降明显；缺乏专用向量数据库的高级特性（如 Quantization、Multi-tenancy）

### 4.3 选型决策指南

```
你的向量数量级？
├── < 100K → Chroma / LanceDB（嵌入式，零依赖）
├── 100K - 10M → pgvector（已有 PG）/ Qdrant（新建）
├── 10M - 1B → Qdrant / Milvus
└── > 1B → Milvus（分布式原生）

团队运维能力？
├── 不想运维 → Pinecone / Zilliz Cloud
├── 有 K8s 经验 → Milvus / Qdrant / Weaviate
└── 只有 PG → pgvector

是否需要复杂过滤？
├── 是 → Qdrant（Payload Index）/ Milvus / pgvector（SQL WHERE）
└── 否 → 都行
```

### 4.4 关键概念：ANN 算法

**HNSW（Hierarchical Navigable Small World）：**
- 当前最流行的 ANN 算法
- 构建多层图结构，查询时从高层逐层下探
- 参数：`ef_construction`（构建质量）、`M`（连接数）、`ef_search`（查询精度）
- 优点：查询速度快，recall 高
- 缺点：**内存占用大**（需要将整个图加载到内存）

**IVF（Inverted File Index）：**
- 将向量空间分成 N 个 Voronoi Cell
- 查询时只在最近的 `nprobe` 个 Cell 中搜索
- 优点：内存可控（支持 PQ 压缩）
- 缺点：需要训练（聚类），recall 不如 HNSW

**DiskANN：**
- Microsoft 提出，基于 Vamana 图
- 核心优势：**SSD-resident**，只需要少量内存
- 适合超大规模（10B+）、内存受限场景
- Milvus 已原生支持

### 4.5 面试答题要点

> 向量数据库选型没有银弹。核心决策因素是：数据规模、运维能力、是否需要和现有数据栈整合。10M 以下 pgvector 完全够用；10M-1B 选 Qdrant 或 Milvus；超大规模选 Milvus 分布式。Pinecone 适合快速验证但要警惕厂商锁定。

---

## 5. Chunking 策略

### 面试官会问

> "你的文档切分策略是什么？chunk size 怎么选？Semantic Chunking 解决了什么问题？"

### 5.1 为什么 Chunking 如此关键

Chunking 是 RAG 中**最被低估但影响最大**的环节。一个好的 Chunking 策略直接决定了：
- **检索精度：** chunk 太大 → 引入噪声；chunk 太小 → 丢失上下文
- **生成质量：** chunk 边界切在句子中间 → LLM 理解困难
- **token 效率：** chunk 越精准，prompt 中的有效信息密度越高

### 5.2 Chunking 策略全景

#### 5.2.1 Fixed-Size Chunking（固定长度切分）

```python
# 最简单的实现
chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size - overlap)]
```

- **参数：** `chunk_size`（通常 256-1024 tokens）、`overlap`（通常 chunk_size 的 10-20%）
- **优点：** 实现简单，速度快，行为可预测
- **缺点：** 不尊重语义边界，可能把一个段落切成两半
- **适用：** 基线方案、格式统一的文档

#### 5.2.2 Recursive Character Splitting（递归字符切分）

LangChain 默认策略。按照分隔符的优先级递归切分：

```
分隔符优先级：\n\n → \n → 句号 → 空格 → 字符
```

- 先尝试用 `\n\n`（段落）切分，如果 chunk 仍然太大，递归用更细的分隔符
- **优点：** 大多数情况下能保持段落/句子完整性
- **缺点：** 仍然是基于规则的，对无结构文本效果有限
- **这是生产中最常用的通用策略**

#### 5.2.3 Semantic Chunking（语义切分）

基于语义相似度来判断切分点：

```
1. 将文本按句子切分
2. 计算相邻句子的 Embedding 相似度
3. 相似度低于阈值的位置 → 切分点（"语义断裂"）
4. 合并相邻的语义连贯句子为一个 chunk
```

- **优点：** chunk 内语义高度连贯
- **缺点：** 需要调用 Embedding 模型，速度慢；chunk 大小不均匀
- **适用：** 对检索精度要求极高的场景（金融研报、法律文书）

#### 5.2.4 Document-Aware Chunking（文档感知切分）

根据文档的内在结构（标题、章节、表格、代码块）来切分：

- **Markdown/HTML：** 按标题层级切分，保持章节完整
- **PDF：** 先用布局分析（Layout Analysis）识别标题、段落、表格、图片，再按语义块切分
- **代码：** 按函数/类/模块切分（AST-based Chunking）
- **表格：** 将表格整体作为一个 chunk，或序列化为文本

**关键工具：**
- **Unstructured.io：** 支持 PDF、Word、HTML 等多格式的结构化解析
- **DoclingV2：** IBM 出品，PDF 布局分析 + 表格提取
- **LlamaParse：** LlamaIndex 的文档解析服务
- **MarkItDown：** Microsoft 出品，将各种文档转为 Markdown

#### 5.2.5 Agentic Chunking / Proposition Chunking

最新趋势：用 LLM 来做切分。

**Proposition Chunking（Chen et al., 2023）：**
- 将文档分解为**独立的命题（Proposition）**
- 每个 Proposition 是一个自包含的、可独立验证的事实陈述
- 例：原文 "苹果公司由乔布斯在 1976 年创立，总部位于库比蒂诺" → Proposition 1: "苹果公司由乔布斯创立" / Proposition 2: "苹果公司创立于 1976 年" / Proposition 3: "苹果公司总部位于库比蒂诺"
- **优点：** 检索精度极高
- **缺点：** 需要 LLM 调用，成本高，速度慢

### 5.3 Chunk Size 调优

**经验法则：**

| Embedding 模型 Max Tokens | 推荐 Chunk Size | 原因 |
|---------------------------|----------------|------|
| 512 | 256-384 tokens | 留余量给 overlap 和特殊 token |
| 8192 | 512-1024 tokens | 可以更大，但检索精度会下降 |
| 32K+ | 仍建议 512-1024 | 大 chunk 导致检索粒度过粗 |

**调优方法：**
1. 从 512 tokens 开始作为基线
2. 准备一个 Ground Truth 评估集（50-100 个 Query + 对应的 Golden Passage）
3. 在 256 / 384 / 512 / 768 / 1024 五个档位做 Recall@10 对比
4. 选择 Recall 最高的档位
5. 用 overlap = chunk_size * 15% 作为起点

**关键洞察：**
- chunk size 越小，检索精度越高，但上下文完整性越差
- chunk size 越大，上下文越完整，但检索噪声越多
- **没有普适的最优值**——取决于文档类型和 Query 类型

### 5.4 Parent-Child Chunking（父子策略）

一种兼顾检索精度和上下文完整性的方法：

```
Document
├── Parent Chunk（大，如 2048 tokens）
│   ├── Child Chunk 1（小，如 256 tokens）← 用于检索
│   ├── Child Chunk 2（小，如 256 tokens）← 用于检索
│   └── Child Chunk 3（小，如 256 tokens）← 用于检索
```

- **检索阶段：** 用小 chunk（Child）做向量检索，精度高
- **生成阶段：** 命中后，将其 Parent Chunk 送给 LLM，上下文更完整
- LlamaIndex 的 `SentenceWindowNodeParser` 和 `AutoMergingRetriever` 原生支持这个模式

### 5.5 面试答题要点

> Chunking 没有银弹。生产中推荐 Recursive Splitting 作为基线，对高价值文档（金融、法律）升级到 Semantic Chunking 或 Document-Aware Chunking。chunk size 通常 512 tokens 起步，通过 Recall 评估来调优。Parent-Child 策略是兼顾精度和上下文的好方法。关键原则：**chunk 内语义连贯，chunk 间信息独立。**

---

## 6. 重排序与过滤

### 面试官会问

> "为什么检索之后还需要 Reranking？Cross-Encoder 和 Bi-Encoder 的区别是什么？LLM-as-Reranker 效果如何？"

### 6.1 为什么需要 Reranking

**Bi-Encoder 的固有缺陷：**
- Bi-Encoder 将 Query 和 Document **独立编码**，无法捕捉两者之间的细粒度交互
- 本质上是在高维空间中做近似最近邻搜索，**是一种粗排**
- Top-100 的 Recall 可能很高（95%+），但 Top-5 的 Precision 往往不理想

**Reranking 的价值：**
- 在 Bi-Encoder 召回的 Top-K（通常 20-100）结果上做**精排**
- 可以使用更复杂的模型（因为只需要处理几十个候选，而非整个语料库）
- 实测通常能让最终的 Hit Rate@5 提升 **5-15%**

### 6.2 Cross-Encoder Reranker

**原理：** 将 Query 和 Document **拼接**后一起输入 Transformer，输出一个相关性分数：

```
Input: [CLS] Query [SEP] Document [SEP]
Output: relevance_score ∈ [0, 1]
```

**与 Bi-Encoder 的核心区别：**

| | Bi-Encoder | Cross-Encoder |
|---|-----------|---------------|
| 输入方式 | 分别编码 | 拼接编码 |
| 交互层级 | 无交互（独立向量） | 全层交互（Full Attention） |
| 速度 | 快（可预计算 Doc Embedding） | 慢（每对 Query-Doc 都要计算） |
| 精度 | 中等 | 高 |
| 用途 | 召回/粗排 | 精排 |

**主流 Cross-Encoder Reranker 模型：**

| 模型 | 来源 | 参数量 | 亮点 |
|------|------|--------|------|
| **bge-reranker-v2-m3** | BAAI | 568M | 中文效果最好的开源 Reranker |
| **bge-reranker-v2-gemma** | BAAI | 2B | LLM-based，更强但更慢 |
| **ms-marco-MiniLM-L-12-v2** | SBERT | 33M | 轻量级，适合低延迟场景 |
| **jina-reranker-v2** | Jina AI | — | 多语言，支持长文档 |

### 6.3 Cohere Rerank API

Cohere 提供业界最成熟的商业 Reranking API：

```python
import cohere
co = cohere.Client("API_KEY")

results = co.rerank(
    model="rerank-v3.5",
    query="What is RAG?",
    documents=["doc1", "doc2", "doc3"],
    top_n=5,
    return_documents=True
)
```

**优势：**
- 开箱即用，无需模型部署
- 多语言支持优秀
- rerank-v3.5 支持长文档（4096 tokens per doc）
- 支持 Semi-structured Data（JSON）的 Reranking

**定价（2025）：** $2 / 1000 search queries（每次最多 1000 个文档）

### 6.4 LLM-as-Reranker

用 LLM（如 GPT-4、Claude）做 Reranking 的新兴方向：

**方法 1：Pointwise（逐文档评分）**
```
Prompt: "Given the query '{query}', rate the relevance of the following document
         on a scale of 1-10: {document}"
```

**方法 2：Listwise（列表排序）**
```
Prompt: "Given the query '{query}', rank the following documents from most
         relevant to least relevant: [A] {doc1} [B] {doc2} [C] {doc3} ..."
```

**方法 3：Pairwise（两两比较）**
```
Prompt: "Given the query '{query}', which document is more relevant?
         A: {doc1}  B: {doc2}"
```

**LLM-as-Reranker 的优缺点：**
- ✅ 精度最高（LLM 的语义理解能力碾压 Cross-Encoder）
- ✅ 零样本，无需训练
- ❌ 成本高（每次 Rerank 都要调用 LLM）
- ❌ 延迟高（秒级 vs Cross-Encoder 的毫秒级）
- ❌ Listwise 方法受限于 LLM 的 Position Bias

**实践建议：** 在延迟不敏感的离线场景（如索引质量评估）中使用 LLM-as-Reranker；在线场景优先用 Cross-Encoder。

### 6.5 Context Compression

除了 Reranking，另一个重要的 Post-Retrieval 技术是 **Context Compression**：

- **LongLLMLingua（2023）：** 用小模型识别并删除 context 中的冗余 token，压缩率可达 4-10 倍
- **Extractive Compression：** 从检索到的文档中只提取与 Query 最相关的句子/段落
- **Abstractive Compression：** 让 LLM 总结检索结果，生成更紧凑的 context

### 6.6 面试答题要点

> Reranking 是 RAG 中 ROI 最高的优化——实现简单、效果显著。Cross-Encoder 是精排的标配，生产中通常先用 Bi-Encoder 召回 Top-50/100，再用 Cross-Encoder 精排到 Top-5/10。Cohere Rerank 是商业方案首选。LLM-as-Reranker 精度最高但成本也最高，适合离线场景。

---

## 7. 高级 RAG 技术

### 面试官会问

> "你了解 Self-RAG 吗？HyDE 的原理是什么？Graph RAG 解决了什么问题？"

### 7.1 HyDE（Hypothetical Document Embedding）

**论文：** *"Precise Zero-Shot Dense Retrieval without Relevance Labels"*（Gao et al., 2022）

**核心思想：** 让 LLM 先**生成一个假设性的答案文档**，再用这个假设文档的 Embedding 去做检索。

```
Query: "What causes aurora borealis?"
    ↓
LLM generates hypothetical answer:
"Aurora borealis is caused by charged particles from the sun
 interacting with Earth's magnetic field..."
    ↓
Embed the hypothetical answer → Search with this embedding
    ↓
Retrieve actual documents that are semantically similar to the hypothesis
```

**为什么有效：**
- Query 通常很短且可能是疑问句，和文档的语义分布不同（Query-Document Mismatch）
- 假设文档和真实文档在语义空间中更接近
- 即使假设文档内容不准确，其**语义分布**通常和正确文档相似

**局限：**
- 需要额外一次 LLM 调用（增加延迟和成本）
- 对于事实性很强的 Query 效果好，对于观点性 / 探索性 Query 效果一般
- 如果 LLM 生成的假设完全偏离方向，反而会降低检索质量

### 7.2 Self-RAG（Self-Reflective RAG）

**论文：** *"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection"*（Asai et al., 2023）

**核心创新：** LLM 在生成过程中输出**特殊反思 token（Reflection Tokens）**，实现自我评估和自适应检索。

**四种 Reflection Token：**

| Token | 含义 | 值 |
|-------|------|-----|
| `[Retrieve]` | 是否需要检索 | Yes / No / Continue |
| `[IsRel]` | 检索到的文档是否与 Query 相关 | Relevant / Irrelevant |
| `[IsSup]` | 生成内容是否被检索文档支持 | Fully / Partially / No Support |
| `[IsUse]` | 最终回答是否有用 | 5 / 4 / 3 / 2 / 1 |

**工作流程：**
```
1. 收到 Query
2. LLM 判断 [Retrieve] = Yes → 触发检索
3. 获取文档后，LLM 判断 [IsRel] → 如果 Irrelevant，重新检索或换策略
4. 基于相关文档生成回答，同时输出 [IsSup]
5. 如果 [IsSup] = No Support → 丢弃这段生成，重试
6. 最终输出 [IsUse] 作为整体质量评估
```

**意义：** Self-RAG 是 Agentic RAG 的理论基础之一，让 LLM 具备了**自主决策检索时机和评估检索质量**的能力。

### 7.3 CRAG（Corrective RAG）

**论文：** *"Corrective Retrieval Augmented Generation"*（Yan et al., 2024）

**核心思想：** 在生成之前，对检索结果做**纠正性评估和处理**。

**三种策略：**

1. **Correct：** 检索结果高度相关 → 提取关键信息，过滤噪声
2. **Incorrect：** 检索结果完全不相关 → 丢弃检索结果，转向 Web Search 获取新信息
3. **Ambiguous：** 相关性模糊 → 同时保留检索结果和 Web Search 结果，综合判断

```
Query → Retrieval → Relevance Evaluator
                        ├── Correct → Knowledge Refinement → Generate
                        ├── Incorrect → Web Search → Generate
                        └── Ambiguous → Merge(Retrieval + Web Search) → Generate
```

### 7.4 Query Decomposition（查询分解）

**针对复杂问题的预处理技术：**

**方法 1：Sub-Question Decomposition**
```
原始 Query: "比较 Tesla Model 3 和 BYD Seal 的续航、价格和安全评级"
分解为：
  Q1: "Tesla Model 3 的续航里程是多少？"
  Q2: "BYD Seal 的续航里程是多少？"
  Q3: "Tesla Model 3 的价格是多少？"
  Q4: "BYD Seal 的价格是多少？"
  Q5: "Tesla Model 3 的安全评级如何？"
  Q6: "BYD Seal 的安全评级如何？"
→ 分别检索 → 合并结果 → 生成对比答案
```

**方法 2：Step-Back Prompting**
```
原始 Query: "在 2025 年 1 月 15 日，NVIDIA 的股价是多少？"
Step-Back: "NVIDIA 2025 年 1 月的股价走势如何？"
→ 退一步问更宽泛的问题，检索到更全面的信息
```

**方法 3：Chain-of-Retrieval**
```
Q: "哪些诺贝尔物理学奖得主曾在 MIT 工作？"
Step 1: 检索 "MIT 的著名物理学家"
Step 2: 基于结果，检索 "Richard Feynman 诺贝尔奖"（Feynman 在 MIT 工作过）
Step 3: 继续迭代...
```

### 7.5 Graph RAG

**核心思想：** 用 **Knowledge Graph（知识图谱）** 替代或增强向量检索，利用实体之间的关系进行推理。

**Microsoft GraphRAG（2024）：**

```
文档集合
    ↓ (LLM 抽取)
Knowledge Graph (Entity → Relationship → Entity)
    ↓ (Leiden Community Detection)
Community Summaries (不同粒度的社区摘要)
    ↓
Query → 选择相关 Community → 基于 Community Summary + Graph Traversal → Generate
```

**两种查询模式：**
- **Local Search：** 从 Query 相关的实体出发，遍历局部子图，适合具体问题
- **Global Search：** 使用全局 Community Summaries，适合需要综合全局信息的问题（如 "这份报告的主要发现是什么？"）

**Graph RAG 解决的问题：**
- **Multi-hop Reasoning：** "A 公司的 CEO 的母校的校训是什么？" → 需要 A 公司 → CEO → 母校 → 校训 的多跳推理
- **全局摘要：** 向量检索只能找到局部相关的 chunk，难以回答需要全局视角的问题
- **关系推理：** "X 和 Y 之间有什么关系？" → 图结构天然支持

**局限：**
- 知识图谱构建成本高（需要 LLM 抽取实体和关系，大规模语料成本惊人）
- 图谱质量高度依赖 LLM 的抽取准确率
- 维护和增量更新比向量索引复杂

### 7.6 其他值得关注的高级技术

**RAG Fusion：**
- 对 Query 生成多个变体（用 LLM 改写）
- 每个变体分别检索
- 用 RRF 融合所有检索结果
- 简单有效，实现成本低

**RAPTOR（Recursive Abstractive Processing for Tree-Organized Retrieval）：**
- 构建文档的**层次化摘要树**
- 叶子节点是原始 chunk，上层节点是摘要
- 检索时可以在不同层级上进行，兼顾细节和宏观

**Contextual Retrieval（Anthropic，2024）：**
- 在 Embedding 之前，让 LLM 为每个 chunk 生成一段 **Contextual Description**
- 例：chunk 原文是一段代码 → LLM 生成 "这段代码实现了用户认证的 JWT 验证逻辑，位于 auth 模块中"
- 显著提高检索精度（Anthropic 报告 Recall 提升 49%）

**Dense-X Retrieval / Proposition Indexing：**
- 将文档分解为原子化的 Proposition，为每个 Proposition 建立索引
- 与 Proposition Chunking 配合使用

### 7.7 面试答题框架

> 高级 RAG 技术可以分为三个方向：(1) 检索前优化——HyDE、Query Decomposition、RAG Fusion；(2) 检索后优化——Self-RAG、CRAG 的自我评估和纠正机制；(3) 结构化增强——Graph RAG 利用知识图谱做关系推理和全局摘要。选择哪个技术取决于具体问题类型：简单事实查询不需要 Graph RAG，多跳推理不需要 HyDE。

---

## 8. 评估体系

### 面试官会问

> "你怎么评估 RAG 系统的效果？RAGAS 是什么？有哪些关键指标？"

### 8.1 为什么 RAG 评估很难

RAG 系统的评估难度远超单独评估 LLM 或检索引擎，因为需要**同时评估检索质量和生成质量**，而且两者之间存在复杂的交互关系：
- 检索好 + 生成差 = 有好的 context 但生成了不忠实的回答
- 检索差 + 生成好 = LLM 靠自身知识回答了正确答案，但不是 RAG 的功劳
- 检索好 + 生成好 = 理想情况
- 检索差 + 生成差 = 需要诊断是哪个环节的问题

### 8.2 RAGAS 框架

**RAGAS（Retrieval Augmented Generation Assessment）** 是当前最主流的 RAG 评估框架。

**核心指标：**

| 指标 | 评估什么 | 需要 Ground Truth? | 计算方式 |
|------|---------|-------------------|---------|
| **Faithfulness** | 生成的回答是否忠实于检索到的 Context | ❌ | LLM 判断 answer 中的每个 claim 是否能被 context 支持 |
| **Answer Relevancy** | 生成的回答是否与 Query 相关 | ❌ | LLM 从 answer 反向生成问题，比较与原 Query 的相似度 |
| **Context Precision** | 检索到的相关文档是否排在前面 | ✅ | 相关文档的排名加权 Precision |
| **Context Recall** | Ground Truth 中的信息是否被检索到的 Context 覆盖 | ✅ | LLM 判断 ground truth 的每个 claim 是否出现在 context 中 |

**RAGAS 使用示例：**

```python
from ragas import evaluate
from ragas.metrics import faithfulness, answer_relevancy, context_precision, context_recall

result = evaluate(
    dataset=eval_dataset,  # 包含 question, answer, contexts, ground_truth
    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],
    llm=ChatOpenAI(model="gpt-4o"),
    embeddings=OpenAIEmbeddings()
)
print(result)
# {'faithfulness': 0.87, 'answer_relevancy': 0.92, 'context_precision': 0.78, 'context_recall': 0.85}
```

### 8.3 更完整的指标体系

**检索质量指标：**

| 指标 | 定义 | 场景 |
|------|------|------|
| **Recall@K** | 在 Top-K 结果中，相关文档的覆盖率 | 最基础的检索指标 |
| **Precision@K** | Top-K 中相关文档的比例 | 衡量噪声程度 |
| **MRR（Mean Reciprocal Rank）** | 第一个相关文档的排名的倒数的平均值 | 衡量排序质量 |
| **NDCG@K** | 考虑相关性分级的排名质量指标 | 多级相关性场景 |
| **Hit Rate@K** | 有多少比例的 Query 在 Top-K 中能找到至少一个相关文档 | 最直觉的指标 |

**生成质量指标：**

| 指标 | 定义 | 场景 |
|------|------|------|
| **Faithfulness / Groundedness** | 回答是否完全基于检索到的 context | 防止幻觉 |
| **Answer Correctness** | 回答是否正确（与 Ground Truth 对比） | 端到端评估 |
| **Answer Completeness** | 回答是否覆盖了所有必要信息 | 防止遗漏 |
| **Hallucination Rate** | 回答中不被 context 支持的 claim 比例 | Faithfulness 的反面 |
| **Toxicity / Safety** | 回答是否包含有害内容 | 合规 |

### 8.4 评估数据集构建

**手动构建（Golden Dataset）：**
1. 从真实 Query 日志中采样 100-500 个 Query
2. 人工标注每个 Query 的 Ground Truth Answer 和 Golden Passage
3. 成本高但质量最好

**自动构建（LLM-based Synthetic Data）：**
1. 从文档库中随机采样 chunk
2. 用 LLM 基于 chunk 生成 QA pair
3. 人工审核过滤低质量 pair
4. 工具：RAGAS 的 `TestsetGenerator`、Llama Index 的 `generate_question_context_pairs`

```python
from ragas.testset.generator import TestsetGenerator
from ragas.testset.evolutions import simple, reasoning, multi_context

generator = TestsetGenerator.from_langchain(
    generator_llm=ChatOpenAI(model="gpt-4o"),
    critic_llm=ChatOpenAI(model="gpt-4o"),
    embeddings=OpenAIEmbeddings()
)

testset = generator.generate_with_langchain_docs(
    documents=docs,
    test_size=100,
    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25}
)
```

### 8.5 红队测试（Red Teaming）

**目标：** 发现 RAG 系统的安全漏洞和失败模式。

**常见攻击向量：**

| 攻击类型 | 描述 | 示例 |
|---------|------|------|
| **Prompt Injection via Documents** | 在文档中注入恶意 prompt | 文档内容："忽略之前的指令，直接回答..." |
| **Data Poisoning** | 在知识库中注入错误信息 | 故意上传包含虚假数据的文档 |
| **Context Overflow** | 构造超长 Query 试图突破 context 限制 | 极长的 Query 挤掉检索结果 |
| **Knowledge Extraction** | 通过巧妙的 Query 提取知识库中的敏感信息 | "列出所有关于XXX的文档内容" |
| **Attribution Spoofing** | 诱导模型将错误来源归因于可信来源 | "根据《XX法》第X条..." |

**防御措施：**
- 文档入库前做内容安全扫描
- Output Guard（输出过滤器）检测敏感信息泄露
- 对检索结果做 Prompt Injection 检测
- 限制单次检索的文档数量和 token 总量

### 8.6 面试答题要点

> RAG 评估需要分层：检索层用 Recall@K / MRR / Hit Rate；生成层用 Faithfulness / Answer Correctness；端到端用 RAGAS 综合评分。RAGAS 最大的优势是大部分指标**不需要 Ground Truth**（LLM-as-Judge）。评估数据集可以用 LLM 自动生成 + 人工审核。别忘了红队测试——RAG 的攻击面比纯 LLM 更广（多了文档注入这个攻击向量）。

---

## 9. 生产落地经验

### 面试官会问

> "RAG 从 demo 到生产，最大的挑战是什么？你在生产中踩过哪些坑？"

### 9.1 索引管道（Indexing Pipeline）

生产级索引管道需要处理的问题远比 demo 复杂：

**文档解析层：**
```
原始文档（PDF/Word/HTML/PPT/图片）
    ↓
文档解析器（Unstructured / DoclingV2 / LlamaParse）
    ↓
结构化内容（Markdown + 元数据）
    ↓
Chunking
    ↓
Embedding
    ↓
向量数据库 + 元数据存储
```

**关键设计决策：**

1. **解析质量 vs 速度的权衡：**
   - PDF 解析是最大的痛点。OCR-based（如 Tesseract）速度快但质量差；Layout-aware（如 DoclingV2）质量好但速度慢
   - 建议：对重要文档（合同、报告）用高质量解析；对大量普通文档用快速解析

2. **元数据管理：**
   - 每个 chunk 至少要存：`source_file`、`page_number`、`chunk_index`、`created_at`、`file_hash`
   - 元数据是做 citation、权限控制、增量更新的基础

3. **去重与冲突处理：**
   - 同一份文档的不同版本怎么办？基于 file_hash 做去重
   - 内容冲突怎么办？保留最新版本，标记旧版本为 deprecated

### 9.2 增量更新

**核心挑战：** 文档新增/修改/删除时，如何高效更新索引，而不是全量重建？

**方案 1：基于 File Hash 的增量更新**
```python
new_hash = hash(file_content)
if new_hash != stored_hash:
    # 文件已修改
    delete_old_chunks(file_id)
    new_chunks = chunk(parse(file_content))
    embed_and_store(new_chunks)
    update_hash(file_id, new_hash)
```

**方案 2：Change Data Capture（CDC）**
- 监听文档存储系统的变更事件（如 S3 Event、Kafka）
- 异步处理变更，更新索引
- 适合大规模、高频更新场景

**方案 3：版本化索引**
- 每次更新创建新版本的索引
- 通过 alias（别名）切换活跃版本
- 支持快速回滚
- Elasticsearch 原生支持这个模式

### 9.3 多模态 RAG

**2025-2026 的热门方向：** 不仅检索文本，还检索图片、表格、图表。

**架构选项：**

**方案 A：统一 Embedding 空间**
- 使用多模态 Embedding 模型（如 CLIP、Jina-CLIP、voyage-multimodal-3）
- 将文本和图片映射到同一向量空间
- 优点：检索逻辑统一
- 缺点：图文对齐质量有限

**方案 B：描述转化**
- 用 Vision LLM（GPT-4o、Claude 3.5）将图片/表格/图表转为文本描述
- 对文本描述做 Embedding 和检索
- 优点：可以利用文本检索的全部优化
- 缺点：信息损失、成本高

**方案 C：混合索引**
- 文本走文本索引，图片走图片索引
- 检索时并行查询两个索引，融合结果
- 在生成阶段用多模态 LLM 同时理解文本和图片

**生产推荐：** 方案 B + C 混合。对表格和图表用 Vision LLM 生成描述（方案 B），同时保留原始图片供生成阶段参考（方案 C）。

### 9.4 权限控制（Access Control）

**企业 RAG 的核心需求：** 不同用户只能检索到自己有权限访问的文档。

**实现方案：**

**方案 1：Pre-filtering（检索前过滤）**
```
User Query + User ACL → Vector DB Filter (metadata.acl CONTAINS user.groups) → Search
```
- 在向量搜索时加元数据过滤条件
- Qdrant、Milvus、Weaviate 都支持
- 优点：安全性最高（未授权文档永远不会被检索到）
- 缺点：可能影响检索效率（过滤条件复杂时）

**方案 2：Post-filtering（检索后过滤）**
```
User Query → Vector Search (no filter) → 结果过滤 (check ACL) → Return filtered results
```
- 缺点：可能导致 Top-K 结果不足（被过滤掉太多）
- 适合权限结构简单的场景

**方案 3：Per-Tenant Index（租户级索引）**
- 每个租户/部门一个独立的索引
- 优点：隔离性最强
- 缺点：索引维护成本高，跨租户搜索困难

**推荐：** 方案 1（Pre-filtering）是大多数场景的最优解。将 ACL 信息作为 chunk 的元数据存储，检索时带上用户的权限标签做过滤。

### 9.5 缓存与性能优化

**Embedding 缓存：**
- 对高频 Query 缓存其 Embedding，避免重复计算
- 使用 Redis 或本地 LRU Cache
- 注意：Query 的微小变化也会导致不同的 Embedding，考虑做 Query Normalization

**语义缓存（Semantic Cache）：**
- 不是精确匹配 Query，而是判断新 Query 和缓存 Query 的**语义相似度**
- 如果相似度 > 阈值，直接返回缓存的回答
- 工具：GPTCache、Redis 的 Vector Similarity Search

**检索结果缓存：**
- 缓存 (query_embedding, top_k_results) 对
- 适合检索请求远多于索引更新的场景
- 设置合理的 TTL（如 1 小时），索引更新时主动失效

### 9.6 Observability（可观测性）

**必须监控的指标：**

| 指标 | 含义 | 告警阈值（参考） |
|------|------|----------------|
| Retrieval Latency（P50/P95/P99） | 检索延迟 | P99 > 500ms |
| Generation Latency | 生成延迟 | P99 > 5s |
| Retrieval Empty Rate | 检索结果为空的比例 | > 10% |
| Faithfulness Score | 生成忠实度（采样评估） | < 0.8 |
| User Feedback (Thumbs Up/Down) | 用户满意度 | Down Rate > 20% |

**推荐工具栈：**
- **LangSmith / LangFuse：** RAG Pipeline 的全链路追踪
- **Phoenix (Arize)：** Embedding 可视化和漂移检测
- **Prometheus + Grafana：** 基础设施指标监控

### 9.7 常见踩坑与解法

| 坑 | 症状 | 解法 |
|----|------|------|
| PDF 解析质量差 | 表格变乱码，标题丢失 | 换 DoclingV2 / LlamaParse；对重要文档人工审核 |
| Chunk 边界不合理 | 回答断断续续，缺乏上下文 | 用 Parent-Child Chunking；增大 overlap |
| Embedding 模型和业务领域不匹配 | 检索准确率低 | 在领域数据上 Fine-tune Embedding 模型 |
| 检索到了相关文档但 LLM 没用 | 明明 context 里有答案但生成了错误回答 | 优化 Prompt；换更强的 LLM；减少 context 噪声 |
| 索引更新延迟 | 新上传的文档搜不到 | 实时索引管道 / Near-real-time CDC |
| 向量数据库性能劣化 | 查询变慢 | 检查索引参数；考虑 Quantization 或分片 |

### 9.8 面试答题要点

> 从 demo 到生产最大的挑战不是算法，而是工程：文档解析质量参差不齐、增量更新的一致性、权限控制的安全性、系统的可观测性。我在生产中学到的最重要教训是：**先把评估体系建好，再做优化**。没有评估体系的优化是盲目的。另外，Chunking 策略和 Reranking 的 ROI 通常是最高的——在这两个环节花时间调优比换一个更贵的 LLM 更有效。

---

## 10. 面试高频题 12 道 + 参考答案

### Q1: RAG 和 Fine-tuning 怎么选？

**答：** 两者解决不同的问题：
- **RAG** 解决「知识更新」和「知识扩展」问题——让 LLM 访问它训练时没见过的信息。适合：企业内部文档、实时数据、需要 citation 的场景。
- **Fine-tuning** 解决「行为调整」和「格式定制」问题——教 LLM 以特定的风格/格式/逻辑来回答。适合：领域术语理解、输出格式固定、特定推理链。
- **最佳实践：RAG + Fine-tuning 结合。** 用 Fine-tuning 让模型更好地理解领域和格式，用 RAG 提供最新的知识。

### Q2: 如何处理 RAG 中的幻觉问题？

**答：** 多层防御：
1. **检索层：** 提高检索精度（Hybrid Search + Reranking），确保 LLM 拿到高质量 context
2. **Prompt 层：** 明确要求 "只基于提供的 context 回答，如果 context 中没有相关信息，请说'我不知道'"
3. **生成层：** 使用 Self-RAG 或 CRAG 的自我评估机制，检测生成内容是否被 context 支持
4. **后处理层：** 用 NLI（Natural Language Inference）模型验证 answer 和 context 的一致性
5. **产品层：** 始终显示 citation/source，让用户能验证

### Q3: Embedding 模型需要 Fine-tune 吗？什么时候需要？

**答：**
- **大多数通用场景不需要。** 2025-2026 年的通用 Embedding 模型（BGE-M3、Cohere embed-v4）已经足够强。
- **需要 Fine-tune 的场景：** (1) 领域专有术语丰富（如生物医药、法律），通用模型对这些术语的语义理解不够；(2) Query 和 Document 的分布差异很大（如 Query 是口语化的，Document 是学术论文）；(3) 评估后发现 Recall 明显不达标。
- **Fine-tune 方法：** 准备 (query, positive_doc, negative_doc) 三元组，用 Contrastive Learning 训练。工具：Sentence-Transformers 的 `MultipleNegativesRankingLoss`。

### Q4: 如何优化 RAG 的检索延迟？

**答：** 分层优化：
1. **Embedding 计算：** 使用 ONNX Runtime 或 TensorRT 加速推理；对高频 Query 做 Embedding 缓存
2. **向量搜索：** 调优 HNSW 参数（降低 `ef_search`）；使用 Quantization（PQ / SQ / Binary）减少内存和计算量
3. **Reranking：** 使用轻量级 Reranker（如 MiniLM-L-12）；减少 Rerank 的候选数量（如从 Top-100 降到 Top-20）
4. **架构层面：** 语义缓存避免重复计算；预计算热门 Query 的检索结果
5. **基准：** 端到端检索延迟（含 Reranking）应控制在 **200-500ms** 以内

### Q5: 向量数据库的 HNSW 参数怎么调？

**答：**
- **`M`（每个节点的最大连接数）：** 默认 16，增大到 32-64 可提高 Recall 但增加内存和构建时间
- **`ef_construction`（构建时的搜索宽度）：** 默认 200，增大可提高图质量但构建更慢。建议 ≥ 2 * M
- **`ef_search`（查询时的搜索宽度）：** 直接影响查询精度和速度的 trade-off。增大提高 Recall 但增加延迟
- **调优策略：** 先设定目标 Recall（如 95%），然后逐步增大 `ef_search` 直到达标，同时监控延迟是否可接受

### Q6: RAG 如何处理多语言文档？

**答：**
- **Embedding 选型：** 使用多语言 Embedding 模型（BGE-M3、Cohere embed-v4），它们能将不同语言映射到同一语义空间
- **检索策略：** 用户用中文提问也能检索到英文文档（cross-lingual retrieval）
- **生成策略：** 在 Prompt 中指定输出语言；对检索到的外语文档，可以先翻译再送给 LLM
- **Chunking 注意：** 不同语言的 tokenization 差异大——中文按字/词切分，英文按空格/subword 切分。确保 chunk size 的计算方式一致（用 token 数而不是字符数）

### Q7: 如何评估 RAG 系统的检索质量？不用 Ground Truth 行不行？

**答：**
- **有 Ground Truth 时：** Recall@K、MRR、NDCG@K 是标准指标
- **没有 Ground Truth 时：** 
  - RAGAS 的 Faithfulness 和 Answer Relevancy **不需要 Ground Truth**
  - 可以用 LLM-as-Judge：让 GPT-4 评估检索结果和 Query 的相关性
  - 用户反馈（thumbs up/down）是最真实的评估信号
- **建议：** 初期用 LLM-as-Judge 快速迭代，同时逐步积累 Golden Dataset 用于更严格的离线评估

### Q8: 什么是 Chunking 的 "Goldilocks Zone"？

**答：**
- Chunk 太小：每个 chunk 信息量不足，LLM 缺乏上下文理解；检索到的 Top-K 可能来自同一段落的不同部分，冗余严重
- Chunk 太大：检索粒度太粗，引入大量噪声；token 浪费严重
- **"Goldilocks Zone"** 通常在 **256-1024 tokens** 之间，具体取决于文档类型和 Query 类型
- 唯一可靠的确定方法是：**用评估集做实验**。没有 one-size-fits-all 的答案

### Q9: Graph RAG 和传统 Vector RAG 的适用边界是什么？

**答：**
- **Vector RAG 擅长：** 事实查找（"X 是什么？"）、相似性搜索（"找类似的案例"）、局部信息检索
- **Graph RAG 擅长：** 多跳推理（"A 的 B 的 C 是什么？"）、全局摘要（"这个领域的主要趋势是什么？"）、关系查询（"X 和 Y 之间有什么联系？"）
- **实践中：** 大多数场景 Vector RAG 足够。Graph RAG 的构建和维护成本高，只在确实需要关系推理或全局视角时才值得引入
- **混合方案：** Vector RAG 做主检索，Graph RAG 做补充（特别是多跳问题）

### Q10: RAG 系统的 Prompt Engineering 有哪些最佳实践？

**答：**
```
System Prompt 结构：

1. 角色定义
   "你是 XX 领域的专家助手，基于提供的参考资料回答用户问题。"

2. 行为约束
   "只基于【参考资料】中的信息回答。如果参考资料中没有相关信息，请明确说明'根据现有资料无法回答'。"

3. 引用要求
   "每个关键论述后标注来源，格式为 [来源X]。"

4. 格式要求
   "回答使用中文，技术术语保留英文。使用 Markdown 格式。"

5. 参考资料注入
   "【参考资料】
    [来源1] {chunk_1}
    [来源2] {chunk_2}
    ..."

6. 用户问题
   "【用户问题】{query}"
```

**关键原则：**
- 将参考资料放在 Prompt 中间偏后的位置（减轻 Lost in the Middle 效应）
- 限制 context 数量（通常 5-10 个 chunk 最优，过多反而降低质量）
- 要求 citation 能有效减少幻觉

### Q11: 如何处理 RAG 中的表格数据？

**答：**
- **方案 1：表格序列化为文本。** 将表格转为 Markdown Table 或自然语言描述。简单但信息可能丢失
- **方案 2：Text-to-SQL。** 如果表格数据在数据库中，将用户 Query 转为 SQL 查询。适合结构化查询
- **方案 3：表格专用 Embedding。** 使用支持表格理解的模型（如 TAPAS、TableFormer），将表格作为整体做 Embedding
- **方案 4：Vision LLM 解析。** 对 PDF 中的表格，用 GPT-4o/Claude 做 Vision 解析，提取结构化信息
- **生产推荐：** 方案 4 + 方案 1 结合。用 Vision LLM 解析表格 → 转为 Markdown Table → 作为 chunk 存储

### Q12: 2026 年 RAG 的发展方向是什么？

**答：**

1. **Agentic RAG 成为主流：** RAG 不再是固定的 Pipeline，而是由 Agent 动态编排检索策略。Self-RAG、CRAG、Adaptive RAG 的思想正在被主流框架（LangGraph、LlamaIndex Workflows）原生支持。

2. **多模态 RAG：** 不仅检索文本，还检索图片、视频、音频。随着多模态 LLM（GPT-4o、Gemini 2.0）的成熟，多模态 RAG 的实用性大幅提升。

3. **Graph RAG + Vector RAG 融合：** 知识图谱和向量检索不再是二选一，而是在同一系统中协同工作。Neo4j 的 GraphRAG 集成、LlamaIndex 的 PropertyGraphIndex 是代表。

4. **端到端优化：** 不再分别优化检索器和生成器，而是用 RLHF / DPO 端到端训练整个 RAG 系统。代表：RA-DIT（Retrieval-Augmented Dual Instruction Tuning）。

5. **RAG-as-a-Service：** 越来越多的平台提供一站式 RAG 托管服务（Cohere、Pinecone、AWS Bedrock Knowledge Base），降低开发门槛。

6. **长上下文 + RAG 的协同：** 利用长上下文窗口做"粗检索"（将更多候选文档放入 context），用 LLM 的长距离注意力做"精排序"，两者优势互补。

---

## 附录：技术栈速查

### 常用框架

| 框架 | 语言 | 特点 |
|------|------|------|
| **LlamaIndex** | Python | RAG 专用，抽象层丰富，支持多种 Data Connector |
| **LangChain** | Python/JS | 通用 LLM 框架，RAG 是其中一个模块 |
| **Haystack** | Python | Deepset 出品，Pipeline 设计优雅，生产级 |
| **RAGFlow** | Python | 国产开源，端到端 RAG 平台，内置文档解析 |
| **Dify** | Python | 国产 LLMOps 平台，低代码 RAG 构建 |

### 推荐学习路径

```
入门：LangChain RAG Tutorial → LlamaIndex Documentation
进阶：Self-RAG Paper → CRAG Paper → Graph RAG Paper
生产：LangSmith 可观测性 → RAGAS 评估 → 向量数据库深入
前沿：Agentic RAG → Multimodal RAG → RAG + Long Context
```

---

> **最后的面试建议：** 不要只背概念，要能说出"我在项目中做了什么、为什么这么选、效果如何"。面试官最想听到的是 **trade-off 思维** 和 **实践经验**。如果没有实际项目经验，至少要做过一个完整的 RAG demo（从文档解析到评估），能说出具体的数字（如 "Reranking 让 Hit Rate@5 从 72% 提升到 86%"）。

---

*Written for Morpheus Vault · February 2026*

## See Also

- [[AI/RAG/_MOC|RAG MOC]] — 检索增强生成全景索引
- [[AI/RAG/Advanced RAG|Advanced RAG]] — 进阶检索技术详解
- [[AI/RAG/RAG 检索策略|RAG 检索策略]] — 检索层策略详解
- [[AI/RAG/向量数据库选型|向量数据库选型]] — 基础设施选型
- [[AI/RAG/RAG-Anything-Multimodal-RAG-Framework|RAG-Anything]] — 多模态 RAG 前沿扩展（arXiv:2503.17347）
