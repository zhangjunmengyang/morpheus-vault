---
title: "T5"
brief: "Google 2019 年提出 T5（Text-to-Text Transfer Transformer）：把所有 NLP 任务统一为 text-to-text 格式（分类/翻译/摘要/QA 全部如此），Encoder-Decoder 架构，预训练用 span corruption 掩码目标。是 instruction tuning 和 prompt engineering 的思想先驱；后续 Flan-T5 系列用 instruction finetuning 进一步强化。"
type: paper
domain: ai/llm/architecture
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm/architecture
  - type/paper
---
# T5

## 概述

T5（Text-to-Text Transfer Transformer）是 Google 在 2019 年发布的模型，论文标题 *"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"*。核心思想极其简洁：**把所有 NLP 任务统一为 text-to-text 格式**。

分类？输入 "classify: ..."，输出 "positive"。翻译？输入 "translate English to German: ..."，输出德语文本。摘要、问答、语义相似度，全部如此。这个统一范式对后来的 instruction tuning 和 prompt engineering 影响深远——从某种意义上说，T5 是 ChatGPT 的思想先驱。

## 架构

T5 采用标准的 **Encoder-Decoder Transformer** 架构，这是它与 GPT（decoder-only）和 BERT（encoder-only）的关键区别：

```
Input: "translate English to German: That is good."
       ↓
   [Encoder] → 编码输入序列的上下文表示
       ↓
   [Decoder] → 自回归生成目标序列
       ↓
Output: "Das ist gut."
```

架构细节：
- **相对位置编码**：不用绝对位置编码（sinusoidal 或 learned），而是用相对位置 bias。每个 attention head 学一组 bucket 化的相对位置偏置，共享于所有层。这让模型对序列长度的泛化更好。
- **Layer Norm 位置**：Pre-Norm（在 attention/FFN 之前做 LayerNorm），训练更稳定。
- **没有 bias**：线性层和 LayerNorm 都不用 bias 项。
- **激活函数**：原版用 ReLU，后来的 T5 v1.1 改用 GeGLU（效果更好）。

模型规模从 Small（60M）到 11B，论文系统性地对比了不同大小的效果。

## Text-to-Text 统一范式

这是 T5 最重要的贡献。所有任务都变成：

```
输入: "task prefix: input text"
输出: "target text"
```

具体例子：

| 任务 | 输入 | 输出 |
|------|------|------|
| 情感分类 | `sst2 sentence: this movie is great` | `positive` |
| 翻译 | `translate English to French: Hello` | `Bonjour` |
| 摘要 | `summarize: [长文本]` | `[摘要]` |
| 语义相似 | `stsb sentence1: A cat sits. sentence2: A cat is sitting.` | `5.0` |

这个设计的深远影响：
1. **统一的训练和推理流程**：不需要针对不同任务设计不同的 head
2. **自然支持 few-shot**：输入中可以包含示例
3. **多任务训练变得 trivial**：混合不同任务的数据即可

## 预训练

### C4 数据集

T5 的预训练数据是 **C4（Colossal Clean Crawled Corpus）**——从 Common Crawl 清洗出的约 750GB 英文文本。清洗规则包括：
- 去重
- 过滤低质量页面（短文本、含脏话等）
- 只保留英文
- 去掉代码（以 `{` 出现频率为启发式）

### Span Corruption 目标

T5 的预训练目标是 **span corruption**（span 级别的 denoising）：

```
原文:    "Thank you for inviting me to your party last week"
mask:    "Thank you <X> me to your party <Y> week"
target:  "<X> for inviting <Y> last <Z>"
```

随机 mask 掉连续的 token span（平均长度 3），模型需要预测被 mask 的内容。相比 BERT 的单 token mask，span corruption：
- 计算效率更高（target 序列更短）
- 能学到更长程的依赖

## 论文中的系统性实验

T5 论文的另一大价值是它做了极其全面的消融实验，几乎是一篇 NLP 迁移学习的综述：

1. **架构对比**：Encoder-Decoder vs Decoder-only vs Prefix LM → Encoder-Decoder 在多数任务上最优
2. **预训练目标**：Language Modeling vs BERT-style vs Span Corruption → Span Corruption 效果最好且效率最高
3. **数据规模**：更多数据持续带来提升（不意外）
4. **模型规模**：更大模型持续带来提升（也不意外）
5. **训练步数 vs 模型大小**：给定计算预算，大模型少训好于小模型多训
6. **多任务策略**：mixing rate 的选择很重要，小任务需要上采样

## 后续演进

- **T5 v1.1**：GeGLU 激活函数、无 dropout 预训练、只用 C4（不混合下游数据）
- **mT5**：多语言版本，覆盖 101 种语言，用 mC4 数据集
- **Flan-T5**：instruction tuning 版本，在 1800+ 任务上做指令微调，大幅提升 zero-shot 能力。Flan-T5 某种程度上是 instruction following 的早期标杆。
- **UL2**：统一预训练目标（混合不同 span 长度和 corruption 率）
- **PaLM 2** 等后续 Google 模型也继承了很多 T5 的设计选择

## 工程价值

T5 的 encoder-decoder 架构在以下场景仍然有独特优势：

1. **翻译和摘要**：encoder 充分理解输入，decoder 生成输出，天然适合 seq2seq 任务
2. **信息检索和 Embedding**：encoder 部分可以单独用来做文本编码
3. **显存效率**：相比同参数量的 decoder-only 模型，encoder-decoder 在条件生成任务上更高效（encoder 只需前向一次）

但在 scaling law 的实践中，decoder-only（GPT 路线）最终胜出，原因可能是：
- 更简单的架构更容易 scale
- 纯生成式预训练（next token prediction）数据利用率更高
- Decoder-only 在 in-context learning 上表现更好

## 相关

- Transformer — T5 的基础架构
- Transformer 通识 — Transformer 家族概览
- [[AI/4-模型/Google/BERT|BERT]] — Encoder-only 对比
- [[AI/4-模型/OpenAI/GPT|GPT]] — Decoder-only 对比
- [[AI/3-LLM/Architecture/Attention 变体综述|Attention 详解]] — 注意力机制细节
- Transformer 位置编码详解 — T5 的相对位置编码
- [[AI/4-模型/Meta/LLaMA|LLaMA]]
- [[AI/4-模型/DeepSeek/DeepSeek-R1|DeepSeek-R1]]
- [[AI/6-应用/Embedding/Embedding|Embedding]]
