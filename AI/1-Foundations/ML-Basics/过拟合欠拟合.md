---
brief: "过拟合与欠拟合——偏差-方差分解的直觉理解；正则化（L1/L2/Dropout）的防止过拟合机制；LLM SFT 场景的过拟合（catastrophic forgetting）和欠拟合（学习率过小）的诊断方法。"
title: "过拟合欠拟合"
type: concept
domain: ai/foundations/ml-basics
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/foundations/ml-basics
  - type/concept
---
# 过拟合欠拟合

**如何解决欠拟合：**

1. 添加其他特征项。组合、泛化、相关性、上下文特征、平台特征等特征是特征添加的重要手段，有时候特征项不够会导致模型欠拟合。
1. 添加多项式特征。例如将线性模型添加二次项或三次项使模型泛化能力更强。例如，FM（Factorization Machine）模型、FFM（Field-aware Factorization Machine）模型，其实就是线性模型，增加了二阶多项式，保证了模型一定的拟合程度。
1. 可以增加模型的复杂程度。
1. 减小正则化系数。正则化的目的是用来防止过拟合的，但是现在模型出现了欠拟合，则需要减少正则化参数。
**如何解决过拟合：**

1. 重新清洗数据，数据不纯会导致过拟合，此类情况需要重新清洗数据。
1. 增加训练样本数量。
1. 降低模型复杂程度。
1. 增大正则项系数。
1. 采用dropout方法，dropout方法，通俗的讲就是在训练的时候让神经元以一定的概率不工作。
1. early stopping。
1. 减少迭代次数。
1. 增大学习率。
1. 添加噪声数据。
1. 树结构中，可以对树进行剪枝。
1. 减少特征项。
欠拟合和过拟合这些方法，需要根据实际问题，实际模型，进行选择。

# **过拟合**

- Choose the right activation function
- Use suitable initialization
- Regularize the weights
- Add dropout between layers
- Batch normalization
### **神经网络过拟合处理**

通常可以通过以下方式避免过度训练。

- 在可能的情况下选择一个大的训练集
- 在训练中随机选择模式
- 在过度训练发生之前停止训练过程（监测训练过程，在错误下降到相当低的水平后停止训练）。 在误差下降到一个相当低的水平后停止)
- 直接在训练模式中引入噪声
- 消除不必要的隐藏层结点和权重
- 添加正则化条款，如L2正则化
- 使用上述的组合
- 使用基于全局优化的训练算法，如遗传算法（GA）。
Overtraining can usually be avoided by:

- choosing a large training set when possible
- selecting the patterns randomly during training
- stopping the training process before excessive training occurs (monitoring the training process and stopping
- after the error drops to a fairly low level)
- introducing noise directly into the training patterns
- eliminate unnecessary hidden-layer nodes & weights
- Adding regularization terms such as L2 regularization
- using a combination of the above
- Using a global optimization based training algorithm, e.g. genetic algorithm (GA)
---

## See Also

- [[AI/1-Foundations/ML-Basics/模型评估|模型评估]] — 过拟合诊断的评估方法
- [[PEFT 参数高效微调|PEFT 参数高效微调]] — LoRA 等 PEFT 方法本质上是正则化策略
- [[AI/1-Foundations/Training/Scaling Laws|Scaling Laws]] — 规模 vs 过拟合：为什么大模型反直觉地不过拟合
-  — ML 基础全图谱
