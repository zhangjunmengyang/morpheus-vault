---
brief: "激活函数——ReLU/GELU/SwiGLU/Sigmoid/Tanh 的数学性质和适用场景；现代 LLM 为何用 SwiGLU（平滑+门控+高效）；激活函数对梯度消失/爆炸的影响；面试经典问题：为什么 ReLU 比 Sigmoid 好？"
title: "激活函数"
type: concept
domain: ai/foundations/ml-basics
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/foundations/ml-basics
  - type/concept
---
# 激活函数

[激活函数](https%3A%2F%2Fgithub.com%2Fscutan90%2FDeepLearning-500-questions%2Fblob%2Fmaster%2Fch03_%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%25E5%259F%25BA%25E7%25A1%2580%2F%25E7%25AC%25AC%25E4%25B8%2589%25E7%25AB%25A0_%25E6%25B7%25B1%25E5%25BA%25A6%25E5%25AD%25A6%25E4%25B9%25A0%25E5%259F%25BA%25E7%25A1%2580.md%2334-%25E6%25BF%2580%25E6%25B4%25BB%25E5%2587%25BD%25E6%2595%25B0)

### **激活函数**

- 神经网络的输入层节点不需要激活函数，在定义输入层时无需担心激活函数。
- 输出层激活函数取决于我们要解决的问题类型。在回归问题中，我们使用线性（恒等）激活函数。在二元分类器中，我们使用 sigmoid 激活函数。在多类分类问题中，我们使用 softmax 激活函数。在多标签分类问题中，我们使用 sigmoid 激活函数，为每个类输出一个概率值。
- 隐藏层中使用非线性激活函数，通过考虑模型的性能或损失函数的收敛性来做出选择。可以从 ReLU 激活函数开始，如果你有一个Dying ReLU 问题，试试leaky ReLU。
- 在 MLP 和 CNN 神经网络模型中，ReLU 是隐藏层的默认激活函数。
- 在 RNN 神经网络模型中，我们对隐藏层使用 sigmoid 或 tanh 函数。 tanh 函数具有更好的性能。
- 只有恒等激活函数被认为是线性的。所有其他激活函数都是非线性的。
- 不要在隐藏层中使用 softmax 和恒等函数，在隐藏层中使用 tanh、ReLU、ReLU 的变体
**Sigmoid**

主要特点：

- 也是逻辑回归模型中使用的逻辑函数。
- sigmoid 函数是一个 s 形图。
- 这是一个非线性函数。
- sigmoid 函数将其输入转换为 0 到 1 之间的概率值。
- 它将大的负值转换为 0，将大的正值转换为 1。
- 对于输入 0，它返回 0.5。所以 0.5 被称为阈值，它可以决定给定的输入属于什么类型的两个类。
由于以下缺点，我们通常不在隐藏层中使用 sigmoid 函数。

- sigmoid 函数存在梯度消失问题。 这也称为梯度饱和。
- sigmoid 函数收敛慢。
- 它的输出不是以零为中心的。 因此，它使优化过程更加困难。
- 由于包含了 e^z 项，因此该函数的计算成本很高。
**Tanh**

主要特点：

- tanh（正切双曲线）函数的输出始终介于 -1 和 +1 之间。
- 像 sigmoid 函数一样，它有一个 s 形图。这也是一个非线性函数。
- 与 sigmoid 函数相比，使用 tanh 函数的一个优点是 tanh 函数以零为中心。这使得优化过程更加容易。
- tanh 函数的梯度比 sigmoid 函数的梯度更陡。
由于以下缺点，我们通常不在隐藏层中使用 tanh 函数。

- tanh 函数存在梯度消失问题。
- 由于包含了 e^z 项，因此该函数的计算成本很高。
**Relu**

主要特点：

- ReLU（整流线性单元）激活函数是 sigmoid 和 tanh 激活函数的绝佳替代品。
- ReLU 发明是深度学习领域最重要的突破之一。
- 不存在梯度消失问题。
- 计算成本很低。认为 ReLU 的收敛速度比 sigmoid 和 tanh 函数快 6 倍。
- 如果输入值为 0 或大于 0，则 ReLU 函数按原样输出输入。如果输入小于 0，则 ReLU 函数输出值 0。
- ReLU 函数由两个线性分量组成。因此，ReLU 函数是一个分段线性函数。所以ReLU 函数是一个非线性函数。
- ReLU 函数的输出范围可以从 0 到正无穷大。
- 收敛速度比 sigmoid 和 tanh 函数快。这是因为 ReLU 函数对一个线性分量具有固定导数（斜率），而对另一个线性分量具有零导数。因此，使用 ReLU 函数的学习过程要快得多。
- 使用 ReLU 可以更快地执行计算，因为函数中不包含指数项。
缺点：

- 使用 ReLU 函数的主要缺点是它有一个dying ReLU问题。（当梯度值过大时，权重更新后为负数，经relu后变为0，导致后面也不再更新）
**Softmax**

主要特点：

- 这也是一个非线性激活函数。
- softmax 函数计算一个事件（类）在 K 个不同事件（类）上的概率值。 它计算每个类别的概率值。 所有概率的总和为 1，这意味着所有事件（类）都是互斥的。
用法：

- 必须在多类分类问题的输出层使用 softmax 函数。
- 不在隐藏层中使用 softmax 函数。
---

## See Also

- [[GELU|GELU]] — GELU 激活函数详解，LLM 标配
- [[Transformer|Transformer 通识]] — 激活函数在 FFN 层的作用
- [[损失函数|损失函数]] — 激活函数与 loss 的关系（Sigmoid + BCE）
- [[AI/1-Foundations/目录|Foundations MOC]] — ML 基础全图谱
