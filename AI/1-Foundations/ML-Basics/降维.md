---
brief: "降维——PCA/t-SNE/UMAP 的原理和应用场景；对 LLM 的意义：理解 Embedding 空间的几何性质，解释 Attention Head 学到的特征；可视化 Token Embedding 分布的分析工具。"
title: "降维"
type: concept
domain: ai/foundations/ml-basics
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/foundations/ml-basics
  - type/concept
---
# 降维

# 特征降维

### 什么是维度

The dimension of a dataset is given as *n*-rows by *p*-columns. We are usually more concerned about *p* – the number of columns. （列数多——纬度低，列数少——维度低）

### 为什么要降维

- Takes up storage space
- Hard to visualise
- Computationally complex (long training time)
- Result in complex models, harder to interpret
- Risk of model overfitting -poor predictions
- Run into the Curse of Dimensionality
### 降维的优点

- data mining algorithms work better
- eliminate irrelevant features and reduce noise
- more understandable model
- more easily visualised
- lower time and memory
维度诅咒（dimensionality curse）

随着维度增加，数据在空间中变得越来越稀疏，分析变得越来越困难

对于分类来说，这可能意味着没有足够的数据对象来创建一个模型，为所有可能的对象可靠地分配一个类别。对于聚类来说，对聚类至关重要的密度和点之间的距离的定义变得不那么有意义。

### **特征选择**

1. 目的：**删掉所有不携带大量信息的特征，不增加新的信息，**找到best set of feature
1. 常见方法：filter method, wrapper method, embedded method
1. **filter method**
1. 皮尔森相关系数法 pearson’s correlation：删除高度相关的feature（r > 0.9）
1. 方差阈值 variance threshold：删除具有低可变性或80%以上的值相似的feature
1. K方检验 chi-squre test：如果该特征与目标变量高度相关，那么我们将保留它
1. F检验 f-test(Anova)：数值输入特征和分类目标特征之间的比率越高，两者之间的依赖性就越高，因此该特征更有可能对模型有用
1. **wrapper method**
1. 优点：效果好，使模型具有更好精度
1. 缺点：计算开销大
1. 常见方法：正向特征选择，反向特征选择，递归特征消除法（RFE）
1. 正向特征选择：从0个特征开始，每次增加一个特征，直到目标个数n
1. 反向特征选择：从全集特征set开始，逐个减少
1. RFE：
1. embedded method
**三种方法特征选择对比：**

![image](IUPed6A9noZ4wGxSPMMcQ3bin8d.png)

### **特征提取**

1. 目的：**通过把原始特征组合的方式，保留所有信息**
1. 方法：PCA，LDA
### 

---

## See Also

- [[AI/1-Foundations/Math/线性代数|线性代数]] — SVD/PCA 的数学基础
- [[AI/1-Foundations/ML-Basics/特征工程|特征工程]] — 降维是特征工程的关键环节
- [[AI/1-Foundations/ML-Basics/数据预处理|数据预处理]] — 降维前的数据准备
-  — ML 基础全图谱
