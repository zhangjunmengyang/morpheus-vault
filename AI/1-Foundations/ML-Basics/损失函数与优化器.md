---
brief: "损失函数与优化器全景——CrossEntropy/InfoNCE/DPO Loss + AdamW/LION/Sophia 的完整对比；学习率调度（Warmup+Cosine/WSD）的数学动机；面试被问「你了解哪些优化器」的深度参考，涵盖 2026 最新进展。"
title: "损失函数与优化器：CrossEntropy/InfoNCE/DPO Loss + AdamW/LION/Sophia + 学习率调度"
date: 2026-02-13
tags:
  - ai/foundations
  - ai/llm/training
  - ai/optimization
  - type/deep-dive
  - interview/hot
status: active
---

# 损失函数与优化器

> 面试中最常被要求"手推公式"的三个方向：Loss Function、Optimizer、Learning Rate Schedule。本文覆盖 LLM 训练中最核心的损失函数、优化器和调度策略，附推导过程。

## 1. 损失函数

### 1.1 Cross-Entropy Loss

LLM pre-training 的核心 loss——衡量模型预测分布与真实 token 分布的差异：

$$\mathcal{L}_{CE} = -\frac{1}{T}\sum_{t=1}^{T} \log p_\theta(x_t | x_{<t})$$

展开单个 token：

$$\log p_\theta(x_t | x_{<t}) = \log \frac{\exp(z_{x_t})}{\sum_{v=1}^{V} \exp(z_v)} = z_{x_t} - \log \sum_{v=1}^{V} \exp(z_v)$$

其中 $z_v$ 是 logits，$V$ 是词表大小。

**与 Perplexity 的关系**：

$$\text{PPL} = \exp(\mathcal{L}_{CE})$$

```python
import torch
import torch.nn.functional as F

# 标准实现
def cross_entropy_loss(logits, targets):
    """
    logits: (batch, seq_len, vocab_size)
    targets: (batch, seq_len)
    """
    # reshape for F.cross_entropy
    B, T, V = logits.shape
    loss = F.cross_entropy(
        logits.view(B * T, V),
        targets.view(B * T),
        ignore_index=-100  # padding token
    )
    return loss

# 面试追问：为什么用 log_softmax + nll_loss 而不是先 softmax 再取 log？
# 答：数值稳定性。log(softmax(x)) 直接计算会有 exp 溢出风险，
#     log_softmax 实现了 x - logsumexp(x)，避免了大数溢出。
```

### 1.2 InfoNCE Loss

对比学习的核心 loss，用于 Embedding 模型与向量数据库（[[AI/6-应用/RAG/_MOC|参见 RAG MOC]]）的训练：

$$\mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(\text{sim}(q, k^+) / \tau)}{\sum_{i=0}^{K} \exp(\text{sim}(q, k_i) / \tau)}$$

- $q$ = query 表示
- $k^+$ = 正样本表示
- $k_i$ = 正样本 + K 个负样本
- $\tau$ = 温度参数
- $\text{sim}$ = 相似度函数（通常是 cosine similarity）

**直觉**：这是一个 (K+1)-way 分类问题——从 K+1 个候选中找出正样本。

```python
def info_nce_loss(query, positive, negatives, temperature=0.07):
    """
    query: (batch, dim)
    positive: (batch, dim)
    negatives: (batch, K, dim)
    """
    # 正样本相似度
    pos_sim = F.cosine_similarity(query, positive, dim=-1)  # (batch,)

    # 负样本相似度
    neg_sim = F.cosine_similarity(
        query.unsqueeze(1), negatives, dim=-1
    )  # (batch, K)

    # 拼接
    logits = torch.cat([pos_sim.unsqueeze(1), neg_sim], dim=1) / temperature
    labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)

    return F.cross_entropy(logits, labels)

# In-batch negatives 变体（更高效）
def in_batch_info_nce(embeddings_a, embeddings_b, temperature=0.07):
    """batch 内其他样本作为负样本"""
    sim_matrix = F.cosine_similarity(
        embeddings_a.unsqueeze(1),
        embeddings_b.unsqueeze(0),
        dim=-1
    ) / temperature  # (batch, batch)
    labels = torch.arange(sim_matrix.size(0), device=sim_matrix.device)
    return F.cross_entropy(sim_matrix, labels)
```

**温度 τ 的作用**：
- τ 小 → 分布更尖锐，hard negatives 权重更高，但梯度可能爆炸
- τ 大 → 分布更平滑，所有负样本权重相近，训练更稳定但可能效率低
- 通常 τ ∈ [0.05, 0.1]

### 1.3 DPO Loss

详见 [[对齐技术综述|对齐技术综述]]，这里给出实现和梯度分析：

$$\mathcal{L}_{DPO} = -\mathbb{E} \left[ \log \sigma \left( \beta \left( \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right) \right]$$

**梯度分析**（面试高频）：

$$\nabla_\theta \mathcal{L}_{DPO} = -\beta \cdot \underbrace{\sigma(\hat{r}_l - \hat{r}_w)}_{\text{权重：错得越多学越多}} \left[ \underbrace{\nabla_\theta \log \pi_\theta(y_w|x)}_{\text{增加 chosen 概率}} - \underbrace{\nabla_\theta \log \pi_\theta(y_l|x)}_{\text{降低 rejected 概率}} \right]$$

其中 $\hat{r} = \beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}$。

直觉：**当模型对 rejected 的隐式 reward 高于 chosen 时（即"判断错误"），梯度权重大，学得更多**。

## 2. 优化器

### 2.1 AdamW

LLM 训练的默认优化器。Adam + **decoupled weight decay**：

$$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \quad \text{(一阶矩估计)}$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \quad \text{(二阶矩估计)}$$
$$\hat{m}_t = \frac{m_t}{1-\beta_1^t}, \quad \hat{v}_t = \frac{v_t}{1-\beta_2^t} \quad \text{(偏差校正)}$$
$$\theta_t = \theta_{t-1} - \eta \left( \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon} + \lambda \theta_{t-1} \right)$$

**AdamW vs Adam + L2**：

```python
# Adam + L2: weight decay 和自适应学习率耦合
# gradient ← g + λθ，然后 Adam 更新
# 问题：自适应学习率会"稀释" weight decay 的效果

# AdamW: weight decay 与梯度更新解耦
# 先 Adam 更新（只用 g），再独立做 θ ← θ - ηλθ
# 这样 weight decay 的力度不受 Adam 的自适应系数影响

class AdamW:
    def step(self):
        for p in params:
            # Adam 部分
            m = beta1 * m + (1 - beta1) * p.grad
            v = beta2 * v + (1 - beta2) * p.grad ** 2
            m_hat = m / (1 - beta1 ** t)
            v_hat = v / (1 - beta2 ** t)
            adam_update = m_hat / (v_hat.sqrt() + eps)

            # Decoupled weight decay
            p.data -= lr * (adam_update + weight_decay * p.data)
```

**LLM 中的典型超参**：
- $\beta_1 = 0.9, \beta_2 = 0.95$（注意不是默认的 0.999）
- $\epsilon = 10^{-8}$
- weight_decay = 0.1
- peak lr: 根据模型大小，通常 $3 \times 10^{-4}$ ~ $1 \times 10^{-4}$

### 2.2 LION (EvoLved Sign Momentum)

Google Brain 通过 **程序搜索** 自动发现的优化器（2023）：

$$m_t = \beta_2 m_{t-1} + (1-\beta_2) g_t$$
$$\theta_t = \theta_{t-1} - \eta \left( \text{sign}(\beta_1 m_{t-1} + (1-\beta_1) g_t) + \lambda \theta_{t-1} \right)$$

**核心特点**：
- 只用 **sign** 更新——每个参数的更新幅度相同（+η 或 -η）
- 内存只需 1 份 momentum（Adam 需要 m + v = 2 份）→ **显存省约 50%**
- 适合大 batch + 大模型

```python
class Lion:
    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):
        self.lr = lr
        self.beta1, self.beta2 = betas
        self.wd = weight_decay
        self.state = {p: torch.zeros_like(p) for p in params}

    def step(self, params):
        for p in params:
            grad = p.grad
            m = self.state[p]

            # 更新方向 = sign(插值 of old momentum and new gradient)
            update = torch.sign(self.beta1 * m + (1 - self.beta1) * grad)

            # Weight decay + update
            p.data -= self.lr * (update + self.wd * p.data)

            # 更新 momentum（注意用 beta2，不是 beta1）
            m.mul_(self.beta2).add_(grad, alpha=1 - self.beta2)
```

**LION vs AdamW**：
| 维度 | AdamW | LION |
|------|-------|------|
| 内存 | 2× 参数（m, v） | 1× 参数（m） |
| 更新规则 | 自适应步长 | sign 统一步长 |
| 学习率 | 通常 1e-4 ~ 3e-4 | 通常 1/3 ~ 1/10 of Adam |
| Weight decay | 0.1 | 通常需要更大（1.0~10.0） |
| 适用场景 | 通用 | 大 batch、视觉任务尤佳 |

### 2.3 Sophia (Second-order Clipped)

Liu et al. (2023) 提出的**准二阶**优化器——用 Hessian 对角线做预条件：

$$\theta_t = \theta_{t-1} - \eta \cdot \text{clip}\left(\frac{m_t}{h_t}, \rho\right)$$

- $m_t$：一阶矩（同 Adam）
- $h_t$：Hessian 对角线的 EMA 估计
- clip 到 $[-\rho, \rho]$ 防止极端更新

**Hessian 估计方法**：
- **Sophia-G**：用 Gauss-Newton-Bartlett 估计（mini-batch gradient 的外积）
- **Sophia-H**：用 Hutchinson's estimator（随机向量的 Hessian-vector product）

**优势**：比 AdamW 快 2× 达到相同 loss（在 GPT-2 规模验证）。
**局限**：Hessian 估计有额外计算成本，每隔 k 步估计一次来摊薄。

## 3. 学习率调度 (Learning Rate Schedule)

### 3.1 Cosine Annealing

LLM pre-training 最常用的调度：

$$\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\left(\frac{t - T_w}{T - T_w} \pi\right)\right)$$

通常配合 **linear warmup**：

```python
def cosine_schedule(step, total_steps, warmup_steps, max_lr, min_lr=0):
    if step < warmup_steps:
        return max_lr * step / warmup_steps
    progress = (step - warmup_steps) / (total_steps - warmup_steps)
    return min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * progress))
```

```
LR
│  ╱╲
│ ╱  ╲
│╱    ╲            ← Cosine decay
│      ╲
│       ╲
│    ╲
│         ╲___
├─────────────── Steps
  warmup  decay
```

### 3.2 WSD (Warmup-Stable-Decay)

MiniCPM / DeepSeek 采用的调度策略：

```
LR
│       ╱────────────────╲
│      ╱                  ╲
│     ╱                    ╲
│    ╱      Stable          ╲___
│   ╱                           
├────────────────────────────── Steps
  warmup    stable phase    decay
```

三个阶段：
1. **Warmup**：线性增加到 peak LR（~2000 steps）
2. **Stable**：保持 peak LR 不变（主要训练阶段）
3. **Decay**：快速衰减到 min LR（最后 ~10% steps）

**优势**：支持 **任意停止点 (anytime stopping)**——stable 阶段的任何 checkpoint 都是接近最优的，方便调整训练长度。

```python
def wsd_schedule(step, total_steps, warmup_steps, decay_steps, max_lr, min_lr=0):
    if step < warmup_steps:
        return max_lr * step / warmup_steps
    elif step < total_steps - decay_steps:
        return max_lr
    else:
        decay_progress = (step - (total_steps - decay_steps)) / decay_steps
        return min_lr + (max_lr - min_lr) * (1 - decay_progress)
```

### 3.3 对比

| 调度策略 | Warmup | 中间 | 末尾 | 适用场景 |
|---------|--------|------|------|---------|
| **Cosine** | Linear | 缓慢衰减 | 趋近 min_lr | 通用，GPT/LLaMA |
| **WSD** | Linear | 恒定 | 快速衰减 | 需要灵活停止点 |
| **Linear Decay** | Linear | 线性衰减 | 0 | 小模型/SFT |
| **Inverse Sqrt** | Linear | $1/\sqrt{t}$ | 缓慢 | Transformer 原版 |

## 4. 面试常考题

### Q1: 手推 Cross-Entropy Loss 对 logits 的梯度
**答**：设 softmax 输出为 $p_i = \frac{e^{z_i}}{\sum_j e^{z_j}}$，目标类为 $y$。则：$\frac{\partial \mathcal{L}}{\partial z_i} = p_i - \mathbb{1}[i=y]$。即：对目标类，梯度 = 预测概率 - 1（模型越不确定，梯度越大）；对非目标类，梯度 = 预测概率（模型越错误地自信，梯度越大）。

### Q2: AdamW 和 Adam + L2 正则有什么区别？为什么 AdamW 更好？
**答**：Adam + L2 将 weight decay 加入梯度（g ← g + λθ），然后经过 Adam 的自适应缩放。问题是自适应学习率会"稀释" weight decay——参数梯度大的维度 weight decay 被弱化，梯度小的维度被强化，与期望相反。AdamW 将 weight decay 从梯度中解耦，直接在参数更新后做 θ ← θ - ηλθ，确保所有参数受到均匀的正则化。Loshchilov & Hutter (2019) 证明这在 LLM 训练中效果显著更好。

### Q3: InfoNCE Loss 的温度参数 τ 有什么作用？
**答**：τ 控制 softmax 的锐度：τ 小 → 分布尖锐，模型对 hard negatives 给予更高权重，学习更精细的区分但可能梯度不稳；τ 大 → 分布平滑，所有负样本权重接近，训练稳定但学习效率低。τ 本质上控制了"对比学习对相似度差异的敏感程度"。可以设为可学习参数（CLIP 中初始 τ=0.07，训练时自动调整）。

### Q4: LION 比 AdamW 省了哪些内存？为什么能 work？
**答**：LION 只维护一阶矩 m（Adam 需要 m + v），节省约 1/3 优化器状态内存。它用 sign(interpolated momentum) 做更新，每个参数更新幅度相同。直觉：sign 操作是一种隐式的梯度归一化，类似于 signSGD 但加了 momentum 平滑。它在大 batch 下特别有效，因为大 batch 梯度方向更稳定，sign 的信息损失更少。代价是需要仔细调 LR（通常为 Adam 的 1/3~1/10）和 weight decay（通常需要更大）。

### Q5: Cosine 和 WSD 学习率调度的核心区别？什么场景选哪个？
**答**：Cosine 从 peak LR 平滑衰减到 min LR，整个训练期间 LR 持续降低。WSD 有一个 stable 阶段保持 peak LR 不变，最后才快速衰减。核心区别：WSD 支持 anytime stopping——stable 阶段的任何 checkpoint 都是可用的，方便根据实际情况调整训练长度；Cosine 需要预先确定总步数。选型：如果训练步数确定选 Cosine（更充分利用每个 step），如果需要灵活停止或做 continual pre-training 选 WSD。

---

## see-also

- [[AI/1-Foundations/目录|Foundations MOC]] — 数学基础全图谱
- [[对齐技术综述|对齐技术综述]] — DPO Loss 的对齐应用背景
- [[AI/3-LLM/SFT/_MOC|SFT MOC]] — Cross-Entropy Loss 在 SFT 中的核心角色
- [[GRPO-Improvement-Panorama-2026|GRPO 改进全景 2026]] — Trust Region / 优化器选择与 RL 训练稳定性
