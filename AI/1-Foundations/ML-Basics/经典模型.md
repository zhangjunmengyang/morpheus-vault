---
brief: "经典 ML 模型——线性回归/Logistic Regression/SVM/随机森林/XGBoost/K-Means 的原理和适用场景；大厂算法面试中 ML 经典模型仍是考察重点；LLM 工程师的传统 ML 知识基线。"
title: "经典模型"
type: concept
domain: ai/foundations/ml-basics
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/foundations/ml-basics
  - type/concept
---
# 经典模型

### **LR**

线性回归

目标：找到一条线，最小化最后一项（预测值到样本点的距离平方和），平方误差

逻辑回归

多分类逻辑回归：计算总概率中的每一类概率，概率最大的为output

### **NB**

贝叶斯计算步骤：

1. 计算先验概率
1. 计算分母证据概率（可选）
1. 计算分母中的证据极大似然概率
1. 计算每一类，代入公式
**拉普拉斯校正 Laplace Correction：**将值为0的feature value设为较小的数值（比如1），防止整体公式都等于0

### **DT**

基本算法：ID3, C4.5, C5.0

算法流程：

1. 如果某个数据集合中的所有样本都属于同一个类C，则C就是根节点
1. 否则选择具有“最大信息的”属性A（A具有v1, v2, …, vn的类），然后把样本点分入这些类里面
1. 递归执行算法，直到所有样本都被划分
**注释：类似于广度优先遍历**

不同决策树算法（选择“最大信息”的过程）：

- information gain：ID3, C4.5
- information gain ratio：C4.5
- Gini index：CART
**启发式搜索**

- 搜索偏向：搜索决策树的空间，从最简单到越来越复杂。从最简单的到越来越复杂的（贪婪的搜索，没有回溯，喜欢小树）。
- 搜索启发式方法。在一个节点上，选择对例子分类最有用的属性。对实例分类最有用的属性，并相应地分割该节点
**信息熵：**信息熵越高，不确定性越高；反之越低

**信息增益：**

信息增益最大的属性即为需要选择的root

**连续值属性划分**：通常相邻数值的中点作为可能的最佳划分点

决策树停止划分的标准 **Stopping Criteria**：

- 所有样本点都被划分结束
- 所有属性都被用到
- min_samples_split 分割结点所需的最小样本数
- min_samples_leaf 最小叶子节点数
- max_depth 最大深度
**增益率：信息增益基础上，考虑了分支的数量和大小**

**基尼系数**

### **NN**

基本结构：

- input + hidden + output
- weight：通常初始化为随机小实数
- activation function f(net)
- 学习规则：Wi(t+1)=Wi(t)+ΔWi(t)，决定了在每轮训练中如何计算权重调整
- 激活函数计算 & 权重调整：每层中计算激活，权重是根据error或distance来调整
可以通过时间、训练轮数、准确率来限制训练程度

神经网络所有要考虑的因素：

- 整体结构：输入+ 隐藏+ 输出
- 内部结构：例如 神经网络画法实例
- 权重
- 学习规则：有监督、无监督、强化学习
- 激活函数（激活计算方式）
建立流程：

输入可能有：**Discrete feature value + Continuous feature value**

### **SVM**

结构：输入层 + 全连接的内积层 + 输出层

### **Cluster & K-means**

### **MLP（BP）**

**训练过程中：向前传播 信号signal，向后传播 错误error**

注释：梯度下降是一种求解方法，BP是其在神经网络中的具体实现

掌握BP计算流程（day2 p43）

### **RNN**

### **LSTM**

### **RBF**

- There are many variations possible in learning strategies 
- basis functions types
- selection of centers
- fixed centers selected randomly
- self-organized learning (clustering)
- supervised selection
- The two-stage training process of RBF permits the use of unlabeled training data (unsupervised training methods) while creating kernel nodes and hence, only a relatively small number of labelled data will be needed to find the output layer parameters.
### **GRNN**

Features:

- Memory-based
- One-pass learning algorithm
- A form of self-growing net with one pattern unit created for each new training pattern or cluster centre.
Applications: Good for forecasting, control and similar applications where the training data set is generated by an unknown probability distribution.

一些优缺点：

# **模型优缺点文档 (1)**

### **LR**

**线性回归特点：**

- 适用于数值型目标
- 服从线性分布则表现更优
- 形如ax + by + cz + d
**逻辑斯蒂回归特点：**

- 为分类问题设计
- 尝试直接估计分类可能性的概率
### **KNN**

- 没有线性决策表面
- 属于计算密集型 computationally intensive
- 距离计算很重要 
- 欧式距离 Euclidean distance
- 汉明距离 Hamming distance
优点：

- 简单的技术，容易实施
- 构建模型的成本很低
- 极为灵活的分类方案
- 最近的邻居分类器是懒惰的学习者
- 缩放问题
- 属性可能必须进行缩放，以防止距离测量 不被其中一个属性所支配
### **NB**

Pros：

- 简单快速，分类效果好
- 当独立性假设成立时 ，表现效果很好
- 训练类别型变量比数值型变量效果更好，数值型的默认假设服从正态分布
Cons：

- 真实生活中很少有完全成立的独立性假设
### **DT**

Pros:

- 易于理解和解释
- 数据准备少，计算量小
- 显示哪些属性对分类最重要
Cons:

- 不能保证产生一个最佳的决策树
- 在许多类和小数据中表现不佳
- 过于复杂的树不能很好地从训练数据中概括出来(过拟合)
应用：

- Customer Relationship Management
- Fraud Detection
- Churn Prediction
- Credit Risk Prediction
- Purchasing Behavior Prediction
- Fault Detection
- Sentiment Analysis
- Investment Solutions
### **SVM**

应用：

- Bioinformatics
- Machine Vision
- Text Categorization
- Handwritten Character Recognition
### **Cluster & K-means**

cluster 应用：

- Cluster Analysis is versatile and can be used in many business problems
- across many domains:
- Sales & Marketing: help marketers discover groups in their customer
- databases, and then use this insight to develop more targeted marketing
- campaigns
- Fraud Detection: Identify groups of customers whose transaction
- behavior is uncharacteristic
- Health & Bioinformatics: help physicians discover groups of patients with
- similar profiles and with a similar risk pattern, and use this insight to
- make predictions about diseases risks
- Insurance: Help identify groups of policy holders with high average claim
- cost
### **CNN & RNN**

CNN vs RNN LSTM

### **LSTM vs RNN**

- RNN has no state; LSTM remembers information by state.
- RNN activation function only has tanh; LSTM introduces sigmoid function through input gate, forgetting gate, output gate and combines with tanh function to add summation operation to reduce the possibility of gradient disappearance and gradient explosion.
- RNN can only deal with short-term dependence problem; LSTM can deal with both short-term and long-term dependence problems.
### **GRNN**

*Advantages*:

- The estimate converges to the true regression surface with increasing samples.
- A GRNN has the ability to work with sparse data and in real-time environments since the regression surface is defined everywhere instantly and the network provides smooth transitions from one observation to another.
- Training a GRNN is fast and straightforward with only a single pass through the training set needed. The network can begin to perform the regression after a single training sample has been presented.
*Disadvantages*:

- *The net may grow to be very large since one pattern unit is added for each pattern in the training set* (unless clustering is used to determine the prototype centres).
- The computation load for the network is relatively heavy.
适用于：自适应控制

### **RBF**

**与BP对比：**

相同点：

1. RBF神经网络中对于权重的求解也可以使用BP算法求解。
不同点：

1. 中间神经元类型不同(RBF:径向基函数；BP:Sigmoid函数)
1. 网络层次数量不同(RBF:3层；BP:不限制)
1. 运行速度的区别(RBF:快；BP:慢)
---

## See Also

- [[机器学习|机器学习]] — 经典模型的宏观框架
- [[损失函数|损失函数]] — 各模型对应的 loss 函数
- [[模型评估|模型评估]] — 如何评估不同经典模型的性能
- [[AI/1-Foundations/目录|Foundations MOC]] — ML 基础全图谱
