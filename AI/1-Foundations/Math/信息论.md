---
brief: "信息论——熵/KL 散度/互信息/交叉熵的数学定义；交叉熵 Loss 的信息论本质（最小化预测分布与真实分布的 KL 散度）；温度参数对 softmax 分布熵的影响；LLM 训练损失函数的数学根基。"
title: "信息论"
type: concept
domain: ai/foundations/math
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/foundations/math
  - type/concept
---
# 信息论

信息论是 Claude Shannon 于 1948 年创立的数学理论，原本用于解决通信系统中的编码和传输问题。在深度学习中，信息论的核心概念——熵、交叉熵、KL 散度——无处不在，从损失函数到正则化，从模型压缩到强化学习，理解信息论是理解现代 AI 的数学基础。

## 核心概念

### 信息量（Information Content）

一个事件 $x$ 的信息量定义为：

$$I(x) = -\log P(x)$$

直觉：越不可能发生的事件，包含的信息量越大。"今天太阳升起"几乎没有信息量，而"今天地震了"信息量很大。

### 熵（Entropy）

随机变量 $X$ 的熵是信息量的期望：

$$H(X) = -\sum_{x} P(x) \log P(x) = \mathbb{E}[-\log P(X)]$$

熵衡量的是一个分布的**不确定性**。均匀分布的熵最大（最不确定），确定性事件的熵为 0。

对于连续分布，对应的是**微分熵**：

$$h(X) = -\int p(x) \log p(x) \, dx$$

**在深度学习中的应用**：
- 分类任务中，模型输出的熵可以衡量预测的不确定性（用于主动学习、OOD 检测）
- RLHF 中的熵正则化：鼓励策略保持探索性，防止过早收敛

### 交叉熵（Cross-Entropy）

给定真实分布 $P$ 和预测分布 $Q$：

$$H(P, Q) = -\sum_{x} P(x) \log Q(x)$$

**交叉熵 = 熵 + KL 散度**：$H(P, Q) = H(P) + D_{\text{KL}}(P \| Q)$

由于 $H(P)$ 是常数（对于给定的训练数据），最小化交叉熵等价于最小化 KL 散度。这就是为什么**交叉熵是分类任务的标准损失函数**。

```python
# PyTorch 中的交叉熵
loss = F.cross_entropy(logits, labels)  # 包含了 softmax + 负对数似然
```

### KL 散度（Kullback-Leibler Divergence）

$$D_{\text{KL}}(P \| Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)} = \mathbb{E}_P\left[\log \frac{P(X)}{Q(X)}\right]$$

KL 散度衡量两个分布之间的"距离"（严格来说不是距离，因为不对称）。

**关键性质**：
- $D_{\text{KL}}(P \| Q) \geq 0$（Gibbs 不等式）
- $D_{\text{KL}}(P \| Q) = 0$ 当且仅当 $P = Q$
- **不对称**：$D_{\text{KL}}(P \| Q) \neq D_{\text{KL}}(Q \| P)$

**在 LLM 中的应用**：
- **知识蒸馏**：最小化 student 和 teacher 输出分布的 KL 散度
- **PPO/GRPO**：用 KL 散度约束新策略不偏离旧策略太远
- **VAE**：KL 散度作为正则项，让后验分布接近先验

### 前向 KL vs 反向 KL

$$\text{前向 KL}: D_{\text{KL}}(P \| Q) \quad \text{（mode-covering，Q 倾向于覆盖 P 的所有模式）}$$
$$\text{反向 KL}: D_{\text{KL}}(Q \| P) \quad \text{（mode-seeking，Q 倾向于集中在 P 的一个模式上）}$$

在 RLHF 中，PPO 通常使用反向 KL $D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})$，这倾向于让策略保持在参考模型附近，而不是试图覆盖所有可能的输出。

### 互信息（Mutual Information）

$$I(X; Y) = D_{\text{KL}}(P(X,Y) \| P(X)P(Y)) = H(X) - H(X|Y)$$

互信息衡量两个变量之间的依赖程度。在特征选择、表示学习（InfoNCE loss）等场景中有重要应用。

CLIP 的对比学习损失本质上是在最大化图像和文本之间的互信息。

## 与深度学习的深层联系

### 最大似然 = 最小化交叉熵

给定数据分布 $P_{\text{data}}$ 和模型分布 $P_\theta$：

$$\arg\max_\theta \mathbb{E}_{x \sim P_{\text{data}}}[\log P_\theta(x)] = \arg\min_\theta H(P_{\text{data}}, P_\theta)$$

这意味着语言模型的训练（最大化对数似然）本质上就是在最小化模型分布和真实数据分布的交叉熵。

### 信息瓶颈（Information Bottleneck）

一种正则化理论：好的表示 $Z$ 应该最大化 $I(Z; Y)$（保留与任务相关的信息）同时最小化 $I(Z; X)$（压缩无关信息）：

$$\min_Z I(Z; X) - \beta I(Z; Y)$$

VAE、Dropout、量化等技术都可以从信息瓶颈的角度理解。

## 相关

## See Also

- [[AI/1-Foundations/Math/概率与分布|概率与分布]] — 熵和 KL 散度的概率论基础
- [[AI/1-Foundations/Math/连续优化|连续优化]] — 最小化 KL 散度 = 优化目标
- [[AI/1-Foundations/DL-Basics/深度学习|深度学习]] — 交叉熵 Loss 的工程实现
- [[AI/3-LLM/RL/Fundamentals/贝尔曼方程|贝尔曼方程]] — RL 中的信息量与奖励结构
- [[AI/3-LLM/RL/Fundamentals/KL散度|KL 散度]] — KL 散度专项深度讲解
- [[AI/1-Foundations/Math/矩阵分解|矩阵分解]] — 线性代数视角的信息压缩
- [[AI/1-Foundations/ML-Basics/损失函数|损失函数]] — 交叉熵 Loss 的信息论本质
- [[AI/2-Agent/Agentic-RL/MIG-Step-Marginal-Information-Gain-Credit-Assignment|MIG：信息论视角的 Credit Assignment]] — 本笔记互信息/条件熵的直接应用：用"条件似然增量"量化每步 Credit（RL 中的信息增益实践）
- [[AI/2-Agent/Agentic-RL/CoT-Monitorability-Information-Theory|CoT 可监控性（信息论）]] — 用信息论量化 CoT 中推理轨迹的可监控程度
