---
brief: "连续优化——梯度下降/Adam/AdamW/LION/Sophia 的原理和 LLM 训练选型；学习率调度（Warmup+Cosine）的数学动机；KL 约束优化（RLHF 中的 PPO Lagrangian）的理论基础。"
title: "连续优化"
type: concept
domain: ai/foundations/math
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/foundations/math
  - type/concept
---
# 连续优化

连续优化是深度学习的计算引擎。神经网络的训练本质上就是一个优化问题：在高维参数空间中寻找使损失函数最小化的点。从最基础的梯度下降到现代的 AdamW，理解优化算法对于调参、诊断训练问题、选择超参数都至关重要。

## 基础框架

### 优化问题的形式化

$$\min_{\theta \in \mathbb{R}^n} L(\theta) = \frac{1}{N} \sum_{i=1}^{N} \ell(f(x_i; \theta), y_i)$$

其中 $\theta$ 是模型参数，$L$ 是损失函数，$\ell$ 是单样本损失。现代 LLM 的参数 $\theta$ 可以有数十亿甚至上万亿维度。

### 梯度与方向导数

梯度 $\nabla L(\theta)$ 指向损失函数增长最快的方向。负梯度方向是**局部最优的下降方向**。

对于深度网络，梯度通过**反向传播（链式法则）** 高效计算：

$$\frac{\partial L}{\partial \theta_l} = \frac{\partial L}{\partial a_L} \prod_{k=l}^{L-1} \frac{\partial a_{k+1}}{\partial a_k} \cdot \frac{\partial a_l}{\partial \theta_l}$$

### 凸优化 vs 非凸优化

深度学习是**非凸优化**——损失曲面有大量鞍点和局部极小值。但经验表明：

1. **大部分局部极小值的损失值相近**（高维空间的特性）
2. **鞍点比局部极小值更成问题**（梯度为零但不是极值）
3. **过参数化的网络更容易优化**（loss landscape 更平滑）

## 优化算法演进

### SGD（随机梯度下降）

$$\theta_{t+1} = \theta_t - \eta \nabla L_{\mathcal{B}}(\theta_t)$$

用 mini-batch $\mathcal{B}$ 的梯度估计全量梯度。引入了噪声，反而有正则化效果。

**SGD + Momentum**：

$$v_t = \mu v_{t-1} + \nabla L_{\mathcal{B}}(\theta_t)$$
$$\theta_{t+1} = \theta_t - \eta v_t$$

动量帮助加速收敛、穿越鞍点、平滑震荡。

### Adam（Adaptive Moment Estimation）

Adam 可能是深度学习最重要的优化器，结合了 Momentum 和 RMSProp：

$$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t \quad \text{（一阶矩，方向）}$$
$$v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2 \quad \text{（二阶矩，步长缩放）}$$

偏差修正：$\hat{m}_t = m_t / (1 - \beta_1^t)$，$\hat{v}_t = v_t / (1 - \beta_2^t)$

参数更新：

$$\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

默认超参数：$\beta_1 = 0.9$，$\beta_2 = 0.999$，$\epsilon = 10^{-8}$

### AdamW（解耦权重衰减）

标准 Adam 中的 L2 正则化实际上和权重衰减不等价（因为自适应步长的存在）。AdamW 将权重衰减从梯度更新中**解耦**出来：

$$\theta_{t+1} = (1 - \lambda\eta) \theta_t - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$

这是 LLM 训练的标准优化器。通常 $\lambda = 0.01$ 或 $0.1$。

## 学习率调度

### Warmup

训练初期逐渐增大学习率：

$$\eta_t = \eta_{\max} \cdot \frac{t}{T_{\text{warmup}}}$$

原因：Adam 的二阶矩估计在初始阶段不准确，大学习率容易导致训练发散。对于大 batch size 和大模型，warmup 尤其重要。

### Cosine Decay

$$\eta_t = \eta_{\min} + \frac{1}{2}(\eta_{\max} - \eta_{\min})\left(1 + \cos\frac{\pi t}{T}\right)$$

Warmup + Cosine Decay 是 LLM 预训练的标准配置：

```python
scheduler = get_cosine_schedule_with_warmup(
    optimizer,
    num_warmup_steps=2000,
    num_training_steps=100000,
)
```

### WSD（Warmup-Stable-Decay）

DeepSeek 和其他一些模型使用的调度策略：warmup → 恒定学习率 → 快速下降。适合在预训练接近结束时做 annealing。

## 大规模训练的优化技巧

### 梯度裁剪（Gradient Clipping）

$$g_t \leftarrow g_t \cdot \frac{\text{max\_norm}}{||g_t||}$$

防止梯度爆炸。LLM 训练通常设 `max_grad_norm = 1.0`。

### 混合精度训练（Mixed Precision）

```python
# FP16/BF16 前向 + FP32 权重更新
with torch.autocast(device_type='cuda', dtype=torch.bfloat16):
    loss = model(input_ids, labels=labels)
loss.backward()
optimizer.step()  # FP32
```

BF16 的动态范围和 FP32 一样（8-bit 指数），不需要 loss scaling，是现代 LLM 训练的默认选择。

### 梯度累积（Gradient Accumulation）

当 GPU 内存不够放下目标 batch size 时：

```python
for i, batch in enumerate(dataloader):
    loss = model(batch) / accumulation_steps
    loss.backward()
    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

等效 batch size = micro_batch_size × accumulation_steps × num_gpus。

## 相关

- [[AI/1-Foundations/Math/信息论]]
- [[AI/1-Foundations/Math/概率与分布]]
- [[AI/1-Foundations/Math/矩阵分解]]
- [[AI/1-Foundations/DL-Basics/深度学习|深度学习]]
- [[AI/3-LLM/Infra/分布式训练|分布式训练]]
- [[AI/1-Foundations/ML-Basics/学习率|学习率]]
- [[AI/1-Foundations/Math/向量微积分|向量微积分]]
- [[AI/1-Foundations/Math/线性代数|线性代数]]
