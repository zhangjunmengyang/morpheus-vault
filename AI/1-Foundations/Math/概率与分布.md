---
brief: "概率与分布——Gaussian/Bernoulli/Categorical/Beta 等分布的 PDF/期望/方差；最大似然估计（MLE）和最大后验（MAP）的推导；理解 LLM 输出的 Categorical 分布、RLHF 的 reward 建模的数学基础。"
title: "概率与分布"
type: concept
domain: ai/foundations/math
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/foundations/math
  - type/concept
---
# 概率与分布

概率论是深度学习的数学语言。从模型输出的 softmax 分布到训练中的随机梯度下降，从 VAE 的后验推断到 RLHF 中的策略分布，几乎每一个核心概念都建立在概率论之上。这篇笔记聚焦于 AI 从业者最需要的概率知识。

## 基础概念

### 条件概率与贝叶斯定理

$$P(A|B) = \frac{P(B|A) P(A)}{P(B)}$$

贝叶斯定理是概率论最重要的定理。在 AI 中：
- **先验 $P(A)$**：模型训练前对参数的信念（如权重初始化、正则化）
- **似然 $P(B|A)$**：给定参数下数据出现的概率（训练目标）
- **后验 $P(A|B)$**：看到数据后对参数的更新信念

最大似然估计（MLE）忽略先验，只最大化似然；最大后验估计（MAP）加上先验，等价于加了正则化的 MLE（L2 正则 ↔ 高斯先验）。

### 期望与方差

$$\mathbb{E}[X] = \sum_x x P(x) \quad \text{或} \quad \int x p(x) dx$$

$$\text{Var}(X) = \mathbb{E}[(X - \mathbb{E}[X])^2] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$$

**在 RL 中的重要性**：
- 策略梯度的方差问题是 RL 的核心挑战
- REINFORCE 的梯度估计：$\nabla_\theta J = \mathbb{E}_{\pi_\theta}[\nabla_\theta \log \pi_\theta(a|s) \cdot R]$
- Baseline 技巧：减去期望奖励来降低方差
- GRPO 使用 group-level 标准化来同时减少偏差和方差

## 关键分布

### 正态分布（Gaussian Distribution）

$$\mathcal{N}(x | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(x-\mu)^2}{2\sigma^2}\right)$$

在 AI 中无处不在：
- **权重初始化**：Kaiming/Xavier 初始化假设权重服从正态分布
- **VAE**：将潜变量建模为正态分布
- **扩散模型**：逐步添加/去除高斯噪声
- **中心极限定理**：解释了为什么 Batch Norm 有效

多维正态分布：$\mathcal{N}(\mathbf{x} | \boldsymbol{\mu}, \boldsymbol{\Sigma})$，协方差矩阵 $\Sigma$ 刻画了变量之间的相关性。

### Categorical 分布

$$P(X = k) = p_k, \quad \sum_{k=1}^K p_k = 1$$

这就是 softmax 输出的分布。分类任务的本质是让模型输出正确类别的 categorical 概率最大化。

在 LLM 中，每次 token 生成都是从一个 vocabulary 大小的 categorical 分布中采样：

```python
logits = model(input_ids)  # [batch, seq_len, vocab_size]
probs = F.softmax(logits / temperature, dim=-1)  # categorical 分布
next_token = torch.multinomial(probs, num_samples=1)
```

### 伯努利分布与二项分布

$$P(X = 1) = p, \quad P(X = 0) = 1 - p$$

二分类任务的基础。二项分布是 $n$ 次独立伯努利试验中成功次数的分布。

Binary Cross-Entropy Loss 本质上就是伯努利分布的负对数似然：

$$L = -[y \log p + (1-y) \log(1-p)]$$

## 高级主题

### 采样方法

LLM 推理中的采样策略直接影响生成质量：

| 方法 | 描述 | 效果 |
|------|------|------|
| Greedy | 每步选概率最大的 token | 确定性但单调 |
| Temperature | $p_i = \frac{\exp(z_i/T)}{\sum \exp(z_j/T)}$ | T↑更随机，T↓更确定 |
| Top-k | 只保留概率最大的 k 个 token | 控制候选集大小 |
| Top-p (Nucleus) | 保留累积概率 ≥ p 的最小 token 集合 | 自适应候选集 |
| Min-p | 过滤概率 < p_max × min_p 的 token | 兼顾质量和多样性 |

### 重参数化技巧（Reparameterization Trick）

VAE 训练的关键技巧。将采样操作转化为确定性操作 + 外部噪声：

$$z = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)$$

这使得梯度可以通过 $\mu$ 和 $\sigma$ 反向传播，解决了"采样操作不可微"的问题。

### 重要性采样（Importance Sampling）

从分布 $q$ 采样来估计分布 $p$ 下的期望：

$$\mathbb{E}_p[f(x)] = \mathbb{E}_q\left[\frac{p(x)}{q(x)} f(x)\right]$$

PPO 中的重要性比率 $\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)}$ 就是重要性采样的应用——用旧策略的数据来更新新策略。

## 概率视角下的深度学习

整个深度学习可以从概率角度重新理解：

- **训练 = 后验推断的近似**：找到最大化数据似然的参数
- **正则化 = 先验**：L2 正则 = 高斯先验，Dropout = 近似贝叶斯推断
- **模型输出 = 条件概率分布**：$P(y|x; \theta)$
- **生成模型 = 建模联合分布**：$P(x)$ 或 $P(x, y)$

## 相关

- [[AI/1-Foundations/Math/信息论]]
- [[AI/1-Foundations/Math/连续优化]]
- [[AI/1-Foundations/Math/矩阵分解]]
- [[AI/1-Foundations/DL-Basics/深度学习|深度学习]]
- [[AI/3-LLM/RL/Fundamentals/贝尔曼方程|贝尔曼方程]]
- [[AI/3-LLM/RL/Fundamentals/KL散度|KL散度]]
- [[AI/1-Foundations/ML-Basics/损失函数|损失函数]]
- [[AI/1-Foundations/Math/线性代数|线性代数]]
