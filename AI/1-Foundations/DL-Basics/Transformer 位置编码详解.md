---
brief: "Transformer 位置编码详解——绝对位置编码（Sinusoidal/Learned）→ 相对位置编码（ALiBi/T5-RPE）→ 旋转位置编码（RoPE）的演进；RoPE 为何成为现代 LLM 标配（外推性好+计算高效）；长上下文处理的位置编码基础。"
title: "Transformer 位置编码详解"
type: concept
domain: ai/foundations/dl-basics
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/foundations/dl-basics
  - type/concept
---
# Transformer 位置编码详解

# 一、概念

**作用：**Transformer 模型本身并不包含处理序列顺序的机制，位置编码为了将 input 转换为语义信息 + 位置信息。

 transformer 的位置编码方法都是着重于构造一个合适的 `f(q, k, v)`函数形式。

**核心特性**：

- 绝对位置：为每个位置输出唯一的编码。
- 相对距离一致性：不同长度的句子之间，任何两个位置之间的的关系应当存在线性变换。
- 有界的：编码值应该是有界的，可以用来表示模型在训练过程中从来没有看到过的句子长度，**排除序号编码。**
- 衰减性：具备远程衰减的特性。**原始的 **`**sin/cos**`** 位置编码，本身并不直接体现“衰减性”**。这正是后续很多研究的改进方向。一个最著名的例子是 **ALiBi (Attention with Linear Biases)**。
最直观想到 sin 表示位置编码，我们需要一个**有界、连续**的函数，最容易想到三角函数。

**第 **`**t**`** 个 token 的位置向量 **`**PE_t**`** 的具体表示是什么？**

它是一个 `d_model` 维的向量。如果我们把维度索引 `i` 从 `0` 到 `d_model/2 - 1`，那么这个向量可以写成：

`PE_t = [ sin(ω₀*t), cos(ω₀*t), sin(ω₁*t), cos(ω₁*t), ..., sin(ωᵢ*t), cos(ωᵢ*t), ... ]`

其中，`ωᵢ = 1 / 10000^(2i/d_model)`，这个衰减权重会直接影响注意力的精度，进一步影响上下文窗口长度。

# **二、绝对位置编码**

1. 为每个位置 `t` 生成一个固定的向量 `PE_t`。
1. 将这个位置向量 `PE_t` **直接加到** 对应位置的词嵌入向量 `Embedding_t` 上。
1. `Final_Input_t = Embedding_t + PE_t`。
常用方法：**Sinusoidal 函数**

![image](Df80dfNGPoOkYuxNmyIcghEpnid.png)

- **优点**:
- **简单高效**: 它可以预先计算好所有位置的编码，然后在训练时直接使用，计算开销小。
- **无参数**: 它不涉及任何需要学习的参数，节省了模型空间。
- **缺点**:
- **外推性差**: 如果模型在训练时见过的最长句子是 512 个词，那么当它在推理时遇到一个 1000 个词的句子时，它不知道第 513 个词及之后的位置编码是什么含义，性能可能会下降。
## **2.1 为什么有个魔法值 10000？**

- 原理上：三角函数是周期函数，如果函数的频率偏大，则不同 t 下的位置向量可能出现重合的情况，也就是出现循环。 
- 目的上：因此就是在降低初期频率变化，避免编码**位置混淆，**Transformer 的核心优势之一是捕捉长距离依赖关系。如果位置编码本身在长距离上是**重复**、混乱的，那模型就失去了判断“远近”的标尺，捕捉长距离依赖的能力会大大减弱。
- 体感上：为什么是 10000，不是 1000 或者 1000000？
- 比如基数设为 1000，**当文本长度超过由基数决定的波长时（大约 1000 * 2π ≈ 6283 个词）**，模型开始混淆绝对位置。它可能无法有效区分文档开头（如第 100 个词）和很后面（如第 6383 个词）的内容，因为它们的低频位置编码可能变得非常相似。
- 如果基数大，模型可以清晰地为几十万长度的文本中的每一个词赋予一个独一无二的绝对位置标识。
- 这个基数 `M` (无论是 1000, 10000, 还是 100000) **必须**大于模型打算处理的**最大上下文长度 (Max Context Length)**。为了扩展上下文，这个需要继续调整比如 50w 或者 100w。
## **2.2 为什么要 sin 和 cos 交替使用？/ 为什么分为奇数和偶数索引？**

- **作用**：
- 确保一定窗口长度内的编码唯一性，避免单一周期函数造成的重复问题。
- 易于泛化外推，有界的可以循环编码。
- **三角函数本身隐含相对位置的关系**：
- 这个矩阵代表着将向量在二维空间内旋转一个角度 `θ`，其中 `θ = ωk`。所以，位置编码的本质就是：在不同频率的二维平面上，随着位置 `t` 的增加，代表位置的向量在不断地**旋转，**而“前进 `k` 步” 就等同于将这个向量再“旋转 `ωk` 的角度”。
- 从 Sinusoidal 到 RoPE 的演进：
1. **抛弃加法**: 我们别在输入端用加法注入绝对编码了。这是一种“污染”，会改变原始词嵌入的语义。
1. **直接利用旋转**: 我们直接把这个旋转的思想应用在自注意力计算的核心——`Q` 和 `K` 的点积上。
# **三、相对位置编码**

当模型计算第 `i` 个词（query）对第 `j` 个词（key）的注意力分数时。就是在计算`q_rotated` 和 `k_rotated` 的点积，这个点积的结果**只和它们的相对位置 i-j 有关，与本身绝对位置无关**。

- 常见相对位置编码：
- **Transformer-XL, DeBERTa**: 在计算注意力分数 `score = q_i · k_j` 的时候，额外加上一个代表相对位置 `i-j` 的偏置项（bias）或可学习的向量。公式会变成类似于 `score = q_i · k_j + q_i · R_{i-j}`，其中 `R_{i-j}` 是一个专门用来表示相对距离 `i-j` 的嵌入向量。
- **RoPE 旋转编码：**它通过在计算 `q` 和 `k` 时，对它们进行“旋转”操作，使得 `q_i` 和 `k_j` 的点积结果**天然地蕴含了它们相对位置 **`**i-j**`** 的信息**。
**优点**

- **泛化能力强**: 由于模型学习的是通用的“相对N步”关系，而不是固定的“第N号”地址，它能更好地处理比训练时更长的序列。
- **更符合注意力机制的直觉**: 注意力本质上就是衡量词与词之间的关系，所以直接将相对位置信息注入到关系计算中，是更自然的设计。
**缺点**

- **实现更复杂**: 它需要修改注意力模块的内部结构，不像绝对编码那样即插即用。
## 3.1 RoPE

**RoPE 的 self-attention 操作的流程是**：

- 对于 token 序列中的每个词嵌入向量，首先计算其对应的 **q 和 k 向量**
- 然后对每个 token 位置都计算对应的**旋转位置编码**
- 接着对每个 token 位置的 q 和 k 向量的元素按照**两两一组**应用旋转变换，最后再计算q和k之间的内积得到 self-attention 的计算结果。
可以看到，RoPE 形式上和 Sinusoidal 位置编码有点相似，只不过 **Sinusoidal 位置编码是加性的，而 RoPE 可以视为乘性的。**

**简单结论：当 θ 为 **`**1 / 10000^(2i/d_model)**`** 时，有远程衰减特性。**

关于远程衰减性的证明：

- [浅析RoPE旋转位置编码的远程衰减特性](https%3A%2F%2Fblog.csdn.net%2Fluxurie%2Farticle%2Fdetails%2F135119538)
- [再探RoPE（二）：为什么RoPE + Bias能在远程衰减和长度外推上发挥重要作用？](https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F17397790476)
- [ROPE的远程衰减](https%3A%2F%2Fjuejin.cn%2Fpost%2F7538783589891768355)
- [RoPE外推的缩放法则 —— 尝试外推RoPE至1M上下文](https%3A%2F%2Fjuejin.cn%2Fpost%2F7293417195437129739)
- [十分钟读懂旋转编码（RoPE）](https%3A%2F%2Fwww.zhihu.com%2Ftardis%2Fzm%2Fart%2F647109286%3Fsource_id%3D1003)
# 四、核心推导

## 4.1 旋转矩阵推导

三角函数的和角公式：

1. `sin(A + B) = sin(A)cos(B) + cos(A)sin(B)`
1. `cos(A + B) = cos(A)cos(B) - sin(A)sin(B)`
在我们的场景中，我们令：

- `A = ωt` (代表当前位置 `t` 的相位角)
- `B = ωk` (代表相对距离 `k` 造成的相位角偏移)
所以，`ω(t+k)` 就是 `A+B`。

位置编码向量 `PE` 是由很多个 `(sin, cos)` 对组成的。我们现在只拿出**其中任意一对**来看，也就是固定一个频率 `ω`。

- 位置 `t` 的编码对是： `[ sin(ωt), cos(ωt) ]`
- 位置 `t+k` 的编码对是： `[ sin(ω(t+k)), cos(ω(t+k)) ]`
**对于 **`**sin**`** 部分：**
`sin(ω(t+k)) = sin(ωt + ωk)`
根据公式1，得到：
`sin(ω(t+k)) = sin(ωt) * cos(ωk) + cos(ωt) * sin(ωk)`

**对于 **`**cos**`** 部分：**
`cos(ω(t+k)) = cos(ωt + ωk)`
根据公式2，得到：
`cos(ω(t+k)) = cos(ωt) * cos(ωk) - sin(ωt) * sin(ωk)`

现在，我们要找到一个矩阵 `M_k`，使得 `M_k * v_t = v_{t+k}`。
`v_{t+k}` 是：
`[ cos(ω(t+k)) ] = [ cos(ωt)cos(ωk) - sin(ωt)sin(ωk) ]`
`[ sin(ω(t+k)) ] [ sin(ωt)cos(ωk) + cos(ωt)sin(ωk) ]`

让我们用矩阵乘法来凑出这个结果：
`M_k * [ cos(ωt) ] = [ cos(ωt)cos(ωk) - sin(ωt)sin(ωk) ]`
`M_k * [ sin(ωt) ] [ sin(ωt)cos(ωk) + cos(ωt)sin(ωk) ]`

得到的矩阵就是标准旋转矩阵：
`M_k = [ cos(ωk) -sin(ωk) ]`
`[ sin(ωk) cos(ωk) ]`

**核心结论**：对于任何固定的偏移量 `k`，从位置 `t` 的编码 `PE_t` 到位置 `t+k` 的编码 `PE_{t+k}`，存在一个**线性变换**（具体来说是旋转）。这个变换**只与偏移量 **`**k**`** 有关，与初始位置 **`**t**`** 无关**。

拓展阅读：[一文看懂 LLaMA 中的旋转式位置编码（Rotary Position Embedding）](https%3A%2F%2Fzhuanlan.zhihu.com%2Fp%2F642884818)

## 4.2 两个位置编码的点积仅取决于偏移量

应用差角公式：

![image](S8NYddJe1oXDYXxFgCxcBT4cn85.png)

## 4.3 位置编码的点积是无向的

也就是证明

![image](LGn4dy5wvoETLHxHw8zcAQdynPh.png)

基于 4.2 的证明，结合 cos 是偶函数可证。

**结论**：位置向量的点积可以用于表示**距离(distance-aware)**，但是它却不能用来表示位置的方向性 **(lack-of-directionality)**。

# 五、改进

## 5.1 长度外推

长度外推问题指的是，在较短长度的序列上训练的Transformer模型，难以在推理时有效处理超过训练时所见过最大长度的序列，导致性能显著下降。换言之，模型无法将在短文本上学到的知识泛化到更长的文本上。这极大地限制了Transformer模型在需要处理长文档、长对话或进行长篇内容生成等场景中的应用。

原理：

- **位置编码（Positional Encoding, PE）的局限性**：在推理时，模型会遇到训练期间从未见过的位置编码，这使得模型难以理解这些新位置的含义。
- **注意力机制分布的变化**：当处理更长的序列时，注意力机制需要处理的token数量远超训练时的数量，这可能导致注意力分布更加分散，降低模型对关键信息的聚焦能力
解决方案，主要可以归纳为以下几类：

### 位置编码优化

鉴于位置编码是导致问题的主要因素之一，大部分研究都集中在设计更具外推性的位置编码方案上。[[5](https%3A%2F%2Fwww.google.com%2Furl%3Fsa%3DE%26q%3Dhttps%253A%252F%252Fvertexaisearch.cloud.google.com%252Fgrounding-api-redirect%252FAUZIYQEWPvOzCENKhFj8sT7F3C2zp1B8CqnqNrL2vDmghzp6KgTM75UkG0mCjvPGe_-pwUHYdl26LEOWnpyX3jHwarrKgaY0XzTeGlZacXbtJnub7hAmjIJpgB7sySrWNVOJtrOOnsQ%253D)]

- **相对位置编码（Relative Position Encoding, RPE）**：与为每个位置分配唯一编码的绝对位置编码不同，相对位置编码关注于token之间的相对距离。这种方式天然地具备更好的外推能力，因为它处理的是相对关系，而不是绝对位置。其中，**旋转位置编码（Rotary Position Embedding, RoPE）** 因其优异的综合性能，已成为当前大语言模型中最主流的位置编码方式之一。RoPE通过旋转矩阵来编码位置信息，使得自注意力机制能够捕捉到token间的相对位置关系。
- **ALiBi（Attention with Linear Biases）**：ALiBi是一种新颖的方法，它不向词嵌入中添加位置编码，而是在计算注意力分数时，直接向注意力矩阵添加一个线性偏差（bias）。这个偏差与token间的距离成正比，距离越远的token对，其注意力分数受到的惩罚就越大，从而将注意力集中在局部范围内。ALiBI被证实拥有出色的长度外推能力。
- **随机化位置编码（Randomized Position Encoding）**：这种方法的核心思想是在训练过程中引入随机的位置编码，从而让模型在训练阶段就“见过”各种可能的位置，打破训练长度与测试长度之间的鸿沟。
### 位置插值（Position Interpolation）

位置插值是一种简单而有效的技术，它通过在推理时对位置编码进行缩放，使得原本超出模型训练长度的位置能够“落入”到已经训练过的位置区间内。例如，一个在2048长度上训练的模型，可以通过将4096个位置的编码线性插值到原始的2048个位置上来处理4096长度的序列。这种方法因其出色的外推性能和易于实现的特点而备受关注。

在位置插值的基础上，还衍生出了一些更先进的方案，例如**NTK-Aware Interpolation**和**YaRN (Yet another RoPE extensioN method)**，它们通过更精细的插值策略进一步提升了模型的长度外推能力。

### 修改注意力机制

除了从位置编码入手，直接修改注意力机制也是一个重要的研究方向。

- **局部注意力（Local Attention）**：强制模型只关注一个固定大小的局部窗口内的上下文信息，从而避免在处理长序列时注意力过于分散。许多外推性好的改进方法在某种意义上都可以看作是局部注意力的变体。
- **稀疏注意力（Sparse Attention）**：通过限制每个token只关注部分上下文，而不是全部上下文，来降低计算复杂度和内存消耗，同时也能在一定程度上缓解长度外推问题。
- **注意力偏差校准（Attention Bias Calibration, ABC）**：这是一种让模型自动学习合适的注意力偏差的技术，通过一个校准阶段来引导模型关注到对于任务更重要的信息，从而实现更好的长度泛化。
### 课程学习与自生成数据

近期的一些研究表明，通过课程学习（Curriculum Learning）和让模型从自己生成的更难的样本中学习，也可以提升其长度泛化能力。模型通过迭代式地生成和学习自己的解决方案，逐步解决更复杂的问题，从而在不改变模型架构的情况下实现对更长序列的泛化。

## 5.2 距离意识

进入attention层之后，内积的距离意识(distance-aware)的模式也遭到了破坏。更详细的细节，可以参见 https://arxiv.org/pdf/1911.04474

在Transformer的论文中，比较了用positional encoding和learnable position embedding(让模型自己学位置参数）两种方法，得到的结论是两种方法对模型最终的衡量指标差别不大。不过在后面的BERT中，已经改成用learnable position embedding的方法了，也许是因为positional encoding在进attention层后一些优异性质消失的原因（猜想）。

# 六、后续

阅读 [探秘Transformer系列](https%3A%2F%2Fwww.zhihu.com%2Fcolumn%2Fc_1889336819960743598) 系列，加深理解

## 相关

- [[LLaMA|LLaMA]]
- [[T5|T5]]
- [[Qwen-VL|Qwen-VL]]
- [[Transformer 通识|Transformer 通识]]
