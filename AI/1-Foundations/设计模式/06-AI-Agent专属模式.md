---
title: AI Agent 专属模式
tags: [design-pattern, ai-agent, llm, multi-agent, agentic-workflow]
date: 2026-02-28
---

# AI Agent 专属模式

> LLM 时代涌现的新模式——这些是传统软件工程书里没有的。
> 它们是 GoF + 分布式模式在 AI 语境下的**重新组合与进化**，但也有真正的新东西。
> 来源：Andrew Ng（DeepLearning.AI）、Anthropic、LangGraph、Google Vertex AI 等实践总结。

---

## 一、单 Agent 内部模式

### 1. 反思模式（Reflection）⭐

**问题**：LLM 一次生成的质量有限，但给它机会检查自己的输出，质量会显著提升。

```
输入  →  [生成]  →  草稿
                     ↓
                  [反思/批评]  →  改进建议
                     ↓
                  [修改]  →  更好的草稿
                     ↓
                  （迭代 N 轮）
                     ↓
                  最终输出
```

**两种变体**：
- **自我反思**：同一个 LLM 既生成又批评（简单，但可能偏见同源）
- **双 Agent 反思**：一个 Agent 生成，另一个 Agent 批评（更可靠，成本更高）

```
生成 Agent  ←→  批评 Agent
   写代码         找 bug
   写文章         挑逻辑漏洞
```

**本质**：用迭代替代一次性生成，质量换时间。
**理论基础**：人类写作也是"写→改→改"，不是一次写完。

**AI 映射**：
- 代码生成 → 自动测试 → 根据错误修复 → 再测试（SWE-bench 的主流路线）
- ExpeL / Reflexion：把反思结果存入记忆，下次任务不重复同样错误
- RAGEN / StarPO：RL 训练时的自我纠错机制

---

### 2. 计划-执行模式（Plan-and-Execute）⭐

**问题**：复杂任务不能边走边看，需要先有全局计划，再逐步执行。

```
用户请求
    ↓
[规划器 Planner]  →  任务分解树
  ├─ 子任务1
  ├─ 子任务2
  └─ 子任务3
    ↓
[执行器 Executor]  →  逐步执行各子任务
    ↓
[重规划]  →  根据执行结果调整计划（可选）
    ↓
最终结果
```

**vs ReAct（边想边做）**：
```
ReAct：        Think → Act → Observe → Think → Act → ...（交替进行）
Plan-Execute： Plan（一次） → Execute → Execute → Execute（计划先行）
```

**适合场景**：
- ReAct 适合短任务、需要即时根据环境反应的场景
- Plan-Execute 适合长任务、需要全局协调的场景

**经典实现**：LangGraph 的 Plan-and-Execute、GPT-Engineer 的多步编码规划

**AI 映射**：
- 软件开发任务（先设计架构 → 再逐步实现各模块）
- 研究任务（先确定研究问题 → 再逐步搜集资料）
- MANUS 等 Agent 的骨架

---

### 3. 工具增强模式（Tool Augmentation）⭐

**问题**：LLM 本身只能处理文本，但真实任务需要访问外联系统、执行代码、搜索实时信息。

```
LLM  →  识别需要调用工具  →  生成工具调用参数
                                    ↓
                           [工具执行层]
                           ├─ 搜索引擎
                           ├─ 代码执行器
                           ├─ 数据库查询
                           ├─ API 调用
                           └─ 文件系统
                                    ↓
                           结果返回 LLM  →  继续推理
```

**核心设计原则**（来自 Token Masking 统一框架）：
- 工具调用的 observation tokens **不参与梯度更新**（工具输出不是 LLM 生成的）
- 工具是 LLM 的感知器官，不是 LLM 的一部分

**MCP（Model Context Protocol）**的本质就是标准化工具调用接口——让任何工具都能被任何 LLM 调用。

**AI 映射**：
- ReAct 的 Act 步骤
- Function Calling / Tool Use（OpenAI / Anthropic API）
- 贾维斯的工具调用层

---

### 4. 记忆增强模式（Memory Augmentation）

**问题**：LLM 的上下文窗口有限，无法记住长期信息；每次对话从零开始。

```
Agent
  ├─ 工作记忆（Context Window）：当前对话，短暂，有限
  ├─ 情节记忆（向量库）：过去经历，语义检索，RAG
  ├─ 语义记忆（知识图谱/文档库）：结构化知识
  └─ 程序记忆（技能库）：可复用的操作序列/工具

每次推理前：检索相关记忆 → 注入上下文
每次推理后：写入新记忆
```

**四层记忆对应人类认知**：
- 工作记忆 ↔ 短时记忆
- 情节记忆 ↔ 个人经历
- 语义记忆 ↔ 世界知识
- 程序记忆 ↔ 技能/习惯

**AI 映射**：
- MemGPT、Letta 的核心架构
- 你们的 Vault（morpheus-vault）就是学者 Scholar 的语义记忆层
- MEMORY.md / heartbeat-state.json 是工作记忆层

---

### 5. 验证-修复循环（Verify-and-Fix Loop）

**问题**：代码/SQL/数学公式等 verifiable 输出，可以自动执行验证，失败就让 LLM 修复。

```
生成输出  →  执行/验证
                ↓
          成功？
          ├─ 是  →  返回结果
          └─ 否  →  错误信息 + 修复提示  →  LLM 重新生成
                         ↑__________________________|
                              （最多重试 N 次）
```

**本质**：把可执行验证变成自动化反馈信号。
**与 RLVR 的关系**：Verify-and-Fix 是推理时的 loop，RLVR 是训练时把这个 loop 的信号变成奖励。

**AI 映射**：
- 代码 Agent（写代码 → 跑单测 → 修 bug）
- SQL Agent（生成 SQL → 执行 → 语法错误/结果异常 → 修正）
- 数学推理（草稿 → 用符号计算器验证 → 修正）

---

## 二、多 Agent 协作模式

### 6. 移交模式（Handoff）⭐

**问题**：一个任务到达某个点，当前 Agent 不再是最合适的处理者——需要移交给专业 Agent。

```
用户: "帮我订机票并安排酒店"

对话 Agent
    ├─ 意图识别：需要订票 + 订酒店
    ├─ 移交 → 机票 Agent（携带上下文）
    └─ 移交 → 酒店 Agent（携带上下文）

关键：移交时必须携带完整上下文（否则用户要重新解释）
```

**移交 vs 委托**：
- **移交（Handoff）**：完全交出控制权，原 Agent 退出
- **委托（Delegation）**：子任务交出去，父 Agent 等结果，保留控制权

**OpenAI Swarm 的核心机制**就是 Handoff——Agent 可以把对话控制权移交给另一个 Agent。

**AI 映射**：
- 客服系统：通用客服 → 专业客服（退款/技术支持）
- 研究助手：问答 Agent → 搜索 Agent → 写作 Agent

---

### 7. 辩论模式（Debate / Multi-Agent Verification）

**问题**：单个 LLM 可能有偏见或幻觉，多个 Agent 相互质疑可以发现更多问题。

```
问题
  ├─  Agent A → 答案A + 论据
  ├─  Agent B → 答案B + 论据
  └─  Agent C → 答案C + 论据
         ↓
  [辩论轮次]：各 Agent 看到对方答案，提出质疑
         ↓
  [裁判 Agent 或投票]：综合得出最终答案
```

**本质**：引入对抗性验证，让错误更难通过。
**理论基础**：MIT 研究表明 Multi-Agent Debate 在数学推理上显著优于单 Agent。

**AI 映射**：
- 代码审查（生成 Agent + 审查 Agent）
- 事实核查（多个 Agent 独立查证，不一致的再深查）
- SCoRe 的两阶段 RL：Phase 1 训练多样性，Phase 2 自我纠错

---

### 8. 混合专家模式（Mixture of Agents，MoA）

**问题**：不同 LLM 在不同任务上各有优势，能不能像 MoE 一样组合它们？

```
输入问题
    ↓
[路由器 / 所有模型并行]
  ├─ Model A（擅长推理）→ 答案A
  ├─ Model B（擅长代码）→ 答案B
  └─ Model C（擅长创意）→ 答案C
    ↓
[聚合器 LLM]  →  综合三个答案  →  最终答案

TogetherAI 的 MoA 实验：GPT-4 级别的质量，用小模型混合实现
```

**vs 传统 MoE**：
- MoE：模型内部的专家，共享权重，token 级路由
- MoA：模型级别的专家，独立模型，响应级路由

**AI 映射**：
- 散播-聚集（Scatter-Gather）在 AI 层的具体实现
- Self-Consistency：同一模型多次采样，取多数答案

---

### 9. 分层规划模式（Hierarchical Planning）

**问题**：复杂任务分解后，子任务还是太复杂，需要多层递归分解。

```
任务（L0）
  ├─ 子任务A（L1）
  │    ├─ 子子任务A1（L2，原子操作）
  │    └─ 子子任务A2（L2，原子操作）
  └─ 子任务B（L1）
       └─ 子子任务B1（L2，原子操作）

每层有自己的规划 Agent，只知道自己这层的问题
```

**本质**：组合模式（Composite）在 Agent 规划上的应用。

**AI 映射**：
- 大型软件工程任务（系统架构师 → 模块负责人 → 开发者）
- Gas Town 项目（Mayor Agent → 专业 Agent 团队）
- 你们军团的 J.A.R.V.I.S. → 各专业 Agent → 子任务执行

---

### 10. 选举/共识模式（Leader Election / Consensus）

**问题**：分布式 Agent 系统中，没有预设的领导者——需要自主选出一个协调者，或者对某个决策达成共识。

```
共识（Consensus）：
  Agent A：我觉得答案是 X
  Agent B：我觉得答案是 X
  Agent C：我觉得答案是 Y
  → 多数投票 → X 胜出

选举（Leader Election）：
  某个 Agent 挂了，触发选举
  → Raft / Paxos 算法 → 新 Leader 产生
  → 新 Leader 接管协调工作
```

**AI 映射**：
- 去中心化多 Agent 系统中的动态协调者选择
- 对抗性 Agent 任务的裁判机制（谁的答案更可信？）

---

## 三、模式的进化方向

```
传统软件设计模式（1994, GoF）
    ↓
分布式系统模式（2000s, 微服务时代）
    ↓
AI Agent 模式（2023-2026）
    ↓
下一步（推测）：
  - 自适应模式：Agent 自主发现并采用最优协作模式
  - 进化模式：系统通过 RL 学习新的协作结构
  - 具身模式：物理世界中 Agent 的协作模式
```

---

## AI Agent 模式全景图

```
单 Agent 内部
┌─────────────────────────────────────────────────────┐
│  输入 → [记忆检索] → [规划] → [工具调用] → [输出]   │
│                         ↑                           │
│              [反思/验证-修复 循环]                   │
└─────────────────────────────────────────────────────┘

多 Agent 协作（选其一或组合）
┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│  管道         │  │   黑板        │  │  监督者       │
│ A→B→C→D      │  │  共享工作区   │  │  中心管控     │
└──────────────┘  └──────────────┘  └──────────────┘
┌──────────────┐  ┌──────────────┐  ┌──────────────┐
│  蜂群         │  │  辩论         │  │  分层规划     │
│  去中心并行   │  │  对抗验证     │  │  递归分解     │
└──────────────┘  └──────────────┘  └──────────────┘

弹性与可靠性（必选项，生产环境）
熔断器 + 重试 + 超时 + 舱壁
```

---

## 完整模式关系图

```
用户请求
    │
    ▼
[API 网关]──────────────────────────────────────────────
    │         鉴权/限速/路由
    ▼
[外观 Facade / Orchestrator]
    │
    ├──[策略模式]── 选路由策略
    │
    ├──[管道/黑板/蜂群]── 选协作模式
    │         │
    │    各 Agent
    │         │
    │    [工具增强] ← MCP / Function Calling
    │    [记���增强] ← RAG / Vector DB
    │    [反思循环] ← 自我批评
    │    [验证修复] ← 可执行验证
    │
    ├──[熔断器/重试/超时]── 弹性保障
    │
    └──[事件溯源/CQRS]── 数据层
```
