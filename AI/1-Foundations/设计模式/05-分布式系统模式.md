---
title: 分布式系统模式
tags: [design-pattern, distributed-systems, microservices, resilience]
date: 2026-02-28
---

# 分布式系统模式

> GoF 是单机代码的设计语言，分布式系统模式是网络时代的设计语言。
> 核心挑战变了：网络会断、机器会挂、延迟不可预测、数据需要一致性。

---

## 一、弹性模式（Resilience Patterns）

### 1. 熔断器（Circuit Breaker）⭐

**问题**：下游服务挂了，你还在不断发请求 → 资源耗尽 → 级联崩溃（雪崩效应）。

```
                     ┌─────────────────────────────┐
                     │        Circuit Breaker       │
请求  →  [熔断器]  → │  Closed（正常，放行）         │  →  下游服务
                     │  Open（熔断，直接拒绝）        │
                     │  Half-Open（探测，少量放行）   │
                     └─────────────────────────────┘

状态转移：
  Closed  →（失败率超阈值）→  Open
  Open    →（冷却时间到）→  Half-Open
  Half-Open →（探测成功）→  Closed
  Half-Open →（探测失败）→  Open
```

**本质**：主动放弃，保护自己，给下游恢复时间。
**现实类比**：家里保险丝——过载时自动断开，不让整个电路烧掉。

**AI 映射**：
- 模型 API 超时 / 限速 → 熔断，切换到 fallback 模型
- 工具调用失败率过高 → 熔断，返回降级响应
- Agent 下游数据库不可用 → 熔断，用缓存数据临时应对

---

### 2. 重试模式（Retry）

**问题**：网络抖动导致请求偶发失败，但其实重试一下就好了。

```
请求失败
  ↓
等待 1s  →  重试
  ↓
等待 2s  →  重试
  ↓
等待 4s  →  重试（指数退避）
  ↓
放弃，返回错误
```

**关键设计**：
- **固定间隔**：简单，但可能造成惊群（所有客户端同时重试）
- **指数退避（Exponential Backoff）**：每次等待时间翻倍
- **加抖动（Jitter）**：在退避时间上加随机量，避免惊群

**注意**：重试只适合**幂等操作**（重复执行结果相同）。非幂等操作（支付、发消息）不能无脑重试。

**AI 映射**：
- LLM API 调用（限速 429 / 超时）→ 指数退避重试
- 工具调用失败 → 重试 + 错误信息反馈给 Agent 调整策略

---

### 3. 舱壁模式（Bulkhead）

**问题**：一个慢请求占满所有线程/连接，导致其他请求也被饿死。

```
没有舱壁：
[服务A请求] × 100  →  占满所有100个线程  →  服务B请求全部等待超时

有舱壁：
[线程池A: 60个] → 服务A独享
[线程池B: 40个] → 服务B独享
服务A挂了，服务B不受影响
```

**本质**：隔离资源，一个部分的故障不蔓延到其他部分。
**现实类比**：船舱隔板——一个舱进水，不沉整艘船。

**AI 映射**：
- 不同 Agent 使用独立的模型推理资源池
- 高优先级任务（实时响应）和低优先级任务（批处理）用不同线程池隔离

---

### 4. 超时模式（Timeout）

**问题**：下游一直不返回，请求永远挂着——不如主动放弃。

```
请求发出  →  等待  →  [超时阈值]  →  放弃，返回错误
                           ↑
                     比 P99 延迟稍大（不太短，不太长）
```

**关键**：超时值怎么设？太短误判，太长资源浪费。通常设为 P99 延迟的 1.5-2 倍。

**AI 映射**：
- 工具调用��时（搜索、代码执行）
- LLM 推理超时 → 触发 fallback
- 心跳检测（超时视为 Agent 挂掉）

---

## 二、数据模式（Data Patterns）

### 5. 事件溯源（Event Sourcing）⭐

**问题**：传统数据库只存"当前状态"，丢失了所有历史——出了问题无法审计，也无法回滚。

```
传统方式（存状态）：
  用户余额 = 100

事件溯源（存事件）：
  事件1：充值 +200
  事件2：消费 -50
  事件3：消费 -50
  当前状态 = 从头重放所有事件 = 100

需要"3天前的余额"？从事件1重放到那个时间点即可
```

**本质**：存变化（事件），不存结果（状态）；状态 = 事件序列的投影。

**三大好处**：
- **完整审计日志**：每一步都有记录
- **时间旅行**：重放到任意历史时刻
- **事件驱动集成**：其他系统订阅事件，松耦合

**AI 映射**：
- Agent 操作日志（每个 tool call、每个决策都作为事件存储）
- 对话历史 = 事件序列（不只存最终状态，存每一轮交互）
- 可回放对话轨迹，Debug Agent 行为

---

### 6. CQRS（命令查询职责分离）

**问题**：读和写的需求完全不同——写需要强一致，读需要高性能——为什么要用同一个模型？

```
传统：一个模型处理所有读写
  ┌──────────────┐
  │   数据库      │  ←→  所有操作
  └──────────────┘

CQRS：读写分离
  Command（写） → 写模型（强一致，范式化）
  Query（读）   → 读模型（高性能，非范式化，可多个视图）

两个模型通过事件同步（通常结合 Event Sourcing）
```

**本质**：读和写用不同的数据模型，各自优化。

**AI 映射**：
- Agent 的写操作（更新记忆、写日志）和读操作（检索上下文）用不同存储
- 向量库（读优化）+ 关系数据库（写优化），CQRS 视角看 RAG 架构

---

### 7. Saga 模式⭐

**问题**：跨多个服务的分布式事务——传统 2PC 太重，微服务不支持。

```
订单流程（跨4个服务）：
  Step1: 扣库存  → Step2: 扣余额  → Step3: 创建物流  → Step4: 发通知

如果 Step3 失败：
  补偿事务（逆序）：
  取消通知 ← 取消物流 ← 退余额 ← 恢复库存
```

**两种实现**：
- **编排式（Choreography）**：每个服务完成后发事件，下一个服��订阅事件自行触发（去中心化）
- **指挥式（Orchestration）**：一个 Saga Orchestrator 依次调用各服务（中心化，更好控制）

**AI 映射**：
- 多步骤 Agent 任务的事务管理（任一步失败 → 触发补偿动作回滚）
- 复杂业务流程（订票：选座→锁座→支付→出票，任何步骤失败都要补偿）

---

## 三、服务治理模式（Service Governance）

### 8. API 网关（API Gateway）

**问题**：客户端需要和 N 个微服务打交道——每个服务的地址不同，鉴权各管各的。

```
客户端  →  API 网关  →  服务A
                    →  服务B
                    →  服务C

网关统一处理：鉴权、限速、路由、负载均衡、日志、SSL卸载
```

**本质**：外观模式（Facade）在网络层的应用。

**AI 映射**：
- LLM API 网关（统一鉴权、限速、模型路由、计费）
- Agent 系统的统一入口（LiteLLM、OpenRouter 的本质就是 API Gateway）

---

### 9. Sidecar 模式

**问题**：每个服务都需要日志收集、健康检测、配置管理……能不能不让业务代码关心这些？

```
[业务容器]  +  [Sidecar 容器]
               ├─ 日志收集
               ├─ 健康检测
               ├─ 服务发现
               └─ 流量监控

两者运行在同一个 Pod/主机，共享网络和存储
业务代码零感知
```

**本质**：把横切关注点（cross-cutting concerns）移到独立的伴生进程。

**AI 映射**：
- Agent 的 Sidecar：安全过滤器（内容审核）、使用量监控、缓存层
- Memory 模块作为 Agent 的 Sidecar（不改主 Agent 逻辑，独立附加记忆能力）

---

### 10. 服务网格（Service Mesh）

**问题**：微服务多了，服务间通信的治理（重试、熔断、追踪、加密）散落在各业务代码里，维护噩梦。

```
没有服务网格：
  每个服务自己实现重试/熔断/追踪（重复造轮子）

有服务网格（Istio/Linkerd）：
  每个服务旁有 Sidecar Proxy（Envoy）
  所有流量经过 Proxy
  治理逻辑统一在控制平面配置（业务代码完全不感知）
```

**本质**：Sidecar 模式的集群化应用——把整个网络治理层抽象出来。

**AI 映射**：
- Multi-Agent 系统的通信治理层（统一配置 Agent 间通信的限速、重试、追踪）
- 未来 Agent 基础设施的演进方向：Agent Mesh

---

### 11. 幂等消费者（Idempotent Consumer）

**问题**：消息队列可能重复投递（at-least-once 语义），消费者收到重复消息可能导致重复操作。

```
消息 ID="abc123"

消费者收到消息 → 检查 ID 是否已处理过
  ├─ 是：直接丢弃（幂等）
  └─ 否：处理 + 记录 ID

无论消息投递几次，结果都一样
```

**AI 映射**：
- Agent 的工具调用去重（同一个 tool call 不重复执行）
- 事件驱动的 Agent 任务（消息重投 → 不重复触发动作）

---

## 四、消息模式（EIP，企业集成模式 65 种的精华）

### 12. 消息路由器（Message Router）

```
消息  →  路由器  →  根据内容/规则  →  目标A
                                 →  目标B
                                 →  目标C
```

**AI 映射**：意图识别后路由到不同 Agent（搜索意图→搜索Agent，写作意图→写作Agent）

---

### 13. 消息过滤器（Message Filter）

```
消息流  →  [过滤器：只通过符合条件的]  →  下游
```

**AI 映射**：内容安全过滤（只有通过审核的消息才发给 Agent）

---

### 14. 聚合器（Aggregator）

```
并行请求A结果 ─→
并行请求B结果 ─→  [聚合器：等齐后合并]  →  完整结果
并行请求C结果 ─→
```

**AI 映射**：
- 并行多路检索结果聚合
- Fan-out 多个 Agent 并行处理，最后汇总答案（Mixture-of-Agents）

---

### 15. 散播-聚集（Scatter-Gather）

```
请求  →  广播给 N 个候选  →  [聚合器：取最好的]  →  最终结果
         （并行执行）
```

**AI 映射**：
- 多模型并行采样，取最高置信度的答案（Self-Consistency 的本质）
- 多个搜索引擎并行查询，聚合结果去重

---

## 分布式模式速查

| 类别 | 模式 | 核心问题 |
|---|---|---|
| 弹性 | 熔断器 | 下游故障不级联 |
| 弹性 | 重试+退避 | 瞬时失败自动恢复 |
| 弹性 | 舱壁 | 故障隔离不蔓延 |
| 弹性 | 超时 | 防止无限等待 |
| 数据 | 事件溯源 | 完整历史可回放 |
| 数据 | CQRS | 读写分离各自优化 |
| 数据 | Saga | 分布式事务补偿 |
| 治理 | API 网关 | 统一入口 |
| 治理 | Sidecar | 横切关注点剥离 |
| 治理 | 服务网格 | 集群级通信治理 |
| 消息 | 路由器 | 按规则分发 |
| 消息 | 聚合器 | 并行结果汇聚 |
| 消息 | 散播-聚集 | 并行采样取最优 |
