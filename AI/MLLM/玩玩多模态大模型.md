---
title: "玩玩多模态大模型"
type: concept
domain: ai/mllm
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/mllm
  - type/concept
---
# 玩玩多模态大模型

> 多模态大模型入门实践笔记 — 从 MiniCPM-o 开始。
> 参考：https://github.com/OpenBMB/MiniCPM-o

## 为什么从 MiniCPM-o 开始

选择 MiniCPM-o 的原因很简单：**小、快、能跑**。

```
模型矩阵对比（2025年初）:
┌──────────────────┬────────┬──────────┬──────────────┐
│ 模型              │ 参数量  │ 单卡推理  │ 开源程度      │
├──────────────────┼────────┼──────────┼──────────────┤
│ MiniCPM-o 2.6    │ 8B     │ ✅ 16GB  │ 全开源        │
│ InternVL2-8B     │ 8B     │ ✅ 16GB  │ 全开源        │
│ Qwen2-VL-7B     │ 8B     │ ✅ 16GB  │ 全开源        │
│ GPT-4o           │ ?      │ ❌ API   │ 闭源          │
│ InternVL2-76B   │ 76B    │ ❌ 多卡   │ 全开源        │
└──────────────────┴────────┴──────────┴──────────────┘
```

MiniCPM-o 的特色是支持**端侧部署**（手机都能跑），并且在同等参数量下效果不错。

## 快速上手

### 安装

```bash
pip install transformers torch accelerate
# MiniCPM-o 的自定义依赖
pip install timm sentencepiece
```

### 基础推理

```python
from transformers import AutoModel, AutoTokenizer
from PIL import Image

model_name = "openbmb/MiniCPM-o-2_6"
model = AutoModel.from_pretrained(model_name, trust_remote_code=True, torch_dtype=torch.float16)
model = model.to("cuda")
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)

# 单图理解
image = Image.open("test.jpg")
question = "描述这张图片的内容"

msgs = [{"role": "user", "content": [image, question]}]
response = model.chat(msgs=msgs, tokenizer=tokenizer)
print(response)
```

### 多图对比

```python
# 多图输入 — 比较两张图片的差异
img1 = Image.open("before.jpg")
img2 = Image.open("after.jpg")

msgs = [{"role": "user", "content": [img1, img2, "这两张图片有什么区别？"]}]
response = model.chat(msgs=msgs, tokenizer=tokenizer)
```

### OCR 场景

```python
# 文档 OCR — MiniCPM-o 在 OCR 上表现不错
doc_image = Image.open("invoice.png")
msgs = [{"role": "user", "content": [doc_image, "请提取发票中的所有信息，以JSON格式输出"]}]
response = model.chat(msgs=msgs, tokenizer=tokenizer)
```

## 几个常见场景的 prompt 技巧

**1. 图表理解** — 要明确告诉模型输出格式：
```
请分析这张图表，以Markdown表格格式输出数据，并总结趋势。
```

**2. 代码截图** — 先识别再解释：
```
请先识别截图中的代码，然后解释这段代码的功能。
```

**3. UI 设计稿** — 转代码：
```
请根据这个设计稿生成对应的 HTML + TailwindCSS 代码。
```

## 多模态模型的通用评估维度

自己玩模型的时候，可以从这几个维度评估：

```python
evaluation_dimensions = {
    "基础视觉": ["物体识别", "场景理解", "颜色/形状/数量"],
    "文字识别": ["印刷体 OCR", "手写体", "多语言", "复杂排版"],
    "推理能力": ["空间关系", "因果推理", "数学/图表"],
    "指令遵循": ["格式控制", "长度控制", "多步骤任务"],
    "实用场景": ["文档理解", "代码识别", "UI 分析"],
}

# 每个维度准备 5-10 个测试用例
# 用 1-5 分评分，记录失败 case 和原因
```

## 我的体验总结

玩了一圈下来的感受：

1. **OCR 场景差异大** — 标准印刷体各家都行，但手写体和复杂排版差异明显
2. **上下文长度是瓶颈** — 高分辨率图片吃大量 token，多图场景容易超限
3. **Prompt 影响巨大** — 同一张图，不同 prompt 效果天差地别
4. **量化有损** — INT4 量化后 OCR 精度明显下降，细节理解能力也变差

对于入门玩家，建议路线：MiniCPM-o / Qwen2-VL-7B 入门 → InternVL2-8B 进阶 → 根据场景选型。

## 相关

- [[AI/MLLM/DeepSeek-VL|DeepSeek-VL]] — DeepSeek 多模态系列
- [[AI/MLLM/InternVL3|InternVL3]] — InternVL 系列
- [[AI/MLLM/Qwen 2.5 VL-Unsloth训练|Qwen 2.5 VL 训练]] — Qwen VLM 微调
- [[AI/MLLM/DeepSeek-OCR 原理|DeepSeek-OCR]] — OCR 专项
- [[AI/MLLM/Seed1.5-VL|Seed1.5-VL]] — 字节方案
