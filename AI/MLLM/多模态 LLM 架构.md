---
title: "å¤šæ¨¡æ€ LLM æ¶æ„å…¨æ™¯"
brief: "VLM çš„ä¸‰å¤§è®¾è®¡é€‰æ‹©â€”â€”Vision Encoderï¼ˆCLIP/SigLIP/InternViTï¼‰ã€Projectorï¼ˆLinear/MLP/Q-Former/Perceiverï¼‰ã€è®­ç»ƒç­–ç•¥ï¼ˆä¸‰é˜¶æ®µ Pretrainâ†’SFTâ†’RLHFï¼‰çš„æ·±åº¦å¯¹æ¯”ï¼Œå« LLaVA/Qwen-VL/InternVL ä¸‰å¤§ç³»åˆ—çš„æ¶æ„å·®å¼‚åˆ†æå’ŒåŠ¨æ€åˆ†è¾¨ç‡/Token å‹ç¼©ç­‰å…³é”®æŠ€æœ¯ã€‚"
date: 2026-02-13
updated: "2026-02-22"
tags:
  - ai/mllm
  - ai/vision-language
  - ai/llm/architecture
  - type/survey
  - interview/hot
status: complete
sources:
  - "Radford et al. 'Learning Transferable Visual Models From Natural Language Supervision (CLIP)' arXiv:2103.00020"
  - "Alayrac et al. 'Flamingo: a Visual Language Model for Few-Shot Learning' arXiv:2204.14198"
  - "Liu et al. 'Visual Instruction Tuning (LLaVA)' arXiv:2304.08485"
  - "Liu et al. 'Improved Baselines with Visual Instruction Tuning (LLaVA-1.5)' arXiv:2310.03744"
  - "Li et al. 'BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models' arXiv:2301.12597"
  - "Chen et al. 'InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks' arXiv:2312.14238"
related:
  - "[[AI/MLLM/MLLM æ¦‚è¿°]]"
  - "[[AI/MLLM/CLIP|CLIP]]"
  - "[[AI/MLLM/BLIP-2|BLIP-2]]"
  - "[[AI/MLLM/InternVL3|InternVL3]]"
  - "[[AI/MLLM/Qwen-VL|Qwen-VL]]"
---

# å¤šæ¨¡æ€ LLM æ¶æ„å…¨æ™¯

> Vision-Language Model çš„æ ¸å¿ƒé—®é¢˜ï¼šå¦‚ä½•è®© LLM "çœ‹æ‡‚" å›¾ç‰‡â€”â€”Vision Encoder + Projector + LLM çš„ä¸‰ä»¶å¥—

## 1. é€šç”¨æ¶æ„

```mermaid
flowchart TD
    IMG["ğŸ–¼ï¸ Image"] --> VE["Vision Encoder<br/>(ViT / SigLIP)"]
    VE -->|"Visual Tokens"| PROJ["Projector<br/>(Linear / MLP / Q-Former)"]
    PROJ -->|"Aligned Tokens"| LLM["LLM<br/>(LLaMA / Qwen)"]
    TXT["ğŸ“ Text Tokens"] --> LLM
    LLM --> OUT["Output Tokens"]
```

**ä¸‰å¤§è®¾è®¡é€‰æ‹©**ï¼š
1. Vision Encoderï¼šæå–è§†è§‰ç‰¹å¾çš„ã€Œçœ¼ç›ã€
2. Projectorï¼šå¯¹é½è§†è§‰-è¯­è¨€ç©ºé—´çš„ã€Œæ¡¥æ¢ã€
3. è®­ç»ƒç­–ç•¥ï¼šåˆ†é˜¶æ®µé‡Šæ”¾å‚æ•°çš„ã€Œè¯¾ç¨‹ã€

## 2. Vision Encoder é€‰å‹

### ä¸»æµé€‰æ‹©

| Encoder | å‚æ•°é‡ | åˆ†è¾¨ç‡ | è®­ç»ƒæ•°æ® | ç‰¹ç‚¹ | ä½¿ç”¨æ¨¡å‹ |
|---------|--------|--------|---------|------|---------|
| CLIP ViT-L/14 (arXiv:2103.00020) | 304M | 224â†’336 | 400M image-text | æœ€ç»å…¸ï¼Œå¯¹é½å¥½ | LLaVA-1.5 |
| SigLIP SO400M | 400M | 384 | 4B pairs | Sigmoid lossï¼Œæ— éœ€è´Ÿæ ·æœ¬ | LLaVA-1.6, PaliGemma |
| InternViT-6B | 6B | 448 | ä¸“æœ‰æ•°æ® | æœ€å¤§å¼€æº ViT | InternVL 2/2.5 |
| EVA-02-E | 4.4B | 224 | LAION | CLIP æ”¹è¿›ç‰ˆ | â€” |
| è‡ªè®­ç»ƒ ViT | å˜åŒ– | åŠ¨æ€ | ä¸“æœ‰ | ä¸ LLM è”åˆè®­ç»ƒ | Qwen-VL, Qwen2-VL |

### åˆ†è¾¨ç‡ç­–ç•¥

```
å›ºå®šåˆ†è¾¨ç‡:
  ä¼˜: ç®€å•é«˜æ•ˆï¼Œtoken æ•°å›ºå®š
  åŠ£: å°ç›®æ ‡/å¯†é›†æ–‡æœ¬è¯†åˆ«å·®
  ä»£è¡¨: LLaVA-1.5 (336Ã—336, 576 tokens)

åŠ¨æ€åˆ†è¾¨ç‡ (AnyRes/NaViT):
  å°†é«˜åˆ†è¾¨ç‡å›¾ç‰‡åˆ‡æˆå¤šä¸ª tileï¼Œæ¯ä¸ª tile ç‹¬ç«‹ç¼–ç ï¼š

  é«˜åˆ†è¾¨ç‡å›¾ç‰‡ (1344Ã—1344)
  â”Œâ”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”
  â”‚tileâ”‚tileâ”‚tileâ”‚tileâ”‚   æ¯ä¸ª tile: 336Ã—336
  â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤   16 tiles Ã— 576 tokens = 9216 tokens
  â”‚tileâ”‚tileâ”‚tileâ”‚tileâ”‚
  â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤   + 1 ä¸ªç¼©ç•¥å›¾ (å…¨å±€ä¸Šä¸‹æ–‡)
  â”‚tileâ”‚tileâ”‚tileâ”‚tileâ”‚
  â”œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”¤   æ€»: ~10K visual tokens
  â”‚tileâ”‚tileâ”‚tileâ”‚tileâ”‚
  â””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”´â”€â”€â”€â”€â”˜

  ä»£è¡¨: LLaVA-NeXT, InternVL 2.5, Qwen2-VL

Qwen2-VL 2D RoPE:
  ä¸åˆ‡ tileï¼Œç›´æ¥ç”¨ ViT å¤„ç†åŠ¨æ€åˆ†è¾¨ç‡
  ç”¨ 2D Rotary Position Embedding ç¼–ç  (h, w) ä½ç½®
  é¿å… tile è¾¹ç•Œä¼ªå½±
```

## 3. Projector è®¾è®¡

### Linear Projection

```python
# LLaVA-1.0: æœ€ç®€å•çš„çº¿æ€§æŠ•å½±
class LinearProjector(nn.Module):
    def __init__(self, vision_dim, llm_dim):
        super().__init__()
        self.proj = nn.Linear(vision_dim, llm_dim)

    def forward(self, visual_features):
        return self.proj(visual_features)  # (B, N_vis, D_vis) â†’ (B, N_vis, D_llm)
```

**ä¼˜ç‚¹**ï¼šå‚æ•°å°‘ã€è®­ç»ƒå¿«ã€ä¸ä¸¢å¤±ä¿¡æ¯ã€‚
**ç¼ºç‚¹**ï¼švisual token æ•°é‡ä¸å˜ï¼ˆ576-10Kï¼‰ï¼Œå ç”¨å¤§é‡ LLM ä¸Šä¸‹æ–‡ã€‚

### MLP Projector

```python
# LLaVA-1.5+, InternVL: 2 å±‚ MLP
class MLPProjector(nn.Module):
    def __init__(self, vision_dim, llm_dim):
        super().__init__()
        self.proj = nn.Sequential(
            nn.Linear(vision_dim, llm_dim),
            nn.GELU(),
            nn.Linear(llm_dim, llm_dim),
        )

    def forward(self, visual_features):
        return self.proj(visual_features)
```

**2025 å¹´ä¸»æµé€‰æ‹©**â€”â€”ç®€å•æœ‰æ•ˆï¼Œæ¯” Linear å¤šä¸€å±‚éçº¿æ€§å˜æ¢ã€‚

### Q-Former (BLIP-2)

å‚è§ [[BLIP-2]]ã€‚é€šè¿‡ä¸€ç»„å¯å­¦ä¹  query tokens ä» visual features ä¸­æå–å›ºå®šæ•°é‡çš„è¡¨ç¤ºï¼š

```python
class QFormer(nn.Module):
    """ç®€åŒ–ç‰ˆ Q-Former"""
    def __init__(self, n_queries=32, vision_dim=1024, hidden_dim=768):
        super().__init__()
        self.queries = nn.Parameter(torch.randn(n_queries, hidden_dim))
        self.cross_attn = nn.MultiheadAttention(hidden_dim, 12)
        self.self_attn = nn.MultiheadAttention(hidden_dim, 12)
        self.proj = nn.Linear(hidden_dim, hidden_dim)

    def forward(self, visual_features):
        # queries ä¸ visual features åš cross attention
        # è¾“å‡ºå›ºå®šæ•°é‡çš„ query tokens (ä¸ visual token æ•°æ— å…³)
        queries = self.queries.unsqueeze(0).expand(visual_features.shape[0], -1, -1)
        out, _ = self.cross_attn(queries, visual_features, visual_features)
        out, _ = self.self_attn(out, out, out)
        return self.proj(out)  # (B, n_queries, D) â†’ å›ºå®š 32 ä¸ª token
```

**ä¼˜ç‚¹**ï¼šå¤§å¹…å‡å°‘ visual token æ•°ï¼ˆ576â†’32ï¼‰ï¼ŒèŠ‚çœ LLM ä¸Šä¸‹æ–‡ã€‚
**ç¼ºç‚¹**ï¼šä¿¡æ¯å‹ç¼©å¯èƒ½ä¸¢å¤±ç»†èŠ‚ï¼Œè®­ç»ƒä¸ç¨³å®šï¼ŒOCR ç­‰ç»†ç²’åº¦ä»»åŠ¡è¡¨ç°å·®ã€‚

### Perceiver Resampler (Flamingo, arXiv:2204.14198)

ç±»ä¼¼ Q-Formerï¼Œä½†ç”¨ Perceiver æ¶æ„ï¼ˆAlayrac et al., 2022ï¼‰ï¼š

```
å¯å­¦ä¹  latent tokens Ã— visual features â†’ cross attention Ã— 6 layers
â†’ è¾“å‡ºå›ºå®šé•¿åº¦ latent tokens â†’ æ³¨å…¥ LLM å„å±‚ (cross attention)

å…³é”®åŒºåˆ«: Flamingo å°† visual tokens é€šè¿‡ cross attention æ³¨å…¥ LLM æ¯å±‚
         è€Œéæ‹¼æ¥åˆ°è¾“å…¥ (LLaVA æ–¹å¼)
```

### Projector å¯¹æ¯”æ€»ç»“

| Projector | Visual Tokens æ•° | å‚æ•°é‡ | ç»†ç²’åº¦èƒ½åŠ› | ä»£è¡¨æ¨¡å‹ |
|-----------|-----------------|--------|-----------|---------|
| Linear | ä¸å˜ (576+) | ~1M | â˜…â˜…â˜…â˜…â˜… | LLaVA-1.0 |
| MLP (2å±‚) | ä¸å˜ (576+) | ~10M | â˜…â˜…â˜…â˜…â˜… | **LLaVA-1.5+, InternVL** |
| Q-Former | å›ºå®š (32-64) | ~100M | â˜…â˜…â˜…â˜†â˜† | BLIP-2 |
| Perceiver | å›ºå®š (64-256) | ~50M | â˜…â˜…â˜…â˜…â˜† | Flamingo |
| C-Abstractor | å‹ç¼© (144) | ~50M | â˜…â˜…â˜…â˜…â˜† | Honeybee |

**2025 è¶‹åŠ¿**ï¼šMLP + åŠ¨æ€åˆ†è¾¨ç‡ + token å‹ç¼©æˆä¸ºä¸»æµï¼ˆInternVL 2.5 ç”¨ pixel shuffle é™ä½ token æ•°ï¼‰ã€‚

## 4. è®­ç»ƒç­–ç•¥

### ä¸‰é˜¶æ®µè®­ç»ƒ

```
é˜¶æ®µ 1: Pre-training (å¯¹é½)
  ç›®æ ‡:    è®© Projector å­¦ä¼šå¯¹é½ vision-language ç©ºé—´
  æ•°æ®:    å¤§è§„æ¨¡ image-caption pairs (~600K-5M)
  å†»ç»“:    Vision Encoder â„ï¸ + LLM â„ï¸
  è®­ç»ƒ:    åªæœ‰ Projector ğŸ”¥
  æ—¶é—´:    æ•°å°æ—¶ (å‡ ç™¾ GPU hours)

é˜¶æ®µ 2: Visual Instruction Tuning (SFT)
  ç›®æ ‡:    è®©æ¨¡å‹å­¦ä¼šå¤šæ¨¡æ€æŒ‡ä»¤éµå¾ª
  æ•°æ®:    é«˜è´¨é‡ visual QA/å¯¹è¯æ•°æ® (~600K-1.5M)
  å†»ç»“:    Vision Encoder â„ï¸ (æˆ–éƒ¨åˆ†è§£å†»)
  è®­ç»ƒ:    Projector ğŸ”¥ + LLM ğŸ”¥
  æ—¶é—´:    æ•°å¤©

é˜¶æ®µ 3: RLHF/DPO (å¯é€‰)
  ç›®æ ‡:    å¯¹é½äººç±»åå¥½ï¼Œå‡å°‘å¹»è§‰
  æ•°æ®:    å¤šæ¨¡æ€åå¥½æ•°æ® (~10K-100K)
  è®­ç»ƒ:    å…¨æ¨¡å‹æˆ– LoRA
  æ–¹æ³•:    RLHF-V, LLaVA-RLHF, Silkie
```

### æ•°æ®æ„é€ 

```python
# Visual Instruction Tuning æ•°æ®æ ¼å¼
{
    "image": "path/to/image.jpg",
    "conversations": [
        {"from": "human", "value": "<image>\nè¿™å¼ å›¾ç‰‡æè¿°äº†ä»€ä¹ˆï¼Ÿ"},
        {"from": "gpt", "value": "è¿™å¼ å›¾ç‰‡å±•ç¤ºäº†ä¸€åªæ©˜çŒ«ååœ¨çª—å°ä¸Š..."},
        {"from": "human", "value": "çŒ«çš„è¡¨æƒ…çœ‹èµ·æ¥æ€æ ·ï¼Ÿ"},
        {"from": "gpt", "value": "çŒ«çš„è¡¨æƒ…çœ‹èµ·æ¥å¾ˆæ”¾æ¾å’Œæ»¡è¶³..."}
    ]
}
```

## 5. ä¸»æµæ¨¡å‹å¯¹æ¯”

### LLaVA ç³»åˆ—

```
LLaVA-1.0 (2023.4):  CLIP ViT-L + Linear + Vicuna 7/13B
LLaVA-1.5 (2023.10): CLIP ViT-L + 2-layer MLP + Vicuna/LLaMA
LLaVA-NeXT (2024.1): + AnyRes åŠ¨æ€åˆ†è¾¨ç‡, æ›´å¤šè®­ç»ƒæ•°æ®
LLaVA-OneVision (2024.8): + å¤šä»»åŠ¡ç»Ÿä¸€è®­ç»ƒ, è§†é¢‘ç†è§£

ä¼˜åŠ¿: ç®€æ´é«˜æ•ˆï¼Œç¤¾åŒºç”Ÿæ€å¥½ï¼Œæ˜“å¤ç°
åŠ£åŠ¿: ä¾èµ–å¤–éƒ¨ ViTï¼Œä¸­æ–‡èƒ½åŠ›ä¸€èˆ¬
```

### Qwen-VL ç³»åˆ—

å‚è§ [[Qwen-VL]]ã€‚

```
Qwen-VL (2023.8):    è‡ªè®­ç»ƒ ViT + Cross-Attn + Qwen 7B
Qwen2-VL (2024.10):  è‡ªè®­ç»ƒ ViT + 2D RoPE + Naive Dynamic Resolution
                     ç‰¹ç‚¹: ä¸åˆ‡ tile! ç”¨ 2D RoPE å¤„ç†ä»»æ„åˆ†è¾¨ç‡
Qwen3-VL (2025):     MoE LLM + æ”¹è¿› ViT + 235B è§„æ¨¡

ä¼˜åŠ¿: ä¸­è‹±åŒè¯­å¼ºï¼ŒOCR/æ–‡æ¡£ç†è§£é¢†å…ˆï¼Œè§†é¢‘ç†è§£
åŠ£åŠ¿: é—­æºè®­ç»ƒæ•°æ®ï¼Œæ¨¡å‹è¾ƒå¤§

Qwen2-VL 2D RoPE åˆ›æ–°:
  ä¼ ç»Ÿ: 2Dâ†’1D flatten + æ ‡å‡† RoPE â†’ ä¸¢å¤±ç©ºé—´ç»“æ„
  Qwen2-VL: å¯¹ (h, w) åˆ†åˆ«ç”¨ç‹¬ç«‹ RoPE â†’ ä¿æŒ 2D ç©ºé—´æ„ŸçŸ¥
```

### InternVL ç³»åˆ—

å‚è§ [[InternVL3]]ã€‚

```
InternVL 1.0 (2023.12): InternViT-6B + QLLaMA
InternVL 1.5 (2024.4):  InternViT-6B + 2-layer MLP + InternLM2
InternVL 2.0 (2024.7):  + åŠ¨æ€åˆ†è¾¨ç‡, Pixel Shuffle å‹ç¼©
InternVL 2.5 (2024.12): + æ”¹è¿›è®­ç»ƒç­–ç•¥, å¤šæ¨¡å‹è§„æ¨¡ (1-78B)
InternVL 3.0 (2025):    + æ¨ç†èƒ½åŠ›å¢å¼º

æ¶æ„: ViT-MLP-LLM (ä¸ LLaVA ç›¸åŒèŒƒå¼)
ç‰¹ç‚¹:
  - InternViT-6B: æœ€å¤§å¼€æºè§†è§‰ç¼–ç å™¨
  - Pixel Shuffle: 4 ä¸ªç›¸é‚» token åˆå¹¶ä¸º 1 ä¸ª â†’ token å‡å°‘ 75%
  - åŠ¨æ€åˆ†è¾¨ç‡: 1-12 tiles Ã— 256 tokens = 256-3072 tokens
```

### ç»¼åˆå¯¹æ¯”

| ç»´åº¦ | LLaVA-1.5/NeXT | Qwen2-VL | InternVL 2.5 |
|------|---------------|----------|-------------|
| Vision Encoder | CLIP ViT-L (304M) | è‡ªè®­ç»ƒ ViT (675M) | InternViT-6B |
| Projector | 2-layer MLP | 2-layer MLP | 2-layer MLP |
| LLM | Vicuna/LLaMA | Qwen2 | InternLM2/Qwen2.5 |
| åˆ†è¾¨ç‡ç­–ç•¥ | AnyRes tile | 2D RoPE åŸç”Ÿ | åŠ¨æ€ tile + PixelShuffle |
| Token å‹ç¼© | æ—  | æ—  | PixelShuffle 4:1 |
| OCR/æ–‡æ¡£ | â˜…â˜…â˜…â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† |
| ä¸­æ–‡èƒ½åŠ› | â˜…â˜…â˜†â˜†â˜† | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… |
| å¼€æºç¨‹åº¦ | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜† | â˜…â˜…â˜…â˜…â˜… |

## 6. å…³é”®æŠ€æœ¯ç»†èŠ‚

### Visual Token å‹ç¼©

é«˜åˆ†è¾¨ç‡å¯¼è‡´ visual token æš´å¢ï¼ˆ10K+ï¼‰ï¼Œå‹ç¼©åŠ¿åœ¨å¿…è¡Œï¼š

```
æ–¹æ³•                  å‹ç¼©æ¯”    è´¨é‡æŸå¤±
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Pixel Shuffle (InternVL) 4:1     æå° (ç©ºé—´ä¿¡æ¯åˆå¹¶åˆ°é€šé“)
Average Pooling         4-16:1   ä¸­ç­‰
Token Merging (ToMe)    2-4:1    å°
C-Abstractor           ~4:1      å°
Perceiver Resampler    ~10:1     ä¸­ç­‰
```

### å¤šæ¨¡æ€å¹»è§‰ (Hallucination)

```
å¸¸è§å¹»è§‰ç±»å‹:
  1. å¯¹è±¡å­˜åœ¨æ€§å¹»è§‰: æè¿°å›¾ä¸­ä¸å­˜åœ¨çš„å¯¹è±¡
  2. å±æ€§å¹»è§‰: é¢œè‰²/ä½ç½®/å¤§å°æè¿°é”™è¯¯
  3. å…³ç³»å¹»è§‰: å¯¹è±¡é—´å…³ç³»æè¿°é”™è¯¯

ç¼“è§£æ–¹æ³•:
  - RLHF-V: ç”¨ç»†ç²’åº¦äººç±»åé¦ˆçº æ­£è§†è§‰å¹»è§‰
  - å¯¹æ¯”å­¦ä¹ : æ­£ç¡®æè¿° vs æ•…æ„åŠ å…¥é”™è¯¯çš„æè¿°
  - Grounding: è¦æ±‚æ¨¡å‹ç»™å‡º bbox åæ ‡å¢å¼ºå®šä½
  - é«˜åˆ†è¾¨ç‡: æ›´æ¸…æ™°çš„è§†è§‰è¾“å…¥ â†’ æ›´å°‘çš„å¹»è§‰
```

## é¢è¯•å¸¸è§é—®é¢˜

### Q1: å¤šæ¨¡æ€ LLM çš„é€šç”¨æ¶æ„åŒ…å«å“ªäº›ç»„ä»¶ï¼Ÿå„è‡ªçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

ä¸‰å¤§ç»„ä»¶ï¼š(1) **Vision Encoder**ï¼ˆé€šå¸¸æ˜¯ ViT/CLIP/SigLIPï¼‰â€”â€”å°†å›¾åƒè½¬åŒ–ä¸º visual token åºåˆ—ï¼Œæå–è§†è§‰è¯­ä¹‰ç‰¹å¾ï¼›(2) **Projector**ï¼ˆLinear/MLP/Q-Formerï¼‰â€”â€”å°† vision space æ˜ å°„åˆ° language spaceï¼Œå®ç°æ¨¡æ€å¯¹é½ï¼›(3) **LLM**ï¼ˆLLaMA/Qwen/InternLMï¼‰â€”â€”åœ¨ç»Ÿä¸€çš„ token ç©ºé—´ä¸­æ¨ç†ã€‚è®¾è®¡çš„æ ¸å¿ƒ trade-off æ˜¯ Projector çš„å¤æ‚åº¦ï¼šç®€å•å¦‚ MLP ä¿ç•™æ‰€æœ‰ä¿¡æ¯ä½† token å¤šï¼Œå¤æ‚å¦‚ Q-Former å‹ç¼© token ä½†å¯èƒ½ä¸¢å¤±ç»†èŠ‚ã€‚2025 å¹´ä¸»æµé€‰æ‹©æ˜¯ **MLP + åŠ¨æ€åˆ†è¾¨ç‡ + token å‹ç¼©**ã€‚

### Q2: ä¸ºä»€ä¹ˆ 2025 å¹´å¤§å¤šæ•° VLM ç”¨ MLP è€Œé Q-Former ä½œä¸º Projectorï¼Ÿ

> æ¥æºï¼šLiu et al. "Improved Baselines with Visual Instruction Tuning (LLaVA-1.5)" arXiv:2310.03744

Q-Former é€šè¿‡å¯å­¦ä¹  queries å°†è§†è§‰ token å‹ç¼©åˆ°å›ºå®šæ•°é‡ï¼ˆå¦‚ 32 ä¸ªï¼‰ï¼Œä½†å­˜åœ¨ä¸‰ä¸ªé—®é¢˜ï¼š(1) **ä¿¡æ¯ä¸¢å¤±**â€”â€”OCRã€ç»†ç²’åº¦è¯†åˆ«ç­‰ä»»åŠ¡éœ€è¦ä¿ç•™åƒç´ çº§ç»†èŠ‚ï¼Œ32 ä¸ª token ä¸å¤Ÿï¼›(2) **è®­ç»ƒä¸ç¨³å®š**â€”â€”cross attention çš„æ”¶æ•›ä¾èµ–ç²¾å¿ƒè®¾è®¡çš„é¢„è®­ç»ƒç­–ç•¥ï¼›(3) **å·¥ç¨‹å¤æ‚**â€”â€”é¢å¤–å¼•å…¥ ~100M å‚æ•°ã€‚MLP ç›´æ¥æŠ•å½±ä¿ç•™æ‰€æœ‰ tokenï¼Œé…åˆ pixel shuffle ç­‰è½»é‡å‹ç¼©å³å¯å¹³è¡¡æ•ˆç‡å’Œç»†èŠ‚ã€‚LLaVA-1.5 çš„å®éªŒè¯æ˜ï¼ˆarXiv:2310.03744ï¼‰ï¼Œç®€å•çš„ 2 å±‚ MLP åœ¨å‡ ä¹æ‰€æœ‰ benchmark ä¸Šä¼˜äº Q-Formerã€‚

### Q3: Qwen2-VL çš„ 2D RoPE å’Œä¼ ç»Ÿ tile åˆ‡åˆ†æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

ä¼ ç»Ÿæ–¹æ³•ï¼ˆLLaVA-NeXTã€InternVLï¼‰å°†é«˜åˆ†è¾¨ç‡å›¾ç‰‡åˆ‡æˆå¤šä¸ª tileï¼ˆå¦‚ 336Ã—336ï¼‰ï¼Œæ¯ä¸ª tile ç‹¬ç«‹ç¼–ç åæ‹¼æ¥ã€‚é—®é¢˜æ˜¯ **tile è¾¹ç•Œä¼ªå½±**â€”â€”è·¨ tile çš„ç›®æ ‡è¢«åˆ‡æ–­ï¼Œæ¨¡å‹çœ‹ä¸åˆ°å®Œæ•´ä¸Šä¸‹æ–‡ã€‚Qwen2-VL è®© ViT ç›´æ¥å¤„ç†ä»»æ„åˆ†è¾¨ç‡ï¼ˆdynamic resolutionï¼‰ï¼Œç”¨ **2D RoPE** ä¸ºæ¯ä¸ª patch ç¼–ç  (h, w) äºŒç»´åæ ‡â€”â€”æ°´å¹³æ–¹å‘å’Œå‚ç›´æ–¹å‘å„ç”¨ç‹¬ç«‹çš„ RoPE é¢‘ç‡ï¼Œä¿æŒäº†å®Œæ•´çš„ 2D ç©ºé—´æ„ŸçŸ¥ã€‚ä»£ä»·æ˜¯ ViT éœ€è¦å¤„ç†å˜é•¿åºåˆ—ï¼Œä½†é…åˆ [[FlashAttention]] å¯ä»¥é«˜æ•ˆå®ç°ã€‚

### Q4: å¤šæ¨¡æ€ LLM çš„ä¸‰é˜¶æ®µè®­ç»ƒå„è‡ªçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ

**é˜¶æ®µ 1 (Pretrain)**ï¼šå†»ç»“ ViT+LLMï¼Œåªè®­ç»ƒ Projectorï¼Œç”¨å¤§è§„æ¨¡ image-caption æ•°æ®ï¼ˆ~5M pairsï¼‰è®© Projector å­¦ä¼šå°†è§†è§‰ç‰¹å¾æ˜ å°„åˆ° LLM çš„è¯åµŒå…¥ç©ºé—´â€”â€”å¯¹é½è€Œéç†è§£ã€‚**é˜¶æ®µ 2 (Visual SFT)**ï¼šè§£å†» LLMï¼ˆViT å¯é€‰ï¼‰ï¼Œç”¨é«˜è´¨é‡çš„å¤šæ¨¡æ€æŒ‡ä»¤æ•°æ®è®­ç»ƒâ€”â€”è®©æ¨¡å‹å­¦ä¼šçœ‹å›¾å›ç­”é—®é¢˜ã€æè¿°åœºæ™¯ã€åš OCR ç­‰ã€‚**é˜¶æ®µ 3 (RLHF/DPO)**ï¼šå¯¹é½äººç±»åå¥½ï¼Œå‡å°‘è§†è§‰å¹»è§‰ã€‚å…³é”®è®¾è®¡ï¼šé˜¶æ®µ 1 åªè®­ Projector æ˜¯å› ä¸ºéšæœºåˆå§‹åŒ–çš„ Projector ä¼šäº§ç”Ÿå™ªå£°æ¢¯åº¦ï¼Œå¦‚æœåŒæ—¶è§£å†» LLM ä¼šç ´åé¢„è®­ç»ƒçŸ¥è¯†ã€‚

### Q5: å¦‚ä½•ç¼“è§£å¤šæ¨¡æ€ LLM çš„è§†è§‰å¹»è§‰é—®é¢˜ï¼Ÿ

è§†è§‰å¹»è§‰ï¼ˆæè¿°å›¾ä¸­ä¸å­˜åœ¨çš„å¯¹è±¡/é”™è¯¯å±æ€§ï¼‰æ˜¯ MLLM æœ€å¤§çš„é—®é¢˜ä¹‹ä¸€ã€‚ç¼“è§£æ–¹æ³•ï¼š(1) **æé«˜åˆ†è¾¨ç‡**â€”â€”AnyRes/åŠ¨æ€åˆ†è¾¨ç‡è®©æ¨¡å‹çœ‹åˆ°æ›´å¤šç»†èŠ‚ï¼Œå‡å°‘é "çŒœ"çš„æƒ…å†µï¼›(2) **RLHF-V**â€”â€”ç”¨ç»†ç²’åº¦äººç±»æ ‡æ³¨ï¼ˆæŒ‡å‡ºå¹»è§‰ä½ç½®ï¼‰åš DPO è®­ç»ƒï¼›(3) **Grounding è®­ç»ƒ**â€”â€”è¦æ±‚æ¨¡å‹ç»™å‡º bbox åæ ‡ï¼Œå¢å¼ºç©ºé—´å®šä½ï¼›(4) **å¯¹æ¯”è®­ç»ƒ**â€”â€”æ„é€ æ­£ç¡®æè¿° vs æ•…æ„åŠ å…¥å¯¹è±¡çš„é”™è¯¯æè¿°ä½œä¸ºåå¥½å¯¹ï¼›(5) **æ¨ç†æ—¶ç­–ç•¥**â€”â€”å¤šæ¬¡é‡‡æ ·å–å…±è¯†ï¼ˆself-consistencyï¼‰ï¼Œæˆ–ç”¨ vision encoder äºŒæ¬¡éªŒè¯ã€‚

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Learning Transferable Visual Models From Natural Language Supervision (CLIP)](https://arxiv.org/abs/2103.00020) â€” Radford et al. 2021ï¼Œè§†è§‰-è¯­è¨€å¯¹é½çš„é‡Œç¨‹ç¢‘ï¼Œå¿…è¯»
- [Flamingo: a Visual Language Model for Few-Shot Learning](https://arxiv.org/abs/2204.14198) â€” Alayrac et al. 2022ï¼ŒPerceiver Resampler + Cross-Attention æ³¨å…¥ LLM
- [Visual Instruction Tuning (LLaVA)](https://arxiv.org/abs/2304.08485) â€” Liu et al. 2023ï¼Œç®€æ´çš„ VLM èŒƒå¼ï¼ŒLinear Projector + SFT
- [Improved Baselines with Visual Instruction Tuning (LLaVA-1.5)](https://arxiv.org/abs/2310.03744) â€” è¯æ˜ 2 å±‚ MLP ä¼˜äº Q-Former â­â­â­â­â­
- [BLIP-2: Bootstrapping Language-Image Pre-training](https://arxiv.org/abs/2301.12597) â€” Q-Former çš„åŸå§‹è®¾è®¡
- [InternVL: Scaling up Vision Foundation Models](https://arxiv.org/abs/2312.14238) â€” æœ€å¤§å¼€æº ViT (6B) + Pixel Shuffle å‹ç¼©

### å®è·µèµ„æº
- [LLaVA GitHub](https://github.com/haotian-liu/LLaVA) â€” æœ€ç®€æ´çš„ MLLM å®ç°
- [InternVL GitHub](https://github.com/OpenGVLab/InternVL) â€” æœ€å¼ºå¼€æº MLLM ç³»åˆ—
- [Qwen-VL ç³»åˆ—](https://github.com/QwenLM/Qwen-VL) â€” ä¸­æ–‡æœ€å¼º MLLM

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **å¤šæ¨¡æ€ RAG**ï¼šç”¨ Vision Encoder å°†å›¾ç‰‡/è¡¨æ ¼å‘é‡åŒ–ï¼Œä¸æ–‡æœ¬ç»Ÿä¸€æ£€ç´¢ï¼ˆå‚è§ [[AI/RAG/RAG-2026-æŠ€æœ¯å…¨æ™¯|RAG 2026 å…¨æ™¯]] çš„å¤šæ¨¡æ€ RAG éƒ¨åˆ†ï¼‰
- **æ–‡æ¡£ç†è§£/OCR**ï¼šåŠ¨æ€åˆ†è¾¨ç‡ + MLP Projector çš„ VLM å¯ç›´æ¥ç†è§£ PDF æˆªå›¾ï¼Œæ›¿ä»£ä¼ ç»Ÿ OCR ç®¡çº¿
- **è§†è§‰é—®ç­”ä¸æè¿°**ï¼šäº§å“å›¾ç‰‡åˆ†æã€åŒ»ç–—å½±åƒåˆç­›ã€å»ºç­‘å›¾çº¸ç†è§£

### å·¥ç¨‹å®ç°è¦ç‚¹
- **Vision Encoder é€‰å‹**ï¼šé€šç”¨åœºæ™¯ CLIP ViT-L / SigLIPï¼›éœ€è¦æœ€å¼ºä¸­æ–‡èƒ½åŠ› InternViT-6Bï¼›è¿½æ±‚è½»é‡ ViT-B/16
- **Projector é€‰å‹**ï¼š2025 å¹´é€‰ 2 å±‚ MLP å°±å¯¹äº†â€”â€”ç®€å•ã€æœ‰æ•ˆã€è®­ç»ƒç¨³å®š
- **åˆ†è¾¨ç‡ç­–ç•¥**ï¼šç®€å•ä»»åŠ¡ç”¨å›ºå®š 336Ã—336ï¼›OCR/æ–‡æ¡£ç”¨åŠ¨æ€åˆ†è¾¨ç‡ï¼ˆAnyRes æˆ– Qwen2-VL 2D RoPEï¼‰
- **Token å‹ç¼©**ï¼šPixel Shuffleï¼ˆInternVLï¼‰4:1 å‹ç¼©æ˜¯å½“å‰æœ€ä¼˜è§£ï¼Œè´¨é‡æŸå¤±æå°

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: ä¸‰å¤§ç»„ä»¶å„è‡ªçš„ä½œç”¨ï¼Ÿ
  A: Vision Encoder æå–è§†è§‰ç‰¹å¾ï¼ˆçœ¼ç›ï¼‰ï¼›Projector å¯¹é½è§†è§‰-è¯­è¨€ç©ºé—´ï¼ˆæ¡¥æ¢ï¼‰ï¼›LLM åšå¤šæ¨¡æ€æ¨ç†ï¼ˆå¤§è„‘ï¼‰
- Q: ä¸ºä»€ä¹ˆè®­ç»ƒæ—¶å…ˆå†»ç»“ ViT å’Œ LLMï¼Ÿ
  A: éšæœºåˆå§‹åŒ–çš„ Projector äº§ç”Ÿå™ªå£°æ¢¯åº¦ï¼ŒåŒæ—¶è§£å†» LLM ä¼šç ´åé¢„è®­ç»ƒçŸ¥è¯†â€”â€”å…ˆè®© Projector å­¦ä¼šå¯¹é½

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- VLM æ¶æ„çš„æ ¸å¿ƒ trade-off æ˜¯ Projector å¤æ‚åº¦â€”â€”ç†è§£è¿™ä¸ª trade-off å°±ç†è§£äº† LLaVA vs BLIP-2 vs Flamingo çš„æœ¬è´¨åŒºåˆ«
- åŠ¨æ€åˆ†è¾¨ç‡ + Token å‹ç¼©æ˜¯ 2025-2026 çš„æŠ€æœ¯ä¸»çº¿ï¼ŒæŒæ¡è¿™ä¸ªæ–¹å‘å¯ä»¥åœ¨é¢è¯•ä¸­å±•ç¤ºå‰æ²¿è®¤çŸ¥

### æœªè§£é—®é¢˜ä¸å±€é™
- **è§†è§‰å¹»è§‰æœªæ ¹æ²»**ï¼šé«˜åˆ†è¾¨ç‡ + RLHF-V ç¼“è§£ä½†æœªè§£å†³ï¼Œæè¿°å›¾ä¸­ä¸å­˜åœ¨å¯¹è±¡çš„é—®é¢˜ä»æ™®é
- **è§†é¢‘ç†è§£æ•ˆç‡**ï¼šé•¿è§†é¢‘çš„ visual token æš´å¢ï¼Œå½“å‰å‹ç¼©æ–¹æ¡ˆï¼ˆPixel Shuffle/ToMeï¼‰å¯¹è§†é¢‘çš„æ•ˆæœä¸å¦‚å›¾åƒ
- **ç»Ÿä¸€ Any-to-Any æ¨¡å‹**ï¼šå›¾æ–‡â†’è§†é¢‘â†’éŸ³é¢‘çš„ç»Ÿä¸€ç”Ÿæˆä»åœ¨æ—©æœŸé˜¶æ®µ

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- Qwen2-VL çš„ 2D RoPE æ€è·¯å¯ä»¥æ¨å¹¿åˆ° 3Dï¼ˆè§†é¢‘çš„æ—¶ç©ºä½ç½®ç¼–ç ï¼‰â€”â€”è¿™æ˜¯è§†é¢‘ç†è§£çš„æ½œåœ¨çªç ´ç‚¹
- ç»“åˆ [[AI/MLLM/MLLM æ¦‚è¿°|MLLM æ¦‚è¿°]] çš„ Any-to-Any æ¶æ„ï¼Œç»Ÿä¸€çš„ Modality Generator å¯èƒ½æ›¿ä»£å½“å‰çš„"å¤–æŒ‚æ‰©æ•£æ¨¡å‹"
- InternViT-6B è¯æ˜äº†å¤§ ViT çš„ä»·å€¼â€”â€”6 ä¸ªæœˆåå¯èƒ½å‡ºç° 10B+ çš„è§†è§‰ç¼–ç å™¨

> ğŸ”— See also: [[AI/MLLM/MLLM æ¦‚è¿°]] â€” MLLM æ•´ä½“æ¦‚è¿°ä¸å‘å±•å†ç¨‹
> ğŸ”— See also: [[AI/MLLM/CLIP|CLIP]] â€” Vision Encoder çš„åŸºçŸ³
> ğŸ”— See also: [[AI/MLLM/BLIP-2|BLIP-2]] â€” Q-Former çš„åŸå§‹è®¾è®¡
