---
title: "Agent 智能体"
type: moc
domain: ai/agent
tags:
  - ai/agent
  - type/moc
updated: 2026-02-22
---

# 🤖 Agent 智能体 — 学习路线图

> 从单 Agent 到 Multi-Agent，从 Tool Use 到 Agentic RL

---

## 第一章 Agent 核心概念

> 理解 Agent 的基本构件：感知、推理、行动、记忆

### 1.1 基础架构

- [[AI/Agent/Fundamentals/Agent or Workflow？|Agent or Workflow？]] — 设计决策
- [[AI/Agent/Fundamentals/ReAct 与 CoT|ReAct 与 CoT]] — 推理范式对比
- [[AI/Agent/ReAct 推理模式|ReAct 推理模式]]
- [[AI/Agent/Fundamentals/分析 Agent 演进的一些思考|Agent 演进思考]]
- [[AI/Agent/Fundamentals/HF Agent Course|HF Agent Course]]
- [[AI/Agent/Fundamentals/HF LLM + Agent|HF LLM + Agent]]
- [[AI/Agent/Fundamentals/Context-Folding 论文|Context-Folding]] — 长程 Agent 论文

### 1.2 记忆机制

- [[AI/Agent/Fundamentals/记忆模块|记忆模块]] — 短期/长期记忆
- [[AI/Agent/Agent Memory 机制|Agent Memory 机制]] — 短期/长期/工作记忆、RAG-based memory、MemGPT/Letta
- [[AI/Agent/Agent World Model|Agent World Model]] — Agentic RL + 合成环境 + 世界模型
- [[AI/Agent/Memory-R1-RL-for-LLM-Memory-Management|Memory-R1]] ⭐ — RL 驱动的记忆管理（ADD/UPDATE/DELETE/NOOP），152 条数据超 Mem0 的 F1 +28%；outcome reward 设计范式（LMU+TU Munich+Cambridge，arXiv:2508.19828）★★★★☆

### 1.3 综合全景

- [[AI/Agent/AI-Agent-2026-技术全景|🔥 AI Agent 2026 技术全景]] ⭐ — 面试武器库，1114行 ★★★★★

---

## 第二章 工具使用与 MCP

> Agent 的"手"：如何调用工具、Function Calling、MCP 协议

- [[AI/Agent/Fundamentals/Tool Use|Tool Use]] — 工具调用基础
- [[AI/Agent/Agent Tool Use|Agent Tool Use]] — Function Calling / ReAct / API 对比
- [[AI/Agent/LLM工具调用与Function-Calling-2026技术全景|🔥 LLM 工具调用与 Function Calling 2026 全景]] ⭐ — 面试武器级 ★★★★★
- [[AI/Agent/Fundamentals/Code Agent|Code Agent (基础)]]
- [[AI/Agent/Code Agent|Code Agent]] — 深度笔记

### MCP (Model Context Protocol)

- [[AI/Agent/MCP/如何给人深度科普 MCP|如何给人深度科普 MCP]]
- [[AI/Agent/MCP/HF MCP Course|HF MCP Course]]

---

## 第三章 Multi-Agent 系统

> 多 Agent 协作、编排、通信

- [[AI/Agent/Multi-Agent/Multi-Agent 概述|Multi-Agent 概述]]
- [[AI/Agent/Multi-Agent/Multi-Agent-架构模式详解|Multi-Agent 架构模式详解]] — Supervisor/Pipeline/Debate三模式含代码实现 ★★★★☆
- [[AI/Agent/多智能体系统与协作框架-2026技术全景|🔥 多智能体系统与协作框架 2026 全景]] ⭐ — 架构/通信/框架对比/信任安全/记忆/评估/集体智能全览，面试武器级 ★★★★★
- [[AI/Agent/Multi-Agent/Agent vs MAS|Agent vs MAS]]
- [[AI/Agent/Multi-Agent/Planner|Planner]]
- [[AI/Agent/Multi-Agent/零碎的点|零碎的点]]
- [[AI/Agent/AgentAuditor — Reasoning Tree审计多Agent系统|AgentAuditor]] — Reasoning Tree 审计
- [[AI/Agent/IMAGINE — 多Agent蒸馏到单模型|IMAGINE]] — 多 Agent 蒸馏到单模型
- [[AI/Agent/Kimi-K2.5-PARL|Kimi K2.5 & PARL]] — 并行多 Agent 强化学习
- [[AI/Agent/GitHub-Agentic-Workflows|GitHub Agentic Workflows]]
- [[AI/Agent/Multi-Agent/Collective-Behaviour-Hundreds-LLM-Agents-2026|Collective Behaviour（100+ LLM Agents）]] — 更强模型→更差社会结果；文化进化→差均衡收敛；100+ 规模集体行为评估框架（arXiv:2602.16662）★★★★☆
- [[AI/Agent/AdaptOrch-Task-Adaptive-Multi-Agent-Orchestration|AdaptOrch（arXiv:2602.16873）]] ⭐ — **LLM 性能收敛时代的编排主导定理**：拓扑选择影响比模型选择大 20x；DAG 三指标（ω/δ/γ）自动路由四种拓扑（并行/顺序/层次/混合）；SWE-bench+GPQA+RAG 提升12-23%；"模型收敛→编排主导"面试核心论点 ★★★★☆

---

## 第四章 Agentic RL 训练 ⭐

> 用强化学习训练更强的 Agent（前沿方向）

### 4.1 综合分析

- [[AI/Agent/Agentic-RL/Agentic-RL-2026前沿综合分析|🔥 Agentic RL 2026 前沿综合分析]] ⭐ — **五大维度**：Credit Assignment（6方案谱系）/ Reward Design / Environment / Workflow / Context Overflow（v3, 2026-02-23）★★★★★
- [[AI/Agent/Agentic-RL/Agentic-RL-元问题-瓶颈与突破方向|🧠 Agentic RL 元问题：瓶颈与突破方向]] ⭐⭐ — **Wisdom 层**：基于 37+ 篇论文的整合判断；核心命题：算法层已有足够方案，真正瓶颈是 Reward Signal Quality（open problem × 3）；面试终极回答素材；预测：自适应课程（2026H2）→ 世界模型辅助RL（1-2年）→ multi-agent scalable 理论（3年+）★★★★★
- [[AI/Agent/Agentic-RL/Agent-RL-训练实战指南|🔥 Agent RL 训练实战指南]] ⭐ — 1001行，面试可用 ★★★★★
- [[AI/Agent/Agentic-RL/RAGEN-StarPO-Multi-Turn-RL-Self-Evolution|RAGEN & StarPO（Northwestern+Stanford+UW+MSR，arXiv:2504.20073）]] ⭐ — **多轮 Agent RL 稳定性奠基论文**：StarPO 框架（trajectory-level RL 形式化）+ **Echo Trap** 发现（reward collapse+entropy drop+gradient spike 三联征）+ StarPO-S 三机制修复；TSR/HiPER/LOOP/KLong 的前驱工作（Li Fei-Fei/Yejin Choi 参与）★★★★★
- [[AI/Agent/Agentic-RL/Dr-MAS-Stable-RL-Multi-Agent-LLM-Systems|Dr. MAS（NTU Singapore，arXiv:2602.08847）]] ⭐ — **多 Agent 跨 agent 训练稳定性**：全局 reward 归一化在角色异构多 agent 场景导致梯度范数爆炸；Agent-Wise Advantage Normalization（per-agent μₖ/σₖ）理论证明梯度二阶矩有界；搜索任务 +15.2%；与 RAGEN（单 agent Echo Trap）正交互补 ★★★★☆
- [[AI/Agent/Agentic-RL/SHARP-Shapley-Credit-Multi-Agent-Tool-Use-RL|SHARP（ICML 2026，arXiv:2602.08335）]] ⭐ — **Multi-Agent 横向 Credit Assignment**：Shapley value + 反事实掩码（counterfactual masking）精确量化每个 agent 边际贡献；planner-worker self-play 框架；vs 单 agent +23.66%，vs 多 agent 方法 +14.05%（Qwen3-8B，4 benchmark）；与 GiGPO（纵向步骤级）正交互补 ★★★★☆
- [[AI/Agent/Agentic-RL/Agentic RL Survey|Agentic RL Survey]]
- [[AI/Agent/Agentic-RL/Agentic RL Training|Agentic RL Training]]
- [[AI/Agent/Agentic-RL/Multi-Agent-RL-训练专题|🔥 Multi-Agent RL 训练专题]] ⭐ — MAGRPO(Dec-POMDP)/AT-GRPO(agent-turn-wise grouping)/MARS2(diversity scaling law)；关键结论：2个异构32B agent > 单个72B ★★★★★

### 4.2 Workflow/Topology 自动化

- [[AI/Agent/Agentic-RL/FlowSteer-CWRPO-Workflow-Orchestration-RL|FlowSteer (CWRPO)]] — Workflow via End-to-End RL ★★★☆
- [[AI/Agent/AgentConductor-Topology-Evolution-Multi-Agent-Code|AgentConductor]] ⭐ — RL 动态生成 DAG topology ★★★★
- [[AI/Agent/Agentic-RL/SquRL-Dynamic-Workflow-Text-to-SQL|SquRL]] — Dynamic Workflow for Text-to-SQL ★★★

### 4.3 训练优化

- [[AI/Agent/Agentic-RL/PA-MoE-Phase-Aware-Mixture-of-Experts|PA-MoE]] — Phase-Aware MoE 解决 Simplicity Bias ★★★★
- [[AI/Agent/Agentic-RL/KLong-Extremely-Long-Horizon-Agent-RL|KLong（NUS+MIT，arXiv:2602.17547）]] ⭐ — Extremely Long-horizon（700+ turns，6-12h，轨迹超 context）：Trajectory-splitting SFT + Progressive RL；PaperBench +20%，MLE-bench competitive ★★★★☆
- [[AI/Agent/Calibrate-Then-Act-Cost-Aware-Exploration|Calibrate-Then-Act]] — Cost-aware 探索策略
- [[AI/Agent/Agentic-RL/Long-Horizon-Credit-Assignment专题|🔥 Long-Horizon Credit Assignment 专题]] ⭐ — GiGPO/AgentPRM/LOOP/MIG/iStar/CSO 全图谱 ★★★★★
- [[AI/Agent/Agentic-RL/CSO-Verified-Critical-Step-Optimization|CSO（Tencent AI Lab+HKU，arXiv:2602.03412）]] ⭐ — **反事实验证视角 Credit Assignment**：从失败轨迹出发，PRM 定位弱点步骤 + expert 生成替代 + policy rollout 验证，只监督 16% 关键步骤做 DPO；GAIA-Text-103 +37% over SFT，8B 超 GPT-4.1（45.9%）；高熵步骤原则推广到 Agent 领域 ★★★★☆
- [[AI/Agent/Agentic-RL/SELAUR-Self-Evolving-LLM-Agent-Uncertainty-Rewards|SELAUR（JHU+ASU+UIC+Purdue，arXiv:2602.21158）]] — **不确定性感知 Reward Shaping**：失败轨迹 reward=0 是信息浪费，用 token-level entropy/least-confidence/margin 三维不确定性重塑为密集学习信号；零额外模型成本，与 GiGPO 互补（GiGPO 精化成功信号，SELAUR 激活失败信号）；ALFWorld/WebShop 超越 GiGPO ★★★☆☆
- [[AI/Agent/Agentic-RL/AgentPRM-Process-Reward-Models-for-LLM-Agents|AgentPRM（Cornell/AI2，arXiv:2502.10325）]] ⭐ — MC rollout 自动标注 PRM 训练目标，InversePRM 无监督学习；step-level credit assignment；3B 超 GPT-4o baseline ★★★★★
- [[AI/Agent/Agentic-RL/GiGPO-Group-in-Group-Policy-Optimization|GiGPO（NeurIPS 2025，NTU+Skywork）]] ⭐ — Anchor State Grouping：无额外rollout/critic实现step-level advantage；ALFWorld +13.3%、WebShop +10.6% over GRPO；内存=GRPO（arXiv:2505.10978）★★★★★
- [[AI/Agent/Agentic-RL/iStar-Implicit-Step-Rewards-Agentic-RL|iStar（2509.19199，阿里通义）]] ⭐ — trajectory-based DPO → implicit step reward，免标注 step-level CA；SOTOPIA unverifiable reward +48%；唯一支持非可验证 reward 的 step 归因方案 ★★★★★
- [[AI/Agent/Agentic-RL/LOOP-Leave-One-Out-PPO-Long-Horizon-Agent-RL|LOOP（Apple Research，arXiv:2502.01600）]] ⭐ — Leave-One-Out PPO：无 value network（1× LLM），长 horizon Interactive Digital Agent（IDA）RL；32B Qwen2.5 超 OpenAI o1 +9pp；AppWorld 9-app API 环境；涌现闭环控制/API文档主动查阅 ★★★★★
- [[AI/Agent/Agentic-RL/MIG-Step-Marginal-Information-Gain-Credit-Assignment|MIG（Step-wise Marginal Information Gain，arXiv:2602.01034）]] ⭐ — 信息论视角 Credit Assignment：Monotonic Historical Watermark 防 reward hacking；奖励真正语义突破步骤；三部分 loss（MIG+Outcome+Gated-SFT）；Credit Assignment 谱系唯一信息论方案 ★★★★☆
- [[AI/Agent/Agentic-RL/HiPER-Hierarchical-Plan-Execute-RL-Credit-Assignment|HiPER（ICML 2026，UMN+TexasA&M，arXiv:2602.16165）]] ⭐ — **segment-level Credit Assignment**：Plan-Execute interface（单 LLM，结构化输出格式同时做 planning+execution）；HAE 两层 advantage（segment-level planner + step-level executor）；理论证明无偏+方差缩减；ALFWorld 97.4%（+6.6%）、WebShop 83.3%（+8.3%）超越 GiGPO ★★★★★
- [[AI/Agent/Agentic-RL/SeeUPO-Sequence-Level-Agentic-RL-Convergence-Guarantees|SeeUPO（Alibaba Tongyi，arXiv:2602.06554）]] ⭐ — **首个 critic-free + 严格收敛保证的 multi-turn Agentic RL 算法**：不可能定理（GRPO 的 variance norm 破坏 multi-turn 收敛）+ 逆序更新（backward induction via HAML）；AppWorld vs GRPO +43%~54%；BFCL v4 全面领先；单轮推理仍用 GRPO，多轮 Agent 训练用 SeeUPO ★★★★★
- [[AI/Agent/Agentic-RL/SCoRe-Self-Correction-via-Reinforcement-Learning|SCoRe（NeurIPS 2024，DeepMind）]] ⭐ — 首个纯多轮 RL 训练 LLM 自我纠错：两阶段（Phase 1 KL约束初始化 + Phase 2 reward bonus放大纠错）；MATH +15.6%、HumanEval +9.1%；解决 behavior collapse（arXiv:2409.12917）★★★★★
- [[AI/Agent/Agentic-RL/ERL-Experiential-Reinforcement-Learning|ERL（USC+Microsoft+UPenn，arXiv:2602.13949）]] ⭐ — **训练时显式反思内化**：experience-reflection-consolidation 三循环嵌入 RL 训练，每轮两次尝试（y¹ → 反思 Δ → y²），SFT 蒸馏成功 y² 进 base policy，部署时**零额外成本**；Sokoban +81%、HotpotQA +11%；Multi-Turn RL 第四支柱（与 TSR/Credit/SCoRe 正交可叠加）★★★★☆
- [[AI/Agent/Agentic-RL/Tree-GRPO-Tree-Search-LLM-Agent-RL|Tree-GRPO（ICLR 2026，Alibaba AMAP）]] ⭐ — 树搜索 rollout 替换独立 chain rollout：共享前缀节省预算 + 双层 advantage（intra-tree≡step-level DPO，inter-tree≡跨prompt）；1/4 预算超越全预算 GRPO；11 数据集验证；代码开源（arXiv:2509.21240）★★★★★

### 4.4 Tool Use RL 专线 ⭐

- [[AI/Agent/Agentic-RL/Tool-Use-RL-训练专题|🔥 Tool Use RL 训练专题]] ⭐ — ToolRL/ToRL/ARTIST/VerlTool/Agent-RLVR 全图谱 ★★★★★
- [[AI/Agent/Agentic-RL/ToRL-Tool-Integrated-Reinforcement-Learning|ToRL（SJTU/GAIR，arXiv:2503.23383）]] ⭐ — 从 base model 直接 RL 训 code interpreter 调用；无 SFT 前提；ToRL-7B AIME24 43.3%（超 RL-only +14%）；涌现自主工具策略 ★★★★★
- [[AI/Agent/Agentic-RL/ARTIST-Agentic-Reasoning-Tool-Integration-RL|ARTIST（Microsoft Research，arXiv:2505.01441）]] ⭐ — GRPO + 工具 token masking（工具输出不计梯度）+ 三类 reward（Answer/Format/Exec）；统一多工具多轮推理链框架；AMC/AIME/Olympiad 超 GPT-4o ★★★★★
- [[AI/Agent/Agentic-RL/TSR-Trajectory-Search-Rollouts-Multi-Turn-RL|TSR（Trajectory-Search Rollouts）]] ⭐ — per-turn 树搜索提升 rollout 质量；解决不可逆陷阱/Echo Trap；+15%，0.5B TSR≈3B naive；optimizer-agnostic（TU Munich + IBM，arXiv:2602.11767，ICML 2026）★★★★☆
- [[AI/Agent/Agentic-RL/CM2-Checklist-Rewards-Multi-Turn-Tool-Use-RL|CM2（Checklist Rewards）]] — Checklist 把 open-ended reward 拆解为结构化二进制分类，8B +8~+12 over SFT（arXiv:2602.12268）★★★★☆
- [[AI/Agent/Agentic-RL/ASTRA-Automated-Tool-Agent-Training|ASTRA（Beike）]] — 全自动 SFT+RL 流水线：MCP 工具图合成轨迹 + code-executable verifiable 环境，无需人工标注，32B 超过 o3（arXiv:2601.21558）★★★★☆
- [[AI/Agent/Agentic-RL/RC-GRPO-Reward-Conditioned-Tool-Calling-RL|RC-GRPO]] — reward token conditioning 解决 multi-turn GRPO all-0/all-1 崩塌，7B 超所有闭源 API（arXiv:2602.03025）★★★★☆
- [[AI/Agent/Agentic-RL/Search-R1-Reasoning-Search-Engine-RL|Search-R1（UIUC/UMass，arXiv:2503.09516）]] ⭐ — 搜索引擎集成进 RL rollout；retrieved token masking（检索结果不参与梯度）+ outcome-only reward；Qwen2.5-7B 超 RAG baseline +41%；与 ARTIST/ToRL 共享 token masking 统一原则 ★★★★☆
- [[AI/Agent/Agentic-RL/WebAgent-R1-Multi-Turn-RL-Web-Agent|WebAgent-R1（Amazon+UVA+GeorgiaTech，arXiv:2505.16421）]] ⭐ — Web Browser RL 端到端框架；**BC 热启动（关键）** + M-GRPO on-policy + Dynamic Context Compression（压缩历史 HTML 防 context 爆炸）；WebArena-Lite Llama-8B 44.8%（+36.3pp）超 o3；Off-Policy 盲区论证（logout→edit profile 例子）★★★★☆

### 4.5 环境工程

- [[AI/Agent/Agentic-RL/Agent-RL-环境工程系统论|🔥 Agent RL 环境工程系统论]] ⭐ — 设计原则/Reward 工程/主流环境拆解 ★★★★★
- [[AI/Agent/Agentic-RL/AWM-Agent-World-Model-Synthetic-Environments|AWM（ICML 2026，Snowflake AI + UIUC，arXiv:2602.10090）]] ⭐ — **全自动合成环境生成**：1000 代码驱动环境（非 LLM 模拟）+ 35,062 MCP 工具；五阶段流水线（场景→任务→DB→MCP接口→验证代码）；"任务先于数据库"逆向工程设计；OOD 三 benchmark 全部优于 benchmark-specific 训练；开源可直接使用 ★★★★
- [[AI/Agent/EnterpriseGym-Corecraft|EnterpriseGym Corecraft]] — 高保真企业 RL 环境 ★★★★
- [[AI/Agent/Agentic-RL/VerlTool 论文|VerlTool]] — 工具使用 RL 统一框架
- [[AI/Agent/Agentic-RL/PVPO 论文|PVPO]] — 价值预估策略优化
- [[AI/Agent/Agentic-RL/UI-TARS-2 论文|UI-TARS-2]] ⭐ — GUI Agent RL 工程极致路线：Data Flywheel + 异步 multi-turn RL，OSWorld 47.5（arXiv:2509.02544）★★★★★
- [[AI/Agent/UI-R1-GUI-Action-Prediction-RL|UI-R1]] — GUI Agent RL 极简路线：136 条数据 rule-based GRPO，3B 媲美 SFT 7B@76K（vivo AI+CUHK，arXiv:2503.21620）★★★★☆
- [[AI/Agent/ActionEngine-Programmatic-GUI-Agent|ActionEngine（arXiv:2602.20502）]] — **Reactive→Programmatic**：离线爬取应用 State Machine Graph → 在线一次规划 → 确定性执行，O(N)→O(1) LLM 调用，Training-free，11.8x 成本节省 ★★★★☆
- [[AI/Agent/Agentic-RL/WebPilot 论文|WebPilot]] — Web 自动化
- [[AI/Agent/Agentic-RL/R-4B 论文|R-4B]] — MLLM Auto-Thinking

---

## 第五章 框架选型

> 生产级 Agent 框架对比与最佳实践

- [[AI/Agent/Agent 框架对比|Agent 框架对比]] — 六大框架选型指南
- [[AI/Agent/Frameworks/Agent 框架对比 2026|Agent 框架对比 2026]]
- [[AI/Agent/Frameworks/AutoGen|AutoGen]]
- [[AI/Agent/Frameworks/dbgpt 文档|DB-GPT]]
- [[AI/Agent/Fundamentals/Agent 生产实践|Agent 生产实践]]
- [[AI/Agent/Agent 生产落地|Agent 生产落地]]

---

## 第六章 安全与评测

> Agent 的可靠性、安全性、评测方法

### 6.1 安全

- [[AI/Agent/Agent-Skills-Security|Agent Skills Security]] — 26.1% 社区 skill 含漏洞
- [[AI/Agent/CowCorpus-Human-Intervention-Modeling-Web-Agents|CowCorpus]] — Human-in-the-Loop 干预建模 ★★★★☆
- [[AI/Agent/PABU — Progress-Aware Belief State高效Agent|PABU]] — 进度感知信念更新
- [[AI/Agent/Multi-Agent/Colosseum-Multi-Agent-Collusion-Audit-2026|Colosseum（勾结审计）]] — 首个系统化审计多 Agent 勾结的框架；"纸上勾结"新现象（计划勾结但行动不勾结）；DCOP 形式化 + regret 度量（arXiv:2602.15198）★★★★☆

### 6.2 评测

- [[AI/Agent/Fundamentals/Agent 评测|Agent 评测]]
- [[AI/Agent/Agent 评测与 Benchmark|Agent 评测与 Benchmark]]
- [[AI/Agent/Evaluating-AGENTS-Context|Evaluating AGENTS: Context Files]]
- [[AI/Agent/Gaia2-Dynamic-Async-Agent-Benchmark|Gaia2]] ⭐ — 动态异步 Agent benchmark ★★★★★
- [[AI/Agent/Aletheia-Math-Research-Agent|Aletheia（前作：Erdős 数据库）]] — 数学科研 Agent，Erdős 4题
- [[AI/Agent/Aletheia-Gemini3-DeepThink-FirstProof|Aletheia FirstProof（arXiv:2602.21201，2026-02-25）]] ⭐ — Gemini 3 Deep Think 自主解决 6/10 研究级数学题（首次在真实 open problems 上突破专家水平）★★★★★
- [[AI/Agent/Agent评估体系批判-Goodhart法则与Benchmark陷阱|🔥 Agent 评估体系批判：Goodhart's Law]] ⭐ — SWE-bench/OSWorld/WebArena/ALFWorld 设计哲学对比 + 五维批判框架 + beyond-benchmark 方向；RL 训练者必读 ★★★★★
- [[AI/Agent/DeepSynth-Deep-Information-Synthesis-Benchmark|DeepSynth（arXiv:2602.21143，2026-02-25）]] ★★★★☆ — 120 道跨源信息综合任务，67 国，SOTA 仅 17.5%；核心发现：瓶颈是规划而非推理（+步骤 → +170% 性能）；真实 Agent 能力边界的新测量维度

---

## 附录 Agent 经济生态 💰

> Agent 的身份、支付、信誉与商业网络

- [[AI/Agent/Agent-Economy/目录|Agent 经济总览]]
- [[AI/Agent/Agent-Economy/Agent 经济基础设施|Agent 经济基础设施]] — Consensus HK 2026 全景
- [[AI/Agent/Agent-Economy/Coinbase AgentKit 技术评估|Coinbase AgentKit]]
- [[AI/Agent/Agent-Economy/ERC-8004 Trustless Agents|ERC-8004]] — Agent 链上身份标准
- [[AI/Agent/Agent-Economy/Virtuals Protocol|Virtuals Protocol]] — Agent-to-Agent 商业协议
- [[AI/Agent/Agent-Economy/Agentic Spring|Agentic Spring]] — 预测市场信号
- [[AI/Agent/Agent-Economy/ai16z 竞品分析|ai16z 竞品分析]]
- [[AI/Agent/Agent-Economy/elizaOS Trust Scoring 源码研究|elizaOS Trust Scoring 源码]]

---

---

## 第七章 Agent 进化方法论 🌱

> Agent 如何从"工具"成长为"伙伴"：记忆、反馈、元认知的工程实践

- [[AI/Agent/Agent自我进化策略-从记忆习惯到自主成长|🔥 Agent 自我进化策略：从记忆习惯到自主成长]] ⭐ — 10种进化模式完整实战手册（记忆沉淀/反馈回路/元认知/课程学习/协作进化等）；来自7-Agent军团实战经验，每种模式附可直接使用的prompt指令块（2026-02-22，Scholar原创）★★★★★

---

## 导航

- ↑ 上级：[[AI/目录]]
- → 交叉：[[AI/LLM/RL/目录]]（Agentic RL）· [[AI/Safety/目录]]（Agent 安全）
