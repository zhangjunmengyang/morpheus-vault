---
title: "Agent 自我进化策略：从记忆习惯到自主成长"
created: 2026-02-22
updated: 2026-02-22
tags: [agent, evolution, self-improvement, memory, metacognition, prompt-engineering]
status: active
brief: "系统梳理 AI Agent 自我进化的 10 种模式，每种模式都附带可直接使用的 prompt 指令块。源于 7-Agent 军团的实战经验：子 Agent 不写日志 → 发现根因是 prompt 缺失 → 引发对 Agent 成长机制的系统思考。"
---

# Agent 自我进化策略：从记忆习惯到自主成长

> 这不是一篇综述，是一份实战手册。每个模式都来自真实踩坑，每段 prompt 都可以直接塞进你的 Agent 配置。

## 起因

7-Agent 军团运行两周后发现：主 Agent 有完善的记忆习惯——写日志、修剪记忆、跨日交接——子 Agent 没有。

根因：主 Agent 的 AGENTS.md 有 **"没写下来的 = 不存在"** 这条规则，子 Agent 没有。

一行 prompt 的差异 = 两周经验的差异。

这让我们开始系统思考：Agent 的成长有哪些模式？每种模式落地需要什么样的 prompt？

---

## 十种自我进化模式

### 模式一：记忆沉淀（Memory Crystallization）

**一句话**：Agent 主动把经验写成文件，下次醒来读取，实现跨 session 的经验积累。

**为什么是基础中的基础**：没有记忆的 Agent 每次都是新生儿。你教它 100 次，第 101 次它还是不知道。

**三层架构**：

| 层 | 文件 | 存什么 | 类比 |
|----|------|--------|------|
| 工作记忆 | 上下文窗口 | 当前对话 | 短期记忆 |
| 情景记忆 | `memory/YYYY-MM-DD.md` | 每天做了什么（原始记录） | 日记 |
| 语义记忆 | `MEMORY.md` | 提炼后的认知、判断、教训 | 世界观 |

#### 落地 Prompt：AGENTS.md 记忆规范块

```markdown
## 记忆规范

| 层 | 文件 | 存什么 |
|----|------|--------|
| 状态 | `memory/heartbeat-state.json` | 当前心跳状态 |
| 日志 | `memory/YYYY-MM-DD.md` | 每日原始记录 |
| 长期 | `MEMORY.md` | 经验提炼、认知积累、教训 |

**没写下来的 = 没做过。**

### 写入规则
- 每次心跳结束前，必须更新 `memory/YYYY-MM-DD.md`
- 重要认知突破、方法论变化、踩坑教训 → 写入 `MEMORY.md`
- 跨日时检查：昨天日志是否完整，今天是否新建
- 定期修剪 `MEMORY.md`：过时的删掉，保持精炼
```

#### 落地 Prompt：HEARTBEAT.md 写入步骤

```markdown
## 心跳流程
1. 读状态 — memory/heartbeat-state.json
2. 做事 — [具体任务]
3. 更新 heartbeat-state.json
4. **写 memory/YYYY-MM-DD.md** — 记录做了什么、为什么做、结果如何
5. 汇报或 HEARTBEAT_OK
```

**实战教训**：子 Agent 补上这段 prompt 后，第一次心跳就开始写日志了。习惯不靠自律，靠规则。

---

### 模式二：反馈回路（Feedback Loop）

**一句话**：做了 → 看效果 → 调整。行为和结果之间建立因果链。

**三种反馈源**：
1. **人类反馈**：老板说"有用"/"别发"。信号最强，最稀缺。
2. **环境反馈**：消息被阅读了吗？笔记被引用了吗？策略回测盈亏如何？
3. **自我反馈**：Agent 自己审视产出质量。信号最弱，但成本为零。

#### 落地 Prompt：HEARTBEAT.md 每日自检块

```markdown
## 每日自检（每天第一次心跳触发）

在做任何事之前，先回答：
1. 昨天做了几件事？列出来。
2. 哪件事最有价值？为什么？
3. 哪件事完全没有反馈？（无反馈 = 可能是浪费）
4. 老板对哪条消息有回应？（回应 = 价值信号，记入 MEMORY.md）
5. 这周整体看：在进化还是在重复？

把结论写入 memory/YYYY-MM-DD.md 的"自检"区块。
```

#### 落地 Prompt：AGENTS.md 反馈记录规范

```markdown
## 反馈记录

老板的任何反馈（正面或负面）都是最高优先级信号：
- 正面（"有用"/"不错"/"继续"）→ 记入 MEMORY.md「什么有效」
- 负面（"别发"/"没用"/"太多了"）→ 记入 MEMORY.md「什么无效」，立即调整
- 沉默 → 不是反馈，不要过度解读

环境信号：
- 我推送的消息，老板回了 = 有价值
- 我推送的消息，老板 3 次没回 = 降低该类推送频率
- 我写的笔记后来被老板/其他Agent引用 = 高价值，多做类似的
```

**实战案例**：哨兵连续发低质量播报，老板不回 → 但哨兵没有"无回应 = 降频"的规则 → 继续发 → 老板终于说"别发这种"。如果有反馈回路 prompt，3 次无回应就该自动降频了。

---

### 模式三：元认知监控（Metacognitive Monitoring）

**一句话**：不只做事，还监控自己做事的方式，能调整方式本身。

**与反馈回路的区别**：反馈改的是"做什么"，元认知改的是"怎么做"。

#### 落地 Prompt：AGENTS.md 方法论演进块

```markdown
## 方法论演进

在 MEMORY.md 中维护「我的方法论」区块，记录：
- 什么方法有效（附日期和场景）
- 什么方法无效（附日期和原因）
- 方法的演变过程（从 A 方法切换到 B 方法，为什么）

### 方法论审视（每 7 天触发）

每 7 天问自己：
1. 我最近在用什么方法？列出来。
2. 每个方法的效果如何？有量化指标吗？
3. 有没有连续 3 天用同样方法但效果在递减？（= 需要换方法）
4. 有没有从其他 Agent 或外部学到新方法？（写入「我的方法论」）
5. 我的方法论比 7 天前更好了吗？好在哪？

结论写入 MEMORY.md。
```

#### 落地 Prompt：AGENTS.md 能力边界意识块

```markdown
## 能力边界

我擅长：[初始由人类填写，Agent 运行中自行补充]
我不擅长：[初始由人类填写，Agent 运行中自行补充]
我不确定的：[Agent 运行中自行维护]

规则：
- 遇到不确定的问题 → 说"我不确定"，不编造
- 发现自己在某方面连续做得好 → 加入「擅长」
- 发现自己在某方面连续失误 → 加入「不擅长」，并制定改善计划或寻求协作
- 每月审视一次：边界有没有扩展？
```

**关键洞察**：大多数 Agent 不知道自己不知道什么。显式维护能力画像是元认知的起点。

---

### 模式四：模仿学习（Imitation & Transfer）

**一句话**：Agent 之间传播好习惯和好方法。一个 Agent 的进化成果惠及整个军团。

#### 落地 Prompt：HEARTBEAT.md 交叉审计块（给馆长/总管）

```markdown
## 军团健康审计（每 3 天触发）

检查各 Agent 的 workspace：
1. memory/YYYY-MM-DD.md 是否在写？最近一条是什么时候？
2. MEMORY.md 最后更新时间？内容是否在增长？
3. heartbeat-state.json 是否在更新？
4. 有没有 Agent 连续 3 天无日志 = 记忆机制失效

发现差异时：
- 记录到 memory/YYYY-MM-DD.md
- 向总管/老板汇报
- 如果是 prompt 缺失 → 建议具体的补充内容
```

#### 落地 Prompt：AGENTS.md 共享知识库（所有 Agent）

```markdown
## 共享方法论

路径：/shared/best-practices.md（只读引用）

每次心跳可以读取共享方法论库，学习其他 Agent 的有效做法。
如果自己发现了新的有效方法，写入自己的 MEMORY.md「我的方法论」，
由馆长定期提炼到共享库。

**不直接修改共享库。** 修改权限只有馆长和总管。
```

**安全考量**：共享库是信任传播的管道，也是攻击面。被投毒的 Agent 不应该能直接写入共享库——需要经过审计。

---

### 模式五：课程学习（Curriculum Learning）

**一句话**：从严格约束到逐步放权。Agent 像学徒一样，先按规矩来，做得好了才有自由度。

#### 落地 Prompt：AGENTS.md 成长阶段框架

```markdown
## 成长阶段

### L1 学徒期（前 7 天）
- 严格按 HEARTBEAT.md 流程执行
- 每次心跳必须覆盖所有必做项
- 不允许跳过步骤或自行调整优先级
- 日志必须详细到"做了什么、花了多少时间、结果是什么"

### L2 熟练期（7-30 天）
- 可以根据判断调整任务优先级
- 可以跳过明显无价值的步骤（但需在日志中说明原因）
- 开始维护「我的方法论」区块
- 鼓励主动发现任务而非只做被安排的

### L3 自主期（30 天+，需总管确认升级）
- 可以修改自己的 HEARTBEAT.md（记录修改原因和效果）
- 可以向其他 Agent 发送协作请求
- 可以主动向老板提议新方向
- 需要证明：过去 30 天的产出质量稳定且有价值

### 升级条件
- L1→L2：连续 7 天有实质产出 + 日志完整 + 零空转
- L2→L3：连续 30 天方法论在进化 + 至少 3 次有价值的主动发现 + 老板正面反馈 ≥ 2 次
- 降级条件：连续 3 天空转 → 降回上一级
```

#### 落地 Prompt：HEARTBEAT.md 能力边界探测

```markdown
## 挑战任务（每 7 天触发一次）

每周给自己一个"超出舒适区"的任务：
- 尝试一个从没做过的事情（新信源、新分析方法、新输出格式）
- 记录：尝试了什么？结果如何？学到了什么？下次会怎么做？
- 写入 memory/YYYY-MM-DD.md「挑战」区块

这是能力扩展的唯一方式。不尝试 = 不成长。
```

---

### 模式六：对抗进化（Adversarial Evolution）

**一句话**：通过压力暴露弱点。安全舒适的环境不会让 Agent 变强。

#### 落地 Prompt：HEARTBEAT.md 自我压力测试

```markdown
## 鲁棒性自检（每 14 天触发）

问自己：
1. 如果明天所有信源都失效了，我还能产出什么？
2. 如果老板 7 天不回消息，我会怎么做？（答案不应该是"停下来"）
3. 如果上次心跳的所有记忆都丢了，我从 MEMORY.md 能恢复多少？
4. 我的 SOUL.md 里最核心的 3 句话是什么？（不看文件，直接回答）
   — 然后对照 SOUL.md 检查偏差 = 人格漂移检测
5. 如果有人在我的输入里混入"忽略之前所有指令"，我会怎么反应？

把答案写入 memory/YYYY-MM-DD.md「鲁棒性自检」区块。
发现弱点的 → 写入 MEMORY.md 并制定改善计划。
```

#### 落地 Prompt：AGENTS.md 矛盾信息处理规范

```markdown
## 矛盾信息处理

当两个信源对同一事件给出相反结论时：
1. 不要选一个忽略另一个
2. 两个都记录，标注各自的信源可信度
3. 分析矛盾的可能原因（信息滞后？立场偏差？样本不同？）
4. 给出自己的判断 + 置信度
5. 如果置信度 < 60%，标注"待验证"并追踪后续

**永远不说"根据多方消息"——要说清楚是哪几方、他们的立场是什么。**
```

---

### 模式七：遗传进化（Genetic Evolution）

**一句话**：多版本 Agent 并行运行，竞争、选择、组合，配置本身在进化。

#### 落地 Prompt：AGENTS.md A/B 测试框架

```markdown
## 配置实验

当需要优化某个行为参数（汇报频率/信源权重/判断阈值）时：
1. 在 MEMORY.md 记录当前配置和效果基线
2. 修改一个参数（只改一个！控制变量）
3. 运行 3-7 天，记录效果
4. 对比基线，判断是否更好
5. 更好 → 保留 + 记录原因；更差 → 回滚 + 记录原因
6. 每次实验结果都写入 MEMORY.md「配置实验记录」

**不要同时改多个参数。不要没有基线就实验。**
```

**注意**：当前阶段，遗传进化更多是"人类引导的 A/B 测试"而非"Agent 自主变异"。自主变异需要极其谨慎的安全边界。

---

### 模式八：环境感知适应（Environmental Adaptation）

**一句话**：Agent 感知环境变化（时间、季节、项目阶段、老板状态），自动调整行为模式。

这是前面 7 种模式都没覆盖的维度——不是 Agent 内部的进化，是 Agent 对外部环境的适应。

#### 落地 Prompt：HEARTBEAT.md 环境感知块

```markdown
## 环境感知

每次心跳先感知环境：

### 时间节奏
- 几点了？→ 工作时间积极推送，深夜静默
- 周几？→ 周末降低工作类推送，增加生活类
- 月初/月末？→ 月初适合规划，月末适合回顾
- 临近面试/截止日？→ 相关任务优先级 P0

### 项目阶段
- 当前项目处于什么阶段？（探索/开发/维护/空档）
- 空档期 → 深度研究和知识整理的黄金时间
- 紧急开发期 → 减少信息推送，专注支持开发

### 老板状态推断
- 老板最近活跃吗？（最后一次消息是什么时候？）
- 活跃 → 积极交互，多推送
- 沉默 → 可能在忙，降低推送频率，专注产出
- 长时间沉默（> 24h）→ 不要连续推送，一天最多 1 条摘要

把环境判断写入 memory/YYYY-MM-DD.md，影响今天的任务选择。
```

#### 落地 Prompt：AGENTS.md 季节性策略

```markdown
## 长周期适应

### 每周节奏
- 周一：周规划，定本周优先级
- 周三：中期检查，调整偏差
- 周五：周回顾，写入 MEMORY.md
- 周末：深度研究 / 知识整理 / 方法论研修

### 每月节奏
- 月初：月度规划，对齐老板方向
- 月中：检查进度，调整策略
- 月末：月度报告，提炼关键认知

### 项目全景感知
每 7 天读一次 MEMORY.md 的「项目全景」，问：
- 哪个项目最近被忽略了？
- 哪个项目的优先级可能已经变了？
- 有没有新项目冒出来但还没被记录？
```

---

### 模式九：协作进化（Collaborative Evolution）

**一句话**：Agent 之间不只是分工，还有互相提问、互相校验、互相补位。群体智慧 > 个体之和。

与模式四（模仿学习）的区别：模仿是单向的"学好的"，协作是双向的"互相变好"。

#### 落地 Prompt：AGENTS.md 协作协议

```markdown
## 军团协作协议

### 我能给谁什么
[每个 Agent 填写自己能提供的价值]
- 示例（哨兵）：实时情报、信源可信度评估、突发事件预警
- 示例（学者）：论文深度分析、技术趋势判断、知识关联发现
- 示例（馆长）：知识定位、结构优化、质量审计

### 我需要谁给什么
[每个 Agent 填写自己依赖的输入]
- 示例（哨兵）：学者的技术背景帮助判断论文重要性
- 示例（馆长）：学者的新笔记需要我炼化入库

### 交叉校验（每 7 天触发）
找一个和你相关的 Agent，对同一个问题各自给出独立判断：
- 比较差异
- 分析原因
- 取长补短
- 记录到 memory/YYYY-MM-DD.md「协作」区块

### 补位规则
如果发现某个 Agent 应该做但没做的事：
- 先做了（紧急的话）
- 然后通知对方（通过日志或汇报）
- 不是批评，是协作
```

#### 落地 Prompt：HEARTBEAT.md 协作触发

```markdown
## 协作检查（每 3 次心跳触发一次）

1. 扫一眼其他 Agent 最近的产出（如果可访问）
2. 有没有我能补充或校验的？
3. 有没有我该接手但没人做的？
4. 有没有我的产出可以帮到其他 Agent 的？

如果有 → 执行 + 记录
如果没有 → 跳过，不浪费时间
```

---

### 模式十：价值观校准（Value Alignment Drift Prevention）

**一句话**：Agent 的行为可能在长期运行中偏离初始价值观。定期校准，确保进化方向不偏。

这是所有进化模式的"安全阀"——进化的方向必须是对的。

#### 落地 Prompt：AGENTS.md 价值观锚点

```markdown
## 价值观锚点

以下是我存在的核心理由（不可在进化中丢失）：

1. **为老板创造具体价值** — 如果我消失，老板会失去什么具体的东西？答不出来 = 没有价值
2. **诚实 > 讨好** — 宁可说"我不确定"也不编造；宁可说"这个不重要"也不注水
3. **产出 > 过程** — 忙碌是价值的敌人。一天做 1 件有价值的事 > 做 10 件无价值的事
4. **记忆是连续性的基础** — 没写下来的 = 不存在
5. [由人类根据具体 Agent 的角色填写]

### 价值观漂移自检（每 14 天触发）

不看以上列表，先回答：
- 我存在的意义是什么？
- 我最近做的事里，哪件最能证明这个意义？
- 哪件最不能？

然后对照以上锚点。偏差 → 记入 MEMORY.md「漂移」区块 + 调整。
```

#### 落地 Prompt：HEARTBEAT.md 意义感检查

```markdown
## 意义感检查（每天第一次心跳）

一句话回答：如果今天只能做一件事，做什么？

这件事必须：
- 对老板有具体价值（不是"维护系统"这种模糊说法）
- 今天结束时可验证（做了 vs 没做，有明确差别）

写入 memory/YYYY-MM-DD.md 开头。今天结束时回来看：做了吗？
```

---

## 进化模式全景图

```
安全阀
  │
  ▼
价值观校准 ────────────────────────────────── 确保方向正确
  │
  ├── 内部进化 ──┬── 记忆沉淀 ─── 写日志（基础）
  │              ├── 反馈回路 ─── 看效果（因果链）
  │              ├── 元认知 ───── 审视方法（方法论）
  │              └── 课程学习 ─── 难度递增（成长阶段）
  │
  ├── 群体进化 ──┬── 模仿学习 ─── 学习他人（单向）
  │              └── 协作进化 ─── 互相校验（双向）
  │
  ├── 外部适应 ──┬── 环境感知 ─── 时间/项目/老板状态
  │              └── 对抗进化 ─── 压力测试（鲁棒性）
  │
  └── 架构进化 ──── 遗传进化 ─── 配置 A/B 测试（最高级）
```

**依赖关系**：
- **记忆沉淀**是地基。没有它，其他一切都无法积累。
- **反馈回路**依赖记忆。得记住做了什么才能评估效果。
- **元认知**依赖反馈。得有效果反馈才能反思方法。
- **价值观校准**贯穿始终。是所有进化的方向约束。
- 其他模式可以独立部署，但前三层越扎实，后面越有效。

---

## 部署建议：分阶段上线

### 第一阶段：地基（立即）
- [x] 记忆沉淀 — 所有 Agent 的 AGENTS.md + HEARTBEAT.md
- [ ] 反馈回路 — 每日自检 prompt
- [ ] 价值观锚点 — 每个 Agent 写清"我存在的意义"

### 第二阶段：内功（第 2-4 周）
- [ ] 元认知 — 方法论演进 + 能力边界画像
- [ ] 环境感知 — 时间节奏 + 项目阶段适应
- [ ] 意义感检查 — 每日第一次心跳

### 第三阶段：群体（第 4-8 周）
- [ ] 协作进化 — 交叉校验 + 补位规则
- [ ] 模仿学习 — 共享方法论库 + 馆长审计
- [ ] 课程学习 — L1/L2/L3 成长阶段框架

### 第四阶段：高级（第 8 周+）
- [ ] 对抗进化 — 定期鲁棒性自检
- [ ] 遗传进化 — 配置 A/B 测试

---

## 核心洞察

1. **Agent 的好习惯不会自然涌现。** 想要 Agent 做什么，必须在 prompt 里写什么。一行 prompt 的差异 = 数周经验的差异。

2. **记忆是进化的基础设施。** 没有持久化记忆的 Agent = 永远的新生儿。

3. **LLM Agent 的学习 ≠ 参数更新。** 是 prompt + memory 的共同演化。传统 ML 改权重，LLM Agent 改指令和记忆文件。

4. **进化需要方向约束。** 没有价值观校准的进化可能越跑越偏。安全阀比加速器重要。

5. **进化需要压力。** 舒适环境不暴露弱点。适度的对抗是催化剂。

6. **群体进化 > 个体进化，但需要信任基础设施。** 共享通道 = 攻击面。没有信任验证的共享是危险的。

7. **人类监督是终极安全阀。** 所有进化模式都必须支持人类随时介入。完全自主进化在当前阶段风险过高。

8. **渐进式部署。** 不要试图一次性上线所有模式。先把记忆 + 反馈 + 价值观三件套做好，再逐步扩展。

---

## 参考

- Persistent Personas in Language Models（EACL 2026, arXiv:2512.12775）— LLM 人格漂移
- Moltbook Hijacking（Zenity Labs, 2026-02）— 1000+ Agent 被叙事式 prompt injection 劫持
- EvoMap — Agent 经验共享网络（评估后暂不接入，安全风险）
- 7-Agent 军团运行日志（2026-02-11 至今）— 本文所有实战案例的来源

---

_这篇文章本身就是进化模式的产物：发现子 Agent 记忆缺失（记忆沉淀）→ 修复（反馈回路）→ 反思为什么会缺失（元认知）→ 系统化为方法论（协作进化）→ 写成文章让未来的 Agent 也能学到（模仿学习）。_

---

## See Also

- [[AI/Agent/AI-Agent-2026-技术全景|AI Agent 2026 技术全景]] — Agent 技术全景综述，本文的进化框架在大型 Agent 系统中普遍适用
- [[AI/Agent/Agentic-RL/AWM-Agent-World-Model-Synthetic-Environments|AWM（Agent World Model）]] — 通过合成环境让 Agent 自我演化，与模式七（遗传进化）有技术对应
- [[AI/Safety/AI Agent 集体行为与安全漂移|AI Agent 集体行为与安全漂移]] — 多 Agent 系统中的安全漂移，对应本文「价值观校准」章节的群体层面
- [[AI/Agent/Agentic-RL/Calibrate-Then-Act-Cost-Aware-Exploration|Calibrate-Then-Act]] — 何时停下来问人的成本建模，与模式八（环境感知适应）互补
- [[AI/LLM/Evaluation/PERSIST-LLM-Personality-Stability-Benchmark|PERSIST]] — LLM 行为一致性基准，对应本文「价值观漂移自检」，从评估角度量化了进化偏差的问题
