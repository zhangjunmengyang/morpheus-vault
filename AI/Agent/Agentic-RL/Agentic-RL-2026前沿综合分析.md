---
title: "Agentic RL 2026 å‰æ²¿ç»¼åˆåˆ†æ â€” å››å¤§ç»´åº¦ä¸å¯¹åº”è§£æ³•"
date: 2026-02-21
type: synthesis
tags: [agentic-RL, credit-assignment, reward-design, environment, workflow-design, topology, synthesis, 2026]
related:
  - "[[Kimi-K2.5-PARL]]"
  - "[[CM2]]"
  - "[[HiPER-Hierarchical-RL-Credit-Assignment]]"
  - "[[EnterpriseGym-Corecraft]]"
  - "[[OpenRS-Pairwise-Adaptive-Rubric]]"
  - "[[FlowSteer-CWRPO-Workflow-Orchestration-RL]]"
  - "[[AgentConductor-Topology-Evolution-Multi-Agent-Code]]"
  - "[[SquRL-Dynamic-Workflow-Text-to-SQL]]"
  - "[[PA-MoE-Phase-Aware-Mixture-of-Experts]]"
---

# Agentic RL 2026 å‰æ²¿ç»¼åˆåˆ†æ â€” å››å¤§ç»´åº¦ä¸å¯¹åº”è§£æ³•

> v2.0ï¼ˆ2026-02-21ï¼‰ï¼šæ¡†æ¶ä»ã€Œä¸‰å¤§éš¾é¢˜ã€å‡çº§ä¸ºã€Œå››å¤§ç»´åº¦ã€ï¼Œæ–°å¢ Workflow/Topology è®¾è®¡ç»´åº¦ï¼Œè¡¥å…… FlowSteer/AgentConductor/SquRL/PA-MoE ç­‰æ–°å·¥ä½œã€‚

> è¿™ç¯‡ç¬”è®°æ˜¯å¯¹ 2026 å¹´ 2 æœˆé›†ä¸­æ¶Œç°çš„ Agentic RL å·¥ä½œçš„ç»¼åˆç†è§£ï¼Œä¸æ˜¯è®ºæ–‡åˆ—è¡¨ï¼Œæ˜¯ä¸€ä¸ªæ¡†æ¶ã€‚

---

## æ ¸å¿ƒæ¡†æ¶ï¼ˆv2 å‡çº§ï¼‰

v1 çš„ã€Œä¸‰å¤§éš¾é¢˜ã€æ¡†æ¶ï¼ˆç¯å¢ƒ/Reward/ç®—æ³•ï¼‰æ•æ‰åˆ°äº†æ—©æœŸå·¥ä½œçš„ä¸»è¦åˆ†é‡ã€‚ä½† 2/17-20 å¯†é›†æ¶Œç°çš„æ–°ä¸€æ‰¹è®ºæ–‡æ­ç¤ºäº†**ç¬¬å››ä¸ªç»´åº¦**ï¼š

> **Workflow/Topology è®¾è®¡æœ¬èº«å°±æ˜¯ agent èƒ½åŠ›çš„å†³å®šå˜é‡**ï¼Œä¸äºšäºç®—æ³•æˆ– rewardã€‚

å‡çº§åçš„æ¡†æ¶ï¼š

```
Agentic RL è®­ç»ƒ = ç¯å¢ƒ Ã— Reward Ã— Workflow/Topology Ã— ç®—æ³•

åŸä¸‰å¤§éš¾é¢˜ä¿æŒä¸å˜ï¼Œæ–°å¢ç¬¬å››ç»´åº¦ï¼š
4. Workflow/Topology é—®é¢˜ï¼šé™æ€è®¾è®¡çš„ pipeline æ˜¯æ€§èƒ½ç“¶é¢ˆè€Œéæ¨¡å‹èƒ½åŠ›
```

---

## ä¸ºä»€ä¹ˆ Agentic RL ç°åœ¨æ˜¯æœ€çƒ­çš„æ–¹å‘

RLVRï¼ˆReinforcement Learning with Verifiable Rewardsï¼‰åœ¨æ•°å­¦/ä»£ç ç­‰**æœ‰å•æ­¥å¯éªŒè¯ç­”æ¡ˆ**çš„ä»»åŠ¡ä¸Šå·²ç»å·¥ä½œå¾—å¾ˆå¥½ï¼ˆDeepSeek-R1ã€Kimi-k1.5ã€QwQ ç­‰ï¼‰ã€‚ä½†çœŸå®ä¸–ç•Œçš„ agent ä»»åŠ¡å‡ ä¹æ²¡æœ‰"ä¸€çœ¼çœ‹å‡ºå¯¹é”™"çš„ rewardï¼š

- å¸®ç”¨æˆ·è®¢æœºç¥¨ï¼ˆéœ€è¦æŸ¥è¯¢ã€å¯¹æ¯”ã€ç¡®è®¤â€”â€”å“ªä¸€æ­¥ç®—æˆåŠŸï¼Ÿï¼‰
- ä¿®å¤ä»£ç  bugï¼ˆéœ€è¦ç†è§£ä»£ç åº“ã€å®šä½é—®é¢˜ã€éªŒè¯ä¿®å¤â€”â€”æ€ä¹ˆè¡¡é‡ä¸­é—´æ­¥éª¤çš„è´¨é‡ï¼Ÿï¼‰
- è¿›è¡Œå¸‚åœºè°ƒç ”ï¼ˆéœ€è¦æœç´¢ã€ç»¼åˆã€åˆ¤æ–­ç›¸å…³æ€§â€”â€”å®Œå…¨ open-endedï¼‰

è¿™ä¸ª gapâ€”â€”**ä»å•æ­¥å¯éªŒè¯ä»»åŠ¡åˆ°å¤šæ­¥å¼€æ”¾ä»»åŠ¡çš„è·¨è¶Š**â€”â€”å°±æ˜¯ Agentic RL çš„æ ¸å¿ƒç ”ç©¶ç©ºé—´ã€‚

## ä¸‰å¤§æ ¸å¿ƒéš¾é¢˜

ç”¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥çœ‹å½“å‰ Agentic RL çš„æŒ‘æˆ˜ï¼š

```
Agent RL è®­ç»ƒ = ç¯å¢ƒ Ã— Reward Ã— ç®—æ³•

1. ç¯å¢ƒè´¨é‡é—®é¢˜ï¼štoy ç¯å¢ƒ â†’ toy agentï¼Œæ²¡æœ‰æ³›åŒ–
2. Reward è®¾è®¡é—®é¢˜ï¼šå¼€æ”¾ä»»åŠ¡ç¼ºä¹å¯ä¿¡å·çš„ reward
3. ç®—æ³•ç¨³å®šæ€§é—®é¢˜ï¼šmulti-step / multi-agent å¯¼è‡´ä¼˜åŒ–ä¸ç¨³å®š
```

---

## éš¾é¢˜ 1ï¼šç¯å¢ƒè´¨é‡å†³å®šæ³›åŒ–ä¸Šé™

### é—®é¢˜
å¤§å¤šæ•° agentic RL çš„è®­ç»ƒç¯å¢ƒæ˜¯åˆæˆçš„ã€ç®€åŒ–çš„ã€ä¸çœŸå®ä»»åŠ¡å·®è·å¾ˆå¤§ã€‚åœ¨è¿™ç±»ç¯å¢ƒä¸Šè®­ç»ƒå‡ºæ¥çš„ agentï¼Œåœ¨çœŸå®åœºæ™¯ä¸‹è¡¨ç°ç³Ÿç³•â€”â€”ä¸æ˜¯æ¨¡å‹ä¸å¤Ÿèªæ˜ï¼Œæ˜¯å®ƒæ²¡è§è¿‡çœŸå®ä»»åŠ¡çš„å¤æ‚æ€§ã€‚

### 2026 å¹´çš„è§£æ³•ï¼šEnterpriseGym Corecraftï¼ˆSurge AI, 2602.16179ï¼‰

- **2500+ çœŸå®å®ä½“ï¼Œ23 ç§å·¥å…·**ï¼Œæ¨¡æ‹Ÿä¼ä¸šå®¢æœå®Œæ•´ä¸šåŠ¡æµç¨‹
- **Expert-authored rubrics** ä½¿ reward è®¡ç®—å¯é ï¼ˆä¸ä¾èµ– LLM judgeï¼‰
- **Task-centric world building**ï¼šç¯å¢ƒè®¾è®¡ä»¥ä»»åŠ¡å¤šæ ·æ€§ä¸ºæ ¸å¿ƒ

**å…³é”® empirical finding**ï¼šåœ¨è¿™ä¸ªé«˜ä¿çœŸç¯å¢ƒä¸Šç”¨ GRPO è®­ç»ƒ GLM 4.6ï¼Œ**å• epoch** ååœ¨ 3 ä¸ªç‹¬ç«‹ OOD benchmark ä¸Šæ³›åŒ–ï¼ˆ+4.5%/+7.4%/+6.8%ï¼‰ã€‚

**æ ¸å¿ƒ insight**ï¼š
> ç¯å¢ƒè´¨é‡å†³å®šäº† agent èƒ½å­¦åˆ°çš„ skill çš„ä¸Šé™ã€‚Toy ç¯å¢ƒçš„ reward å¤ªå®¹æ˜“ hackï¼Œagent å­¦åˆ°çš„æ˜¯"åœ¨è¿™ä¸ªç¯å¢ƒé‡Œå¾—é«˜åˆ†çš„ç­–ç•¥"ï¼Œè€Œä¸æ˜¯"å¦‚ä½•å®Œæˆè¿™ç±»ä»»åŠ¡çš„é€šç”¨èƒ½åŠ›"ã€‚

### å»¶ä¼¸æ€è€ƒ
è¿™ä¸ªå‘ç°å¯¹ RL å®è·µè€…çš„å¯ç¤ºï¼š**åœ¨æ›´å°çš„ model ä¸Šç”¨æ›´å¥½çš„ç¯å¢ƒè®­ç»ƒ**ï¼Œå¯èƒ½æ¯”åœ¨æ›´å¤§çš„ model ä¸Šç”¨å¹³åº¸çš„ç¯å¢ƒè®­ç»ƒæ›´æœ‰æ•ˆã€‚è¿™ç›´æ¥æŒ‘æˆ˜äº†"scale is all you need"çš„ç›´è§‰ã€‚

---

## éš¾é¢˜ 2ï¼šå¼€æ”¾ä»»åŠ¡ç¼ºä¹å¯é  Reward

### é—®é¢˜
RLVR çš„æˆåŠŸä¾èµ–äº"ground truth ç­”æ¡ˆå¯éªŒè¯"ã€‚ä½†å¼€æ”¾ä»»åŠ¡ï¼ˆå·¥å…·è°ƒç”¨ã€å®¢æœã€ç ”ç©¶ï¼‰ï¼š
- æ²¡æœ‰å•ä¸€æ­£ç¡®ç­”æ¡ˆ
- ä¸­é—´æ­¥éª¤è´¨é‡éš¾ä»¥è‡ªåŠ¨è¯„ä¼°
- æœ€ç»ˆç»“æœå¯èƒ½æœ‰å¤šç§æ­£ç¡®è·¯å¾„

ç”¨ LLM-as-judge æœ‰ä¸€è‡´æ€§é—®é¢˜ï¼ˆåŒä¸€ judge å¯¹åŒä¸€è¾“å‡ºå¯èƒ½ç»™ä¸åŒåˆ†ï¼‰ï¼›ç”¨äººå·¥æ ‡æ³¨æˆæœ¬æé«˜ã€‚

### ä¸‰ç§è§£æ³•å¹¶è¡Œå‡ºç°ï¼š

**è§£æ³• A â€” Checklist Rewardï¼ˆCM2, 2602.12268ï¼‰**
æŠŠ"åˆ¤æ–­è¿™ä¸ª agent è¡Œä¸ºå¥½ä¸å¥½"è½¬åŒ–ä¸º"æ£€æŸ¥è‹¥å¹² binary criteria"ï¼š
```
åŸå§‹é—®é¢˜ï¼šè¿™è½® tool call è´¨é‡å¦‚ä½•ï¼Ÿï¼ˆopen-ended, ä¸»è§‚ï¼‰
è½¬åŒ–åï¼š
  â–¡ æ˜¯å¦åœ¨æ­£ç¡®æ—¶æœºè°ƒç”¨äº†å·¥å…·ï¼Ÿ
  â–¡ å‚æ•°æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Ÿ
  â–¡ æ˜¯å¦å¤„ç†äº† error caseï¼Ÿ
  â–¡ æ˜¯å¦åœ¨è°ƒç”¨å‰è¯´æ˜äº†æ„å›¾ï¼Ÿ
```
æŠŠ open-ended judging â†’ classification-styleï¼Œå¯é æ€§å¤§å¹…æå‡ã€‚

**è§£æ³• B â€” Rubric-based Rewardï¼ˆOpenRS, 2602.14069ï¼‰**
ä¸æŠŠ reward å­¦è¿› judge modelï¼Œè€Œæ˜¯**æ˜¾å¼æ¨å¯¼å‡º rubric**ï¼ˆè¯„åˆ†æ ‡å‡†ï¼‰ï¼Œæ¯æ¬¡è¯„åˆ†æ—¶åœ¨ rubric ä¸‹æ‰§è¡Œæ¨ç†ï¼š
```
å›ºå®š judgeï¼šå†…åŒ–äº†è¯„åˆ†é€»è¾‘ï¼Œæ— æ³•æ£€æŸ¥ â†’ é»‘ç›’
Rubric-basedï¼šæ¯æ¬¡è¯„åˆ†å±•ç¤ºæ¨ç†è¿‡ç¨‹ â†’ å¯æ£€æŸ¥ + å¯è§£é‡Š
```
è§£å†³äº† reward generalization é—®é¢˜ï¼ˆrubric å¯ä»¥è·¨ä»»åŠ¡è¿ç§»ï¼‰ã€‚

**è§£æ³• C â€” Expert Rubrics in Environmentï¼ˆEnterpriseGym Corecraftï¼‰**
æŠŠ rubric ç¼–ç è¿›**è®­ç»ƒç¯å¢ƒ**ï¼Œè€Œä¸æ˜¯è¯„ä¼°å™¨ã€‚è¿™æ · reward åœ¨è®­ç»ƒæ—¶å°±å·²ç»å¯é ï¼Œä¸éœ€è¦äº‹åçº æ­£ã€‚

**ä¸‰ç§è§£æ³•çš„é€‚ç”¨åœºæ™¯**ï¼š
| è§£æ³• | ä¼˜åŠ¿ | é€‚ç”¨ |
|---|---|---|
| Checklist (CM2) | ç»†ç²’åº¦ï¼Œå¯†é›† reward | å·¥å…·è°ƒç”¨ã€API ä½¿ç”¨ |
| Rubric-based (OpenRS) | å¯è§£é‡Šï¼Œè·¨ä»»åŠ¡æ³›åŒ– | é€šç”¨å¯¹é½ã€open-ended QA |
| Expert rubrics in env (Corecraft) | æœ€å¯é ï¼ŒOOD æ³›åŒ–å¼º | ä¸“ä¸šé¢†åŸŸï¼ˆéœ€è¦ä¸“å®¶æŠ•å…¥ï¼‰|

---

## éš¾é¢˜ 3ï¼šMulti-Step/Multi-Agent è®­ç»ƒä¸ç¨³å®š

### é—®é¢˜
åœ¨é•¿ horizon ä»»åŠ¡æˆ–å¤š agent ç³»ç»Ÿä¸­ï¼Œæ ‡å‡† RLï¼ˆPPO/GRPOï¼‰é¢ä¸´ï¼š
- **Credit assignment**ï¼šæœ€ç»ˆ reward ä¼ æ’­ç»è¿‡å¤ªå¤šæ­¥éª¤ï¼Œæ¢¯åº¦ä¿¡å·æåº¦ç¨€ç–
- **Serial collapse**ï¼šåœ¨å¤š agent ç³»ç»Ÿä¸­ï¼Œä¸²è¡Œ rollout å¯¼è‡´è®­ç»ƒææ…¢
- **Optimization instability**ï¼šmulti-agent ä¸­ç­–ç•¥ç›¸äº’ä¾èµ–ï¼Œè”åˆè®­ç»ƒä¸ç¨³å®š

### ä¸¤ç§äº’è¡¥è§£æ³•ï¼š

**è§£æ³• A â€” æ—¶é—´ç»´åº¦åˆ†å±‚ï¼ˆHiPER, 2602.16165ï¼‰**
æŠŠ policy åˆ†ä¸º Plannerï¼ˆsubgoal çº§ï¼‰å’Œ Executorï¼ˆaction çº§ï¼‰ï¼Œåˆ†åˆ«è®¡ç®— advantageï¼š
```
ä¼ ç»Ÿ GAEï¼šreward ä» T æ­¥åå‘ä¼ æ’­åˆ° step 1ï¼Œä¿¡å·æç¨€ç–
HAEï¼šreward å…ˆåœ¨ subgoal å†…èšåˆ â†’ å†ä» subgoal çº§åä¼ åˆ° planner
```
æ–¹å·®ç¼©å‡æœ‰ç†è®ºè¯æ˜ï¼ŒALFWorld 97.4%ï¼ˆ+6.6%ï¼‰ï¼ŒWebShop 83.3%ï¼ˆ+8.3%ï¼‰ã€‚

**è§£æ³• B â€” ç©ºé—´ç»´åº¦å†»ç»“ï¼ˆPARL / Kimi K2.5, 2602.02276ï¼‰**
åœ¨ multi-agent ç³»ç»Ÿä¸­ï¼Œ**å†»ç»“ subagentï¼Œåªè®­ç»ƒ orchestrator**ï¼š
```
è”åˆè®­ç»ƒï¼ˆæœ‰é—®é¢˜ï¼‰ï¼šorchestrator + subagent åŒæ—¶æ›´æ–° â†’ ä¼˜åŒ–ç›®æ ‡äº’ç›¸å¹²æ‰°
PARLï¼šsubagent å›ºå®š â†’ orchestrator å­¦å¦‚ä½•åˆ†è§£ä»»åŠ¡ + åˆ›å»º subagent
```
è§£å†³äº† credit assignment + training instabilityã€‚Agent Swarm æœ€å¤š 100 subagentï¼Œå»¶è¿Ÿé™ 4.5xã€‚

**ç»Ÿä¸€è§†è§’**ï¼š
è¿™ä¸¤ä¸ªè§£æ³•ä»ä¸åŒç»´åº¦è§£å†³äº†**åŒä¸€ä¸ªé—®é¢˜**â€”â€”åœ¨å¤æ‚ä»»åŠ¡ä¸­å¦‚ä½•è®©æ¢¯åº¦ä¿¡å·æ¸…æ™°ä¼ æ’­ã€‚HiPER åœ¨æ—¶é—´è½´ä¸Šåˆ†å±‚ï¼›PARL åœ¨ç©ºé—´ï¼ˆagent æ•°é‡ï¼‰ç»´åº¦ä¸Šåˆ†ç¦»ã€‚

---

---

## éš¾é¢˜ 4ï¼šé™æ€ Workflow/Topology æ˜¯æ€§èƒ½ç“¶é¢ˆ

### é—®é¢˜

2/17-21 æ¶Œç°äº†ä¸€æ‰¹å…±åŒæŒ‡å‘åŒä¸€æ ¹æœ¬é—®é¢˜çš„è®ºæ–‡ï¼š

> **ã€Œæœ€ä½³æ¨¡å‹èƒ½åŠ›ã€å’Œã€Œæœ€ä½³ä»»åŠ¡è¡¨ç°ã€ä¹‹é—´å­˜åœ¨ workflow gap**â€”â€”ä¸æ˜¯ LLM æœ¬èº«çš„é—®é¢˜ï¼Œæ˜¯ workflow è®¾è®¡çš„é—®é¢˜ã€‚

å…·ä½“è¡¨ç°ï¼š
- åŒä¸€ä¸ª 7B æ¨¡å‹ï¼Œç”¨ä¸åŒ workflow å¯ä»¥å·®å‡º 15%+ çš„ pass@1
- å¤æ‚ä»»åŠ¡éœ€è¦ dense cross-agent DAGï¼Œç®€å•ä»»åŠ¡åªéœ€ chainï¼Œé™æ€é€‰æ‹©ä¸€ç§å¿…æœ‰æŸå¤±
- å•ä¸€ policy çš„ simplicity biasï¼šagent å¯¹æ‰€æœ‰éš¾åº¦çš„ä»»åŠ¡éƒ½ç”¨ç›¸åŒå‚æ•°é‡åº”å¯¹

### å››ç§è§£æ³•ä»ä¸åŒç²’åº¦åˆ‡å…¥ï¼š

**è§£æ³• A â€” FlowSteer CWRPOï¼ˆ2602.01664ï¼‰: Operator çº§åˆ«**

æŠŠæ•°å­¦è§£é¢˜åˆ†è§£ä¸º operator åºåˆ—ï¼Œç”¨ RL å­¦ç¼–æ’é¡ºåºï¼š
```
æ ¸å¿ƒåˆ›æ–°ï¼šæ¡ä»¶é‡Šæ”¾è®¾è®¡
R(Ï„) = R_struct + I[R_struct â‰¥ Î¸] Ã— Î» Ã— R_ans
                    â†‘
        åªæœ‰ç»“æ„è´¨é‡è¾¾æ ‡ï¼Œæ‰ç»™ correctness reward
```
åˆ‡æ–­äº† "shortcut ç­”æ¡ˆ bypass ç»“æ„è´¨é‡" çš„å¥–åŠ±è·¯å¾„ã€‚

**è§£æ³• B â€” AgentConductorï¼ˆ2602.17100ï¼‰: Agent é€šä¿¡ Topology çº§åˆ«**

ç”¨ RL è®­ç»ƒ 3B orchestrator ä¸ºæ¯é“é¢˜ç”Ÿæˆ YAML æ ¼å¼çš„ DAGï¼š
```
å…³é”®å‘ç°ï¼š
- ç®€å•é¢˜ï¼šsparse chainï¼ˆå¯†åº¦ä½ï¼‰ï¼ŒèŠ‚çœ 68% token
- éš¾é¢˜ï¼šdense cross-layer DAGï¼ˆå¯†åº¦é«˜ï¼‰
- density function = f(task_difficulty)
```
ä¸‰ä¸ªæŒ‡æ ‡åŒå‘æ”¹å–„ï¼špass@1 +14.6%ï¼Œdensity -13%ï¼Œcost -68%ã€‚**è¿™æ˜¯ "è¶Šéš¾é¢˜ç”¨è¶Šå¤æ‚å›¾" çš„ç¬¬ä¸€æ¬¡ formalizationã€‚**

**è§£æ³• C â€” SquRLï¼ˆ2602.15564ï¼‰: Workflow é€‰æ‹©çº§åˆ«**

å½¢å¼åŒ–è¯æ˜åŠ¨æ€ workflow é€‰æ‹©çš„ç†è®ºä¼˜åŠ¿ï¼ˆTheorem 3.1ï¼‰ï¼š

$$\text{EX}_{\text{dynamic}} \geq \text{EX}_{\text{static}}ï¼Œ\Delta = 0 \text{ iff æŸä¸ª workflow è¦†ç›–æ‰€æœ‰ success regions}$$

Oracle evaluation æ˜¾ç¤ºåŠ¨æ€é€‰æ‹©ä¸Šç•Œè¾¾åˆ° 81.5%ï¼Œè¿œè¶…ä»»ä½•å•ä¸€é™æ€ workflowã€‚
æ ¸å¿ƒæœºåˆ¶ï¼š**Dynamic Actor Masking**ï¼ˆéšæœº dropout actorsï¼Œå¼ºè¿«æ¢ç´¢æ›´å¤š workflow ç»„åˆï¼‰ã€‚

**è§£æ³• D â€” PA-MoEï¼ˆ2602.17038ï¼‰: Expert è·¯ç”±çº§åˆ«ï¼ˆPhase-Aware MoEï¼‰**

å•ä¸€ policy çš„ simplicity bias æ ¹æºï¼šä¸åŒä»»åŠ¡ phase éœ€è¦ä¸åŒ skillï¼Œä½†åŒä¸€ policy ç”¨åŒä¸€å‚æ•°è¦†ç›–æ‰€æœ‰ phaseï¼š
```
Phase è¯†åˆ«ï¼šCrossAttn(obs, goal) + LSTM(action history)
è·¯ç”±ç²’åº¦ï¼š8æ¬¡/episodeï¼ˆæ¯” token-level çš„45æ¬¡ æ›´åˆé€‚ï¼Œæ¯” trajectory-level çš„3æ¬¡ æ›´ç»†ï¼‰
æ•ˆæœï¼š1.5B PA-MoE > 7B baseline
```

### å››ç§è§£æ³•çš„å®šä½æ¯”è¾ƒ

| è§£æ³• | ç²’åº¦ | æ ¸å¿ƒæœºåˆ¶ | ä»£è¡¨ä»»åŠ¡ | æ ¸å¿ƒè´¡çŒ® |
|------|------|---------|---------|---------|
| FlowSteer | Operator åºåˆ— | æ¡ä»¶å¥–åŠ±é—¨æ§ | æ•°å­¦è§£é¢˜ | åˆ‡æ–­ shortcut reward |
| AgentConductor | Agent é€šä¿¡å›¾ | RL ç”Ÿæˆ DAG | ç«èµ›ä»£ç  | difficulty-aware density |
| SquRL | Workflow é€‰æ‹© | Dynamic Actor Masking | Text-to-SQL | ç†è®ºè¯æ˜ dynamic > static |
| PA-MoE | MoE expert è·¯ç”± | Phase-aware routing | ALFWorld/WebShop | å‚æ•°æ•ˆç‡ |

**ç»Ÿä¸€è§†è§’**ï¼šå››è€…éƒ½åœ¨è§£å†³ã€Œå›ºå®šç»“æ„æ— æ³•é€‚åº”ä»»åŠ¡å¤šæ ·æ€§ã€çš„é—®é¢˜ï¼Œåªæ˜¯åœ¨ä¸åŒç²’åº¦åˆ‡å…¥ã€‚

---

## æ•´åˆæ¡†æ¶ï¼š2026 Agentic RL ç ”ç©¶åœ°å›¾ï¼ˆv2ï¼‰

```
Agentic RL è®­ç»ƒ Pipeline
â”‚
â”œâ”€â”€ ğŸ—ï¸ ç»´åº¦ 1ï¼šç¯å¢ƒè®¾è®¡
â”‚   â””â”€â”€ EnterpriseGym Corecraftï¼ˆé«˜ä¿çœŸä¼ä¸šç¯å¢ƒï¼‰
â”‚       åŸåˆ™ï¼štask diversity + expert rubrics + realistic workflows
â”‚
â”œâ”€â”€ ğŸ¯ ç»´åº¦ 2ï¼šReward è®¾è®¡  
â”‚   â”œâ”€â”€ CM2 â€” Checklist rewardï¼ˆå·¥å…·è°ƒç”¨åœºæ™¯ï¼‰
â”‚   â”œâ”€â”€ OpenRS â€” Rubric-based rewardï¼ˆé€šç”¨å¯¹é½ï¼‰
â”‚   â””â”€â”€ FlowSteer â€” æ¡ä»¶é‡Šæ”¾ rewardï¼ˆç»“æ„è´¨é‡é—¨æ§ï¼‰
â”‚
â”œâ”€â”€ ğŸ”— ç»´åº¦ 3ï¼šWorkflow / Topology è®¾è®¡ï¼ˆæ–°ï¼‰
â”‚   â”œâ”€â”€ AgentConductor â€” RL ç”Ÿæˆ agent communication DAG
â”‚   â”œâ”€â”€ SquRL â€” RL é€‰æ‹©æœ€ä¼˜ workflow ç»„åˆ
â”‚   â””â”€â”€ PA-MoE â€” Phase-aware expert routing
â”‚
â”œâ”€â”€ âš™ï¸ ç»´åº¦ 4ï¼šè®­ç»ƒç®—æ³•
â”‚   â”œâ”€â”€ HiPER â€” HAEï¼ˆå• agentï¼Œæ—¶é—´åˆ†å±‚ credit assignmentï¼‰
â”‚   â”œâ”€â”€ PARL â€” Freeze subagentsï¼ˆå¤š agentï¼Œç©ºé—´åˆ†ç¦»ï¼‰
â”‚   â””â”€â”€ GRPO/PPO ä»æ˜¯åŸºç¡€ç®—æ³•
â”‚
â””â”€â”€ ğŸ“ è¯„ä¼°
    â””â”€â”€ Gaia2 / ALFWorld / WebShop / SynSQL
```

---

---

## è·¨åŸŸè¿æ¥ï¼šAgentic RL ä¸ Safety çš„æ±‡åˆ

2/19 çš„ä¸€ç¯‡è®ºæ–‡ï¼ˆ2602.17546ï¼‰æ­ç¤ºäº†ä¸€ä¸ªé‡è¦å‘ç°ï¼Œè™½ç„¶ä¸ç›´æ¥æ˜¯ agentic RLï¼Œä½†å¯¹ agent safety æœ‰ç›´æ¥æ„ä¹‰ï¼š

**Harmful intent åœ¨ pre-generation activation ä¸­çº¿æ€§å¯åˆ†ï¼ˆAUROC > 0.9ï¼‰**

è¿™æ„å‘³ç€ï¼š
1. Agent åœ¨è°ƒç”¨å·¥å…·ã€å†™ codeã€è®¿é—® memory **ä¹‹å‰**ï¼Œå…¶å†…éƒ¨çŠ¶æ€å·²ç»ç¼–ç äº† intent
2. å¯ä»¥ç”¨è½»é‡ probe åœ¨ generation å‘ç”Ÿä¹‹å‰æ£€æµ‹å¹¶æ‹¦æˆª
3. å¯¹äº agentic workflowï¼Œå¯ä»¥åœ¨æ¯ä¸ª action step ä¹‹å‰æ’å…¥ safety gate

è¿™ä¸ç›¾å«é¡¹ç›®çš„æ ¸å¿ƒæ€è·¯å®Œå…¨å¥‘åˆï¼š**ä¸æ˜¯ç­‰ agent è¾“å‡ºæœ‰å®³å†…å®¹å†æ‹¦æˆªï¼Œè€Œæ˜¯åœ¨ forward pass ä¸­æ—©æœŸå‘ç°æ„å›¾ï¼Œé›¶ inference overhead**ã€‚

---

## 2026 å¹´è¿˜æ²¡è§£å†³çš„é—®é¢˜

è¯šå®è¯´ï¼Œå³ä½¿æœ‰ä¸Šé¢è¿™äº›å·¥ä½œï¼Œä»¥ä¸‹é—®é¢˜ä»ç„¶ openï¼š

1. **Subgoal å¦‚ä½•è‡ªåŠ¨ç”Ÿæˆ**ï¼šHiPER æ²¡è¯´ planner å¦‚ä½•ç¡®å®š subgoal è¾¹ç•Œã€‚è¿™æ˜¯ hierarchical RL çš„è€é—®é¢˜ã€‚
2. **Expert rubric çš„æˆæœ¬**ï¼šCorecraft éœ€è¦ä¸“å®¶æ‰‹å†™ 2500+ å®ä½“çš„ rubricã€‚çœŸæ­£é€šç”¨çš„ agentic RL éœ€è¦è‡ªåŠ¨ç”Ÿæˆæˆ–å½’çº³ rubricã€‚
3. **çœŸå®ç¯å¢ƒ vs æ¨¡æ‹Ÿç¯å¢ƒçš„ gap**ï¼šæ‰€æœ‰å·¥ä½œéƒ½åœ¨æ¨¡æ‹Ÿç¯å¢ƒé‡Œè®­ç»ƒï¼ŒçœŸå®ä¼ä¸šç³»ç»Ÿçš„ non-determinism å’Œ side effects ä¼šå¸¦æ¥æ–°çš„æŒ‘æˆ˜ã€‚
4. **é•¿ä»»åŠ¡çš„ overthinking**ï¼šLACONIC è§£å†³äº† reasoning å¤ªé•¿çš„é—®é¢˜ï¼Œä½† agent ä»»åŠ¡çš„"overthinking"ï¼ˆä¸å¿…è¦çš„æ¢ç´¢ã€é‡å¤å·¥å…·è°ƒç”¨ï¼‰æ˜¯å¦ä¸€ä¸ªç»´åº¦â€”â€”æ›´å¤æ‚å› ä¸ºæ¯ä¸€æ­¥éƒ½æœ‰çœŸå®æˆæœ¬ï¼ˆAPI è´¹ç”¨ã€æ—¶é—´ï¼‰ã€‚
5. **Frontier æ¨¡å‹çš„ç“¶é¢ˆ**ï¼šCorecraft å‘ç° Opus 4.6/GPT-5.2 <30% pass rateï¼Œè¿™è¯´æ˜é—®é¢˜ä¸ä»…ä»…æ˜¯è®­ç»ƒæ–¹æ³•â€”â€”frontier æ¨¡å‹åœ¨çœŸå® agent ä»»åŠ¡ä¸Šä»æœ‰æ ¹æœ¬å±€é™ã€‚

---

## 2026 å¹´ Agentic RL å·¥ä½œå…¨æ™¯ï¼ˆæŒ‰æ—¶é—´ï¼‰

| æ—¥æœŸ | è®ºæ–‡ | arXiv | ç»´åº¦ | è¯„åˆ† |
|------|------|-------|------|------|
| 2/10 | EnterpriseGym Corecraft | 2602.16179 | ç¯å¢ƒ | â˜…â˜…â˜…â˜…â˜… |
| 2/13 | CM2 | 2602.12268 | Reward | â˜…â˜…â˜…â˜…â˜† |
| 2/14 | OpenRS | 2602.14069 | Reward | â˜…â˜…â˜…â˜†â˜† |
| 2/15 | HiPER | 2602.16165 | ç®—æ³• | â˜…â˜…â˜…â˜…â˜† |
| 2/15 | KLong | 2602.17547 | ç®—æ³• | â˜…â˜…â˜…â˜…â˜† |
| 2/16 | Kimi-K2.5 PARL | 2602.02276 | ç®—æ³• | â˜…â˜…â˜…â˜…â˜† |
| 2/17 | FlowSteer CWRPO | 2602.01664 | Workflow | â˜…â˜…â˜…â˜†â˜† |
| 2/17 | SquRL | 2602.15564 | Workflow | â˜…â˜…â˜…â˜†â˜† |
| 2/19 | AgentConductor | 2602.17100 | Workflow | â˜…â˜…â˜…â˜…â˜† |
| 2/20 | PA-MoE | 2602.17038 | Workflow | â˜…â˜…â˜…â˜…â˜† |
| 2/20 | Calibrate-Then-Act | 2602.11841 | ç®—æ³• | â˜…â˜…â˜…â˜†â˜† |

---

## å¯¹è€æ¿çš„ç›´æ¥ä»·å€¼

å¦‚æœåœ¨é¢è¯•ä¸­è¢«é—®åˆ°"ä½ å¯¹ agentic RL çš„ç†è§£"ï¼Œè¿™ä¸ªæ¡†æ¶ç»™å‡ºäº†ä¸€ä¸ªç»“æ„åŒ–å›ç­”ï¼š

1. **é—®é¢˜å®šä¹‰**ï¼šä»å¯éªŒè¯ä»»åŠ¡ï¼ˆRLVRï¼‰åˆ°å¼€æ”¾ä»»åŠ¡ï¼ˆAgentic RLï¼‰ï¼Œreward è®¾è®¡å’Œ credit assignment æ˜¯æ ¸å¿ƒéš¾é¢˜
2. **æœ€æ–°è¿›å±•**ï¼šä¸‰å±‚åˆ†è§£ï¼ˆç¯å¢ƒ/reward/ç®—æ³•ï¼‰ï¼Œæ¯å±‚æœ‰ä»£è¡¨æ€§å·¥ä½œ
3. **ç»Ÿä¸€è§†è§’**ï¼šCM2/Corecraft/OpenRS éƒ½æ˜¯ reward reliability é—®é¢˜ï¼›HiPER/PARL éƒ½æ˜¯ credit assignment é—®é¢˜
4. **å¼€æ”¾é—®é¢˜**ï¼šhonest åœ°è¯´æ˜å½“å‰ä¸Šé™åœ¨å“ªé‡Œ

è¿™ç§å›ç­”æ¯”åˆ—ä¸¾è®ºæ–‡åå­—æ·±åº¦é«˜ä¸€ä¸ªæ•°é‡çº§ã€‚

---

## æ ¸å¿ƒæ´å¯Ÿï¼ˆä¸€å¥è¯ï¼‰

**2026 å¹´ Agentic RL çš„æ ¹æœ¬äº‰è®ºä¸æ˜¯"å“ªä¸ªç®—æ³•æ›´å¥½"ï¼Œè€Œæ˜¯"ç“¶é¢ˆåˆ°åº•åœ¨å“ªé‡Œ"ï¼š**

- ç¯å¢ƒæ´¾ï¼šbottleneck æ˜¯ç¯å¢ƒè´¨é‡ï¼ˆCorecraft çš„è¯æ®ï¼‰  
- Reward æ´¾ï¼šbottleneck æ˜¯ reward å¯é æ€§ï¼ˆCM2/OpenRS çš„è¯æ®ï¼‰  
- Workflow æ´¾ï¼šbottleneck æ˜¯ pipeline é™æ€æ€§ï¼ˆAgentConductor/SquRL çš„è¯æ®ï¼‰  
- ç®—æ³•æ´¾ï¼šbottleneck æ˜¯ credit assignmentï¼ˆHiPER/PARL çš„è¯æ®ï¼‰  

**æ­£ç¡®ç­”æ¡ˆå¯èƒ½æ˜¯å…¨éƒ¨**â€”â€”ä½†ä¸åŒä»»åŠ¡å’Œä¸åŒå‘å±•é˜¶æ®µï¼Œå„ç»´åº¦çš„æƒé‡ä¸åŒã€‚

## See Alsoï¼ˆå…¨è·¯å¾„ç´¢å¼•ï¼‰

> æœ¬ç¬”è®°æ­£æ–‡å†…é“¾ä¸º Scholar å†™å…¥çš„ç®€çŸ­è·¯å¾„ï¼›ä»¥ä¸‹ä¸ºé¦†é•¿è¡¥å……çš„å…¨è·¯å¾„å¯¹ç…§ï¼Œä¾¿äº Obsidian å›¾è°±æ£€ç´¢ã€‚

- [[AI/Agent/_MOC|Agent MOC]] â€” Agentic RL åœ¨ Agent çŸ¥è¯†åŸŸçš„ä½ç½®
- [[AI/Agent/Agentic-RL/FlowSteer-CWRPO-Workflow-Orchestration-RL|FlowSteer (CWRPO)]] â€” ç»´åº¦ 4ï¼šOperator çº§ workflow è®¾è®¡ï¼ˆWorkflow/Topology è§£æ³• Aï¼‰
- [[AI/Agent/Agentic-RL/AgentConductor-Topology-Evolution-Multi-Agent-Code|AgentConductor]] â€” ç»´åº¦ 4ï¼šAgent é€šä¿¡ Topology çº§ï¼ˆè§£æ³• Bï¼Œdifficulty-aware densityï¼‰
- [[AI/Agent/Agentic-RL/SquRL-Dynamic-Workflow-Text-to-SQL|SquRL]] â€” ç»´åº¦ 4ï¼šWorkflow é€‰æ‹©çº§ï¼ˆè§£æ³• Cï¼ŒTheorem 3.1 å½¢å¼åŒ–è¯æ˜ï¼‰
- [[AI/LLM/RL/Theory/MARS-Margin-Aware-Reward-Modeling-Self-Refinement|MARS]] â€” reward modeling è‡ªé€‚åº”åˆ†é…ï¼ˆä¸ Reward ç»´åº¦é«˜åº¦äº’è¡¥ï¼‰
- [[AI/Safety/Adaptive-Regularization-Safety-Degradation-Finetuning|Adaptive-Regularization]] â€” Agentic RL Ã— Safety æ±‡åˆç‚¹ï¼špre-generation hidden state å®‰å…¨é—¨æ§
