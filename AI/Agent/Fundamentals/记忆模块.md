---
brief: "Agent 记忆模块——四类记忆架构：Working Memory（context）/ Episodic Memory（历史轨迹）/ Semantic Memory（知识库）/ Procedural Memory（技能）；外部记忆（Vector DB）vs 内部参数记忆的工程权衡。"
title: "记忆模块"
type: concept
domain: ai/agent/fundamentals
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/agent/fundamentals
  - type/concept
---
# 记忆模块

> Agent 的第二大核心能力。没有记忆的 Agent 每次对话都是从零开始，无法积累经验。

阅读：https://mp.weixin.qq.com/s/CogB7fEQmSfbeudMN9F0Uw

## 记忆的分类

### 短期记忆 (Working Memory)

就是 context window。当前对话的所有历史信息都在这里。

```python
# 本质上就是 messages 列表
messages = [
    {"role": "system", "content": "You are..."},
    {"role": "user", "content": "第一轮问题"},
    {"role": "assistant", "content": "第一轮回答"},
    {"role": "user", "content": "第二轮问题"},  # 可以引用第一轮
    # ...
]
# 限制：context window 有上限（128k-1M tokens）
# 超了就得 truncate 或 summarize
```

短期记忆的核心问题：**context window 管理**。超长对话怎么办？

```python
# 策略 1: 滑动窗口 — 简单但丢信息
messages = messages[-MAX_TURNS:]

# 策略 2: 摘要压缩 — 保留语义但丢细节
if len(messages) > THRESHOLD:
    summary = llm.summarize(messages[:HALF])
    messages = [summary_msg] + messages[HALF:]

# 策略 3: 重要性评分 + 选择性保留
# 对每条消息打分，保留重要的，丢弃闲聊
```

### 长期记忆 (Long-term Memory)

跨会话持久化的信息。需要外部存储。

```python
# 典型实现：向量数据库
import chromadb

collection = chromadb.create_collection("user_memories")

# 写入记忆
collection.add(
    documents=["用户喜欢简洁的回答风格"],
    metadatas=[{"type": "preference", "timestamp": "2026-02-13"}],
    ids=["mem_001"]
)

# 检索相关记忆
results = collection.query(
    query_texts=["用户偏好"],
    n_results=5
)
```

### 情景记忆 (Episodic Memory)

记住具体事件和经历。类比人类的 "我记得上周二我们讨论过 X"。

```python
# 结构化的事件记录
class Episode:
    timestamp: str
    context: str      # 什么场景
    action: str       # 做了什么
    outcome: str      # 结果如何
    reflection: str   # 经验总结

# 用于 few-shot：遇到类似场景时检索过去的经验
```

### 语义记忆 (Semantic Memory)

抽象的知识和事实。类比人类的常识。

```python
# 知识图谱 或 结构化数据库
knowledge = {
    "user_name": "Peter",
    "tech_stack": ["Python", "Spark", "LLM"],
    "preferences": {
        "language": "中文",
        "style": "简洁技术风格"
    }
}
```

## 记忆的写入时机

这是工程上最重要的设计决策：什么时候写入记忆？

```
方案 1: 每轮对话后自动提取 — 噪音大，但不会遗漏
方案 2: 模型主动决定 — 依赖模型判断力
方案 3: 用户明确指示 — 最可靠但依赖用户配合
方案 4: 定期 reflection — 类似日记，批量提炼
```

我的实践：**方案 4 最靠谱**。实时写入噪音太多，而且每轮都调 embedding API 太贵。不如积累一段时间后统一做一次 reflection，提炼出关键信息。

## 记忆检索的质量

检索召回的质量直接决定记忆是否有用：

```python
# 朴素的 embedding 检索往往不够好
# 问题："上次那个 bug 怎么修的"
# 如果只用语义相似度，可能召回一堆 "bug" 相关但不相关的内容

# 改进方案：
# 1. Hybrid search: 向量 + 关键词（BM25）
# 2. 元数据过滤：时间范围、标签
# 3. Reranking：召回后用 cross-encoder 重排

def retrieve_memories(query, user_id, top_k=5):
    # 第一步：粗召回
    candidates = vector_search(query, top_k=20)
    candidates += keyword_search(query, top_k=10)
    
    # 第二步：过滤
    candidates = [m for m in candidates if m.user_id == user_id]
    
    # 第三步：精排
    ranked = reranker.rank(query, candidates)
    return ranked[:top_k]
```

## 用户偏好的特殊处理

用户偏好应该由 chat 层面来解决，不需要进入 Agent 的记忆系统。原因：

1. 偏好是结构化的（语言、风格、格式），用 system prompt 表达更直接
2. 偏好变化频率低，不需要每次检索
3. 偏好冲突时需要明确的优先级，向量检索处理不好这种逻辑

```python
# 偏好放 system prompt
system_prompt = f"""
用户偏好：
- 语言：{user.preferred_language}
- 回答风格：{user.style}
- 技术水平：{user.expertise_level}
"""
# 而不是混在向量记忆里
```

## 相关

- [[AI/Agent/Fundamentals/Tool Use|Tool Use]] — Agent 的另一核心能力
- [[AI/Agent/Agentic-RL/Agentic RL Training|Agentic RL Training]] — 用 RL 优化 Agent 行为
- [[AI/LLM/Application/Prompt-Engineering-概述|Prompt Engineering]] — system prompt 设计
- [[AI/Agent/Agentic-RL/AWM-Agent-World-Model-Synthetic-Environments|AWM]] — 世界模型作为Agent记忆的扩展：DB-as-state是"外置记忆"的工程实现
- [[AI/LLM/Evaluation/PERSIST-LLM-Personality-Stability-Benchmark|PERSIST]] — 记忆的一致性问题：记忆模块存储的信息需要与Agent人格行为保持一致（PERSIST测量正是这种跨时间稳定性）
