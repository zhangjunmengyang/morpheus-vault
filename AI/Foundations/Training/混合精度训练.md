---
title: "混合精度训练详解"
date: 2026-02-14
tags: [training, mixed-precision, fp16, bf16, interview]
type: note
---

# 混合精度训练详解

## 1. 为什么需要混合精度

### 1.1 显存节省

以一个 7B 参数模型为例：

| 精度 | 每参数字节 | 模型权重 | 优化器状态（Adam） | 梯度 | 总计 |
|------|-----------|---------|-------------------|------|------|
| FP32 | 4B | 28 GB | 56 GB | 28 GB | ~112 GB |
| 混合精度（FP16/BF16 + FP32 master） | — | 14 GB (FP16) + 28 GB (FP32 master) | 56 GB | 14 GB | ~112 GB* |
| 纯 FP16（不推荐） | 2B | 14 GB | 28 GB | 14 GB | ~56 GB |

> *混合精度的显存节省主要体现在 **activation memory**（中间激活值），而非参数本身。激活值用 FP16 存储，可节省近一半激活显存。配合 activation checkpointing，效果更显著。

### 1.2 训练加速

- **Tensor Core 加速**：NVIDIA GPU（V100+）的 Tensor Core 对 FP16/BF16 矩阵乘法有硬件加速
  - V100：FP16 比 FP32 快 ~8x（125 TFLOPS vs 15.7 TFLOPS）
  - A100：BF16/FP16 达 312 TFLOPS，FP32 为 19.5 TFLOPS
  - H100：FP8 达 ~1979 TFLOPS，BF16 达 ~989 TFLOPS
- **通信带宽减半**：分布式训练中梯度同步的数据量减半
- **内存带宽提升**：数据搬运量减少，缓解 memory-bound 瓶颈

**实际加速效果**：混合精度训练通常带来 **1.5x - 2x** 的端到端训练速度提升。

---

## 2. 数值格式对比

### 2.1 位宽结构

```
FP32:  1 sign + 8 exponent + 23 mantissa  = 32 bits
FP16:  1 sign + 5 exponent + 10 mantissa  = 16 bits
BF16:  1 sign + 8 exponent +  7 mantissa  = 16 bits
FP8 (E4M3): 1 sign + 4 exponent + 3 mantissa = 8 bits
FP8 (E5M2): 1 sign + 5 exponent + 2 mantissa = 8 bits
```

### 2.2 关键指标对比

| 格式 | 位宽 | 动态范围 | 精度（尾数位） | 最大值 | 最小正规数 |
|------|------|---------|---------------|--------|-----------|
| FP32 | 32 | ±3.4×10³⁸ | 23 bits (~7 位十进制) | 3.4e38 | 1.2e-38 |
| FP16 | 16 | ±6.5×10⁴ | 10 bits (~3 位十进制) | 65504 | 6.1e-5 |
| BF16 | 16 | ±3.4×10³⁸ | 7 bits (~2 位十进制) | 3.4e38 | 1.2e-38 |
| FP8 E4M3 | 8 | ±448 | 3 bits | 448 | 0.015625 |
| FP8 E5M2 | 8 | ±57344 | 2 bits | 57344 | 6.1e-5 |

**核心权衡：**
- FP16：精度高但动态范围小 → 容易溢出（overflow/underflow）
- BF16：动态范围大但精度低 → 与 FP32 直接截断兼容
- FP8 E4M3：精度优先 → 适合前向传播
- FP8 E5M2：范围优先 → 适合反向传播（梯度）

---

## 3. 混合精度训练原理

### 3.1 经典混合精度（Micikevicius et al., 2018）

```
┌─────────────────────────────────────────────┐
│            FP32 Master Weights               │
│           (权威权重副本)                      │
└──────────┬────────────────────┬──────────────┘
           │ cast to FP16       │ update with FP32
           ▼                    │
┌──────────────────┐           │
│  FP16 Forward    │           │
│  (前向计算)       │           │
└────────┬─────────┘           │
         │                     │
         ▼                     │
┌──────────────────┐           │
│  Loss Scaling    │           │
│  (放大 loss)      │           │
└────────┬─────────┘           │
         │                     │
         ▼                     │
┌──────────────────┐           │
│  FP16 Backward   │           │
│  (反向计算)       │           │
└────────┬─────────┘           │
         │                     │
         ▼                     │
┌──────────────────┐           │
│  Unscale Grads   │───────────┘
│  + FP32 Update   │
│  (还原梯度+更新)  │
└──────────────────┘
```

**三个关键技术：**

1. **FP32 Master Weights**：保留一份 FP32 的权重副本，所有权重更新在 FP32 上进行。原因：FP16 下 `weight + small_gradient` 可能因精度不足被直接丢弃（当 gradient / weight < 2⁻¹⁰ 时）。

2. **FP16 计算**：前向和反向传播都用 FP16，利用 Tensor Core 加速。部分操作（如 softmax、layer norm、loss 计算）仍用 FP32 以保证数值稳定性。

3. **Loss Scaling**：将 loss 乘以一个 scale factor（如 1024 或动态调整），使反向传播中的梯度值被放大，防止 FP16 下的小梯度 underflow 为 0。更新权重前再除回来。

### 3.2 需要保持 FP32 的操作

不是所有操作都能安全地用 FP16：

| 操作 | 原因 | 处理 |
|------|------|------|
| Softmax | 指数运算易溢出，需要数值稳定 | FP32 计算 |
| LayerNorm / RMSNorm | 方差计算对精度敏感 | FP32 accumulation |
| Loss 计算 | cross-entropy 中的 log 对小值敏感 | FP32 计算 |
| 权重更新 | 小梯度会被 FP16 吞掉 | FP32 master weights |
| 大规模 reduction | 累加大量小值需要高精度 | FP32 accumulator |

---

## 4. BF16 vs FP16

### 4.1 为什么现代 LLM 训练优先选择 BF16

| 维度 | FP16 | BF16 |
|------|------|------|
| 动态范围 | ±6.5×10⁴ | ±3.4×10³⁸（与 FP32 相同） |
| 精度 | 10 位尾数 | 7 位尾数 |
| 需要 Loss Scaling | ✅ 必须 | ❌ 通常不需要 |
| 溢出风险 | 高（梯度 > 65504 就溢出） | 极低 |
| 与 FP32 转换 | 需要特殊处理 | 直接截断尾数即可 |
| 硬件支持 | V100+ | A100+（V100 不支持） |

**BF16 的核心优势——动态范围：**

在 LLM 训练中，梯度值的分布跨越很大范围：
- 早期层梯度可能很大（>100）
- 深层梯度可能极小（<1e-7）
- Loss spike 时梯度可能突发增大

FP16 最大只能表示 65504，超出就变成 `inf`，导致训练崩溃。BF16 的动态范围和 FP32 一样（~3.4e38），几乎不会溢出。

**BF16 的劣势——精度：**

7 位尾数意味着约 2 位十进制精度。对大多数深度学习计算这足够了，但在某些需要精确数值的场景（如 eval metrics 计算）可能不够。

### 4.2 实际选择策略

```
有 A100/H100？ ──yes──→ 用 BF16（省心、稳定）
      │
      no（V100）
      │
      ▼
用 FP16 + Dynamic Loss Scaling
（需要额外调参，可能遇到 loss spike）
```

---

## 5. Loss Scaling

### 5.1 为什么 FP16 需要 Loss Scaling

FP16 的最小正规数是 ~6.1e-5。在反向传播中，很多梯度值小于这个阈值，会被直接 underflow 为 0：

```
FP16 可表示范围: [6.1e-5, 65504]（正数部分）

梯度值分布:
  ████████████████████           ← 很多梯度在 1e-7 ~ 1e-4 范围
           ████████████████████  ← FP16 能表示的范围
  ^^^^^^^^^                      ← 这部分会 underflow 为 0！
```

Loss Scaling 将 loss 放大 → 所有梯度等比放大 → 小梯度被"推入"FP16 可表示范围 → 更新前再缩放回来。

### 5.2 静态 vs 动态 Loss Scaling

**静态 Loss Scaling：**
- 固定 scale factor（如 128、1024、8192）
- 简单但不灵活
- 过小：无法拯救所有 underflow 的梯度
- 过大：可能导致梯度 overflow（变成 inf）

**动态 Loss Scaling（推荐）：**

```python
# PyTorch GradScaler 的核心逻辑
scale = initial_scale  # 如 2^16 = 65536
growth_interval = 2000  # 每 N 步无 inf 就翻倍
consecutive_ok = 0

for step in training:
    scaled_loss = loss * scale
    scaled_loss.backward()
    
    if has_inf_or_nan(gradients):
        scale /= 2          # 发现 overflow，缩小 scale
        skip_this_step()     # 跳过本次更新
        consecutive_ok = 0
    else:
        optimizer.step()     # 正常更新（梯度先 / scale）
        consecutive_ok += 1
        if consecutive_ok >= growth_interval:
            scale *= 2       # 一段时间没 overflow，尝试增大 scale
            consecutive_ok = 0
```

### 5.3 为什么 BF16 不需要 Loss Scaling

BF16 的动态范围和 FP32 一样大（8 位指数），可以表示到 ~1.2e-38 的极小值。正常训练中的梯度值几乎不会 underflow。因此：
- 不需要 loss scaling 机制
- 不存在因 overflow 而跳过训练步的问题
- 训练更稳定，需要调的超参更少

---

## 6. FP8 训练（H100+）

### 6.1 两种 FP8 格式

NVIDIA H100（Hopper 架构）引入原生 FP8 支持，有两种格式：

| 格式 | 结构 | 最大值 | 精度 | 适用场景 |
|------|------|--------|------|---------|
| **E4M3** | 1+4+3 | 448 | 较高（3 位尾数） | **前向传播**（activation 和 weight） |
| **E5M2** | 1+5+2 | 57344 | 较低（2 位尾数） | **反向传播**（梯度需要更大动态范围） |

**为什么要两种格式？**
- 前向传播中的 activation 和 weight 值范围相对集中，需要更高精度 → E4M3
- 反向传播中的梯度值范围波动大，需要更大动态范围 → E5M2

### 6.2 Per-Tensor Scaling

FP8 的动态范围极小，必须配合 per-tensor scaling：

```python
# 对每个 tensor 独立计算 scale factor
def compute_scale(tensor, fp8_max):
    amax = tensor.abs().max()
    scale = fp8_max / amax  # 将 tensor 最大值映射到 FP8 最大值
    return scale

# 前向传播
x_scale = compute_scale(x, E4M3_MAX)  # 448
x_fp8 = cast_to_fp8(x * x_scale)

w_scale = compute_scale(w, E4M3_MAX)
w_fp8 = cast_to_fp8(w * w_scale)

# 矩阵乘法在 FP8 完成，结果还原
y = matmul_fp8(x_fp8, w_fp8) / (x_scale * w_scale)
```

**Delayed Scaling**：使用前一步的 amax 统计信息来计算当前步的 scale，避免额外的 amax 计算开销。

### 6.3 NVIDIA TransformerEngine

TransformerEngine 是 NVIDIA 提供的库，封装了 FP8 训练的复杂性：

```python
import transformer_engine.pytorch as te

# 替换标准 Linear 层
model.layer = te.Linear(hidden_size, ffn_size)

# 使用 fp8_autocast 上下文
with te.fp8_autocast(enabled=True):
    output = model(input)
    loss = criterion(output, target)
    loss.backward()
```

**核心功能：**
- 自动管理 FP8 cast 和 scaling
- 支持 delayed scaling 策略
- 内置 Transformer 层的 FP8 优化实现
- 与 Megatron-LM、NeMo 等框架集成

**FP8 训练的实际效果：**
- 相比 BF16，H100 上 FP8 训练可额外加速 ~30-50%
- 精度损失通常在 0.1-0.3% 以内（benchmark 评测）
- 目前主要用于大规模预训练，SFT/RLHF 中的应用仍在探索

---

## 7. 与分布式训练的关系

### 7.1 DeepSpeed ZeRO 中的精度策略

**ZeRO Stage 1/2/3 的精度处理：**

```
ZeRO-2 典型配置:
┌─────────────────────────────────────────┐
│  FP32 Master Weights (分片存储)          │
│  → 每个 GPU 只持有 1/N 的 FP32 权重      │
├─────────────────────────────────────────┤
│  FP16/BF16 Working Copy (全量)           │
│  → 前向/反向计算使用                      │
├─────────────────────────────────────────┤
│  FP16/BF16 Gradients (分片后聚合)        │
│  → reduce-scatter 通信用 FP16/BF16       │
├─────────────────────────────────────────┤
│  FP32 Optimizer States (分片存储)        │
│  → Adam 的 m/v 始终 FP32                │
└─────────────────────────────────────────┘
```

**DeepSpeed 配置示例：**

```json
{
  "bf16": { "enabled": true },
  "zero_optimization": {
    "stage": 2,
    "allgather_partitions": true,
    "reduce_scatter": true,
    "overlap_comm": true
  }
}
```

**通信精度**：默认使用 FP16/BF16 通信，节省带宽。可配置 `communication_data_type` 指定通信精度。

### 7.2 FSDP 中的精度策略

PyTorch FSDP（Fully Sharded Data Parallel）提供三种精度控制：

```python
from torch.distributed.fsdp import MixedPrecision

mp_policy = MixedPrecision(
    param_dtype=torch.bfloat16,      # 前向/反向计算精度
    reduce_dtype=torch.bfloat16,     # 梯度 all-reduce 精度
    buffer_dtype=torch.bfloat16,     # buffer 精度（如 BatchNorm 统计量）
)

model = FSDP(
    model,
    mixed_precision=mp_policy,
    # ...
)
```

| 参数 | 控制内容 | 典型设置 |
|------|---------|---------|
| `param_dtype` | 参与计算的参数精度 | BF16 |
| `reduce_dtype` | All-reduce 通信精度 | BF16（或 FP32 更稳定） |
| `buffer_dtype` | 模型 buffer 精度 | BF16 |

**FSDP 的权重精度流程：**
- 分片存储：FP32（或 BF16，取决于 `param_dtype`）
- all-gather 聚合：cast 到 `param_dtype`
- 前向/反向计算：`param_dtype`
- 梯度 reduce-scatter：`reduce_dtype`
- 优化器更新：FP32（始终）

### 7.3 通信压缩

分布式训练中，梯度通信是主要瓶颈之一：

| 策略 | 方法 | 节省 |
|------|------|------|
| FP16/BF16 通信 | 梯度 cast 为半精度再通信 | 50% 带宽 |
| FP8 通信 | 梯度量化为 FP8（实验性） | 75% 带宽 |
| 梯度压缩 | Top-K、Random-K | 90%+（但可能影响收敛） |
| PowerSGD | 低秩近似梯度 | 自适应 |

---

## 8. 面试常见问题

### Q1：混合精度训练为什么需要保留一份 FP32 的 master weights？

**要点：**
- 核心问题是**权重更新的精度不足**
- 当 `learning_rate × gradient` 远小于当前权重值时，FP16 的 10 位尾数精度无法表示这个微小变化
- 具体来说，FP16 下 `1.0 + 0.0001 = 1.0`（因为 0.0001 < 2⁻¹⁰ ≈ 0.001）
- FP32 有 23 位尾数精度，可以准确累积这些微小更新
- 方案：计算用 FP16（利用 Tensor Core 加速），更新用 FP32（保证精度）

### Q2：BF16 和 FP16 的核心区别是什么？为什么大模型训练更倾向 BF16？

**要点：**
- BF16 和 FP16 都是 16 位，但位宽分配不同
  - FP16：5 位指数 + 10 位尾数 → 精度高，范围小（max 65504）
  - BF16：8 位指数 + 7 位尾数 → 精度低，范围大（max 3.4e38）
- LLM 训练中更需要**大动态范围**而非高精度：
  - 梯度值跨越很大范围，小范围容易 overflow/underflow
  - 模型足够大时对个别值的精度不那么敏感
- BF16 不需要 loss scaling → 训练更简单稳定
- BF16 是 FP32 直接截断尾数得到的 → 转换无开销
- 劣势：V100 不支持 BF16，A100+ 才有硬件支持

### Q3：Loss Scaling 的原理是什么？动态 Loss Scaling 如何工作？

**要点：**
- **问题**：FP16 的最小正规数 ~6e-5，很多梯度值比这小，会 underflow 为 0
- **方案**：将 loss 乘以一个大数（scale factor）→ 反向传播中所有梯度等比放大 → 更新前除以 scale 还原
- **动态策略**：
  - 初始 scale 较大（如 2¹⁶）
  - 如果检测到梯度中有 inf/nan → 缩小 scale（÷2），跳过本步更新
  - 如果连续 N 步（如 2000 步）没有 inf → 增大 scale（×2）
- **BF16 为什么不需要**：8 位指数提供与 FP32 相同的动态范围，正常训练梯度不会 underflow

### Q4：FP8 训练相比 BF16 有什么优势和挑战？

**要点：**
- **优势**：
  - H100 上 FP8 Tensor Core 吞吐量比 BF16 高 ~2x
  - 显存节省约 50%（activation 和 weight 存储）
  - 通信量减少（如果用 FP8 通信）
- **挑战**：
  - 动态范围极小（E4M3 max 448），必须做 per-tensor scaling
  - 需要两种格式（E4M3 用于前向，E5M2 用于反向）
  - Scaling 策略（delayed scaling）引入额外复杂性
  - 对某些任务可能有精度损失，需要仔细验证
- **现状**：大规模预训练中已被验证可行（Meta LLaMA 3 训练使用了 FP8），但还不如 BF16 成熟

### Q5：在 DeepSpeed ZeRO 或 FSDP 中，混合精度是如何与分片策略配合的？

**要点：**
- **存储**：FP32 master weights 和 optimizer states 被分片到各 GPU → 每 GPU 只存 1/N
- **计算**：需要完整权重时，通过 all-gather 收集并 cast 到 BF16/FP16
- **通信**：梯度 reduce-scatter 用 BF16/FP16 → 通信量是 FP32 的一半
- **更新**：每个 GPU 只更新自己持有的 FP32 分片
- **关键配置**：
  - FSDP 的 `MixedPrecision` 可分别控制 param/reduce/buffer 精度
  - DeepSpeed 可通过 `bf16.enabled` + `zero_optimization` 联合配置
- 混合精度和分片是正交的优化，组合使用效果最好

### Q6：混合精度训练中常见的数值问题有哪些？如何排查？

**要点：**
- **常见问题**：
  - **Loss 变成 NaN/Inf**：通常是 FP16 overflow → 检查是否启用了 loss scaling
  - **训练后期 loss spike**：梯度突增超出 FP16 范围 → 考虑切换到 BF16
  - **精度下降**：某些敏感操作用了低精度 → 检查 softmax/layernorm 是否用 FP32
  - **Loss scaling 频繁跳步**：scale 过大或梯度不稳 → 降低初始 scale 或排查模型问题
- **排查方法**：
  - 监控 loss scale 值的变化趋势
  - 检查梯度的 min/max/mean 统计信息
  - 对比 FP32 baseline，定位精度损失出现的位置
  - 用 `torch.autograd.detect_anomaly()` 定位产生 NaN 的操作
- **经验法则**：能用 BF16 就用 BF16，省去大量数值调试工作
