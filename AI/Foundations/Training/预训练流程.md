---
brief: "预训练流程——LLM 从数据→分词→打包→训练→评估的完整流程；数据 packing/目标函数设计/checkpoint 策略；分布式训练（DP/TP/PP/SP）的基础概念；理解 GPT-4/Llama 等大模型训练工程的入门参考。"
title: "LLM 预训练流程"
date: 2026-02-14
tags: [training, pretraining, data, interview]
type: note
---

# LLM 预训练流程

## 1. 预训练全流程概述

```
原始数据 → 数据清洗 → Tokenization → 模型初始化 → 预训练 → Checkpoint → 评估
```

### 1.1 端到端流程

| 阶段 | 关键动作 | 产出 |
|------|---------|------|
| **数据收集** | 爬取 Common Crawl、书籍、代码、Wiki 等 | 原始语料（数十 TB） |
| **数据清洗** | 去重、质量过滤、毒性过滤、PII 去除 | 清洗后语料（数 TB） |
| **Tokenization** | 训练 BPE/SentencePiece tokenizer，将文本转 token 序列 | tokenizer 模型 + token IDs |
| **模型初始化** | 确定架构（层数/隐藏维度/头数），随机初始化或从已有 checkpoint 加载 | 初始权重 |
| **预训练** | Next-token prediction（Causal LM）或 MLM，分布式训练 | 训练中的 checkpoint |
| **Checkpoint 保存** | 定期保存模型权重 + optimizer state + scheduler state | 完整 checkpoint |
| **评估** | Perplexity、下游 benchmark（MMLU/HellaSwag/ARC 等） | 评估报告 |

### 1.2 Tokenization 要点

- **BPE（Byte Pair Encoding）**：GPT 系列使用，从字符级别迭代合并高频 pair
- **SentencePiece**：LLaMA/Gemma 使用，支持 Unigram 和 BPE，直接在原始文本上操作（不依赖预分词）
- **Tiktoken**：OpenAI 的快速 BPE 实现，GPT-4 使用 `cl100k_base`（100k vocab）
- vocab size 通常 32k–128k，越大 → 序列越短 → 训练越快，但 embedding 参数更多
- 多语言场景需确保 tokenizer 对非英语语言的 **fertility**（每词平均 token 数）合理

### 1.3 模型初始化

- 权重初始化通常用 **truncated normal**，std 与层数相关（如 $\sigma = 0.02$ 或 $\sigma = 1/\sqrt{d_{model}}$）
- 残差连接的输出层可用 $1/\sqrt{2N}$ 缩放（N = 层数），参考 GPT-2 做法
- Embedding 层和 LM head 是否 **tie weights** 是设计选择（LLaMA 不 tie，GPT-2 tie）
- μP（Maximal Update Parameterization）可以让超参从小模型直接迁移到大模型

---

## 2. 数据清洗 Pipeline

```
Raw Crawl → URL 过滤 → 文本提取 → 语言识别 → 质量过滤 → 去重 → 毒性过滤 → PII 去除 → 最终语料
```

### 2.1 去重（Deduplication）

**为什么要去重？** 重复数据导致模型对特定文本过拟合，降低泛化能力，且训练 FLOPs 浪费。

| 方法 | 原理 | 适用场景 |
|------|------|---------|
| **Exact dedup** | 对整个文档计算 SHA-256 hash，完全相同则去除 | 快速第一轮去重 |
| **MinHash + LSH** | 将文档转为 n-gram 集合 → MinHash 签名 → LSH 分桶 → 桶内比较 Jaccard 相似度 | 近似去重，工业标准 |
| **Suffix Array** | 找到文档间共享的长子串并移除 | 段落级去重（ExactSubstr） |
| **Bloom Filter** | 对 n-gram 做 membership 检测，去除高重复 n-gram 的文档 | 内存高效的行级去重 |

**MinHash 细节：**
- 将文档分解为 n-gram（通常 5-gram 或 13-gram）
- 用 k 个 hash 函数分别对所有 n-gram 求最小值 → 得到长度为 k 的签名
- 两文档签名一致的比例 ≈ Jaccard 相似度
- LSH 将签名分成 b 个 band，每个 band r 行，band 完全一致的文档对成为候选
- 阈值 ≈ $(1/b)^{1/r}$，可调 b 和 r 控制召回率 vs 精度

### 2.2 质量过滤

- **规则过滤**：移除过短/过长文档、非自然语言（代码注释、广告模板）、特殊字符比例过高
- **语言模型打分**：用小型 KenLM/fastText 模型对文档打分，保留高质量文本
  - 如 CCNet pipeline：用 Wiki 训练 KenLM，对 Common Crawl 文档按 perplexity 分桶
- **分类器过滤**：训练 binary classifier（如 fastText），正例=高质量来源（Wiki/书籍），负例=随机网页
- **Heuristic 规则**（参考 Gopher/C4）：
  - 停用词比例过低 → 可能是列表/代码 → 过滤
  - "lorem ipsum" 等模板文本 → 过滤
  - 连续重复段落 → 过滤

### 2.3 毒性过滤

- 用 **Perspective API** 或 **toxicity classifier**（如 Jigsaw 模型）对文档打分
- 阈值过滤：toxicity > 0.5 的文档直接移除或降采样
- 关键词黑名单：补充规则过滤明显有害内容
- 注意：过度过滤会移除对少数群体的正常讨论（bias-safety tradeoff）

### 2.4 PII 去除

- **正则表达式**：匹配邮箱、电话号码、身份证号、信用卡号等
- **NER 模型**：识别人名、地址等实体，替换为占位符 `[PERSON]` `[ADDRESS]`
- **IP 地址/URL**：根据策略保留或替换
- 实践中常用 **presidio**（Microsoft 开源 PII 检测工具）

---

## 3. 常见预训练数据集

| 数据集 | 规模 | 来源 | 特点 |
|--------|------|------|------|
| **C4** | ~800GB | Common Crawl (2019-04) | 较早期标准，过滤规则简单，heuristic 质量过滤 |
| **The Pile** | ~825GB (300B tokens) | 22 个子集混合 | Eleuther AI 开源，含 PubMed/ArXiv/GitHub/StackExchange 等多样来源 |
| **RedPajama v1** | ~1.2T tokens | 复现 LLaMA 训练数据配方 | Together AI 开源，7 个子集（CC/C4/GitHub/Wiki/Books/ArXiv/StackExchange） |
| **RedPajama v2** | ~30T tokens（raw） | 84 个 Common Crawl dump | 附带质量信号，用户自选过滤策略 |
| **FineWeb** | ~15T tokens | Common Crawl（95 个 dump） | HuggingFace 出品，精心清洗，DCLM 基础，质量极高 |
| **FineWeb-Edu** | ~1.3T tokens | FineWeb 子集 | 用教育质量分类器筛选，适合知识密集型训练 |
| **CulturaX** | ~6.3T tokens | mC4 + OSCAR 清洗合并 | 覆盖 167 种语言，多语言预训练首选 |
| **DCLM** | ~4T tokens | Common Crawl | Apple/UW 出品，model-based 质量过滤，开源 baseline pipeline |
| **Dolma** | ~3T tokens | 多源混合 | AI2 为 OLMo 准备的数据集，完全开源可复现 |
| **StarCoder Data** | ~250B tokens | GitHub（许可证过滤） | The Stack v2，80+ 编程语言，代码预训练专用 |

### 数据混合比例（Data Mix）

经典配比参考（LLaMA 风格）：

| 来源 | 比例 | 说明 |
|------|------|------|
| Common Crawl（清洗后） | 67% | 主体通用知识 |
| C4 | 15% | 补充通用知识 |
| GitHub | 4.5% | 代码能力 |
| Wikipedia | 4.5% | 事实知识 |
| Books | 4.5% | 长文本理解 |
| ArXiv | 2.5% | 科学推理 |
| StackExchange | 2% | QA 格式数据 |

- 数据混合比例对模型能力影响极大，是核心超参之一
- 代码比例提升可增强推理能力（Codex 效应）
- 多语言模型需要平衡语言比例，低资源语言可 **上采样**（temperature sampling：$p_i' \propto p_i^{1/T}$，T<1 提升低资源）

---

## 4. 训练配置

### 4.1 Optimizer

| 优化器 | 公式要点 | 使用场景 |
|--------|---------|---------|
| **AdamW** | Adam + 解耦权重衰减；$\theta_{t+1} = \theta_t - \eta(\hat{m}_t / (\sqrt{\hat{v}_t} + \epsilon) + \lambda\theta_t)$ | 工业标准，绝大多数 LLM 使用 |
| **Adafactor** | 分解二阶矩为行列因子，节省显存 | T5/PaLM 使用，省显存但可能不稳定 |
| **LAMB/LARS** | 按层自适应学习率，适合超大 batch | 大 batch size 场景 |
| **Lion** | 只用 sign 更新，显存更低 | Google Brain 提出，实验性 |
| **8-bit Adam** | 用 8-bit 量化 optimizer state | 用 bitsandbytes 实现，省显存 |

**AdamW 典型超参：**
- $\beta_1 = 0.9, \beta_2 = 0.95$（LLaMA/GPT-3 的选择）
- $\epsilon = 10^{-8}$
- weight decay $\lambda = 0.1$
- 注意：**embedding、LayerNorm、bias 通常不加 weight decay**

### 4.2 Learning Rate Schedule

```
LR
 ^
 |     /‾‾‾‾‾‾‾‾‾‾‾\
 |    /               \
 |   /                  \
 |  /                    \___________
 | /                                  
 +──────────────────────────────────→ step
   warmup    cosine decay      min_lr
```

- **Warmup**：前 0.1%–2% 的 step 线性从 0 增到 peak LR
  - 典型 warmup steps：2000
  - 作用：避免初始阶段 loss spike，让 Adam 的二阶矩估计稳定
- **Cosine Decay**：从 peak LR 余弦衰减到 min LR
  - $\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{t}{T}\pi))$
  - min LR 通常为 peak LR 的 1/10
- **Peak LR** 选择：与模型大小负相关
  - 7B: ~3e-4, 13B: ~3e-4, 65B: ~1.5e-4, 175B: ~6e-5

### 4.3 Batch Size 与 Gradient Accumulation

- **Effective batch size** = micro_batch_size × gradient_accumulation_steps × num_GPUs
- 大模型通常用 **大 batch**（数百万 tokens/step）
  - LLaMA 65B: 4M tokens/batch
  - GPT-3 175B: 3.2M tokens/batch
- **Batch size warmup**：训练初期用小 batch，逐步增大到目标值
  - 好处：初期小 batch 提供更多 update steps，优化更精细
- **Gradient accumulation**：在显存不足时，用多次 forward-backward 累积梯度再更新
  - 等价于更大的 batch size，但增加训练时间

### 4.4 序列长度与 Packing

- **Sequence length**：通常 2048（GPT-3）或 4096（LLaMA），现代模型 8192+
- **Document packing**：将多个短文档拼接填满一个序列，用 `<eos>` 分隔
  - 提升 GPU 利用率，避免 padding 浪费
  - 需要 attention mask 防止跨文档 attend
- 部分实现用 **intra-document attention mask**，或直接让模型学会忽略跨文档上下文

---

## 5. 训练稳定性

### 5.1 梯度裁剪（Gradient Clipping）

```python
# 全局范数裁剪（最常用）
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
```

- **Global norm clipping**：计算所有参数梯度的 L2 范数，若超过阈值则等比缩放
- 典型阈值：1.0（几乎所有大模型都用这个值）
- 作用：防止梯度爆炸，尤其是遇到异常 batch 时

### 5.2 Warmup 的作用

1. **Adam 估计不准**：训练初期 $\hat{v}_t$ 是少量样本的估计，偏差大 → 大 LR 会导致不稳定
2. **Loss landscape 陡峭**：初始随机权重附近 loss 变化剧烈，小 LR 更安全
3. **LayerNorm 未稳定**：初始阶段 LayerNorm 的统计量波动大

### 5.3 Loss Spike 处理

**Loss spike**：训练中 loss 突然飙升，常见原因和处理：

| 原因 | 症状 | 处理 |
|------|------|------|
| **异常数据 batch** | 单次 spike 后自行恢复 | 通常可忽略，或清洗对应数据 |
| **学习率过高** | 频繁 spike | 降低 peak LR |
| **数值不稳定** | spike 后不恢复或 NaN | 回滚到之前的 checkpoint 继续训练 |
| **数据重复** | 周期性 spike（每 epoch 同一位置） | 改进去重 |

**PaLM 论文的做法**：遇到 loss spike → 从 spike 前 ~100 steps 的 checkpoint 回滚 → 跳过导致 spike 的数据 → 继续训练。

### 5.4 NaN Debugging

NaN 出现的常见原因：

1. **FP16 溢出**：梯度值超出 FP16 范围（65504）
   - 解决：使用 **BF16**（范围与 FP32 相同）或 **loss scaling**（AMP）
2. **除零**：LayerNorm 中方差为 0，或 softmax 中 exp 溢出
   - 解决：增大 $\epsilon$，使用数值稳定的实现
3. **Learning rate 过高**：参数更新过大导致数值发散
   - 解决：降低 LR，增加 warmup
4. **Inf × 0**：混合精度中的计算顺序问题
   - 解决：用 `torch.nan_to_num` 或检查中间值

**调试技巧：**
```python
# 监控梯度范数
for name, param in model.named_parameters():
    if param.grad is not None:
        grad_norm = param.grad.norm()
        if torch.isnan(grad_norm) or torch.isinf(grad_norm):
            print(f"NaN/Inf gradient in {name}")
```

### 5.5 其他稳定性技巧

- **Embedding 层缩放**：将 embedding 输出乘以 $\sqrt{d_{model}}$（参考 Transformer 原始论文）
- **Pre-LayerNorm vs Post-LayerNorm**：Pre-LN 更稳定，几乎所有现代 LLM 使用 Pre-LN
- **QK-Norm**：对 attention 的 Q 和 K 做 LayerNorm，防止 attention logits 过大（Gemma 2 使用）
- **z-loss**：对 softmax logits 施加正则化，防止 logits 漂移过大（PaLM 使用）
  - $L_z = \alpha \cdot \log^2(\sum_i e^{z_i})$
- **Gradient checkpointing**：以计算换显存，但不影响稳定性

---

## 6. Checkpoint 管理与续训策略

### 6.1 Checkpoint 内容

一个完整的 checkpoint 应包含：

```python
checkpoint = {
    "model_state_dict": model.state_dict(),          # 模型权重
    "optimizer_state_dict": optimizer.state_dict(),    # Adam 的 m 和 v
    "scheduler_state_dict": scheduler.state_dict(),    # LR scheduler 状态
    "step": global_step,                               # 当前步数
    "epoch": epoch,                                    # 当前 epoch
    "loss": current_loss,                              # 当前 loss
    "rng_states": {                                    # 随机数状态
        "python": random.getstate(),
        "numpy": np.random.get_state(),
        "torch": torch.random.get_rng_state(),
        "cuda": torch.cuda.get_rng_state_all(),
    },
    "config": model_config,                            # 模型配置
    "data_state": dataloader_state,                    # 数据加载位置
}
```

### 6.2 保存策略

- **定期保存**：每 N steps 保存（如每 1000 steps）
- **保留策略**：只保留最近 K 个 checkpoint + 若干里程碑 checkpoint
- **异步保存**：保存操作在后台线程进行，不阻塞训练
- **分布式保存**：用 `torch.distributed.checkpoint`（DCP）保存分片 checkpoint
  - 好处：每个 rank 保存自己的 shard，不需要先 gather 到 rank 0
  - 支持 reshape（不同并行度的 checkpoint 可以互相加载）

### 6.3 续训策略（Resumption）

**完全续训**（同配置恢复）：
1. 加载模型权重 + optimizer state + scheduler state
2. 恢复 RNG state 和数据位置
3. 从中断处继续，结果应完全可复现

**变更续训**（改配置继续）：
- **改 batch size**：调整 gradient accumulation steps，保持 LR schedule 按 token 数计算
- **改并行策略**：用 universal checkpoint（如 DeepSpeed 的 zero_to_fp32.py 转换后重新分片）
- **改序列长度**：RoPE 位置编码天然支持，但需调整 batch size 保持总 token 数一致
- **改数据混合**：直接修改 data mix，optimizer state 可保留

**长期训练的 checkpoint 策略**：
- 训练数月的大模型，checkpoint 需要跨文件系统备份
- 单个 checkpoint 可达 TB 级（175B 模型 FP32 约 700GB）
- 建议：用 BF16 保存权重，FP32 保存 optimizer state

---

## 7. 训练成本估算

### 7.1 FLOPs 公式

**单次前向传播 FLOPs**（Transformer）：

$$C_{forward} \approx 2 \times P \times T$$

其中 P = 参数量，T = 训练 token 数。

**包含反向传播**（反向约为前向的 2 倍）：

$$C_{total} \approx 6 \times P \times T$$

> 这是 Kaplan et al. (2020) 和 Hoffmann et al. (2022, Chinchilla) 使用的近似公式。

**更精确的估算**（考虑 attention 的 QKV 计算）：

$$C_{total} \approx 6PT + 12 \times L \times H \times S^2 \times T / S$$

其中 L = 层数，H = 隐藏维度，S = 序列长度。当 S 较大时第二项不可忽略。

### 7.2 GPU 小时计算

$$\text{GPU hours} = \frac{C_{total}}{\text{GPU FLOPS} \times \text{MFU}}$$

| GPU | BF16 FLOPS | 典型 MFU |
|-----|-----------|---------|
| A100 80GB | 312 TFLOPS | 40–55% |
| H100 SXM | 990 TFLOPS（BF16 + Tensor Core） | 40–55% |
| H200 | ~990 TFLOPS | 45–55% |

**MFU（Model FLOPs Utilization）**：实际计算效率 / 理论峰值。通常 40%–55%。

**示例：训练 7B 模型 2T tokens**
```
C = 6 × 7e9 × 2e12 = 8.4e22 FLOPs
H100 effective = 990e12 × 0.45 = 4.455e14 FLOPS
GPU_hours = 8.4e22 / (4.455e14 × 3600) ≈ 52,400 GPU hours
```
用 512 张 H100：约 102 小时 ≈ 4.3 天

### 7.3 Chinchilla Scaling Law

**Compute-optimal 训练**（Hoffmann et al., 2022）：

$$P_{opt} \approx 0.057 \times C^{0.49}, \quad T_{opt} \approx 0.293 \times C^{0.51}$$

经验法则：**token 数 ≈ 20 × 参数量**。

| 参数量 | Chinchilla 最优 Token 数 | 典型实际 Token 数 |
|-------|------------------------|-----------------|
| 7B | ~140B | 1T–2T（过训练以获得更强小模型） |
| 13B | ~260B | 1T–2T |
| 70B | ~1.4T | 2T–15T |

> 现代趋势：**过训练小模型**（LLaMA 3 用 15T tokens 训练 8B 模型），推理成本更低。

### 7.4 成本估算

| 模型规模 | 训练 Token 数 | 估算 GPU 小时 (H100) | 估算成本（$2/H100·hr） |
|---------|-------------|---------------------|----------------------|
| 7B | 2T | ~52k | ~$104k |
| 13B | 2T | ~97k | ~$194k |
| 70B | 2T | ~524k | ~$1.05M |
| 70B | 15T | ~3.9M | ~$7.8M |
| 405B | 15T | ~24M | ~$48M |

---

## 8. 面试题及回答要点

### Q1：预训练数据的去重为什么重要？MinHash 的原理是什么？

**回答要点：**
- 重复数据会导致模型**记忆**而非学习泛化特征，训练 loss 虚低但泛化能力差
- 研究表明去重后模型在下游任务上表现更好（Deduplicating Training Data Makes Language Models Better, Lee et al. 2022）
- MinHash 原理：
  1. 文档 → n-gram 集合
  2. 多个 hash 函数分别取最小值 → 签名向量
  3. 签名相似度 ≈ Jaccard 相似度（理论保证）
  4. LSH 分桶加速相似文档检索，避免 O(n²) 比较
- 去重发生在多个粒度：精确去重（文档级）→ 模糊去重（MinHash 文档级）→ 子串去重（段落级）

### Q2：为什么现在主流用 BF16 而不是 FP16？

**回答要点：**
- FP16 的表示范围只有 ±65504，大模型训练中梯度/激活值容易溢出
- BF16 的指数位与 FP32 相同（8 位），范围与 FP32 一致（±3.4e38），不容易溢出
- BF16 的精度较低（尾数 7 位 vs FP16 的 10 位），但对训练影响不大
- FP16 需要 **loss scaling**（dynamic loss scaling）来避免下溢，BF16 通常不需要
- BF16 在 A100/H100 上有原生硬件支持（Tensor Core），速度与 FP16 相当
- 实践中 BF16 训练更稳定、配置更简单，几乎是当前标准

### Q3：如何选择学习率和 batch size？有什么经验法则？

**回答要点：**
- **学习率**：
  - 与模型大小大致反比：模型越大 → LR 越小
  - 通常通过小规模实验（如 1B 模型）sweep 找到最优 LR，再用 μP 等方法迁移
  - Cosine schedule 几乎是标准配置，warmup 2000 steps
- **Batch size**：
  - Chinchilla 建议 batch size 与 loss 相关：$B_{crit} \propto L^{-1}$
  - 实践中 batch size 受显存限制，用 gradient accumulation 模拟大 batch
  - 过大的 batch 可能导致泛化性下降（sharp minima）
  - Batch size warmup 是常用技巧
- **两者的关系**：LR 和 batch size 存在 **linear scaling rule**（batch 翻倍 → LR 也可翻倍），但大 batch 时这个规则不完全成立

### Q4：训练过程中遇到 loss spike 怎么排查和处理？

**回答要点：**
- **先看 spike 幅度和恢复情况**：
  - 小幅 spike 且自动恢复 → 通常是异常数据，可忽略
  - 大幅 spike 不恢复 → 需要干预
- **排查步骤**：
  1. 检查梯度范数：spike 时梯度范数是否激增
  2. 检查对应的数据 batch：是否有异常长文本、特殊字符、重复内容
  3. 检查学习率：是否刚进入高 LR 阶段
  4. 检查 loss 的各个组成部分：是某一类数据的 loss 还是全局 loss
- **处理方式**：
  - 回滚到 spike 前的 checkpoint（PaLM 做法）
  - 跳过导致 spike 的数据
  - 降低学习率
  - 增加梯度裁剪阈值的监控
- **预防措施**：z-loss 正则化、QK-Norm、严格的数据清洗

### Q5：给你 1000 张 H100，你怎么训练一个 70B 模型？

**回答要点：**

**1. 并行策略：**
- **Tensor Parallelism (TP)**：TP=8（同一节点内，NVLink 互连）
- **Pipeline Parallelism (PP)**：PP=4（跨节点，将模型纵向切分为 4 段）
- **Data Parallelism (DP)**：DP = 1000 / (8×4) ≈ 31 路（ZeRO Stage 1）
- 或用 FSDP (ZeRO Stage 3)：省显存但通信开销更大

**2. 训练配置：**
- BF16 混合精度
- AdamW ($\beta_1=0.9, \beta_2=0.95$, wd=0.1)
- Cosine LR schedule, peak LR ~1.5e-4, warmup 2000 steps
- Sequence length 4096, batch size ~4M tokens/step
- Gradient clipping max_norm=1.0

**3. 数据：**
- 准备 2T+ tokens 清洗语料
- 数据混合：~70% web + 15% code + 5% wiki + 5% books + 5% 其他
- Online tokenization 或 pre-tokenized memmap

**4. 基础设施：**
- 约 125 个 8×H100 节点
- InfiniBand/RoCE 网络，至少 400Gbps 节点间带宽
- 分布式文件系统（Lustre/GPFS）存放数据和 checkpoint
- 每 500–1000 steps 保存 checkpoint

**5. 时间和成本估算：**
- FLOPs = 6 × 70e9 × 2e12 = 8.4e23
- MFU ~45%, H100 effective = 445 TFLOPS
- 总 GPU hours ≈ 524k，1000 卡约 22 天
- 成本约 $1M（按 $2/GPU·hr）

---

## 参考资料

- Kaplan et al., "Scaling Laws for Neural Language Models" (2020)
- Hoffmann et al., "Training Compute-Optimal Large Language Models" (Chinchilla, 2022)
- Touvron et al., "LLaMA: Open and Efficient Foundation Language Models" (2023)
- Chowdhery et al., "PaLM: Scaling Language Modeling with Pathways" (2022)
- Penedo et al., "The FineWeb Datasets" (2024)
- Li et al., "DataComp-LM (DCLM)" (2024)
- Lee et al., "Deduplicating Training Data Makes Language Models Better" (2022)

---

## See Also

- [[AI/Foundations/Training/Scaling Laws|Scaling Laws]] — 预训练的核心决策依据：模型/数据/计算的最优配比
- [[AI/LLM/Pretraining/LLM预训练与分布式训练2026全景|LLM 预训练 2026 全景]] — 工程全景：分布式训练/数据工程/优化技巧（2183行深度版）
- [[AI/Foundations/Training/Training Loss 分析|Training Loss 分析]] — 预训练过程中 loss 曲线的解读
- [[AI/Foundations/目录|Foundations MOC]] — 训练基础全图谱
