---
title: "LLM 训练数据构建"
date: 2026-02-14
tags: [training, data, sft, interview]
type: note
---

> [!warning] 重复笔记
> 同名深入版：[[AI/LLM/SFT/训练数据构建]]
> 本篇为 Foundations 面试准备版，建议以 LLM 版为主

# LLM 训练数据构建

## 1. 预训练数据

### 1.1 数据清洗 Pipeline

预训练数据从原始爬取到可用，需要经过多阶段清洗：

```
Raw Crawl → Language ID → URL Filtering → Content Extraction
    → Quality Scoring → Deduplication → PII Removal → Final Corpus
```

**各阶段要点：**

| 阶段 | 方法 | 说明 |
|------|------|------|
| Language ID | fastText lid | 过滤非目标语言，阈值通常 0.65+ |
| URL Filtering | 黑名单 + 启发式规则 | 去成人站、spam 农场、低质聚合站 |
| Content Extraction | trafilatura / resiliparse | 从 HTML 提取正文，去 boilerplate |
| Quality Scoring | 分类器 / perplexity | 用 Wikipedia 训练的分类器打分，或用 KenLM 算 PPL |
| Deduplication | MinHash / Exact dedup | 文档级 + 段落级去重，MinHash (n-gram=5, threshold=0.8) |
| PII Removal | 正则 + NER | 去邮箱、电话、身份证号等 |

### 1.2 常见预训练数据集

| 数据集 | 规模 | 特点 |
|--------|------|------|
| **C4** | ~750GB | 从 Common Crawl 清洗，T5 使用，规则简单但有效 |
| **The Pile** | ~825GB | EleutherAI 出品，22 个子集混合（学术论文、代码、书籍等） |
| **RedPajama v2** | ~30T tokens | 开源复现 LLaMA 训练数据，含质量信号标注 |
| **FineWeb** | ~15T tokens | HuggingFace 出品，基于 Common Crawl，精细清洗 |
| **DCLM** | ~4T tokens | Apple 开源，强调数据质量筛选方法论 |
| **CulturaX** | ~6.3T tokens | 多语言清洗数据，167 种语言 |

### 1.3 数据配比策略

数据配比直接影响模型能力分布。核心原则：**不是越多越好，而是配比要对**。

**典型配比参考（LLaMA 风格）：**

| 数据类型 | 比例 | 说明 |
|----------|------|------|
| 网页文本 | ~60-70% | 通用语言能力的基础 |
| 代码 | ~10-15% | 提升推理能力，不只是写代码 |
| 学术论文 | ~5% | 科学知识、严谨表达 |
| 书籍 | ~5-10% | 长上下文理解、叙事能力 |
| 数学 | ~3-5% | 数学推理，需要高质量数据 |
| 多语言 | ~5-10% | 按语言资源丰富度加权 |
| 对话/QA | ~2-3% | 提升对话和问答能力 |

**关键发现：**
- **代码数据的溢出效应**：加入代码训练能提升非代码任务的推理能力（Codex 论文观察）
- **数学数据需要精选**：质量差的数学数据不如不加，proof-pile 等高质量来源更有效
- **多语言的权重策略**：常用 temperature sampling（T=0.3-0.7）来平衡高低资源语言
- **数据重复的影响**：Chinchilla 之后发现适量重复（2-4 epochs）对小数据集可接受，但过度重复导致记忆化

---

## 2. SFT 指令数据

### 2.1 数据格式

标准格式为 instruction-input-output 三元组：

```json
{
  "instruction": "将以下英文翻译成中文",
  "input": "The quick brown fox jumps over the lazy dog.",
  "output": "敏捷的棕色狐狸跳过了懒狗。"
}
```

多轮对话格式（ChatML 风格）：

```json
{
  "messages": [
    {"role": "system", "content": "你是一个有帮助的助手"},
    {"role": "user", "content": "解释量子纠缠"},
    {"role": "assistant", "content": "量子纠缠是..."}
  ]
}
```

### 2.2 构建方法

**人工标注：**
- 最高质量但成本最高（$10-50/条高质量数据）
- 需要详细的标注指南（annotation guideline）
- 关键：标注员的多样性和专业性，避免同质化
- InstructGPT 使用 ~13k 条人工标注数据作为 SFT seed

**Self-Instruct（Wang et al., 2022）：**
- 用少量 seed instructions（175 条）引导 LLM 生成新指令
- Pipeline：生成指令 → 判断是否分类任务 → 生成 input → 生成 output
- 优点：成本低、可大规模生成
- 缺点：多样性有上限、可能放大模型偏见
- Alpaca 用此方法从 GPT-3.5 生成 52k 条数据

**Evol-Instruct（WizardLM, 2023）：**
- 对已有指令进行"进化"——增加约束、深化、具体化、增加推理步骤
- 进化方向：深度进化（更复杂）+ 广度进化（更多样）
- 比 Self-Instruct 生成的数据复杂度更高、能力覆盖更广

**其他方法：**
- **FLAN Collection**：将已有 NLP 数据集转化为指令格式，1800+ 任务
- **ShareGPT / WildChat**：收集真实用户与 ChatGPT 的对话
- **Orca 方法**：让强模型输出思维链（CoT），用于训练弱模型

### 2.3 质量 > 数量：LIMA 的启示

**LIMA（Zhou et al., 2023）** 的核心发现：

> 仅用 **1000 条**精选高质量 SFT 数据微调 LLaMA-65B，效果可媲美 GPT-4（人类评估中 43% 的情况优于 DaVinci003）。

**关键启示：**
- **Superficial Alignment Hypothesis**：模型的知识和能力几乎全部来自预训练，SFT 只是教模型"用什么格式/风格回答"
- 数据质量的优先级远高于数量——1000 条精选 > 52000 条机器生成
- 多样性比重复更重要——覆盖不同任务类型和领域
- 后续 DEITA、Instruction Mining 等工作进一步验证了数据选择的重要性

---

## 3. RLHF 偏好数据

### 3.1 Chosen / Rejected Pair 构建

偏好数据的基本形式：

```json
{
  "prompt": "解释相对论",
  "chosen": "相对论是爱因斯坦提出的...(详细、准确、有条理的回答)",
  "rejected": "相对论就是说时间会变慢...(简略、不准确的回答)"
}
```

**构建方法：**

| 方法 | 流程 | 优缺点 |
|------|------|---------|
| 人工排序 | 模型生成 N 个回答 → 标注员排序 | 质量最高，成本高 |
| 二元比较 | 模型生成 2 个回答 → 标注员选 better | 简单高效，信息量有限 |
| AI 反馈（RLAIF） | 用 Claude/GPT-4 做评判 | 可扩展，但有偏见 |
| 隐式偏好 | 用户点赞/复制/regenerate 行为 | 真实但噪声大 |

### 3.2 标注一致性

偏好标注的一致性是核心挑战：

- **人类标注员间一致性**（inter-annotator agreement）通常只有 60-75%
- 原因：主观偏好、指南理解差异、任务复杂度
- **处理方法**：
  - 多人标注取多数票（通常 3-5 人）
  - 计算 Cohen's Kappa / Fleiss' Kappa 监控质量
  - 分离"明确偏好"和"边际差异"，边际 pair 可丢弃
  - 使用 confidence-weighted 训练，低一致性样本降权

### 3.3 Anthropic HH-RLHF 数据集

- **规模**：~170k 偏好对
- **组成**：Helpful（有帮助性）+ Harmless（无害性）两部分
- **标注方式**：人类与模型对话，选择更好的回答
- **特点**：
  - 红队数据含对抗性 prompt，是安全对齐的重要资源
  - Helpful 和 Harmless 可能存在张力（过于安全 = 不够有帮助）
  - 后续 UltraFeedback、Nectar 等数据集在规模和多样性上进一步扩展

---

## 4. 数据质量评估方法

### 4.1 Perplexity Filtering

用一个参考语言模型（如 KenLM 或小型 GPT）计算文本困惑度：

- **低 PPL**：接近训练分布，可能是高质量文本（如 Wikipedia 风格）
- **极高 PPL**：可能是乱码、机器生成的垃圾、非自然文本
- **极低 PPL**：可能是重复文本、模板化内容

**实践要点：**
- 通常用在 Wikipedia 上训练的 KenLM 作为参考模型
- 过滤阈值需要根据数据分布调整，不能一刀切
- CCNet 方法：按 PPL 分桶（head/middle/tail），取 head 部分
- 局限：PPL 偏好 Wikipedia 风格，会误杀口语化但有价值的内容

### 4.2 Deduplication

**Exact Deduplication：**
- 基于 hash（SHA256）去重完全相同的文档
- 快速但只能处理完全重复

**Near-duplicate（MinHash LSH）：**
- 将文档表示为 n-gram 集合
- 用 MinHash 生成签名（通常 128-256 个 hash 函数）
- LSH（Locality-Sensitive Hashing）进行近似最近邻搜索
- 阈值 Jaccard similarity > 0.8 视为近重复
- 工具：datasketch、text-dedup

**SimHash：**
- 将文档映射为固定长度的二进制指纹
- 通过 Hamming 距离判断相似度
- 比 MinHash 更节省内存，但精度略低

**Suffix Array（Lee et al., 2022）：**
- 子串级别去重，可发现段落级重复
- Google 用于 C4 去重，发现大量跨文档重复段落

### 4.3 Toxic Content Filtering

多层过滤策略：

| 层级 | 方法 | 工具 |
|------|------|------|
| 关键词 | 黑名单词表匹配 | 自定义词表 |
| 分类器 | 毒性分类模型 | Perspective API、HateBERT、ToxiGen |
| URL | 域名黑名单 | UT1 blacklist、自维护列表 |
| LLM 审核 | 用 LLM 判断内容安全性 | Llama Guard、GPT-4 审核 |

**注意事项：**
- 过度过滤会导致特定群体、方言、文化表达被误删
- 需要在安全和多样性间取平衡
- 应保留部分"带有上下文的敏感内容"以让模型学会拒绝

---

## 5. 合成数据

### 5.1 强模型生成弱模型训练数据

核心思路：用 GPT-4 / Claude 级别的强模型生成数据，训练开源小模型。

**典型案例：**
- **Alpaca**：GPT-3.5 生成 52k 指令数据 → 微调 LLaMA-7B
- **Vicuna**：收集 ShareGPT 对话 → 微调 LLaMA-13B
- **Orca / Orca 2**：让 GPT-4 生成带 CoT 的回答 → 训练小模型学习推理过程
- **Phi 系列**：微软用 GPT-4 生成"教科书质量"数据训练小模型，效果显著

**关键问题：**
- **Model Collapse**：用合成数据迭代训练可能导致分布退化
- **License 争议**：用商业 API 输出训练模型的法律灰区
- **天花板效应**：学生模型很难超越教师模型的能力上限

### 5.2 Self-Play

模型与自身对弈/对话来生成训练数据：

- **Constitutional AI（Anthropic）**：模型自我批评 → 修改回答 → 形成偏好对
- **Self-Play Fine-Tuning（SPIN）**：当前模型 vs 上一版本模型，生成偏好数据
- **Debate**：两个模型辩论，评判模型打分
- **LMSYS Chatbot Arena**：本质是人类参与的 self-play 评估

### 5.3 知识蒸馏数据

不只是模仿输出，而是蒸馏中间知识：

- **Logit 蒸馏**：教师模型输出 soft label（概率分布），学生学习分布而非 hard label
- **Chain-of-Thought 蒸馏**：让教师输出推理过程，学生学习"怎么想"而非"答案是什么"
- **Feature 蒸馏**：对齐中间层表示（如 attention pattern、hidden states）
- **Step-by-step Distillation（Hsieh et al., 2023）**：提取推理步骤作为额外监督信号

---

## 6. 面试常见问题

### Q1：预训练数据清洗中，去重为什么如此重要？有哪些去重方法？

**要点：**
- 重复数据导致模型记忆化（memorization），而非泛化
- 训练时重复数据被过采样，导致分布偏移
- Lee et al. (2022) 发现 C4 中存在大量近重复内容，去重后下游任务提升显著
- **方法层次**：Exact dedup（SHA256 hash）→ Near-dedup（MinHash LSH, Jaccard > 0.8）→ Substring dedup（Suffix Array）
- 实际工程中通常多级联合使用，先 exact 再 near-duplicate

### Q2：SFT 数据的质量和数量哪个更重要？用什么指标评估质量？

**要点：**
- **质量远重要于数量**——LIMA 用 1000 条精选数据就达到了很好效果
- Superficial Alignment Hypothesis：SFT 只是调整输出格式/风格，知识来自预训练
- 质量评估维度：准确性、完整性、格式规范性、无害性
- 评估方法：人工抽检、用强模型（GPT-4）自动评分、IFD score（Instruction Following Difficulty）
- 实践建议：先用少量高质量数据训练 baseline，再逐步扩展

### Q3：RLHF 中偏好数据的标注一致性问题如何解决？

**要点：**
- 人类标注一致性通常只有 60-75%，这是固有挑战
- 多人标注取多数票（3-5 人），并计算 inter-annotator agreement
- 将偏好对按 margin 分级：clear win / marginal / tie
- Tie 和 marginal 样本可以降权或丢弃
- 使用 soft labels（如 60-40 而非 1-0）替代硬标签
- RLAIF 替代方案：用 LLM 做评判减少人工成本，但需注意位置偏见等 bias

### Q4：合成数据训练有什么风险？如何缓解？

**要点：**
- **Model Collapse**：迭代使用合成数据训练会导致模型多样性和质量退化
- **Distribution Shift**：合成数据分布与真实数据不同，可能偏向特定风格
- **Hallucination 传播**：教师模型的错误会被学生模型继承和放大
- **缓解方法**：
  - 混合真实数据和合成数据（如 70% 真实 + 30% 合成）
  - 对合成数据进行质量过滤（验证事实准确性）
  - 每轮合成用新的教师模型，避免反馈循环
  - 使用多个不同教师模型生成以增加多样性

### Q5：如何设计预训练数据的配比策略？代码数据为什么重要？

**要点：**
- 没有万能配比，需根据目标能力分布调整
- 通用配比参考：网页 60-70%、代码 10-15%、学术 5%、书籍 5-10%、数学 3-5%
- **代码数据的溢出效应**：代码的结构化特性有助于提升逻辑推理和 planning 能力
- 多语言使用 temperature sampling 平衡高低资源语言
- 数据配比可以在训练过程中动态调整（curriculum learning）
- 关键实验：DoReMi（Google）使用小代理模型自动搜索最优配比

### Q6：FineWeb 和 RedPajama 这类开源数据集相比早期的 C4/The Pile 有什么改进？

**要点：**
- **更大规模**：从 TB 级到数十 TB 级，token 量级提升 10-20 倍
- **更精细的清洗**：
  - FineWeb 使用多阶段质量过滤 + 精心调优的去重参数
  - RedPajama v2 为每个文档提供 40+ 维度的质量信号
- **更好的可复现性**：完整公开处理 pipeline 和配置
- **数据溯源**：支持追踪每条数据的来源和处理历史
- **持续更新**：定期纳入新的 Common Crawl 快照
- 实际影响：用 FineWeb 训练的模型在同等规模下 benchmark 分数显著高于 C4

---

## See Also

- [[AI/LLM/SFT/训练数据构建|训练数据构建（SFT 深度版）]] — 本文面试版，深度版含 ChatML 格式 + quality filtering 代码 + 生产数据 pipeline
- [[AI/LLM/Application/Synthetic-Data/合成数据与数据飞轮-2026技术全景|合成数据与数据飞轮 2026 全景]] ⭐ — 当实际数据不够时，合成数据是训练数据构建的核心扩展路径；Self-Instruct/RLAIF/数据飞轮
- [[AI/LLM/Training/LLM数据工程2026技术全景|LLM 数据工程 2026 全景]] — 预训练数据工程全景（3778行）；本文聚焦 SFT 数据，全景版覆盖预训练→SFT完整链路
- [[AI/Foundations/Training/预训练流程|预训练流程]] — 数据构建是预训练流程的第一步；FineWeb 的质量过滤方法与 SFT 数据过滤共享 deduplication + quality signal 设计模式
