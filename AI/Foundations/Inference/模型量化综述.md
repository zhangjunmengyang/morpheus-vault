---
title: "LLM 模型量化综述"
date: 2026-02-14
tags: [inference, quantization, optimization, interview]
type: note
---

# LLM 模型量化综述

## 1. 为什么要量化

LLM 参数量动辄数十到数百 B，FP16 下一个 70B 模型需要 ~140GB 显存，单卡根本放不下。量化是将高精度浮点权重（FP16/BF16）映射到低精度整数（INT8/INT4）或低精度浮点（FP8/NF4）的过程。

| 维度 | FP16 (70B) | INT4 (70B) | 收益 |
|------|-----------|-----------|------|
| 显存 | ~140 GB | ~35 GB | **4× 压缩** |
| 带宽 | 受 HBM 带宽瓶颈 | 传输量减少 | **Decode 吞吐 ↑** |
| 成本 | 需多卡/A100 80G | 单卡 A100 40G 可装 | **硬件门槛 ↓** |

核心动机：

- **显存**：让大模型装进更少的 GPU — 70B INT4 可跑在单张 A100 40G 上
- **速度**：LLM decode 阶段是 memory-bound，减少 weight 传输量直接提升 token/s
- **成本**：同等服务质量下，GPU 数量可减少 2-4 倍，推理成本等比下降

## 2. 量化基础概念

### 2.1 对称量化 vs 非对称量化

**对称量化（Symmetric）**：
```
x_q = round(x / scale)
scale = max(|x|) / (2^(b-1) - 1)
```
- zero_point = 0，映射关系关于原点对称
- 实现简单，计算快；但对分布不对称的激活值（如 ReLU 后全正）浪费表示范围

**非对称量化（Asymmetric）**：
```
x_q = round(x / scale) + zero_point
scale = (max(x) - min(x)) / (2^b - 1)
zero_point = round(-min(x) / scale)
```
- 多一个 zero_point 参数，能更好覆盖非对称分布
- 实现稍复杂，推理时需额外处理 zero_point

**实践选择**：权重通常近似对称分布 → 对称量化；激活值分布偏斜 → 非对称量化更优。

### 2.2 量化粒度：Per-Tensor / Per-Channel / Per-Group

| 粒度 | 共享参数范围 | 精度 | 开销 |
|------|------------|------|------|
| **Per-Tensor** | 整个 tensor 共享一组 (scale, zp) | 最低 | 最小 |
| **Per-Channel** | 每个输出通道一组 | 中等 | 中等 |
| **Per-Group** | 每 g 个元素一组（常见 g=128） | 最高 | 最大 |

Per-Group (group_size=128) 是当前主流选择，在精度和存储开销之间取得最佳平衡。GPTQ/AWQ 默认都使用 group_size=128。

### 2.3 量化对象

- **Weight-only 量化**：只量化权重，激活保持 FP16。Decode 阶段 memory-bound，weight-only 即可获得大部分加速。GPTQ、AWQ、GGUF 均属此类。
- **Weight + Activation 量化**：权重和激活都量化为 INT8/FP8。Prefill 阶段 compute-bound，W+A 量化可利用 INT8/FP8 Tensor Core 加速矩阵乘。SmoothQuant 属此类。

## 3. PTQ 方法：GPTQ、AWQ、SmoothQuant

PTQ（Post-Training Quantization）= 训练后量化，不需要重新训练模型，仅用少量校准数据即可完成。

### 3.1 GPTQ

**核心思想**：基于 OBQ（Optimal Brain Quantization）的逐层权重量化。将量化误差通过 Hessian 信息补偿到未量化的权重上。

**关键步骤**：
1. 对每一层，计算权重矩阵的 Hessian 逆 $H^{-1}$（用校准数据的激活估计）
2. 逐列量化权重，每量化一列后将误差按 Hessian 补偿分散到剩余列
3. 使用 lazy batch update 加速（每 128 列一批）

**特点**：
- 精度高，INT4 下 perplexity 损失小（~0.5 PPL on LLaMA-7B）
- 量化速度适中（70B 约需数小时 on A100）
- 支持 group quantization，通常 group_size=128
- 与 vLLM、HuggingFace 生态集成好（Marlin kernel 加速）

### 3.2 AWQ（Activation-Aware Weight Quantization）

**核心思想**：不是所有权重同等重要。通过观察激活值的分布，找到"重要"的权重通道（对应激活值大的通道），对这些通道做 per-channel scaling 保护。

**关键步骤**：
1. 用校准数据统计每个通道的激活值均值 $s_j = \text{mean}(|X_j|)$
2. 对权重做 per-channel 缩放：$W_j' = W_j \cdot s_j^\alpha$，对激活做反缩放：$X_j' = X_j / s_j^\alpha$
3. 搜索最优 $\alpha$（通常在 [0, 1] 之间 grid search）
4. 在缩放后的权重上做标准 RTN（Round-To-Nearest）量化

**特点**：
- 思路简洁，实现高效
- 量化速度快（比 GPTQ 快很多，70B 约需 ~20min）
- INT4 精度与 GPTQ 相当或略优
- **vLLM 官方推荐的量化方式**，有专门的高性能 AWQ kernel

### 3.3 SmoothQuant

**核心思想**：激活值量化的难点在于 outlier（异常大的激活值集中在少数通道）。SmoothQuant 将激活的量化难度"迁移"到权重上。

**关键步骤**：
1. 统计激活每通道的最大值 $s_j = \max(|X_j|)^\alpha / \max(|W_j|)^{1-\alpha}$
2. 对激活做 per-channel 除法：$\hat{X} = X \cdot \text{diag}(s)^{-1}$
3. 对权重做 per-channel 乘法：$\hat{W} = W \cdot \text{diag}(s)$
4. 平滑后 W 和 X 都可以用 per-tensor INT8 量化

**特点**：
- **W8A8**：权重和激活都是 INT8，可利用 INT8 Tensor Core
- 适合 Prefill 阶段的 compute-bound 场景
- 配合 FP8（H100）效果更佳
- TensorRT-LLM 原生支持

### 3.4 三者对比

| 方法 | 量化目标 | 位宽 | 核心创新 | 最佳场景 |
|------|---------|------|---------|---------|
| **GPTQ** | Weight-only | W4 | Hessian 补偿 | 通用部署 |
| **AWQ** | Weight-only | W4 | 激活感知缩放 | vLLM 部署 |
| **SmoothQuant** | W+A | W8A8 | 难度迁移 | 高吞吐 Prefill |

## 4. GGUF/GGML：llama.cpp 量化格式

### 4.1 背景

GGML（Georgi Gerganov's ML library）是 llama.cpp 底层的张量库。**GGUF** 是 GGML 的新一代文件格式（取代旧的 GGML 格式），单文件包含模型元数据 + 量化权重。

### 4.2 量化类型命名规则

格式：`Q{bits}_{variant}_{size}`

| 量化类型 | 说明 | 精度 | 大小 (7B) |
|---------|------|------|----------|
| `Q2_K` | 2-bit, K-Quant | 最低 | ~2.8 GB |
| `Q3_K_S/M/L` | 3-bit, Small/Medium/Large | 低 | ~3.2-3.6 GB |
| `Q4_0` | 4-bit, 基础 round-to-nearest | 中 | ~3.8 GB |
| `Q4_K_M` | 4-bit, K-Quant, Medium | **推荐** | ~4.1 GB |
| `Q4_K_S` | 4-bit, K-Quant, Small | 中 | ~3.9 GB |
| `Q5_K_M` | 5-bit, K-Quant, Medium | 较高 | ~4.8 GB |
| `Q6_K` | 6-bit, K-Quant | 高 | ~5.5 GB |
| `Q8_0` | 8-bit, 基础 | 最高 | ~7.2 GB |
| `IQ4_XS` | 4-bit, importance matrix | 高（同位宽最优） | ~3.9 GB |

**K-Quant**：不同层使用不同位宽的混合量化，attention 层用更高精度，FFN 层用更低精度。S/M/L 代表高精度层的比例。

**IQ 系列**：使用 importance matrix（Fisher 信息矩阵近似）指导量化，同位宽下精度最优，但量化速度更慢。

### 4.3 实践建议

- **消费级 GPU/CPU 推理首选 GGUF**
- 精度 vs 大小平衡：**Q4_K_M** 是最常用的"甜点"
- 追求品质：Q5_K_M 或 Q6_K
- 极限压缩：IQ4_XS > Q4_K_S > Q3_K_M
- 与 AWQ/GPTQ 不同，GGUF 同时支持 CPU 和 GPU 推理

## 5. QAT vs PTQ 对比

| 维度 | PTQ | QAT |
|------|-----|-----|
| **全称** | Post-Training Quantization | Quantization-Aware Training |
| **过程** | 模型训练完成后直接量化 | 训练过程中模拟量化误差 |
| **训练成本** | ✅ 零（仅需少量校准数据） | ❌ 高（需完整训练流程） |
| **精度** | INT8 几乎无损；INT4 有轻微损失 | INT4 精度优于 PTQ |
| **典型方法** | GPTQ, AWQ, SmoothQuant | LLM-QAT, BitNet, AQLM |
| **适用场景** | 大多数部署场景 | 追求极致 INT4/INT2 精度 |

**面试要点**：当前 LLM 领域 PTQ 是绝对主流。QAT 成本太高（需要完整的训练资源），只有模型开发者才会做。但 BitNet（1-bit LLM）这类从头训练的方案属于 QAT 思路，是未来方向。

## 6. 数据格式适用场景

### INT8（W8A8）
- **场景**：精度要求高、不追求极致压缩
- **方法**：SmoothQuant、基础 absmax 量化
- **部署**：TensorRT-LLM INT8 模式，vLLM 的 `bitsandbytes` INT8
- **损失**：几乎无损（< 0.1 PPL degradation）

### INT4（W4A16）
- **场景**：显存受限、需要单卡部署大模型
- **方法**：GPTQ、AWQ、GGUF Q4_K_M
- **部署**：vLLM + AWQ、llama.cpp + GGUF
- **损失**：轻微（0.3-0.8 PPL degradation depending on model）

### FP8（E4M3 / E5M2）
- **场景**：H100/H200 等新硬件，追求 compute-bound 阶段加速
- **格式**：E4M3（推理常用，4-bit 指数 + 3-bit 尾数），E5M2（训练常用）
- **部署**：TensorRT-LLM FP8 模式，vLLM FP8 量化
- **特点**：精度接近 FP16，速度接近 INT8。**H100 上的最佳选择**
- **损失**：极小（< 0.1 PPL degradation）

### NF4（4-bit NormalFloat）
- **场景**：QLoRA 微调
- **来源**：bitsandbytes 提出，假设权重服从正态分布，设计最优 4-bit 量化点
- **特点**：比均匀 INT4 更适合正态分布权重，常配合 double quantization 进一步压缩
- **损失**：比 INT4 RTN 更低（同为 4-bit，PPL 更优）

## 7. 量化对模型质量的影响

### 7.1 一般规律

1. **模型越大，量化越鲁棒**：70B INT4 的质量 > 7B FP16 的质量
2. **INT8 几乎无损**：所有模型 INT8 量化后 PPL 劣化 < 0.1
3. **INT4 有可接受的损失**：用好的方法（GPTQ/AWQ），PPL 劣化在 0.3-1.0
4. **低于 INT4 快速恶化**：INT3/INT2 质量下降显著，需要 QAT 或特殊方法

### 7.2 哪些能力最先受损

- **长尾知识**（冷门事实）最先丢失
- **数学推理**对量化敏感
- **指令遵循**（格式、约束）受量化影响较大
- **通用对话能力**受影响最小

### 7.3 评估指标

- **Perplexity (PPL)**：最基础的量化质量指标
- **下游任务准确率**：MMLU、HumanEval 等
- **人工评测**：MT-Bench、Chatbot Arena（量化模型 vs 原始模型盲评）

## 8. 部署选择指南

```
┌─────────────────────────────────────────────────────┐
│                 我该用什么量化方案？                    │
├─────────────────────────────────────────────────────┤
│                                                     │
│  vLLM 部署（数据中心）                                │
│  └→ AWQ (INT4) — 官方推荐，有 Marlin kernel          │
│  └→ FP8 (H100) — 精度最高，TPS 最优                  │
│  └→ GPTQ (INT4) — 也可以，但 AWQ 生态更好             │
│                                                     │
│  llama.cpp 部署（边缘/消费级）                         │
│  └→ GGUF Q4_K_M — 精度/大小最佳平衡                   │
│  └→ GGUF Q5_K_M — 追求更高精度                       │
│  └→ GGUF IQ4_XS — 同位宽最高精度                     │
│                                                     │
│  TensorRT-LLM 部署（最高性能）                        │
│  └→ FP8 (H100/H200) — 首选                          │
│  └→ INT8 (SmoothQuant) — A100 等旧卡                 │
│  └→ INT4 (AWQ) — 极限显存压缩                        │
│                                                     │
│  微调场景                                            │
│  └→ NF4 + QLoRA (bitsandbytes)                      │
│                                                     │
└─────────────────────────────────────────────────────┘
```

### 速查表

| 部署目标 | 推荐方案 | 量化方法 | 格式 |
|---------|---------|---------|------|
| vLLM + A100 | W4A16 | AWQ | safetensors |
| vLLM + H100 | FP8 | FP8 dynamic | safetensors |
| llama.cpp + 消费 GPU | W4 mixed | K-Quant | GGUF |
| llama.cpp + CPU only | W4/W5 mixed | K-Quant | GGUF |
| TensorRT-LLM + H100 | FP8 | TRT 内建 | TRT engine |
| TensorRT-LLM + A100 | W8A8 | SmoothQuant | TRT engine |
| QLoRA 微调 | NF4 | bitsandbytes | HF format |

## 9. 面试常见问题及回答要点

### Q1: 为什么 LLM 推理量化后速度能提升？不是 INT 运算本身更快吗？

**A**：LLM decode 阶段是 **memory-bandwidth bound**，不是 compute-bound。每生成一个 token，需要加载整个模型权重，但只做 batch_size × hidden_dim 的计算。瓶颈在显存带宽。INT4 权重体积是 FP16 的 1/4，传输量减少 4×，所以速度提升。这和"INT 运算更快"是两个不同的事情。

### Q2: GPTQ 和 AWQ 怎么选？

**A**：精度上两者接近（都是 INT4 weight-only）。选择看部署栈：
- **vLLM** → AWQ（官方推荐，有专门优化的 Marlin kernel）
- **HuggingFace TGI** → GPTQ（历史支持更好）
- **量化速度** → AWQ 更快（不需要 Hessian 计算）

### Q3: 什么是 K-Quant？和普通量化有什么区别？

**A**：K-Quant 是 llama.cpp 中的 **混合精度量化方案**。不是所有层都用同一位宽——对模型质量影响大的层（如 attention 的 Q/K/V 投影）用更高精度（5-6 bit），不太敏感的层（如 FFN）用更低精度（3-4 bit）。这样在相同平均位宽下获得更好的精度。S/M/L 后缀表示高精度层的比例。

### Q4: SmoothQuant 和 AWQ 的思路有什么相似之处？

**A**：两者都利用了 **per-channel 缩放** 来处理量化难题，但方向相反：
- **SmoothQuant**：将激活的量化难度迁移到权重上（激活 ÷ s，权重 × s），目标是让 W 和 A 都能用 INT8
- **AWQ**：用激活值大小指导权重的缩放保护（重要通道放大再量化），目标是 weight-only INT4 更准确

### Q5: FP8 和 INT8 有什么区别？什么时候用哪个？

**A**：
- **INT8**：均匀量化，适合权重。A100 及更早 GPU 支持 INT8 Tensor Core
- **FP8 (E4M3)**：非均匀量化，能更好表示接近 0 的小值。**H100 才有原生 FP8 Tensor Core**
- 选择：有 H100 → FP8（精度更高、速度更快）；A100/旧卡 → INT8

### Q6: 量化后模型出现质量问题，怎么排查？

**A**：
1. 跑 PPL 对比：量化前后 PPL 差距 > 1.0 说明量化有问题
2. 检查校准数据：是否与目标任务域匹配
3. 检查异常层：某些层的量化误差可能特别大（可用 mixed precision，对敏感层保持高精度）
4. 尝试更高精度：INT4 不行换 INT8，或 Q4_K_M 不行换 Q5_K_M
5. 评估下游任务而非只看 PPL：有些情况 PPL 差距小但特定任务下降明显

### Q7: 为什么不直接用 FP16 部署，非要量化？

**A**：成本账。以 LLaMA-70B 为例：
- FP16：需要 2×A100 80G（~$4/hr on cloud）
- INT4 (AWQ)：需要 1×A100 40G（~$1/hr on cloud）
- 同时 INT4 的 decode 吞吐更高（减少 weight 传输）
- 4× 成本差距在大规模部署中非常显著
