---
brief: "LLM 推理服务架构——从单机推理到生产级服务的工程架构；Continuous Batching/Prefill-Decode 分离/多卡 Tensor Parallel 的核心设计；vLLM/TGI/TensorRT-LLM 架构对比；面试系统设计题参考。"
title: "LLM 推理服务架构"
date: 2026-02-14
tags: [inference, serving, vllm, architecture, interview]
type: note
---

> [!info] 📖 版本说明
> 本篇为**面试速查版**（简洁直接）。深度工程版：[[AI/LLM/Inference/推理服务架构|推理服务架构工程深度版]]

# LLM 推理服务架构

## 推理的两阶段

LLM 推理本质是两个截然不同的计算阶段：

### Prefill（预填充）
- **任务**：处理整个输入 prompt，计算所有 token 的 KV Cache
- **特征**：**计算密集型**（compute-bound），大量矩阵乘法可高度并行
- **瓶颈**：GPU 算力
- **延迟指标**：TTFT（Time To First Token）

### Decode（解码）
- **任务**：逐 token 自回归生成，每步只处理 1 个新 token
- **特征**：**内存密集型**（memory-bound），需要读取全部 KV Cache 但计算量小
- **瓶颈**：显存带宽
- **延迟指标**：TBT（Time Between Tokens）/ TPOT（Time Per Output Token）

### 为什么这个区分重要
两阶段的资源需求完全不同，混在一起会互相拖累。这是后面所有优化的出发点。

## 核心优化技术

### Continuous Batching（持续批处理）

**Static Batching（传统）：**
- 一批请求必须等最长的那个生成完，才能处理下一批
- 短请求被长请求拖累，GPU 利用率低

**Continuous Batching（Orca 2022）：**
- 请求完成一个就释放一个，新请求随时插入
- 每个 iteration 动态调整 batch 组成
- 吞吐量提升 **2-10x**
- vLLM、SGLang 等现代引擎的标配

### PagedAttention（vLLM 核心创新）

灵感来自操作系统的**虚拟内存分页**：

**传统方式的问题：**
- KV Cache 预分配连续显存，按最大序列长度分
- 实际使用率只有 20-40%，大量显存浪费

**PagedAttention 方案：**
- 将 KV Cache 分成固定大小的 block（如 16 tokens/block）
- 用 block table 映射逻辑位置到物理显存块
- 按需分配，用完回收
- 显存利用率接近 100%
- 支持 **Copy-on-Write**：beam search 等场景共享 KV Cache 前缀

### Chunked Prefill

**问题**：长 prompt 的 prefill 可能占用 GPU 数秒，阻塞其他请求的 decode

**方案**：将长 prefill 拆成固定大小的 chunk，与 decode 请求交替执行
- 保证 decode 请求的 TBT 不被 prefill 阻塞
- 在 TTFT 和 TBT 之间取得平衡
- Sarathi-Serve、vLLM v0.3+ 支持

### Prefill-Decode 分离（PD Disaggregation）

**核心思想**：既然两阶段资源需求不同，为什么不分开部署？

| 方案 | 思路 |
|------|------|
| **DistServe** (2024) | Prefill 节点（高算力GPU）+ Decode 节点（高带宽GPU），通过 KV Cache 传输连接 |
| **Mooncake** (月之暗面) | 类似分离架构，优化了 KV Cache 的跨节点传输 |
| **Splitwise** (微软) | 动态决定 prefill/decode 的资源分配比例 |

**优势**：各阶段独立扩缩容，资源利用率最优
**挑战**：KV Cache 跨节点传输的延迟和带宽开销

## 推理中的并行策略

### Tensor Parallelism（TP）
- 将单层的权重矩阵切分到多 GPU
- 每步需要 all-reduce 通信
- 适合**低延迟**场景（单请求跨多卡）
- 典型：TP=2/4/8 在单节点内

### Pipeline Parallelism（PP）
- 不同层放在不同 GPU
- 适合**高吞吐**场景
- 延迟高于 TP（micro-batch 流水线填充）
- 跨节点时通信量更小

### Expert Parallelism（EP）
- 专为 MoE 模型设计
- 不同 expert 放在不同 GPU
- 通信模式：all-to-all（每 token 路由到对应 expert 所在 GPU）

## 主流推理引擎对比

| 引擎 | 核心特点 | 优势场景 | 缺点 |
|------|----------|----------|------|
| **vLLM** | PagedAttention、Continuous Batching | 通用高吞吐服务 | 长序列性能一般 |
| **SGLang** | RadixAttention（前缀缓存）、零开销调度 | 多轮对话、共享前缀 | 生态略小于 vLLM |
| **TensorRT-LLM** | NVIDIA 官方优化、FP8/INT4 kernel | 追求极致单卡性能 | 仅 NVIDIA GPU、配置复杂 |
| **MLC-LLM** | 跨平台（手机/浏览器/嵌入式） | 端侧部署 | 吞吐不如服务端引擎 |
| **llama.cpp** | 纯 CPU/Metal 推理、GGUF 格式 | 个人设备、Mac | 不适合高并发服务 |
| **DeepSpeed-MII** | DeepSpeed 生态、模型并行 | 超大模型分布式 | 更新频率低 |

### 选型建议
- **通用在线服务** → vLLM 或 SGLang
- **多轮对话/Agent** → SGLang（前缀缓存优势大）
- **NVIDIA 极致性能** → TensorRT-LLM
- **端侧/手机** → MLC-LLM 或 llama.cpp
- **MoE 大模型** → SGLang（EP 支持好）

## SLO 设计

| 指标 | 含义 | 典型要求 |
|------|------|----------|
| **TTFT** | 首 token 延迟 | <500ms（对话）、<2s（长文档） |
| **TBT/TPOT** | token 间延迟 | <50ms（流式输出感觉流畅） |
| **E2E Latency** | 端到端延迟 | 取决于输出长度 |
| **Throughput** | 每秒处理 token 数 | 取决于业务量 |
| **P99 Latency** | 99 分位延迟 | 通常是 P50 的 2-3 倍以内 |

### 吞吐 vs 延迟的 Trade-off
- 加大 batch size → 吞吐↑ 延迟↑
- TP 增加 → 延迟↓ 但通信开销↑
- 分离部署 → 两者可独立优化

## 面试常见问题

### Q1: vLLM 的 PagedAttention 是什么？为什么重要？
**答**：借鉴操作系统虚拟内存分页思想，将 KV Cache 分成固定大小 block 按需分配。解决了 KV Cache 显存碎片化和浪费问题，将显存利用率从 ~30% 提升到接近 100%。还支持 CoW 机制用于 beam search 等场景的 KV Cache 共享。

### Q2: Prefill 和 Decode 的区别？为什么要分离？
**答**：Prefill 是计算密集型（处理整个 prompt），Decode 是内存密集型（逐 token 生成需要读 KV Cache）。资源需求完全不同，分离后可以用不同规格的 GPU 独立扩缩容，避免互相干扰。

### Q3: Continuous Batching 相比 Static Batching 的优势？
**答**：Static Batching 中整个 batch 必须等最慢的请求完成，短请求被长请求拖累。Continuous Batching 允许请求动态加入和退出 batch，每个 iteration 都可以调整 batch 组成，吞吐量提升 2-10 倍。

### Q4: SGLang 和 vLLM 怎么选？
**答**：通用高吞吐服务选 vLLM（社区大、兼容好）。多轮对话和 Agent 场景选 SGLang（RadixAttention 自动复用前缀 KV Cache，多轮对话性能优势明显）。MoE 模型推理 SGLang 的 EP 支持也更好。

### Q5: 如何估算一个模型需要多少显存做推理？
**答**：模型权重 + KV Cache。权重 = 参数量 × 精度字节数（如 7B FP16 ≈ 14GB）。KV Cache = 2 × layers × hidden_dim × seq_len × batch_size × 精度字节数。实际还要加上 activation memory 和框架开销，通常总量是权重的 1.2-2 倍（取决于并发和序列长度）。

### Q6: FP8 推理和 INT8 推理的区别？
**答**：FP8 保留浮点格式（有指数位），动态范围大，对 outlier 友好，H100/H200 原生支持；INT8 是定点整数，需要 calibration 确定 scale，实现简单但对异常值敏感。实践中 FP8 在 NVIDIA 新卡上质量和速度都更优，INT8 在旧卡（A100 等）上更实用。

---

## See Also

- [[AI/LLM/Inference/推理服务架构|推理服务架构（LLM 深度版）]] — 本文面试版，深度版含 vLLM/TensorRT-LLM/生产部署完整方案
- [[AI/Foundations/Inference/KV Cache|KV Cache]] — 推理服务架构的核心内存管理机制；PagedAttention/Continuous Batching 都是围绕 KV Cache 的架构创新
- [[AI/LLM/Inference/LLM推理优化2026全景|LLM 推理优化 2026 全景]] ⭐ — 推理服务工程全景；本文提供概念框架，全景版提供2026年工业界最新实践
- [[AI/LLM/Frameworks/Jet-RL-FP8-On-Policy-RL-Training|Jet-RL（FP8 On-Policy RL）]] — FP8 推理在 RL 训练场景的应用；同样的 FP8 精度在推理服务和 RL 训练中的工程挑战不同
