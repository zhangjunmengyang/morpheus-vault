---
title: "LLM 采样策略详解"
date: 2026-02-14
tags:
  - inference
  - sampling
  - decoding
  - interview
type: note
---

> [!warning] 重复笔记
> 同名深入版：[[AI/LLM/Inference/采样策略]]
> 本篇为 Foundations 面试准备版，建议以 LLM 版为主

# LLM 采样策略详解

语言模型在每个时间步输出一个 **词表大小的 logits 向量**，经 softmax 转为概率分布。**采样策略（Decoding Strategy）** 决定了如何从这个分布中选择下一个 token——它直接影响生成文本的质量、多样性和确定性。

---

## 1 Greedy Decoding

**原理**：每一步选概率最高的 token。

$$x_t = \arg\max_{x} P(x \mid x_{<t})$$

| 优点 | 缺点 |
|------|------|
| 实现最简单，速度最快 | 确定性输出，完全没有多样性 |
| 适合唯一正确答案的场景 | 容易陷入重复循环 |
| | 无法回溯——局部最优 ≠ 全局最优 |

> **一句话**：Greedy 是所有采样策略的 baseline，理解它才能理解后续方法在"修正"什么。

---

## 2 Beam Search

**原理**：同时维护 $b$ 条候选序列（beam），每步扩展所有候选并保留得分最高的 $b$ 条。

- **Beam Width $b$**：$b=1$ 退化为 Greedy；$b$ 越大搜索越充分，但计算和显存开销线性增长。
- **长度惩罚（Length Penalty）**：防止 beam search 偏好短序列，通常对 log-probability 除以 $\text{length}^\alpha$。

### 为什么 LLM 时代用得少了？

1. **计算成本**：LLM 参数量巨大，维护多条 beam 的 KV Cache 显存开销不可接受。
2. **开放式生成**：Beam Search 倾向生成"安全但无聊"的文本，缺乏创造性。
3. **采样方法更灵活**：Temperature + Top-P 组合在多样性和质量之间取得了更好的平衡。
4. **应用场景变化**：机器翻译、摘要等 beam search 擅长的"有标准答案"任务占比下降。

> **面试要点**：Beam Search 在 NMT 时代是标配，但在 GPT 类模型中几乎不用——要能说清原因。

---

## 3 Temperature Sampling

**原理**：在 softmax 之前将 logits 除以温度参数 $T$：

$$P(x_i) = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}$$

| $T$ 值 | 效果 |
|---------|------|
| $T \to 0$ | 分布趋近 one-hot → 退化为 Greedy |
| $T = 1$ | 原始分布，不做任何调整 |
| $T > 1$ | 分布变平坦，增加随机性和多样性 |
| $T \to \infty$ | 趋近均匀分布，完全随机 |

**直觉**：Temperature 控制的是模型的"自信程度"——低温让模型更果断，高温让模型更"放飞自我"。

> **常用范围**：代码生成 0.0-0.2，事实问答 0.3-0.5，创意写作 0.7-1.0，头脑风暴 1.0-1.5。

---

## 4 Top-K Sampling

**原理**：每步只保留概率最高的 $K$ 个 token，将其余 token 的概率置零，然后在这 $K$ 个中重新归一化后采样。

- **$K$ 值选择**：
  - $K=1$：Greedy Decoding
  - 常见值：$K = 40 \sim 100$
  - $K$ 过小 → 多样性不足；$K$ 过大 → 引入低质量 token

### Top-K 的根本问题

**$K$ 是固定的，但概率分布是动态的。**

- 当模型非常确信时（分布尖锐），$K=50$ 会引入大量不合理的 token。
- 当模型不确定时（分布平坦），$K=50$ 可能把合理的候选截掉。

这个问题直接催生了 Top-P Sampling。

---

## 5 Top-P (Nucleus) Sampling

> Holtzman et al., 2020 — *"The Curious Case of Neural Text Degeneration"*

**原理**：选择概率从高到低排列的最小 token 集合，使其累积概率 $\geq p$：

$$\text{Nucleus}(p) = \min \left\{ V' \subseteq V : \sum_{x \in V'} P(x) \geq p \right\}$$

然后在这个集合中重新归一化并采样。

### Top-P vs Top-K 对比

| 维度 | Top-K | Top-P |
|------|-------|-------|
| 阈值类型 | 固定数量 | 动态，基于累积概率 |
| 分布自适应 | ❌ 不适应 | ✅ 自动调整候选集大小 |
| 尖锐分布 | 可能引入噪声 | 自动缩小候选集 |
| 平坦分布 | 可能截断合理候选 | 自动扩大候选集 |
| 常用值 | K = 40~100 | p = 0.9~0.95 |

> **Top-P 的核心优势**：候选集大小随分布形状动态变化，比 Top-K 更鲁棒。

---

## 6 Min-P Sampling

> 2023 年由社区提出，已被 llama.cpp / vLLM / HuggingFace 等主流框架采纳。

**原理**：设定一个相对阈值 $p_{\min}$，只保留概率 $\geq p_{\max} \times p_{\min}$ 的 token，其中 $p_{\max}$ 是当前步最高概率。

$$\text{Keep}(x_i) \iff P(x_i) \geq p_{\min} \cdot \max_j P(x_j)$$

### 为什么 Min-P 更好？

- **动态阈值**：与 Top-P 一样自适应，但直觉更清晰——"只保留与最佳候选相比不太差的"。
- **避免 Top-P 的尾部问题**：Top-P 在平坦分布下可能累积大量低质量 token 才达到阈值；Min-P 直接用相对比例截断。
- **更少超参数敏感性**：$p_{\min} = 0.05 \sim 0.1$ 在大多数场景下都表现不错。

---

## 7 重复惩罚机制

自回归模型天然倾向重复，以下三种惩罚从不同角度解决这个问题：

### 7.1 Repetition Penalty

对已出现过的 token 的 logits 施加乘性惩罚：

$$z_i' = \begin{cases} z_i / \theta & \text{if } z_i > 0 \text{ and } x_i \in \text{generated} \\ z_i \times \theta & \text{if } z_i < 0 \text{ and } x_i \in \text{generated} \end{cases}$$

- $\theta > 1$ 时降低已生成 token 的概率。
- 常用值：$\theta = 1.0 \sim 1.3$。

### 7.2 Frequency Penalty（OpenAI）

按 token **出现次数** 线性惩罚：$z_i' = z_i - \alpha \cdot \text{count}(x_i)$

- 出现越多惩罚越重，有效抑制高频重复。

### 7.3 Presence Penalty（OpenAI）

只看 token **是否出现过**（二值）：$z_i' = z_i - \beta \cdot \mathbb{1}[x_i \in \text{generated}]$

- 鼓励模型引入新话题，不关心重复次数。

| 惩罚类型 | 适用场景 |
|----------|----------|
| Repetition Penalty | 通用防重复 |
| Frequency Penalty | 抑制特定词的过度使用 |
| Presence Penalty | 鼓励话题多样性 |

---

## 8 实际应用中的组合策略

在生产环境中，**很少单独使用某一种策略**，而是组合使用：

### 最常见组合：Temperature + Top-P

```
temperature: 0.7
top_p: 0.9
```

- Temperature 控制整体随机性
- Top-P 截掉尾部噪声
- 两者互补：Temperature 调"放不放飞"，Top-P 调"兜底安全网"

### 典型场景配置

| 场景 | Temperature | Top-P | 其他 |
|------|-------------|-------|------|
| **代码生成** | 0.0 ~ 0.2 | 0.95 | 低温保证正确性 |
| **事实问答** | 0.0 ~ 0.3 | 0.9 | 接近 Greedy，减少幻觉 |
| **对话聊天** | 0.7 ~ 0.9 | 0.9 | 平衡自然度和准确性 |
| **创意写作** | 0.8 ~ 1.2 | 0.95 | 高温 + 宽候选集 |
| **头脑风暴** | 1.0 ~ 1.5 | 1.0 | 最大多样性 |

### 组合顺序

实际实现中，采样流程通常是：

```
logits → Temperature scaling → Top-K filter → Top-P filter → (Min-P filter) → Repetition Penalty → Sample
```

---

## 9 面试常见问题及回答要点

### Q1: Temperature 和 Top-P 都能控制随机性，为什么要同时用？

**答**：它们作用于不同层面。Temperature 改变整个概率分布的形状（熵），Top-P 在改变后的分布上截断尾部。单用 Temperature 高温可能采到极低概率的垃圾 token；单用 Top-P 无法控制分布的"尖锐/平坦"程度。组合使用实现了"先调形状，再切尾巴"的两阶段控制。

### Q2: Top-K 和 Top-P 哪个更好？为什么？

**答**：Top-P 通常更好，因为它是自适应的。Top-K 用固定数量 $K$ 截断，无法适应不同时间步的概率分布形状——模型确信时会引入噪声，不确信时会丢掉合理候选。Top-P 的候选集大小随分布动态变化，更鲁棒。但 Top-K 实现更简单，在某些框架中效率更高。

### Q3: 为什么 LLM 不用 Beam Search？

**答**：三个原因——（1）显存：每条 beam 需独立 KV Cache，$b$ 条 beam 显存翻 $b$ 倍，LLM 本身就很大；（2）质量：Beam Search 倾向生成高概率但"无聊"的文本，与开放式生成的需求矛盾；（3）效率：采样只需一次 forward pass，Beam Search 需要维护和比较多条候选。

### Q4: Greedy Decoding 一定比采样差吗？

**答**：不一定。在有唯一正确答案的任务（如数学计算、结构化输出、代码补全）中，Greedy（T=0）往往是最好的选择。采样引入的随机性在这些场景下反而是噪声。**没有"最好"的策略，只有最匹配任务的策略。**

### Q5: 如何选择采样参数？

**答**：实用经验——（1）先确定任务类型：确定性任务 → 低温/Greedy，开放式任务 → 中高温；（2）Temperature 定大方向，Top-P 做安全网，0.7/0.9 是不错的起点；（3）如果出现重复，加 Repetition Penalty 1.1-1.2；（4）A/B 测试，没有放之四海而皆准的参数。

### Q6: Min-P 会取代 Top-P 吗？

**答**：Min-P 在社区中获得了很大关注，它用相对阈值替代累积概率，直觉更清晰且避免了 Top-P 在平坦分布下的尾部问题。但 Top-P 已经是工业标准（OpenAI API 默认参数），短期内不太可能被取代。两者可以结合使用。

---

## 参考资料

- Holtzman et al. (2020). *The Curious Case of Neural Text Degeneration* — 提出 Top-P/Nucleus Sampling
- Fan et al. (2018). *Hierarchical Neural Story Generation* — Top-K Sampling
- Keskar et al. (2019). *CTRL* — Repetition Penalty
- Min-P Sampling: [GitHub PR in llama.cpp](https://github.com/ggerganov/llama.cpp/pull/3841)

---

## See Also

- [[AI/LLM/Inference/采样策略|采样策略（LLM 深度版）]] — 本文面试版，深度版含 Min-P/典型采样/Contrastive Search 前沿方案
- [[AI/Foundations/Inference/Speculative Decoding|Speculative Decoding]] — 采样策略的工程延伸：Speculative Decoding 的 token 验收用"修正采样"保证分布等价性
- [[AI/LLM/RL/Theory/RLRR-Reference-Guided-Alignment-Non-Verifiable|RLRR（参考引导对齐）]] — 采样策略与对齐的交叉：temperature/top-p 控制 diversity，RLRR 在 non-verifiable 任务上用参考引导取代 reward，两者共同决定输出分布
- [[AI/LLM/Inference/LLM推理优化2026全景|LLM 推理优化 2026 全景]] ⭐ — 推理优化全景；采样策略是推理质量控制的最后一环
