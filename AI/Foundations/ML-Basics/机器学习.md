---
title: "机器学习"
type: tutorial
domain: ai/foundations/ml-basics
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/foundations/ml-basics
  - type/tutorial
---
# 机器学习

机器学习是让计算机从数据中自动学习规律，而不需要显式编程的方法论。虽然深度学习在近年几乎垄断了 AI 的注意力，但经典机器学习的核心概念——偏差-方差权衡、正则化、交叉验证、特征工程——仍然是理解现代 AI 不可或缺的基础。

> 参考：西瓜书（https://github.com/datawhalechina/pumpkin-book?tab=readme-ov-file）  
> 主流机器学习算法：https://github.com/datawhalechina/daily-interview/tree/master

## 学习范式

### 监督学习（Supervised Learning）

给定标注数据 $\{(x_i, y_i)\}_{i=1}^N$，学习映射 $f: X \to Y$。

- **分类**：$y$ 是离散的（如垃圾邮件检测）
- **回归**：$y$ 是连续的（如房价预测）

核心算法：线性回归、逻辑回归、SVM、决策树、随机森林、XGBoost、神经网络

### 无监督学习（Unsupervised Learning）

只有输入 $\{x_i\}$，发现数据的内在结构。

- **聚类**：K-Means、DBSCAN、层次聚类
- **降维**：PCA、t-SNE、UMAP
- **密度估计**：GMM、KDE
- **自编码器**：学习压缩表示

### 自监督学习（Self-Supervised Learning）

从数据本身构造监督信号。现代 LLM 的预训练（next token prediction）本质上就是自监督学习。

- **对比学习**：SimCLR、MoCo、CLIP
- **掩码预测**：BERT 的 MLM、MAE 的 masked image modeling
- **自回归**：GPT 系列

### 强化学习（Reinforcement Learning）

Agent 通过与环境交互来学习策略。在 LLM 领域，RLHF/GRPO 等方法用 RL 来对齐模型。

## 核心概念

### 偏差-方差权衡（Bias-Variance Tradeoff）

模型的泛化误差可以分解为：

$$\text{Error} = \text{Bias}^2 + \text{Variance} + \text{Noise}$$

- **偏差（Bias）**：模型的假设空间太受限，欠拟合
- **方差（Variance）**：模型对训练数据过于敏感，过拟合

传统 ML 需要在两者之间找平衡。有趣的是，深度学习打破了这个权衡——**过参数化的模型可以同时实现低偏差和低方差**（double descent 现象）。但理解这个框架仍然有助于诊断模型问题。

### 正则化

控制模型复杂度，防止过拟合：

| 方法 | 机制 | 对应先验 |
|------|------|----------|
| L2 正则化（Ridge） | $\lambda ||\theta||^2$ | 高斯先验 |
| L1 正则化（Lasso） | $\lambda ||\theta||_1$ | 拉普拉斯先验，产生稀疏解 |
| Dropout | 随机丢弃神经元 | 近似贝叶斯推断 |
| Early Stopping | 提前停止训练 | 隐式正则化 |
| Data Augmentation | 扩充训练数据 | 不变性先验 |

### 交叉验证

评估模型泛化能力的黄金标准：

```python
# k-fold 交叉验证
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
```

在 LLM 时代，交叉验证不太实际（训练成本太高），但对于传统 ML 任务和超参数搜索仍然不可或缺。

## 经典算法速览

### 线性模型

$$f(x) = w^T x + b$$

简单、可解释、计算高效。逻辑回归在工业界仍然是 baseline 的首选。

### 决策树 & 集成方法

- **决策树**：通过递归二分来构建决策规则，可解释性极强
- **随机森林**：Bagging + 特征采样，减少方差
- **XGBoost/LightGBM**：Boosting，逐步纠正前一棵树的错误

在**结构化数据**（表格数据）上，XGBoost 系列至今仍然是最强的方法，在 Kaggle 竞赛中统治力惊人。**深度学习在结构化数据上并没有明显优势**。

### SVM（支持向量机）

$$\min_{w,b} \frac{1}{2}||w||^2 + C \sum_i \max(0, 1 - y_i(w^T x_i + b))$$

核心思想：找到最大间隔超平面。通过核函数技巧可以处理非线性问题。在小样本、高维数据上仍然有价值。

### k-NN（k 近邻）

不需要训练，直接在推理时找最近的 k 个邻居投票。简单但在高维空间中效率低下（维度灾难）。

**在 LLM 时代的回归**：RAG 系统的检索本质上就是在 embedding 空间中做最近邻搜索。

## 特征工程

在深度学习之前，特征工程是 ML 的核心竞争力。虽然深度学习号称"自动特征提取"，但在实践中：

1. **结构化数据**仍然需要特征工程
2. **数据预处理**（缺失值、异常值、编码）影响巨大
3. **领域知识**永远有价值

## 从 ML 到 DL 的范式转变

| 维度 | 传统 ML | 深度学习 |
|------|---------|----------|
| 特征 | 手工设计 | 自动学习 |
| 数据量 | 小数据有效 | 大数据越多越好 |
| 可解释性 | 相对透明 | 黑盒 |
| 计算需求 | CPU 即可 | 需要 GPU |
| 适用场景 | 结构化数据 | 非结构化数据（文本、图像、音频） |

## 相关

- [[深度学习]]
- [[信息论]]
- [[概率与分布]]
- [[连续优化]]
