---
title: "机器学习"
type: tutorial
domain: ai/foundations/ml-basics
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/foundations/ml-basics
  - type/tutorial
---
# 机器学习

[深度学习 500 问](https%3A%2F%2Fgithub.com%2Fscutan90%2FDeepLearning-500-questions)

## 机器学习方式

### **监督学习**

 特点：监督学习是使用已知正确答案的示例来训练网络。已知数据和其一一对应的标签，训练一个预测模型，将输入数据映射到标签的过程。

 常见应用场景：监督式学习的常见应用场景如分类问题和回归问题。

 算法举例：常见的有监督机器学习算法包括支持向量机(Support Vector Machine, SVM)，朴素贝叶斯(Naive Bayes)，逻辑回归(Logistic Regression)，K近邻(K-Nearest Neighborhood, KNN)，决策树(Decision Tree)，随机森林(Random Forest)，AdaBoost以及线性判别分析(Linear Discriminant Analysis, LDA)等。深度学习(Deep Learning)也是大多数以监督学习的方式呈现。

### **非监督式学习**

 定义：在非监督式学习中，数据并不被特别标识，适用于你具有数据集但无标签的情况。学习模型是为了推断出数据的一些内在结构。

 常见应用场景：常见的应用场景包括关联规则的学习以及聚类等。

 算法举例：常见算法包括Apriori算法以及k-Means算法。

### **半监督式学习**

 特点：在此学习方式下，输入数据部分被标记，部分没有被标记，这种学习模型可以用来进行预测。

 常见应用场景：应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，通过对已标记数据建模，在此基础上，对未标记数据进行预测。

 算法举例：常见算法如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM）等。

### **弱监督学习**

 特点：弱监督学习可以看做是有多个标记的数据集合，次集合可以是空集，单个元素，或包含多种情况（没有标记，有一个标记，和有多个标记）的多个元素。 数据集的标签是不可靠的，这里的不可靠可以是标记不正确，多种标记，标记不充分，局部标记等。已知数据和其一一对应的弱标签，训练一个智能算法，将输入数据映射到一组更强的标签的过程。标签的强弱指的是标签蕴含的信息量的多少，比如相对于分割的标签来说，分类的标签就是弱标签。

 算法举例：举例，给出一张包含气球的图片，需要得出气球在图片中的位置及气球和背景的分割线，这就是已知弱标签学习强标签的问题。

 在企业数据应用的场景下， 人们最常用的可能就是监督式学习和非监督式学习的模型。 在图像识别等领域，由于存在大量的非标识的数据和少量的可标识数据， 目前半监督式学习是一个很热的话题。

## 监督学习的经典流程

## 常用分类算法

[表格内容，请参考原文档]

算法

优点

缺点

Bayes 贝叶斯分类法

1）所需估计的参数少，对于缺失数据不敏感。
2）有着坚实的数学基础，以及稳定的分类效率。

1）需要假设属性之间相互独立，这往往并不成立。（喜欢吃番茄、鸡蛋，却不喜欢吃番茄炒蛋）。
2）需要知道先验概率。
3）分类决策存在错误率。

Decision Tree决策树

1）不需要任何领域知识或参数假设。
2）适合高维数据。
3）简单易于理解。
4）短时间内处理大量数据，得到可行且效果较好的结果。
5）能够同时处理数据型和常规性属性。

1）对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。
2）易于过拟合。
3）忽略属性之间的相关性。
4）不支持在线学习。

SVM支持向量机

1）可以解决小样本下机器学习的问题。
2）提高泛化性能。
3）可以解决高维、非线性问题。超高维文本分类仍受欢迎。
4）避免神经网络结构选择和局部极小的问题。

1）对缺失数据敏感。
2）内存消耗大，难以解释。
3）运行和调参略烦人。

KNN K近邻

1）思想简单，理论成熟，既可以用来做分类也可以用来做回归；
2）可用于非线性分类；
3）训练时间复杂度为O(n)；
4）准确度高，对数据没有假设，对outlier不敏感；

1）计算量太大。
2）对于样本分类不均衡的问题，会产生误判。
3）需要大量的内存。
4）输出的可解释性不强。

Logistic Regression逻辑回归

1）速度快。
2）简单易于理解，直接看到各个特征的权重。
3）能容易地更新模型吸收新的数据。
4）如果想要一个概率框架，动态调整分类阀值。

特征处理复杂。需要归一化和较多的特征工程。

Neural Network 神经网络

1）分类准确率高。
2）并行处理能力强。
3）分布式存储和学习能力强。
4）鲁棒性较强，不易受噪声影响。

1）需要大量参数（网络拓扑、阀值、阈值）。
2）结果难以解释。
3）训练时间过长。

Adaboosting

1）adaboost是一种有很高精度的分类器。
2）可以使用各种方法构建子分类器，Adaboost算法提供的是框架。
3）当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单。
4）简单，不用做特征筛选。
5）不用担心overfitting。

对outlier比较敏感

---

## See Also

- [[AI/Foundations/ML-Basics/机器学习|机器学习]] — 同方向综述笔记，两篇互补
- [[AI/Foundations/ML-Basics/模型评估|模型评估]] — 知识点中的评估方法详解
- [[AI/Foundations/ML-Basics/损失函数|损失函数]] — 各算法损失函数汇总
- [[AI/Foundations/_MOC|Foundations MOC]] — ML 基础全图谱
