---
title: "深度学习"
type: tutorial
domain: ai/foundations/dl-basics
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/foundations/dl-basics
  - type/tutorial
---
# 深度学习

深度学习是机器学习的一个子领域，核心思想是通过多层非线性变换（深度神经网络）自动学习数据的层次化表示。从 2012 年 AlexNet 在 ImageNet 上的突破开始，深度学习在短短十几年内彻底改变了 CV、NLP、语音、推荐系统等几乎所有 AI 领域。

## 核心概念

### 神经网络的本质

一个神经网络本质上是一个参数化的函数 $f(x; \theta)$，它将输入 $x$ 映射到输出 $y$。"深度"指的是这个函数由多层简单变换复合而成：

$$f = f_L \circ f_{L-1} \circ \cdots \circ f_1$$

每一层 $f_l$ 通常是一个线性变换加非线性激活：

$$f_l(x) = \sigma(W_l x + b_l)$$

**通用近似定理**告诉我们，一个足够宽的单层网络可以近似任意连续函数。但实践中，**深度**（多层）比**宽度**更高效——这是深度学习最核心的归纳偏置。

### 反向传播与梯度下降

训练神经网络的核心算法是**反向传播（Backpropagation）** + **梯度下降**：

1. **前向传播**：计算预测值 $\hat{y} = f(x; \theta)$
2. **计算损失**：$L = \mathcal{L}(\hat{y}, y)$
3. **反向传播**：利用链式法则计算 $\nabla_\theta L$
4. **参数更新**：$\theta \leftarrow \theta - \eta \nabla_\theta L$

链式法则是反向传播的数学基础：

$$\frac{\partial L}{\partial W_l} = \frac{\partial L}{\partial a_L} \cdot \frac{\partial a_L}{\partial a_{L-1}} \cdots \frac{\partial a_{l+1}}{\partial a_l} \cdot \frac{\partial a_l}{\partial W_l}$$

### 常用激活函数

| 函数 | 公式 | 特点 |
|------|------|------|
| ReLU | $\max(0, x)$ | 简单高效，但有 dead neuron 问题 |
| GELU | $x \cdot \Phi(x)$ | Transformer 标配，更平滑 |
| SwiGLU | $\text{Swish}(xW) \otimes (xV)$ | 现代 LLM 标配（LLaMA、Qwen） |
| Sigmoid | $1/(1+e^{-x})$ | 输出归 (0,1)，梯度消失严重 |

### 优化器

- **SGD + Momentum**：经典，收敛稳定但慢
- **Adam**：自适应学习率，几乎是默认选择
  $$m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t, \quad v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$$
  $$\theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{v}_t} + \epsilon} \hat{m}_t$$
- **AdamW**：解耦权重衰减，LLM 训练的标配
- **学习率调度**：Warmup + Cosine Decay 是最常见的组合

## 关键架构

### CNN（卷积神经网络）

利用**局部连接**和**权值共享**两个归纳偏置，天然适合处理具有空间结构的数据（图像、时序）。关键里程碑：AlexNet → VGG → ResNet → EfficientNet。

ResNet 引入的**残差连接**是深度学习最重要的架构创新之一：

$$y = F(x) + x$$

它解决了深层网络的梯度消失问题，使训练上百层甚至上千层网络成为可能。这个思想后来被 Transformer 全面继承。

### RNN / LSTM / GRU

处理序列数据的经典架构。LSTM 通过门控机制（遗忘门、输入门、输出门）解决了 vanilla RNN 的梯度消失问题。但 RNN 的串行计算特性严重限制了并行化，在 Transformer 出现后逐渐退出主流。

### Transformer

2017 年 "Attention Is All You Need" 提出，现在是几乎所有 SOTA 模型的基础。核心组件：

- **Self-Attention**：$\text{Attention}(Q,K,V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$
- **Multi-Head Attention**：多个注意力头并行，捕捉不同的语义关系
- **FFN**：两层全连接网络，是 Transformer 的"记忆模块"
- **残差连接 + Layer Norm**：稳定训练

Transformer 的革命性在于：**用注意力机制替代循环，实现了完全并行化**。

## 正则化技术

1. **Dropout**：训练时随机丢弃神经元，推理时缩放。简单有效的集成学习近似
2. **Batch Normalization**：加速训练、允许更大学习率。但在 NLP 中被 Layer Norm 替代
3. **Layer Normalization**：Transformer 的标配，对每个样本独立归一化
4. **RMSNorm**：Layer Norm 的简化版，去掉了均值中心化，现代 LLM 的默认选择
5. **权重衰减（L2 正则化）**：$L_{\text{total}} = L + \lambda ||\theta||^2$

## 学习资源

| 资源 | 适合谁 |
|------|--------|
| CS231n (Stanford) | CV 方向入门 |
| CS224n (Stanford) | NLP 方向入门 |
| d2l.ai（动手学深度学习） | 代码驱动学习 |
| 3Blue1Brown 神经网络系列 | 直觉建立 |
| Fast.ai | 实践优先 |

## 一点思考

深度学习的成功靠的不是理论上的优美，而是三件事的汇合：**数据规模**（互联网的数据洪流）、**算力**（GPU/TPU）、**简单且有效的归纳偏置**（卷积、注意力、残差）。

理论解释至今远远落后于实践。为什么 SGD 能找到好的极小值？为什么过参数化模型反而泛化好？这些问题还没有满意的答案。但这不妨碍它成为人类历史上最有影响力的工程工具之一。

## 相关

- [[AI/Foundations/ML-Basics/机器学习|机器学习]]
- [[AI/Foundations/Math/信息论|信息论]]
- [[AI/Foundations/Math/概率与分布|概率与分布]]
- [[AI/Foundations/Math/连续优化|连续优化]]
- [[AI/CV/ViT|ViT]]
- [[AI/LLM/Architecture/GPT|GPT]]
- [[AI/LLM/Architecture/BERT|BERT]]
