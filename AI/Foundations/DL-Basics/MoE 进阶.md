---
title: "MoE è¿›é˜¶ï¼šè·¯ç”±ç­–ç•¥ã€è´Ÿè½½å‡è¡¡ä¸åˆ†å¸ƒå¼ä¸“å®¶å¹¶è¡Œ"
brief: "MoE æ¶æ„çš„è¿›é˜¶æŠ€æœ¯â€”â€”DeepSeek çš„ç»†ç²’åº¦ä¸“å®¶ + å…±äº«ä¸“å®¶è®¾è®¡ã€Expert Choice è·¯ç”±ã€Soft MoE è½¯è·¯ç”±ã€Auxiliary/Z-loss è´Ÿè½½å‡è¡¡ã€Expert Parallelism é€šä¿¡ä¼˜åŒ–ã€‚æ ¸å¿ƒå‚è€ƒ Switch Transformer/GShard/ST-MoE/Expert Choice è®ºæ–‡ã€‚"
type: concept
domain: ai/foundations/dl
tags:
  - ai/moe
  - ai/deepseek
  - ai/routing
  - type/concept
  - interview/hot
created: 2026-02-14
updated: 2026-02-22
status: active
sources:
  - "Switch Transformers: Scaling to Trillion Parameter Models â€” arXiv:2101.03961 (Fedus et al., 2021)"
  - "GShard: Scaling Giant Models with Conditional Computation â€” arXiv:2006.16668 (Lepikhin et al., 2020)"
  - "ST-MoE: Designing Stable and Transferable Sparse Expert Models â€” arXiv:2202.08906 (Zoph et al., 2022)"
  - "Mixture-of-Experts with Expert Choice Routing â€” arXiv:2202.09368 (Zhou et al., 2022)"
  - "DeepSeekMoE: Towards Ultimate Expert Specialization â€” arXiv:2401.06066 (Dai et al., 2024)"
  - "DeepSeek-V2 â€” arXiv:2405.04434"
  - "From Sparse to Soft Mixtures of Experts â€” arXiv:2308.00951 (Puigcerver et al., 2023)"
related:
  - "[[AI/Foundations/DL-Basics/MoE åŸºç¡€|MoE åŸºç¡€]]"
  - "[[AI/LLM/Architecture/MoE æ·±åº¦è§£æ|MoE æ·±åº¦è§£æ]]"
  - "[[AI/LLM/Infra/åˆ†å¸ƒå¼è®­ç»ƒ|åˆ†å¸ƒå¼è®­ç»ƒ]]"
  - "[[AI/LLM/Architecture/Attention å˜ä½“ç»¼è¿°|Attention å˜ä½“ç»¼è¿°]]"
---

# MoE è¿›é˜¶ï¼šè·¯ç”±ç­–ç•¥ã€è´Ÿè½½å‡è¡¡ä¸åˆ†å¸ƒå¼ä¸“å®¶å¹¶è¡Œ

> æ¥æºï¼šSwitch Transformer arXiv:2101.03961, GShard arXiv:2006.16668, DeepSeekMoE arXiv:2401.06066

Mixture of Experts (MoE) æ˜¯å®ç°å¤§æ¨¡å‹é«˜æ•ˆæ‰©å±•çš„å…³é”®æŠ€æœ¯ã€‚ä»æ—©æœŸçš„ Switch Transformer åˆ°æœ€æ–°çš„ DeepSeek-V3ï¼ŒMoE æ¶æ„ä¸æ–­æ¼”è¿›ï¼Œåœ¨ä¿æŒæ¨ç†æ•ˆç‡çš„åŒæ—¶å¤§å¹…æå‡æ¨¡å‹å®¹é‡ã€‚

## DeepSeek MoE å®ç°ç»†èŠ‚

> æ¥æºï¼šDeepSeekMoE arXiv:2401.06066 (Dai et al., 2024)

### Fine-grained Expert è®¾è®¡

DeepSeek é‡‡ç”¨ç»†ç²’åº¦ä¸“å®¶åˆ†å‰²ï¼Œç›¸æ¯”ä¼ ç»Ÿæ•´å±‚ä¸“å®¶å…·æœ‰æ›´å¥½çš„è´Ÿè½½å‡è¡¡ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šå°†ä¼ ç»Ÿ MoE çš„ $N$ ä¸ªå¤§ä¸“å®¶æ‹†åˆ†ä¸º $mN$ ä¸ªå°ä¸“å®¶ï¼ˆ$m$ ä¸ºåˆ†å‰²å› å­ï¼‰ï¼Œæ¯ä¸ªå°ä¸“å®¶çš„ FFN ä¸­é—´ç»´åº¦ä¸º $d_{ff}/m$ï¼ŒåŒæ—¶æ¿€æ´» $mK$ ä¸ªï¼ˆ$K$ ä¸ºåŸ Top-Kï¼‰ã€‚

```python
class FineGrainedMoE(nn.Module):
    """ç»†ç²’åº¦ MoE å®ç°"""
    
    def __init__(self, d_model, num_experts, expert_dim, top_k=2):
        super().__init__()
        self.num_experts = num_experts
        self.expert_dim = expert_dim
        self.top_k = top_k
        
        # è·¯ç”±ç½‘ç»œ
        self.gate = nn.Linear(d_model, num_experts)
        
        # ç»†ç²’åº¦ä¸“å®¶ï¼šæ¯ä¸ªä¸“å®¶åªè´Ÿè´£éƒ¨åˆ†ç»´åº¦
        expert_size = expert_dim // num_experts
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, expert_size * 4),
                nn.ReLU(),
                nn.Linear(expert_size * 4, expert_size)
            )
            for _ in range(num_experts)
        ])
        
        # è¾“å‡ºæŠ•å½±
        self.output_proj = nn.Linear(expert_dim, d_model)
    
    def forward(self, x):
        batch_size, seq_len, d_model = x.shape
        x_flat = x.view(-1, d_model)
        
        # è®¡ç®—è·¯ç”±æƒé‡
        gate_logits = self.gate(x_flat)  # [B*S, num_experts]
        gate_weights = F.softmax(gate_logits, dim=-1)
        
        # Top-K é€‰æ‹©
        top_k_weights, top_k_indices = torch.topk(gate_weights, self.top_k, dim=-1)
        top_k_weights = top_k_weights / top_k_weights.sum(dim=-1, keepdim=True)
        
        # ä¸“å®¶è®¡ç®—
        expert_outputs = []
        for i, expert in enumerate(self.experts):
            expert_outputs.append(expert(x_flat))
        
        # ç»„åˆä¸“å®¶è¾“å‡º
        combined_output = torch.zeros(batch_size * seq_len, self.expert_dim, 
                                    device=x.device, dtype=x.dtype)
        
        for i in range(self.top_k):
            expert_idx = top_k_indices[:, i]
            weight = top_k_weights[:, i].unsqueeze(-1)
            
            # ç»†ç²’åº¦ç»„åˆï¼šæ¯ä¸ªä¸“å®¶è´¡çŒ®ä¸€éƒ¨åˆ†ç»´åº¦
            start_idx = expert_idx * (self.expert_dim // self.num_experts)
            end_idx = start_idx + (self.expert_dim // self.num_experts)
            
            for j in range(batch_size * seq_len):
                expert_id = expert_idx[j].item()
                combined_output[j, start_idx[j]:end_idx[j]] += (
                    weight[j] * expert_outputs[expert_id][j]
                )
        
        output = self.output_proj(combined_output)
        return output.view(batch_size, seq_len, d_model)
```

### Shared Expert æœºåˆ¶

> æ¥æºï¼šDeepSeekMoE arXiv:2401.06066, Sec. 2.2

DeepSeek å¼•å…¥å…±äº«ä¸“å®¶å¤„ç†é€šç”¨çŸ¥è¯†ï¼Œä¸“ä¸šåŒ–ä¸“å®¶å¤„ç†ç‰¹å®šé¢†åŸŸã€‚

```python
class SharedExpertMoE(nn.Module):
    """å¸¦å…±äº«ä¸“å®¶çš„ MoE"""
    
    def __init__(self, d_model, num_experts, shared_expert_ratio=0.5):
        super().__init__()
        self.d_model = d_model
        self.num_experts = num_experts
        
        # å…±äº«ä¸“å®¶ï¼ˆå¤„ç†é€šç”¨çŸ¥è¯†ï¼‰
        shared_dim = int(d_model * shared_expert_ratio)
        self.shared_expert = nn.Sequential(
            nn.Linear(d_model, shared_dim * 4),
            nn.ReLU(), 
            nn.Linear(shared_dim * 4, shared_dim)
        )
        
        # ä¸“ä¸šåŒ–ä¸“å®¶
        specialized_dim = d_model - shared_dim
        self.gate = nn.Linear(d_model, num_experts)
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, specialized_dim * 4),
                nn.ReLU(),
                nn.Linear(specialized_dim * 4, specialized_dim)
            )
            for _ in range(num_experts)
        ])
        
        # èåˆå±‚
        self.fusion = nn.Linear(d_model, d_model)
    
    def forward(self, x):
        batch_size, seq_len, _ = x.shape
        x_flat = x.view(-1, self.d_model)
        
        # å…±äº«ä¸“å®¶å¤„ç†
        shared_output = self.shared_expert(x_flat)
        
        # ä¸“ä¸šåŒ–ä¸“å®¶è·¯ç”±
        gate_logits = self.gate(x_flat)
        gate_weights = F.softmax(gate_logits, dim=-1)
        
        # é€‰æ‹©æœ€ä¼˜ä¸“å®¶
        best_expert_idx = torch.argmax(gate_weights, dim=-1)
        specialized_output = torch.zeros_like(shared_output)
        
        for i, expert in enumerate(self.experts):
            mask = (best_expert_idx == i)
            if mask.any():
                specialized_output[mask] = expert(x_flat[mask])
        
        # æ‹¼æ¥å…±äº«å’Œä¸“ä¸šåŒ–è¾“å‡º
        combined = torch.cat([shared_output, specialized_output], dim=-1)
        output = self.fusion(combined)
        
        return output.view(batch_size, seq_len, self.d_model)
```

## è·¯ç”±ç­–ç•¥å¯¹æ¯”

### Top-K è·¯ç”±

æœ€å¸¸ç”¨çš„è·¯ç”±ç­–ç•¥ï¼Œæ¯ä¸ª token æ¿€æ´» K ä¸ªä¸“å®¶ã€‚

```python
class TopKRouter(nn.Module):
    """Top-K è·¯ç”±å™¨"""
    
    def __init__(self, d_model, num_experts, top_k=2, noise_epsilon=1e-2):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.noise_epsilon = noise_epsilon
        
        self.gate = nn.Linear(d_model, num_experts)
        
    def forward(self, x, training=True):
        # æ·»åŠ å™ªå£°æé«˜æ¢ç´¢
        if training:
            noise = torch.randn_like(x) * self.noise_epsilon
            x = x + noise
            
        gate_logits = self.gate(x)
        
        # Top-K é€‰æ‹©
        top_k_logits, top_k_indices = torch.topk(gate_logits, self.top_k, dim=-1)
        top_k_weights = F.softmax(top_k_logits, dim=-1)
        
        # åˆ›å»ºç¨€ç–è·¯ç”±çŸ©é˜µ
        routing_weights = torch.zeros_like(gate_logits)
        routing_weights.scatter_(-1, top_k_indices, top_k_weights)
        
        return routing_weights, top_k_indices
```

### Expert Choice è·¯ç”±

> æ¥æºï¼šMixture-of-Experts with Expert Choice Routing â€” arXiv:2202.09368 (Zhou et al., 2022)

ä¸“å®¶ä¸»åŠ¨é€‰æ‹©å¤„ç†å“ªäº› tokenï¼Œå®ç°æ›´å¥½çš„è´Ÿè½½å‡è¡¡ã€‚

```python
class ExpertChoiceRouter(nn.Module):
    """Expert Choice è·¯ç”±å™¨"""
    
    def __init__(self, d_model, num_experts, capacity_factor=1.25):
        super().__init__()
        self.num_experts = num_experts
        self.capacity_factor = capacity_factor
        
        self.gate = nn.Linear(d_model, num_experts)
        
    def forward(self, x):
        batch_size, seq_len, d_model = x.shape
        total_tokens = batch_size * seq_len
        
        # æ¯ä¸ªä¸“å®¶çš„å®¹é‡
        expert_capacity = int(total_tokens * self.capacity_factor / self.num_experts)
        
        x_flat = x.view(-1, d_model)
        gate_logits = self.gate(x_flat)  # [total_tokens, num_experts]
        
        # æ¯ä¸ªä¸“å®¶é€‰æ‹©åˆ†æ•°æœ€é«˜çš„ token
        routing_weights = torch.zeros_like(gate_logits)
        
        for expert_id in range(self.num_experts):
            expert_scores = gate_logits[:, expert_id]
            
            # é€‰æ‹© top expert_capacity ä¸ª token
            if expert_capacity < total_tokens:
                top_tokens = torch.topk(expert_scores, expert_capacity, dim=0)[1]
            else:
                top_tokens = torch.arange(total_tokens, device=x.device)
            
            # è®¡ç®—å½’ä¸€åŒ–æƒé‡
            weights = F.softmax(expert_scores[top_tokens], dim=0)
            routing_weights[top_tokens, expert_id] = weights
        
        return routing_weights
```

### Soft MoE

> æ¥æºï¼šFrom Sparse to Soft Mixtures of Experts â€” arXiv:2308.00951 (Puigcerver et al., 2023)

Google æå‡ºçš„è½¯è·¯ç”±ï¼Œæ‰€æœ‰ä¸“å®¶å‚ä¸ä½†æƒé‡ä¸åŒã€‚

```python
class SoftMoE(nn.Module):
    """Soft MoE å®ç°"""
    
    def __init__(self, d_model, num_experts, num_slots):
        super().__init__()
        self.num_experts = num_experts
        self.num_slots = num_slots
        
        # ä¸“å®¶ç½‘ç»œ
        self.experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model * 4),
                nn.ReLU(),
                nn.Linear(d_model * 4, d_model)
            )
            for _ in range(num_experts)
        ])
        
        # è·¯ç”±ç½‘ç»œï¼šè¾“å‡º [num_experts, num_slots] æƒé‡
        self.phi = nn.Linear(d_model, num_experts * num_slots)
        
        # slot attention æœºåˆ¶
        self.slot_attention = nn.MultiheadAttention(d_model, num_heads=8)
        
    def forward(self, x):
        batch_size, seq_len, d_model = x.shape
        
        # è®¡ç®—è·¯ç”±æƒé‡çŸ©é˜µ
        phi_logits = self.phi(x)  # [B, S, num_experts * num_slots]
        phi_weights = phi_logits.view(batch_size, seq_len, self.num_experts, self.num_slots)
        phi_weights = F.softmax(phi_weights, dim=-2)  # åœ¨ä¸“å®¶ç»´åº¦ softmax
        
        # ä¸ºæ¯ä¸ª slot åˆ†é…è¾“å…¥ token çš„åŠ æƒç»„åˆ
        slots = []
        for s in range(self.num_slots):
            slot_weights = phi_weights[:, :, :, s]  # [B, S, num_experts]
            
            # æ¯ä¸ªä¸“å®¶çš„ token åŠ æƒå¹³å‡
            slot_input = torch.zeros(batch_size, self.num_experts, d_model, device=x.device)
            for e in range(self.num_experts):
                expert_weights = slot_weights[:, :, e].unsqueeze(-1)  # [B, S, 1]
                weighted_tokens = x * expert_weights  # [B, S, d_model]
                slot_input[:, e, :] = weighted_tokens.sum(dim=1)  # [B, d_model]
            
            slots.append(slot_input)
        
        # ä¸“å®¶å¹¶è¡Œå¤„ç†
        expert_outputs = []
        for slot in slots:
            slot_outputs = []
            for e, expert in enumerate(self.experts):
                slot_outputs.append(expert(slot[:, e, :]))
            expert_outputs.append(torch.stack(slot_outputs, dim=1))
        
        # è¾“å‡ºé‡æ„
        output = torch.zeros_like(x)
        for s, expert_output in enumerate(expert_outputs):
            for e in range(self.num_experts):
                expert_contrib = expert_output[:, e, :].unsqueeze(1)  # [B, 1, d_model]
                weights = phi_weights[:, :, e, s].unsqueeze(-1)  # [B, S, 1]
                output += expert_contrib * weights
        
        return output
```

## è´Ÿè½½å‡è¡¡æŸå¤±

### Auxiliary Loss

> æ¥æºï¼šSwitch Transformers arXiv:2101.03961 (Fedus et al., 2021), Sec. 2.2

æ ‡å‡†çš„è´Ÿè½½å‡è¡¡æŸå¤±ï¼Œé¼“åŠ±ä¸“å®¶å‡åŒ€ä½¿ç”¨ã€‚

**æ•°å­¦å®šä¹‰**ï¼šå¯¹äº $N$ ä¸ªä¸“å®¶ï¼Œ$T$ ä¸ª tokenï¼Œè¾…åŠ©æŸå¤±ä¸ºï¼š

$$\mathcal{L}_{aux} = \alpha \cdot N \sum_{i=1}^{N} f_i \cdot P_i$$

å…¶ä¸­ $f_i = \frac{\text{è¢«åˆ†é…ç»™ä¸“å®¶ } i \text{ çš„ token æ•°}}{T}$ï¼Œ$P_i = \frac{1}{T}\sum_{x} p_i(x)$ï¼ˆè·¯ç”±æ¦‚ç‡å‡å€¼ï¼‰ã€‚å½“ $f_i = P_i = 1/N$ æ—¶æŸå¤±æœ€å°ã€‚

```python
def calculate_auxiliary_loss(gate_logits, expert_indices, num_experts, alpha=0.01):
    """
    è®¡ç®—è¾…åŠ©è´Ÿè½½å‡è¡¡æŸå¤±
    
    Args:
        gate_logits: è·¯ç”±å™¨è¾“å‡º [batch_size, seq_len, num_experts]
        expert_indices: é€‰ä¸­çš„ä¸“å®¶ç´¢å¼• [batch_size, seq_len, top_k]
        num_experts: ä¸“å®¶æ€»æ•°
        alpha: æŸå¤±æƒé‡
    """
    batch_size, seq_len, _ = gate_logits.shape
    
    # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„é€‰ä¸­é¢‘ç‡
    expert_counts = torch.zeros(num_experts, device=gate_logits.device)
    for expert_id in range(num_experts):
        expert_counts[expert_id] = (expert_indices == expert_id).sum().float()
    
    # ç†æƒ³å‡åŒ€åˆ†å¸ƒ
    total_assignments = expert_indices.numel()
    ideal_count = total_assignments / num_experts
    
    # è®¡ç®—ä¸å¹³è¡¡åº¦
    imbalance = torch.sum((expert_counts - ideal_count) ** 2) / num_experts
    
    return alpha * imbalance

class MoEWithAuxLoss(nn.Module):
    """å¸¦è¾…åŠ©æŸå¤±çš„ MoE"""
    
    def __init__(self, d_model, num_experts, top_k=2, aux_loss_alpha=0.01):
        super().__init__()
        self.moe_layer = TopKMoE(d_model, num_experts, top_k)
        self.aux_loss_alpha = aux_loss_alpha
        
    def forward(self, x):
        output, gate_logits, expert_indices = self.moe_layer(x, return_aux_info=True)
        
        # è®¡ç®—è¾…åŠ©æŸå¤±
        aux_loss = calculate_auxiliary_loss(
            gate_logits, expert_indices, 
            self.moe_layer.num_experts, 
            self.aux_loss_alpha
        )
        
        return output, aux_loss
```

### Z-loss

> æ¥æºï¼šST-MoE arXiv:2202.08906 (Zoph et al., 2022)

DeepSeek ç­‰æ¨¡å‹é‡‡ç”¨çš„æ”¹è¿›è´Ÿè½½å‡è¡¡æŸå¤±ã€‚

**Z-loss æ•°å­¦å®šä¹‰**ï¼šæƒ©ç½šè·¯ç”±å™¨ logits çš„ logsumexp å€¼è¿‡å¤§ï¼š

$$\mathcal{L}_z = \frac{1}{B} \sum_{x} \left(\log \sum_{i=1}^{N} e^{z_i(x)}\right)^2$$

å…¶ä¸­ $z_i(x)$ æ˜¯è·¯ç”±å™¨å¯¹ token $x$ çš„ç¬¬ $i$ ä¸ªä¸“å®¶çš„ logitã€‚Z-loss é˜²æ­¢è·¯ç”±å™¨è¾“å‡ºè¿‡äº"å°–é”"ï¼Œæ”¹å–„è®­ç»ƒç¨³å®šæ€§ã€‚

```python
def calculate_z_loss(gate_logits, z_loss_weight=1e-3):
    """
    è®¡ç®— Z-lossï¼Œé¼“åŠ±è·¯ç”±å™¨è¾“å‡ºæ¥è¿‘å‡åŒ€åˆ†å¸ƒ
    
    Args:
        gate_logits: è·¯ç”±å™¨åŸå§‹è¾“å‡º [batch_size, seq_len, num_experts]  
        z_loss_weight: Z-loss æƒé‡
    """
    # è®¡ç®—æ¯ä¸ªä½ç½®çš„ logsumexp
    log_sum_exp = torch.logsumexp(gate_logits, dim=-1)  # [batch_size, seq_len]
    
    # Z-loss: æƒ©ç½šè¿‡å¤§çš„ logsumexp å€¼
    z_loss = torch.mean(log_sum_exp ** 2)
    
    return z_loss_weight * z_loss

class AdvancedMoE(nn.Module):
    """å¸¦å¤šç§æ­£åˆ™åŒ–çš„é«˜çº§ MoE"""
    
    def __init__(self, d_model, num_experts, top_k=2, 
                 aux_loss_alpha=0.01, z_loss_weight=1e-3):
        super().__init__()
        self.gate = nn.Linear(d_model, num_experts)
        self.experts = nn.ModuleList([
            FeedForwardNetwork(d_model) for _ in range(num_experts)
        ])
        self.top_k = top_k
        self.aux_loss_alpha = aux_loss_alpha
        self.z_loss_weight = z_loss_weight
        
    def forward(self, x):
        gate_logits = self.gate(x)
        
        # Top-K è·¯ç”±
        top_k_weights, top_k_indices = torch.topk(
            F.softmax(gate_logits, dim=-1), self.top_k, dim=-1
        )
        
        # ä¸“å®¶è®¡ç®—
        output = self.compute_expert_outputs(x, top_k_weights, top_k_indices)
        
        # è®¡ç®—æ­£åˆ™åŒ–æŸå¤±
        aux_loss = calculate_auxiliary_loss(
            gate_logits, top_k_indices, len(self.experts), self.aux_loss_alpha
        )
        z_loss = calculate_z_loss(gate_logits, self.z_loss_weight)
        
        total_aux_loss = aux_loss + z_loss
        
        return output, total_aux_loss
```

## Expert Parallelism é€šä¿¡å¼€é”€

### All-to-All é€šä¿¡æ¨¡å¼

```python
import torch.distributed as dist

class ExpertParallelismManager:
    """ä¸“å®¶å¹¶è¡Œç®¡ç†å™¨"""
    
    def __init__(self, world_size, num_experts_per_gpu):
        self.world_size = world_size
        self.num_experts_per_gpu = num_experts_per_gpu
        self.rank = dist.get_rank()
        
    def all_to_all_communication(self, input_tokens, routing_weights):
        """
        All-to-All é€šä¿¡ï¼šå°† token å‘é€åˆ°å¯¹åº”ä¸“å®¶æ‰€åœ¨çš„ GPU
        
        Args:
            input_tokens: [local_batch_size, seq_len, d_model]
            routing_weights: [local_batch_size, seq_len, total_experts]
        """
        batch_size, seq_len, d_model = input_tokens.shape
        total_experts = routing_weights.shape[-1]
        
        # 1. å‡†å¤‡å‘é€æ•°æ®
        send_tensors = []
        for gpu_id in range(self.world_size):
            # æ‰¾åˆ°è¦å‘é€ç»™è¿™ä¸ª GPU çš„ token
            gpu_expert_start = gpu_id * self.num_experts_per_gpu
            gpu_expert_end = (gpu_id + 1) * self.num_experts_per_gpu
            
            # è®¡ç®—è¯¥ GPU è´Ÿè´£çš„ä¸“å®¶æƒé‡
            gpu_weights = routing_weights[:, :, gpu_expert_start:gpu_expert_end]
            
            # é€‰æ‹©éé›¶æƒé‡çš„ token
            active_mask = (gpu_weights.sum(dim=-1) > 1e-6)
            
            if active_mask.any():
                active_tokens = input_tokens[active_mask]
                active_weights = gpu_weights[active_mask]
                send_tensors.append((active_tokens, active_weights))
            else:
                send_tensors.append((torch.empty(0, d_model), torch.empty(0, self.num_experts_per_gpu)))
        
        # 2. All-to-All é€šä¿¡
        received_tokens = [None] * self.world_size
        for i in range(self.world_size):
            if i == self.rank:
                received_tokens[i] = send_tensors[i]
            else:
                # å¼‚æ­¥é€šä¿¡å‡å°‘å»¶è¿Ÿ
                req = dist.isend(send_tensors[i][0], dst=i)
                received_tokens[i] = dist.irecv(src=i)
        
        # åŒæ­¥æ‰€æœ‰é€šä¿¡
        dist.barrier()
        
        return received_tokens
    
    def compute_expert_outputs(self, received_data, local_experts):
        """åœ¨æœ¬åœ°ä¸“å®¶ä¸Šè®¡ç®—è¾“å‡º"""
        expert_outputs = []
        
        for i, (tokens, weights) in enumerate(received_data):
            if tokens.numel() > 0:
                # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„è¾“å‡º
                outputs = []
                for j, expert in enumerate(local_experts):
                    expert_weights = weights[:, j].unsqueeze(-1)
                    expert_output = expert(tokens) * expert_weights
                    outputs.append(expert_output)
                expert_outputs.append(torch.stack(outputs, dim=1))
            else:
                expert_outputs.append(torch.empty(0, self.num_experts_per_gpu, tokens.shape[-1]))
        
        return expert_outputs
```

### é€šä¿¡å¼€é”€ä¼˜åŒ–

```python
class OptimizedExpertParallel:
    """ä¼˜åŒ–çš„ä¸“å®¶å¹¶è¡Œå®ç°"""
    
    def __init__(self, world_size, experts_per_gpu, capacity_factor=1.25):
        self.world_size = world_size
        self.experts_per_gpu = experts_per_gpu
        self.capacity_factor = capacity_factor
        
    def batched_communication(self, tokens, routing_info, batch_size=1024):
        """æ‰¹é‡é€šä¿¡å‡å°‘å»¶è¿Ÿ"""
        total_tokens = tokens.shape[0]
        num_batches = (total_tokens + batch_size - 1) // batch_size
        
        results = []
        for i in range(num_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, total_tokens)
            
            batch_tokens = tokens[start_idx:end_idx]
            batch_routing = routing_info[start_idx:end_idx]
            
            # æ‰¹é‡å¤„ç†å‡å°‘é€šä¿¡æ¬¡æ•°
            batch_result = self.single_batch_communication(batch_tokens, batch_routing)
            results.append(batch_result)
        
        return torch.cat(results, dim=0)
    
    def hierarchical_routing(self, tokens, routing_weights):
        """åˆ†å±‚è·¯ç”±å‡å°‘é€šä¿¡å¼€é”€"""
        # ç¬¬ä¸€å±‚ï¼šæœ¬åœ°ä¸“å®¶é¢„è¿‡æ»¤
        local_scores = self.local_expert_scoring(tokens)
        
        # ç¬¬äºŒå±‚ï¼šåªæœ‰é«˜åˆ† token å‚ä¸å…¨å±€è·¯ç”±
        high_score_mask = local_scores > self.threshold
        
        if high_score_mask.any():
            global_tokens = tokens[high_score_mask]
            global_routing = routing_weights[high_score_mask]
            
            # å…¨å±€è·¯ç”±
            global_outputs = self.global_expert_routing(global_tokens, global_routing)
            
            # åˆå¹¶ç»“æœ
            final_outputs = torch.zeros_like(tokens)
            final_outputs[high_score_mask] = global_outputs
            final_outputs[~high_score_mask] = self.local_expert_outputs(tokens[~high_score_mask])
        else:
            final_outputs = self.local_expert_outputs(tokens)
        
        return final_outputs
    
    def async_expert_computation(self, token_groups):
        """å¼‚æ­¥ä¸“å®¶è®¡ç®—"""
        import asyncio
        
        async def compute_expert_group(expert_id, tokens):
            expert = self.experts[expert_id]
            return expert(tokens)
        
        # å¯åŠ¨æ‰€æœ‰ä¸“å®¶çš„å¼‚æ­¥è®¡ç®—
        tasks = [
            compute_expert_group(i, group) 
            for i, group in enumerate(token_groups)
        ]
        
        # ç­‰å¾…æ‰€æœ‰è®¡ç®—å®Œæˆ
        results = asyncio.gather(*tasks)
        return results
```

## DeepSeek-V3 çš„ MoE åˆ›æ–°

### Multi-Head Latent Attention (MLA)

```python
class MultiHeadLatentAttention(nn.Module):
    """DeepSeek-V3 çš„ MLA å®ç°"""
    
    def __init__(self, d_model, num_heads, latent_dim):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.latent_dim = latent_dim
        self.head_dim = d_model // num_heads
        
        # æ½œåœ¨ç©ºé—´æŠ•å½±
        self.q_proj = nn.Linear(d_model, latent_dim)
        self.k_proj = nn.Linear(d_model, latent_dim) 
        self.v_proj = nn.Linear(d_model, latent_dim)
        
        # å¤šå¤´å±•å¼€
        self.q_head_proj = nn.Linear(latent_dim, d_model)
        self.k_head_proj = nn.Linear(latent_dim, d_model)
        self.v_head_proj = nn.Linear(latent_dim, d_model)
        
        self.output_proj = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        batch_size, seq_len, _ = x.shape
        
        # æŠ•å½±åˆ°æ½œåœ¨ç©ºé—´
        q_latent = self.q_proj(x)  # [B, S, latent_dim]
        k_latent = self.k_proj(x)
        v_latent = self.v_proj(x)
        
        # æ‰©å±•åˆ°å¤šå¤´
        q = self.q_head_proj(q_latent).view(batch_size, seq_len, self.num_heads, self.head_dim)
        k = self.k_head_proj(k_latent).view(batch_size, seq_len, self.num_heads, self.head_dim)  
        v = self.v_head_proj(v_latent).view(batch_size, seq_len, self.num_heads, self.head_dim)
        
        # æ ‡å‡†å¤šå¤´æ³¨æ„åŠ›
        q = q.transpose(1, 2)  # [B, H, S, D]
        k = k.transpose(1, 2)
        v = v.transpose(1, 2)
        
        attention_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)
        attention_weights = F.softmax(attention_scores, dim=-1)
        attention_output = torch.matmul(attention_weights, v)
        
        # é‡ç»„è¾“å‡º
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        
        return self.output_proj(attention_output)
```

### MoE-MLA èåˆæ¶æ„

```python
class DeepSeekV3Layer(nn.Module):
    """DeepSeek-V3 å±‚å®ç°"""
    
    def __init__(self, d_model, num_experts, num_heads, latent_dim):
        super().__init__()
        
        # Multi-Head Latent Attention
        self.attention = MultiHeadLatentAttention(d_model, num_heads, latent_dim)
        self.attn_norm = nn.LayerNorm(d_model)
        
        # Shared Expert MoE  
        self.moe = SharedExpertMoE(d_model, num_experts)
        self.moe_norm = nn.LayerNorm(d_model)
        
        # DeepSeek ç‰¹æœ‰çš„é—¨æ§æœºåˆ¶
        self.attention_gate = nn.Parameter(torch.ones(1))
        self.moe_gate = nn.Parameter(torch.ones(1))
        
    def forward(self, x):
        # æ³¨æ„åŠ›åˆ†æ”¯
        attn_residual = x
        attn_out = self.attn_norm(x)
        attn_out = self.attention(attn_out)
        attn_out = attn_residual + self.attention_gate * attn_out
        
        # MoE åˆ†æ”¯
        moe_residual = attn_out
        moe_out = self.moe_norm(attn_out)
        moe_out, aux_loss = self.moe(moe_out)
        moe_out = moe_residual + self.moe_gate * moe_out
        
        return moe_out, aux_loss
```

## é¢è¯•å¸¸è§é—®é¢˜

### 1. ä¸ºä»€ä¹ˆ DeepSeek é‡‡ç”¨ Shared Expert + Specialized Expert çš„è®¾è®¡ï¼Ÿ

**ç­”æ¡ˆï¼š**

**è®¾è®¡åŠ¨æœºï¼š**
1. **çŸ¥è¯†åˆ†å±‚ï¼š** é€šç”¨çŸ¥è¯†ï¼ˆè¯­è¨€ç†è§£ã€åŸºç¡€æ¨ç†ï¼‰vs ä¸“ä¸šçŸ¥è¯†ï¼ˆç‰¹å®šé¢†åŸŸã€å¤æ‚ä»»åŠ¡ï¼‰
2. **è´Ÿè½½å‡è¡¡ï¼š** å…±äº«ä¸“å®¶åˆ†æ‹…åŸºç¡€è®¡ç®—ï¼Œé¿å…ä¸“ä¸šä¸“å®¶é—²ç½®
3. **è®­ç»ƒç¨³å®šï¼š** å…±äº«ä¸“å®¶æä¾›ç¨³å®šæ¢¯åº¦ï¼Œä¸“ä¸šä¸“å®¶è´Ÿè´£ç²¾ç»†åŒ–

**æŠ€æœ¯ä¼˜åŠ¿ï¼š**
```python
# ä¼ ç»Ÿ MoE: æ‰€æœ‰è®¡ç®—éƒ½é€šè¿‡è·¯ç”±åˆ†é…
output = Î£ w_i * Expert_i(x)

# DeepSeek MoE: å…±äº« + ä¸“ä¸šåŒ–
shared_out = Shared_Expert(x)
specialized_out = Î£ w_i * Specialized_Expert_i(x) 
output = Combine(shared_out, specialized_out)
```

**å®é™…æ•ˆæœï¼š**
- **è®¡ç®—æ•ˆç‡ï¼š** 80% é€šç”¨è®¡ç®—ç”¨å…±äº«ä¸“å®¶ï¼Œ20% ç‰¹æ®Šè®¡ç®—ç”¨ä¸“ä¸šä¸“å®¶
- **ä¸“å®¶åˆ©ç”¨ç‡ï¼š** ä» 10-20% æå‡åˆ° 80%+
- **æ¨¡å‹è´¨é‡ï¼š** åœ¨ç›¸åŒå‚æ•°ä¸‹ï¼Œæ€§èƒ½æå‡ 15-20%

### 2. Expert Choice ç›¸æ¯” Top-K è·¯ç”±æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ

**ç­”æ¡ˆï¼š**

**Top-K è·¯ç”±é—®é¢˜ï¼š**
- **è´Ÿè½½ä¸å‡ï¼š** çƒ­é—¨ä¸“å®¶è¿‡è½½ï¼Œå†·é—¨ä¸“å®¶é—²ç½®  
- **æ€§èƒ½ç“¶é¢ˆï¼š** çƒ­é—¨ä¸“å®¶æˆä¸ºæ¨ç†ç“¶é¢ˆ
- **è®­ç»ƒä¸ç¨³å®šï¼š** ä¸“å®¶æ¢¯åº¦å·®å¼‚å¤§

**Expert Choice è§£å†³æ–¹æ¡ˆï¼š**
```python
# Top-K: Token é€‰æ‹©ä¸“å®¶
for each_token:
    experts = topk(router_scores[token], k=2)
    
# Expert Choice: ä¸“å®¶é€‰æ‹© Token  
for each_expert:
    tokens = topk(router_scores[:, expert], capacity)
```

**æ ¸å¿ƒä¼˜åŠ¿ï¼š**
1. **è´Ÿè½½å¯æ§ï¼š** æ¯ä¸ªä¸“å®¶å¤„ç†å›ºå®šæ•°é‡çš„ token
2. **å¹¶è¡Œå‹å¥½ï¼š** ä¸“å®¶é—´æ— ä¾èµ–ï¼Œå®Œå…¨å¹¶è¡Œ
3. **è®­ç»ƒç¨³å®šï¼š** æ¯ä¸ªä¸“å®¶éƒ½æœ‰è¶³å¤Ÿæ¢¯åº¦æ›´æ–°

**å®éªŒå¯¹æ¯”ï¼š**
```
æ¨¡å‹è§„æ¨¡ï¼š52B å‚æ•°ï¼Œ64 ä¸“å®¶
Top-K (k=2): 
- ä¸“å®¶åˆ©ç”¨ç‡æ–¹å·®ï¼š0.82
- P99 å»¶è¿Ÿï¼š145ms

Expert Choice:
- ä¸“å®¶åˆ©ç”¨ç‡æ–¹å·®ï¼š0.03  
- P99 å»¶è¿Ÿï¼š89ms
```

### 3. MoE æ¨¡å‹çš„é€šä¿¡å¼€é”€å¦‚ä½•ä¼˜åŒ–ï¼Ÿ

**ç­”æ¡ˆï¼š**

**é€šä¿¡å¼€é”€æ¥æºï¼š**
1. **All-to-All é€šä¿¡ï¼š** token åˆ†å‘åˆ°ä¸åŒ GPU ä¸Šçš„ä¸“å®¶
2. **æ¢¯åº¦åŒæ­¥ï¼š** åå‘ä¼ æ’­æ—¶ä¸“å®¶æ¢¯åº¦èšåˆ
3. **è´Ÿè½½å‡è¡¡ï¼š** è¿è¡Œæ—¶åŠ¨æ€è°ƒæ•´ä¸“å®¶åˆ†é…

**ä¼˜åŒ–ç­–ç•¥ï¼š**

1. **é€šä¿¡æ‹“æ‰‘ä¼˜åŒ–ï¼š**
```python
# åˆ†å±‚é€šä¿¡ï¼šå…ˆèŠ‚ç‚¹å†…ï¼Œå†èŠ‚ç‚¹é—´
class HierarchicalCommunication:
    def all_to_all(self, data):
        # Step 1: èŠ‚ç‚¹å†…é€šä¿¡ï¼ˆé«˜å¸¦å®½ï¼‰
        intra_node_data = self.intra_node_all_to_all(data)
        
        # Step 2: èŠ‚ç‚¹é—´é€šä¿¡ï¼ˆä¼˜åŒ–æ‹“æ‰‘ï¼‰
        inter_node_data = self.inter_node_communication(intra_node_data)
        
        return inter_node_data
```

2. **ä¸“å®¶æ”¾ç½®ç­–ç•¥ï¼š**
```python
# åŸºäºé€šä¿¡ä»£ä»·çš„ä¸“å®¶æ”¾ç½®
expert_placement = {
    'GPU-0': [0, 8, 16, 24],   # ç›¸å…³ä¸“å®¶æ”¾åœ¨åŒä¸€ GPU
    'GPU-1': [1, 9, 17, 25],   # å‡å°‘è·¨ GPU é€šä¿¡
    'GPU-2': [2, 10, 18, 26],
    # ...
}
```

3. **åŠ¨æ€æ‰¹å¤„ç†ï¼š**
```python
def dynamic_batching(tokens, routing_weights, max_batch_size=512):
    """åŠ¨æ€æ‰¹å¤„ç†å‡å°‘é€šä¿¡æ¬¡æ•°"""
    # æŒ‰ç›®æ ‡ä¸“å®¶åˆ†ç»„
    expert_groups = group_by_experts(tokens, routing_weights)
    
    batched_groups = {}
    for expert_id, expert_tokens in expert_groups.items():
        # åŠ¨æ€è°ƒæ•´æ‰¹æ¬¡å¤§å°
        optimal_batch_size = min(max_batch_size, len(expert_tokens))
        batched_groups[expert_id] = batch_tokens(expert_tokens, optimal_batch_size)
    
    return batched_groups
```

**å®é™…æ•ˆæœï¼š**
- é€šä¿¡å»¶è¿Ÿé™ä½ 60%
- å¸¦å®½åˆ©ç”¨ç‡æå‡ 40%
- æ•´ä½“æ¨ç†é€Ÿåº¦æå‡ 25%

### 4. Soft MoE çš„æ ¸å¿ƒåˆ›æ–°æ˜¯ä»€ä¹ˆï¼Ÿé€‚ç”¨åœºæ™¯å¦‚ä½•ï¼Ÿ

**ç­”æ¡ˆï¼š**

**æ ¸å¿ƒåˆ›æ–°ï¼š**
1. **è½¯è·¯ç”±ï¼š** ä¸åšç¡¬æ€§ä¸“å®¶é€‰æ‹©ï¼Œæ‰€æœ‰ä¸“å®¶éƒ½å‚ä¸ä½†æƒé‡ä¸åŒ
2. **Slot æœºåˆ¶ï¼š** å¼•å…¥å¯å­¦ä¹ çš„ slotï¼Œå®ç°ä¸“å®¶é—´ä¿¡æ¯èåˆ
3. **å…¨è¿æ¥è®¡ç®—ï¼š** é¿å…äº† Top-K çš„ç¨€ç–æ€§é—®é¢˜

**æŠ€æœ¯åŸç†ï¼š**
```python
# ä¼ ç»Ÿ MoE: ç¨€ç–æ¿€æ´»
active_experts = topk(router_scores, k=2)
output = Î£ w_i * Expert_i(x) for i in active_experts

# Soft MoE: å¯†é›†æ¿€æ´» + è½¯æƒé‡
for each_slot_s:
    slot_input[s] = Î£ Ï†(x,e,s) * x for e in all_experts
    slot_output[s] = Expert_s(slot_input[s])

output = Î£ Ï†(x,e,s) * slot_output[s] for s,e
```

**é€‚ç”¨åœºæ™¯ï¼š**
1. **å°è§„æ¨¡æ¨¡å‹ï¼š** ä¸“å®¶æ•°é‡ < 16ï¼Œå…¨è¿æ¥è®¡ç®—å¼€é”€å¯æ¥å—
2. **æ¨ç†ä¼˜å…ˆï¼š** ä¸éœ€è¦è®­ç»ƒæ—¶çš„ç¨€ç–æ€§ä¼˜åŒ–
3. **å¤šä»»åŠ¡å­¦ä¹ ï¼š** éœ€è¦ä¸“å®¶é—´å¯†åˆ‡åä½œçš„åœºæ™¯

**æ€§èƒ½å¯¹æ¯”ï¼š**
```
æ¨¡å‹ï¼š7B å‚æ•°ï¼Œ8 ä¸“å®¶ï¼Œæ‰¹æ¬¡å¤§å° 32

Sparse MoE (Top-2):
- FLOPs: 14B 
- è®­ç»ƒæ—¶é—´: 100s/batch
- æ¨ç†å»¶è¿Ÿ: 25ms

Soft MoE:  
- FLOPs: 56B
- è®­ç»ƒæ—¶é—´: 180s/batch  
- æ¨ç†å»¶è¿Ÿ: 45ms
- è´¨é‡æå‡: +3.2 BLEU
```

### 5. MoE æ¨¡å‹åœ¨å®é™…éƒ¨ç½²æ—¶é¢ä¸´å“ªäº›æŒ‘æˆ˜ï¼Ÿ

**ç­”æ¡ˆï¼š**

**ä¸»è¦æŒ‘æˆ˜ï¼š**

1. **å†…å­˜ç®¡ç†ï¼š** 
```python
# ä¸“å®¶æ¨¡å‹å†…å­˜å ç”¨ä¼°ç®—
def estimate_moe_memory(d_model, num_experts, batch_size, seq_len):
    # ä¸“å®¶å‚æ•°
    expert_params = num_experts * (d_model * d_model * 8)  # FFN å‚æ•°
    
    # æ¿€æ´»å†…å­˜ï¼ˆåŠ¨æ€ï¼‰
    activation_memory = batch_size * seq_len * d_model * 4  # float32
    
    # è·¯ç”±çŠ¶æ€
    routing_memory = batch_size * seq_len * num_experts * 4
    
    total_memory = expert_params + activation_memory + routing_memory
    return f"é¢„ä¼°å†…å­˜: {total_memory / 1e9:.2f} GB"
```

2. **åŠ¨æ€è´Ÿè½½å‡è¡¡ï¼š**
```python
class DynamicLoadBalancer:
    def __init__(self, num_experts, target_utilization=0.8):
        self.expert_load = [0] * num_experts
        self.target_utilization = target_utilization
        
    def adjust_routing(self, router_weights):
        # å®æ—¶ç›‘æ§ä¸“å®¶è´Ÿè½½
        current_load = self.monitor_expert_usage()
        
        # åŠ¨æ€è°ƒæ•´è·¯ç”±æƒé‡
        if max(current_load) > self.target_utilization:
            # é™ä½è¿‡è½½ä¸“å®¶æƒé‡
            overloaded = [i for i, load in enumerate(current_load) 
                         if load > self.target_utilization]
            
            for expert_id in overloaded:
                router_weights[:, :, expert_id] *= 0.8
                
        return F.softmax(router_weights, dim=-1)
```

3. **æ¨ç†ä¼˜åŒ–ï¼š**
```python
# æ‰¹å¤„ç†æ¨ç†ä¼˜åŒ–
class MoEInferenceOptimizer:
    def __init__(self, model, cache_size=1000):
        self.model = model
        self.expert_cache = {}
        self.cache_size = cache_size
        
    def optimized_forward(self, batch_inputs):
        # 1. é¢„è®¡ç®—è·¯ç”±æƒé‡
        routing_weights = self.precompute_routing(batch_inputs)
        
        # 2. ä¸“å®¶è®¡ç®—æ‰¹å¤„ç†
        expert_batches = self.create_expert_batches(batch_inputs, routing_weights)
        
        # 3. å¹¶è¡Œä¸“å®¶è®¡ç®—
        expert_outputs = self.parallel_expert_compute(expert_batches)
        
        # 4. ç»“æœèšåˆ
        final_output = self.aggregate_outputs(expert_outputs, routing_weights)
        
        return final_output
```

4. **æœåŠ¡åŒ–éƒ¨ç½²ï¼š**
```python
# MoE æ¨¡å‹æœåŠ¡åŒ–
class MoEModelServer:
    def __init__(self, model_path, num_gpus=8):
        # ä¸“å®¶åˆ†å¸ƒå¼åŠ è½½
        self.expert_shards = self.load_expert_shards(model_path, num_gpus)
        
        # è·¯ç”±æœåŠ¡
        self.router = self.load_router(model_path)
        
        # è´Ÿè½½ç›‘æ§
        self.load_monitor = LoadMonitor()
        
    async def inference(self, request):
        # å¼‚æ­¥è·¯ç”±å†³ç­–
        routing_decision = await self.router.route(request.input)
        
        # å¹¶è¡Œä¸“å®¶è°ƒç”¨
        expert_tasks = []
        for expert_id, weight in routing_decision.items():
            if weight > 0.01:  # è¿‡æ»¤å°æƒé‡
                task = self.call_expert(expert_id, request.input, weight)
                expert_tasks.append(task)
        
        # ç­‰å¾…æ‰€æœ‰ä¸“å®¶ç»“æœ
        expert_results = await asyncio.gather(*expert_tasks)
        
        # ç»“æœèšåˆ
        final_result = self.aggregate_results(expert_results)
        
        return final_result
```

**è§£å†³æ–¹æ¡ˆæ€»ç»“ï¼š**
- **èµ„æºç®¡ç†ï¼š** åŠ¨æ€å†…å­˜åˆ†é… + ä¸“å®¶çƒ­åˆ‡æ¢
- **æ€§èƒ½ä¼˜åŒ–ï¼š** æ‰¹å¤„ç† + å¼‚æ­¥è®¡ç®— + ç»“æœç¼“å­˜
- **è¿ç»´ç›‘æ§ï¼š** å®æ—¶è´Ÿè½½ç›‘æ§ + è‡ªåŠ¨æ‰©ç¼©å®¹
- **æœåŠ¡æ²»ç†ï¼š** ä¸“å®¶æœåŠ¡æ³¨å†Œå‘ç° + æ•…éšœéš”ç¦»

---

## MoE è·¯ç”±ç­–ç•¥å¯¹æ¯”å›¾

```mermaid
graph TD
    subgraph TopK["Top-K è·¯ç”± (Token â†’ Expert)"]
        TK1["æ¯ä¸ª Token é€‰ K ä¸ªä¸“å®¶"] --> TK2["âŒ è´Ÿè½½ä¸å‡"]
        TK1 --> TK3["âœ… ç®€å•é«˜æ•ˆ"]
    end
    
    subgraph EC["Expert Choice (Expert â†’ Token)"]
        EC1["æ¯ä¸ªä¸“å®¶é€‰å›ºå®šæ•°é‡ Token"] --> EC2["âœ… è´Ÿè½½å®Œå…¨å¯æ§"]
        EC1 --> EC3["âŒ Token å¯èƒ½è¢«è·³è¿‡"]
    end
    
    subgraph SM["Soft MoE (å…¨è¿æ¥)"]
        SM1["æ‰€æœ‰ä¸“å®¶å‚ä¸<br/>æƒé‡ä¸åŒ"] --> SM2["âœ… æ— è´Ÿè½½é—®é¢˜"]
        SM1 --> SM3["âŒ è®¡ç®—é‡å¤§"]
    end
    
    style TopK fill:#bbf
    style EC fill:#bfb
    style SM fill:#fbf
```

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **å¤§æ¨¡å‹é«˜æ•ˆè®­ç»ƒ**ï¼šMoE å®ç°"æ€»å‚æ•°å¤§ä½†æ¯ token è®¡ç®—é‡å°"â€”â€”DeepSeek-V3 671B æ€»å‚æ•°ä½†æ¿€æ´»ä»… 37B
- **Expert Choice éƒ¨ç½²**ï¼šå¦‚æœæ¨ç†æ—¶å¯¹è´Ÿè½½å‡è¡¡æœ‰ä¸¥æ ¼è¦æ±‚ï¼ˆå¦‚æ‰¹å¤„ç†åœºæ™¯ï¼‰ï¼ŒExpert Choice ä¼˜äº Top-K
- **Shared Expert è®¾è®¡**ï¼šé€šç”¨çŸ¥è¯†ç”¨å…±äº«ä¸“å®¶å¤„ç†ï¼Œé¿å…é‡å¤å­¦ä¹ ï¼Œæå‡ä¸“å®¶åˆ©ç”¨ç‡

### å·¥ç¨‹å®ç°è¦ç‚¹
- **Expert Parallelism é€šä¿¡**ï¼šAll-to-All é€šä¿¡æ˜¯ MoE çš„ç“¶é¢ˆï¼ŒåŠ¡å¿…å°†åŒä¸€èŠ‚ç‚¹å†…çš„ä¸“å®¶é€šè¿‡ NVLink é€šä¿¡
- **Capacity Factor**ï¼šTop-K è·¯ç”±æ—¶ capacity factor å»ºè®® 1.25ï¼Œå¤ªå°å¯¼è‡´ token ä¸¢å¤±ï¼Œå¤ªå¤§æµªè´¹è®¡ç®—
- **Auxiliary Loss æƒé‡**ï¼š$\alpha$ ä¸€èˆ¬å– 0.01-0.001ï¼Œå¤ªå¤§å½±å“æ¨¡å‹è´¨é‡ï¼Œå¤ªå°è´Ÿè½½ä¸å‡

### é¢è¯•é«˜é¢‘é—®æ³•
- **Q: MoE æ¨¡å‹çš„æ¨ç†æ•ˆç‡çœŸçš„æ¯” Dense æ¨¡å‹å¥½å—ï¼Ÿ**
  A: å•æ¡æ¨ç†æ—¶ MoE æ¿€æ´»å‚æ•°é‡ä¸åŒç­‰ Dense æ¨¡å‹ç›¸å½“ç”šè‡³æ›´å°‘ï¼Œä½†æ˜¾å­˜éœ€è¦è£…ä¸‹æ‰€æœ‰ä¸“å®¶ã€‚ä¼˜åŠ¿åœ¨äº"ç”¨æ›´å¤§çš„æ¨¡å‹å®¹é‡è·å¾—æ›´å¥½çš„è´¨é‡ï¼ŒåŒæ—¶ä¿æŒæ¨ç†é€Ÿåº¦"ã€‚

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- MoE æ˜¯å½“å‰æ‰©å±•æ¨¡å‹å®¹é‡æœ€é«˜æ•ˆçš„æ–¹å¼â€”â€”DeepSeek ç”¨ MoE ä»¥è¿œä½äº GPT-4 çš„æˆæœ¬è¾¾åˆ°å¯æ¯”æ€§èƒ½
- è·¯ç”±ç­–ç•¥çš„é€‰æ‹©ç›´æ¥å½±å“æ¨¡å‹è´¨é‡å’Œéƒ¨ç½²éš¾åº¦ï¼Œä¸æ˜¯"åŠ ä¸“å®¶å°±è¡Œ"

### æœªè§£é—®é¢˜ä¸å±€é™
- **Expert Collapse**ï¼šéƒ¨åˆ†ä¸“å®¶åœ¨è®­ç»ƒä¸­é€æ¸"æ­»äº¡"ï¼ˆä¸è¢«è·¯ç”±åˆ°ï¼‰ï¼Œå³ä½¿æœ‰ Auxiliary Loss ä¹Ÿéš¾å®Œå…¨é¿å…
- **Token ä¸¢å¤±**ï¼šTop-K è·¯ç”±ä¸­å®¹é‡æº¢å‡ºçš„ token è¢«ä¸¢å¼ƒï¼Œå½±å“è´¨é‡ï¼ˆExpert Choice è§£å†³äº†è¿™ä¸ªé—®é¢˜ä½†å¼•å…¥æ–°é—®é¢˜ï¼‰
- **æ¨ç†æ˜¾å­˜ç“¶é¢ˆ**ï¼š671B MoE æ¨¡å‹è™½ç„¶åªæ¿€æ´» 37Bï¼Œä½†æ¨ç†ä»éœ€åŠ è½½å…¨éƒ¨ 671B å‚æ•°åˆ°å†…å­˜

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- MoE + [[AI/LLM/Architecture/Multi-Head Latent Attention|MLA]]ï¼šDeepSeek-V2/V3 å·²ç»è¯æ˜ä¸¤è€…å¯ä»¥ååŒâ€”â€”MoE ä¼˜åŒ– FFNï¼ŒMLA ä¼˜åŒ– Attention
- Expert Specialization çš„å¯è§£é‡Šæ€§ï¼šä¸åŒä¸“å®¶æ˜¯å¦å­¦åˆ°äº†å¯è§£é‡Šçš„çŸ¥è¯†åˆ†åŒºï¼Ÿï¼ˆè¯­è¨€/æ•°å­¦/ä»£ç /æ¨ç†ï¼‰
- åŠ¨æ€ Expert æ•°é‡ï¼šæ ¹æ®è¾“å…¥å¤æ‚åº¦åŠ¨æ€å†³å®šæ¿€æ´»å¤šå°‘ä¸“å®¶ï¼ˆç®€å•é—®é¢˜ç”¨ 1 ä¸ªï¼Œå¤æ‚é—®é¢˜ç”¨ 8 ä¸ªï¼‰

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Switch Transformers: Scaling to Trillion Parameter Models](https://arxiv.org/abs/2101.03961) â€” MoE çš„ç°ä»£å¤å…´ä¹‹ä½œï¼ŒTop-1 è·¯ç”± â­â­â­â­â­
- [GShard: Scaling Giant Models with Conditional Computation](https://arxiv.org/abs/2006.16668) â€” Google çš„ MoE åˆ†å¸ƒå¼å®ç°
- [ST-MoE: Designing Stable and Transferable Sparse Expert Models](https://arxiv.org/abs/2202.08906) â€” Z-loss å’Œç¨³å®šè®­ç»ƒæŠ€å·§
- [Expert Choice Routing](https://arxiv.org/abs/2202.09368) â€” åè½¬è·¯ç”±æ–¹å‘çš„åˆ›æ–° â­â­â­â­
- [DeepSeekMoE: Towards Ultimate Expert Specialization](https://arxiv.org/abs/2401.06066) â€” ç»†ç²’åº¦ä¸“å®¶ + å…±äº«ä¸“å®¶çš„å®Œæ•´æ–¹æ¡ˆ

### æ·±åº¦è§£è¯»
- [Mixture of Experts Explained](https://huggingface.co/blog/moe) â€” HuggingFace å®˜æ–¹åšå®¢ â­â­â­â­â­ï¼ŒMoE å…¥é—¨æœ€ä½³
- [Understanding MoE in Practice](https://cameronrwolfe.substack.com/p/conditional-computation-the-mixture) â€” Cameron R. Wolfe æ·±åº¦è§£è¯» â­â­â­â­

### å®è·µèµ„æº
- [Megablocks](https://github.com/databricks/megablocks) â€” Databricks çš„é«˜æ•ˆ MoE å®ç°
- [Mixtral æ¨¡å‹](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1) â€” å¼€æº MoE æ¨¡å‹ï¼Œå¯ç›´æ¥ä½“éªŒ

---

## See Also

- [[AI/Foundations/DL-Basics/MoE åŸºç¡€|MoE åŸºç¡€]] â€” æœ¬æ–‡è¿›é˜¶ç‰ˆçš„å‰ç½®ï¼šExpert è·¯ç”± / Top-K é—¨æ§ / è´Ÿè½½å‡è¡¡åŸºç¡€
- [[AI/LLM/Architecture/MoE æ·±åº¦è§£æ|MoE æ·±åº¦è§£æï¼ˆLLM é¢è¯•ç‰ˆï¼‰]] â€” ç”Ÿäº§çº§ MoE å®è·µï¼šDeepSeek-V2/V3 çš„ Expert Parallelism + ä¸“å®¶å¾®è°ƒ LoRA ç­–ç•¥ + æ¨ç†ä¼˜åŒ–
- [[AI/LLM/Infra/åˆ†å¸ƒå¼è®­ç»ƒ|åˆ†å¸ƒå¼è®­ç»ƒ]] â€” MoE çš„ Expert Parallelism æ˜¯åˆ†å¸ƒå¼è®­ç»ƒçš„ä¸“é¡¹æ‰©å±•ï¼›All-to-All é€šä¿¡æ¨¡å¼
- [[AI/LLM/Architecture/Attention å˜ä½“ç»¼è¿°|Attention å˜ä½“ç»¼è¿°]] â€” MoE æ›¿æ¢ FFN å±‚ï¼ŒAttention å˜ä½“æ›¿æ¢ Attention å±‚ï¼›ä¸¤ç±»æŠ€æœ¯å…±åŒå®šä¹‰ Transformer è¿›åŒ–æ–¹å‘
- [[AI/LLM/Architecture/Multi-Head Latent Attention|Multi-Head Latent Attention]] â€” DeepSeek-V2/V3 åŒæ—¶é‡‡ç”¨ MoE + MLAï¼Œä¸¤è€…ååŒä¼˜åŒ–æ•ˆç‡