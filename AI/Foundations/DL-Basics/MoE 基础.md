---
title: "MOE 基础（一）"
type: concept
domain: ai/foundations/dl-basics
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/foundations/dl-basics
  - type/concept
---
# MOE 基础（一）

# 一、介绍

模型规模是提升模型性能的关键因素之一。**在有限的计算资源预算下，用更少的训练步数训练一个更大的模型，往往比用更多的步数训练一个较小的模型效果更佳。**

### 优势

混合专家模型 (MoE) 的一个显著优势是它们能够在远少于稠密模型所需的计算资源下进行有效的预训练。这意味着在相同的计算预算条件下，您可以显著扩大模型或数据集的规模。**特别是在预训练阶段，与稠密模型相比，混合专家模型通常能够更快地达到相同的质量水平。**

### 劣势

- 训练挑战: 虽然 MoE 能够实现更高效的计算预训练，但它们在微调阶段往往面临泛化能力不足的问题，长期以来易于引发过拟合现象。
- 推理挑战: MoE 模型虽然可能拥有大量参数，但在推理过程中只使用其中的一部分，这使得它们的推理速度快于具有相同数量参数的稠密模型。然而，这种模型需要将所有参数加载到内存中，因此对内存的需求非常高。以 Mixtral 8x7B 这样的 MoE 为例，需要足够的 VRAM 来容纳一个 47B 参数的稠密模型。之所以是 47B 而不是 8 x 7B = 56B，是因为在 MoE 模型中，只有 FFN 层被视为独立的专家，而模型的其他参数是共享的。此外，假设每个令牌只使用两个专家，那么推理速度 (以 FLOPs 计算) 类似于使用 12B 模型 (而不是 14B 模型)，因为虽然它进行了 2x7B 的矩阵乘法计算，但某些层是共享的。
### 原始 transformer

![image](assets/EGPIdd2fioDR3ZxtjSmcyDTfnwf.png)

先回顾原始 tranformer 的结构，然后看 MOE 如何应用。

**原始 transformer**：一个Transformer Bolck中主要包含三部分 MultiheadAttention(多头注意力)、FFN(前馈神经网络) 和 Add&Norm，其中的MultiheadAttention是由多层的 self-attention 搭建而来的，而FFN则是由两个线性变换层和激活函数组成的

### MOE 应用——Switch Transformers

混合专家模型 (MoE) 作为一种基于 Transformer 架构的模型，混合专家模型主要由两个关键部分组成:

- 稀疏 MoE 层: 这些层代替了传统 Transformer 模型中的前馈网络 (FFN) 层。MoE 层包含若干“专家”(例如 8 个)，每个专家本身是一个独立的神经网络。在实际应用中，这些专家通常是前馈网络 (FFN)，但它们也可以是更复杂的网络结构，甚至可以是 MoE 层本身，从而形成层级式的 MoE 结构。
- 门控网络或路由: 这个部分用于决定哪些令牌 (token) 被发送到哪个专家。例如，在下图中，“More”这个令牌可能被发送到第二个专家，而“Parameters”这个令牌被发送到第一个专家。有时，一个令牌甚至可以被发送到多个专家。令牌的路由方式是 MoE 使用中的一个关键点，因为路由器由学习的参数组成，并且与网络的其他部分一同进行预训练。
![image](assets/Clybd8mwdo2Szuxi8GHcocoinGb.png)

用混合专家模型 (MoE) 层替换了前馈网络 (FFN) 层。Switch Transformers 提出了一个 Switch Transformer 层，它接收两个输入 (两个不同的令牌) 并拥有四个专家。

与最初使用至少两个专家的想法相反，Switch Transformers 采用了简化的单专家策略。

### 发展

- 起源：混合专家模型 (MoE) 的理念起源于 1991 年的论文 [Adaptive Mixture of Local Experts](https%3A%2F%2Fwww.cs.toronto.edu%2F~hinton%2Fabsps%2Fjjnh91.pdf)。这个概念与集成学习方法相似，旨在为由多个单独网络组成的系统建立一个监管机制。在这种系统中，每个网络 (被称为“专家”) 处理训练样本的不同子集，**专注于输入空间的特定区域**。**门控网络决定了分配给每个专家的权重**。在训练过程中，这些专家和门控网络都同时接受训练，以优化它们的性能和决策能力。
- 发展：
- 组件专家: 在传统的 MoE 设置中，整个系统由一个门控网络和多个专家组成。在支持向量机 (SVMs) 、高斯过程和其他方法的研究中，MoE 通常被视为整个模型的一部分。然而，[Eigen、Ranzato 和 Ilya 的研究](https%3A%2F%2Farxiv.org%2Fabs%2F1312.4314) 探索了将 MoE 作为更深层网络的一个组件。
- 创新点：将 MoE 嵌入到多层网络中的某一层，使得模型既大又高效。
- 条件计算: 传统的神经网络通过每一层处理所有输入数据。在这一时期，Yoshua Bengio 等研究人员开始探索基于输入令牌动态激活或停用网络组件的方法。
- 创新点：激活或停用网络组件
-  稀疏性：137B 的 LSTM (由 Schmidhuber 提出)。通过引入稀疏性，这项工作在**保持极高规模的同时实现了快速的推理速度**。这项工作主要集中在翻译领域，但面临着如高通信成本和训练不稳定性等多种挑战。
# 二、稀疏性

- 传统：在传统的稠密模型中，所有的参数都会对所有输入数据进行处理。
- 稀疏：相比之下，稀疏性允许我们仅针对整个系统的某些特定部分执行计算。这意味着并非所有参数都会在处理每个输入时被激活或使用，而是根据输入的特定特征或需求，只有部分参数集合被调用和运行。
### 门控网络

一个可学习的门控网络 (G) 决定将输入的哪一部分发送给哪些专家 (E)

允许门控输出为 0， G (门控网络的输出) 为 0时，对应专家不激活，实现非激活状态。

- 经典门控函数 softmax：
- Shazeer 等人的工作还探索了其他的门控机制，其中包括带噪声的 TopK 门控 (Noisy Top-K Gating)。这种门控方法引入了一些可调整的噪声，然后保留前 k 个值。添加一些噪声：
1. 添加一些噪声
1. 选择保留前 K 个值
![image](assets/KTfZdOjJxosVBDxaDa2cSoCOnXd.png)

1. 应用 Softmax 函数
负无穷时 softmax 函数为 0，实现专家的非激活

关于稀疏的专家也有相应的观点：

只有所有专家都参与进来，混合专家才有效。如果只有一个专家处于活跃状态（例如， g(x)=[0,1,0,0]*g*(*x*)=[0,1,0,0] )，那么这就是浪费。

此外，如果我们一直处于这种状态，那么未使用的专家的梯度将为零，因此他们将不会收到任何梯度并得到改善。因此，使用混合专家的主要考虑因素之一是确保所有专家都能被输入使用。

# 三、训练 & 微调

### 并行训练

MOE 并行是并行计算的关键方式之一，专家被放置在不同的节点上。**如果与数据并行结合，每个节点拥有不同的专家，数据在所有节点之间分割。**

在专家并行中，专家被放置在不同的节点上，每个节点处理不同批次的训练样本。对于非 MoE 层，专家并行的行为与数据并行相同。对于 MoE 层，序列中的令牌被发送到拥有所需专家的节点。

下图是对并行方式的直观理解：

![image](assets/N1vadJ3XVoAHC3xIgIBcJUOKnTg.png)

### 路由

考虑任务级别的路由。

推荐材料，路由详解：https://www.53ai.com/news/finetuning/2024102523879.html

### 微调

**过拟合问题**：

稠密模型和稀疏模型在过拟合的动态表现上存在显著差异。稀疏模型更易于出现过拟合现象，因此在处理这些模型时，尝试更强的内部正则化措施是有益的，比如使用更高比例的 dropout。

- 可以为稠密层设定一个较低的 dropout 率
- 而为稀疏层设置一个更高的 dropout 率，以此来优化模型性能。
**适用场景**：

Switch Transformers 的作者观察到：

- 在相同的预训练困惑度下，稀疏模型在下游任务中的表现不如对应的稠密模型，特别是在重理解任务 (如 SuperGLUE) 上。
- 对于知识密集型任务 (如 TriviaQA)，稀疏模型的表现异常出色。
- 作者还观察到，在微调过程中，较少的专家的数量有助于改善性能。另一个关于泛化问题确认的发现是，模型在小型任务上表现较差，但在大型任务上表现良好。
![image](assets/Kqykd3bPSoFpK3xBJEgcKh7Hnmh.png)

**在小任务 (左图) 中，我们可以看到明显的过拟合，因为稀疏模型在验证集中的表现要差得多。在较大的任务 (右图) 中，MoE 则表现良好。该图来自 ST-MoE 论文**。

**freeze**：

- 冻结所有非专家层的权重：实践中，这会导致性能大幅下降，但这符合我们的预期，因为混合专家模型 (MoE) 层占据了网络的主要部分。
- 仅冻结 MoE 层的参数：这种方法几乎与更新所有参数的效果相当。这种做法可以加速微调过程，并降低显存需求。
![image](assets/BWzfdRQTkonCugxfSwJcq7ctnag.png)

**通过仅冻结 MoE 层，我们可以在保持质量的同时加快训练速度。该图来自 ST-MoE 论文。**

学习率 & batch size：

![image](assets/AMLodIqkYovPknxr8thch81lnHd.png)

**提高学习率和调小批量可以提升稀疏模型微调质量。该图来自 ST-MoE 论文。**

### 稀疏 vs 稠密

稀疏混合专家模型 (MoE) 适用于拥有多台机器且要求高吞吐量的场景。在固定的预训练计算资源下，稀疏模型往往能够实现更优的效果。相反，在显存较少且吞吐量要求不高的场景，稠密模型则是更合适的选择。
