---
title: "LLM 长上下文处理技术"
date: 2026-02-14
tags: [architecture, long-context, rope, attention, interview]
type: note
---

> [!warning] 重复笔记
> 同名深入版：[[AI/LLM/Architecture/长上下文处理]]
> 本篇为 Foundations 面试准备版，建议以 LLM 版为主

# LLM 长上下文处理技术

## 1. 长上下文的核心挑战

### 注意力机制的 O(n²) 复杂度

标准 Self-Attention 的计算和内存复杂度均为 O(n²)：

```
Attention(Q, K, V) = softmax(QKᵀ / √d_k) · V
```

- QKᵀ 矩阵大小为 n × n，n 为序列长度
- 序列长度翻倍 → 计算量和显存需求均增长 4 倍
- 128K tokens 的 QKᵀ 矩阵需要 ~64GB 显存（FP16），这还只是注意力本身

### KV Cache 显存爆炸

推理时为了避免重复计算，需要缓存每一层、每一个 token 的 Key 和 Value：

```
KV Cache 大小 = 2 × n_layers × n_heads × d_head × seq_len × dtype_size
```

以 Llama 3 70B 为例（80 层、64 头、128 维、FP16）：
- 4K context: ~2.5 GB
- 128K context: ~80 GB
- 1M context: ~640 GB（需要多卡 or 量化）

**优化手段：**
- **GQA（Grouped Query Attention）**：多个 Query Head 共享一组 KV Head，减少 KV Cache 大小（Llama 2 70B: 8 KV Heads vs 64 Q Heads → 8x 压缩）
- **MQA（Multi-Query Attention）**：极端版 GQA，所有 Query Head 共享 1 组 KV（但质量损失较大）
- **KV Cache 量化**：将缓存从 FP16 量化到 INT8/INT4
- **PagedAttention（vLLM）**：类似操作系统的分页机制，按需分配 KV Cache 显存

### 位置编码外推失败

大多数模型训练时只见过有限长度的位置编码（如 4K/8K），超出训练长度后位置编码信号退化，模型性能急剧下降。这不是 "拉长就能用" 的问题——需要专门的位置编码扩展技术。

---

## 2. RoPE 位置编码扩展

RoPE（Rotary Position Embedding）是当前主流的位置编码方式（Llama/Qwen/Mistral 等均采用），通过旋转矩阵编码相对位置信息。

### 基本原理

```
f(x_m, m) = x_m · e^{imθ}
```

对于 d 维向量，每两个维度为一组，旋转角度为：
```
θ_j = base^{-2j/d},  j = 0, 1, ..., d/2 - 1
```

其中 `base` 默认为 10000。

### NTK-aware Scaling

**思想：** 不是简单地缩放位置索引（线性插值），而是修改 RoPE 的 base 参数，在**高频维度保持分辨率、低频维度扩展范围**。

```python
# 线性插值（Position Interpolation）—— 损失局部分辨率
theta_j = base^{-2j/d} / scale_factor

# NTK-aware —— 保持高频分辨率
base_new = base * scale_factor^{d/(d-2)}
theta_j = base_new^{-2j/d}
```

**直觉：** 高频分量（j 小）编码局部位置关系，低频分量（j 大）编码全局位置。NTK-aware 只扩展低频部分，不损失局部精度。

### YaRN（Yet another RoPE extensioN）

在 NTK-aware 基础上增加两个改进：
1. **分段策略**：将 RoPE 维度分为三组，对不同频率的维度采用不同的缩放策略
   - 高频维度：不缩放（保持原样）
   - 低频维度：完全插值
   - 中间维度：线性插值过渡
2. **注意力缩放因子**：补偿位置插值导致的注意力分布变化

**效果：** Llama 2 7B 训练 4K → YaRN 扩展到 128K，PPL 仅略微上升，且只需极少量继续训练即可恢复。

### Dynamic NTK

**思想：** 不使用固定的缩放因子，而是根据当前序列的实际长度动态计算 base：

```python
if seq_len > trained_len:
    base_new = base * ((scale_factor * seq_len / trained_len) - (scale_factor - 1))^{d/(d-2)}
```

优势是无需提前知道目标上下文长度，推理时自适应调整。

---

## 3. 稀疏注意力

核心思路：不是所有 token 都需要关注所有其他 token，通过限制注意力模式降低复杂度。

### Sliding Window Attention（Mistral）

每个 token 只关注前面固定窗口内的 token：

```
窗口大小 W = 4096
token_i 只看 [i-W, i] 范围内的 token
```

**关键洞察：** 通过多层堆叠，信息可以间接传播到更远的位置：
- 第 1 层：每个 token 看到 W 个 token
- 第 L 层：理论感受野为 L × W 个 token
- Mistral 7B: W=4096, L=32 → 理论感受野 = 131K

**优势：** KV Cache 大小固定为 O(W)，不随序列长度增长。

### DeepSeek Sparse Attention（NSA）

DeepSeek V3/R1 使用的 **Native Sparse Attention** 设计：

- **压缩注意力（Compressed Attention）**：对历史 token 做分块压缩（block-wise pooling），用少量压缩 token 代表大段历史
- **选择性注意力（Selected Attention）**：基于压缩 token 的注意力分数，选择重要的原始 block 做精细注意力
- **滑动窗口注意力**：对最近的 token 保持全注意力

```
Query → 同时关注三个部分：
  1. 压缩后的全局信息（粗粒度，低成本）
  2. 被选中的重要历史 block（细粒度，按需）
  3. 最近的滑动窗口（细粒度，固定成本）
```

### Longformer Patterns

组合多种注意力模式：
- **局部滑动窗口**：所有 token 的默认模式
- **全局注意力**：特定 token（如 [CLS]、问题 token）可看到全部序列
- **空洞注意力（Dilated）**：间隔采样，扩大感受野

---

## 4. Ring Attention：分布式长序列处理

> 论文：*Ring Attention with Blockwise Transformers for Near-Infinite Context* (2024)

**解决的问题：** 单 GPU 显存放不下超长序列的 KV Cache。

**核心思想：** 将序列分块分配到多个 GPU 上，通过 **环形通信** 实现分布式注意力计算。

**工作流程：**

```
GPU 0: tokens [0, n/4)        KV blocks for chunk 0
GPU 1: tokens [n/4, n/2)      KV blocks for chunk 1  
GPU 2: tokens [n/2, 3n/4)     KV blocks for chunk 2
GPU 3: tokens [3n/4, n)       KV blocks for chunk 3

Step 1: 每个 GPU 用本地 Q 和本地 KV 计算局部注意力
Step 2: 将 KV block 沿环形传递给下一个 GPU
Step 3: 每个 GPU 用本地 Q 和接收到的 KV 计算部分注意力
Step 4: 重复 Step 2-3，直到每个 GPU 都看过所有 KV blocks
Step 5: 合并所有部分注意力结果（online softmax）
```

**关键技术：**
- **计算-通信重叠（Overlap）**：在传输下一个 KV block 的同时计算当前 block 的注意力
- **Blockwise Parallel Transformer**：按 block 计算 attention，内存需求从 O(n) 降到 O(block_size)
- **因果掩码优化**：自回归模型中，很多 block 对是零贡献的（未来不看过去），可以直接跳过

**效果：** 理论上可以处理任意长度的序列，上限取决于 GPU 数量而非单卡显存。

---

## 5. 上下文压缩

### Gisting / AutoCompressor

**思想：** 将长上下文压缩为少量的 "gist tokens"（摘要向量），作为 soft prompt 注入模型。

**AutoCompressor 流程：**
```
长文档（10000 tokens）
  → 分段处理：每段 1000 tokens
  → 每段通过模型生成 50 个 summary tokens
  → 所有 summary tokens 拼接（500 tokens）
  → 作为前缀 + 新查询 → 模型生成回答
```

**优势：** 20x 压缩比，推理时大幅减少 KV Cache
**劣势：** 有损压缩，细节信息可能丢失

### LLMLingua / LongLLMLingua

**思想：** 基于 perplexity（困惑度）的 **prompt 压缩**——去掉模型认为"显而易见"的 token。

**流程：**
1. 用小模型（如 GPT-2）计算 prompt 中每个 token 的 perplexity
2. 低 perplexity 的 token = 对模型来说信息量低 = 可以删除
3. 保留高 perplexity（信息量大）的 token
4. 压缩后的 prompt 送入大模型

**压缩效果：** 通常可以压缩 2-5x，且对答案质量影响很小。

**LongLLMLingua 改进：**
- 考虑 query 相关性：与问题相关的 token 即使 perplexity 低也保留
- 粗到细的压缩策略：先文档级筛选，再 token 级压缩

---

## 6. Landmark Attention / StreamingLLM

### Landmark Attention

**思想：** 在每个文档块的末尾插入一个 **landmark token**，全注意力只在 landmark tokens 之间计算，通过 landmark 间接访问对应块的内容。

```
[block_1 tokens] [LM_1] [block_2 tokens] [LM_2] ... [block_n tokens] [LM_n] [query]
                                                                                ↓
query 先 attend 到所有 LM tokens → 找到相关 block → 对相关 block 做全注意力
```

**效果：** 类似一个可学习的"目录"，让模型快速定位相关信息。

### StreamingLLM

> 论文：*Efficient Streaming Language Models with Attention Sinks* (2024)

**发现：** LLM 推理时，**最初几个 token（attention sinks）** 即使内容无关，也会吸引大量注意力分数。直接丢弃它们会导致输出崩坏。

**方案：** 保留初始 token + 滑动窗口的 KV Cache 组合：

```
KV Cache = [前 4 个 token（attention sinks）] + [最近 W 个 token（滑动窗口）]
```

**效果：** 用固定大小的 KV Cache 实现理论上无限长度的流式推理，但代价是中间历史信息会丢失（只有局部记忆）。

**适用场景：** 长对话、流式文本生成、实时翻译等不需要精确回忆远期信息的场景。

---

## 7. 实际应用：上下文长度的演进

### 里程碑

| 时间 | 模型 | 上下文长度 | 关键技术 |
|------|------|-----------|----------|
| 2023.03 | GPT-4 | 8K / 32K | - |
| 2023.07 | Claude 2 | 100K | - |
| 2023.11 | GPT-4 Turbo | 128K | - |
| 2024.02 | Gemini 1.5 Pro | 1M | Ring Attention + MoE |
| 2024.03 | Claude 3 | 200K | - |
| 2024.06 | Gemini 1.5 Pro | 2M（实验） | - |
| 2024.11 | Llama 3.1 | 128K | YaRN + 继续预训练 |
| 2025.01 | DeepSeek V3 | 128K | NSA + MLA |
| 2025.04 | Llama 4 Scout | 10M | - |
| 2025.06 | Claude 4 | 200K | - |

### 关键趋势

1. **MLA（Multi-head Latent Attention）**：DeepSeek 提出，将 KV 压缩到低维潜空间，KV Cache 压缩到 GQA 的 ~1/7
2. **MoE + 长上下文**：稀疏激活降低计算成本，使长上下文在效率上可行
3. **继续预训练（Continual Pre-training）**：在长文档数据上继续训练，让模型"学会"利用长上下文
4. **分阶段扩展**：先在短序列上训练，逐步增加序列长度（Llama 3.1: 8K → 128K）

---

## 8. 长上下文 ≠ 有效利用

### Needle in a Haystack 测试

在不同长度的文档中、不同位置插入一条关键信息（"needle"），测试模型能否准确提取。

**典型发现：**
- **Lost in the Middle**：大多数模型对文档中间位置的信息提取能力最差，开头和结尾较好
- **长度衰减**：随着上下文长度增加，即使在开头/结尾的信息，提取准确率也会下降
- **能力差异大**：标称支持 128K 的模型，在 64K 以上可能已经开始性能退化

### 多针测试（Multi-Needle）

插入多条分散的关键信息，测试模型能否同时找到所有信息并正确整合。这比单针测试更贴近实际应用，也更具挑战性。

### RULER Benchmark

更全面的长上下文评估：
- **单针/多针提取**
- **多键多值查找**
- **变量追踪（Variable Tracking）**：跨长上下文追踪变量赋值
- **聚合任务**：在长上下文中统计特定信息的出现次数等

**核心启示：**
> 标称上下文长度 ≠ 有效上下文长度。选择模型时，应关注其在 **有效长度**（effective context length）上的实际表现，而非 marketing 数字。

### 实践建议

1. **不要盲目塞满上下文**：更多内容 ≠ 更好的回答，噪声会降低质量
2. **关键信息前置**：利用 "primacy bias" 将最重要的内容放在 prompt 开头
3. **结合 RAG**：长上下文用于保持对话历史，RAG 用于精准知识检索，两者互补
4. **分块处理 + 汇总**：对超长文档，先分块提取再汇总，比一次性塞入效果更好

---

## 9. 面试常见问题及回答要点

### Q1：为什么需要位置编码？RoPE 相比 Sinusoidal PE 和 ALiBi 有什么优势？

**回答要点：**
> Transformer 的 Self-Attention 是置换不变的（permutation invariant），不含位置信息，所以需要显式注入位置信号。
>
> **三种主流方案对比：**
> - **Sinusoidal PE（原始 Transformer）**：绝对位置编码，加在输入上，理论上可外推但实际效果差
> - **ALiBi（BLOOM）**：不用位置编码，而是在注意力分数上加一个与距离成正比的偏置（距离越远惩罚越大），天然支持外推但偏好局部信息
> - **RoPE**：通过旋转矩阵编码**相对位置**，同时保留了绝对位置信息；与 NTK-aware/YaRN 结合后外推能力强；是目前 Llama/Qwen/Mistral 系列的标配
>
> RoPE 的核心优势在于将位置信息融入 Q、K 的内积运算，使得注意力分数天然包含相对位置信号。

### Q2：KV Cache 是什么？为什么是长上下文的瓶颈？有哪些优化方法？

**回答要点：**
> 自回归生成时，每个新 token 需要 attend 到之前所有 token 的 K 和 V。为了避免对历史 token 重复计算，把每层的 K、V 缓存起来就是 KV Cache。
>
> **瓶颈分析：** 以 70B 模型为例，128K 上下文的 KV Cache 约 80GB，接近一张 H100 的全部显存。
>
> **优化方法：**
> 1. **GQA**：减少 KV Head 数量（Llama 2 70B: 8 KV Heads → 8x 压缩）
> 2. **MLA**：将 KV 投影到低维潜空间（DeepSeek V3: ~7x 压缩 vs GQA）
> 3. **KV Cache 量化**：FP16 → INT8/INT4
> 4. **PagedAttention（vLLM）**：避免显存碎片化
> 5. **Token 级剪枝**：丢弃注意力分数低的 token 的 KV（如 H2O, ScissorHands）

### Q3：解释 Ring Attention 的原理，它如何实现超长上下文？

**回答要点：**
> Ring Attention 解决的是单 GPU 放不下整个序列 KV Cache 的问题。核心思路是把序列切分到多个 GPU 上，每个 GPU 持有一段 Q 和一段 KV。
>
> 关键机制是**环形传递**：每个 GPU 先用本地 KV 算一部分注意力，然后把自己的 KV block 传给环上的下一个 GPU，同时从前一个 GPU 接收新的 KV block，再算一轮。转 N-1 次后，每个 GPU 就看过了所有 KV。
>
> 两个工程关键点：
> 1. **计算-通信重叠**：在传输数据的同时做计算，隐藏通信延迟
> 2. **在线 softmax（Flash Attention 风格）**：部分注意力结果可以增量合并，不需要存全局 QKᵀ 矩阵

### Q4：StreamingLLM 的 attention sink 现象是什么？为什么会出现？

**回答要点：**
> Attention sink 是指 LLM 推理时，序列最开头的几个 token（通常 1-4 个）会吸引异常高的注意力分数，即使这些 token 的内容与当前生成完全无关。
>
> **可能原因：**
> - 自回归训练中，第一个 token 是所有后续 token 都能 attend 到的唯一公共 token
> - 模型可能学会把"不知道该关注谁"的注意力倾倒到第一个 token 上（类似垃圾桶机制）
> - softmax 的性质要求注意力分布归一化，总有 token 需要承担"填充"角色
>
> **实践意义：** 不能简单地用滑动窗口丢弃最早的 token，必须保留 attention sinks，否则生成质量急剧下降。StreamingLLM 正是利用这一发现实现了固定 KV Cache 的无限长推理。

### Q5：实际应用中，长上下文和 RAG 应该怎么选？可以结合吗？

**回答要点：**
> 不是二选一，而是互补关系：
>
> **长上下文的优势：** 保持完整的对话历史和会话状态；处理整本文档/代码库时无需切分；端到端简单，不需要检索基础设施
>
> **RAG 的优势：** 知识库可以无限大（不受上下文窗口限制）；检索精准度高于"大海捞针"；成本更低（只传相关片段）
>
> **最佳实践是结合使用：**
> - 长上下文用于：对话历史、当前工作文档、用户偏好等**会话级状态**
> - RAG 用于：知识库检索、实时信息获取等**知识级查询**
> - 具体比例取决于场景：客服系统更依赖 RAG，代码助手更依赖长上下文

---

## See Also

- [[AI/LLM/Architecture/长上下文技术|长上下文技术（LLM 深度版）]] — 本文面试版，深度版含 RoPE 外推数学推导 + YaRN/NTK-Aware 完整比较 + Ring Attention 实现
- [[AI/LLM/Architecture/Transformer 位置编码|Transformer 位置编码]] — 长上下文问题的核心：RoPE 外推困难源于位置编码的训练分布限制
- [[AI/Foundations/Inference/KV Cache|KV Cache]] — 长上下文的工程瓶颈：128K 上下文 KV Cache 显存占用是推理部署的核心挑战，与本文的 position encoding 问题形成"理论限制+工程限制"双重约束
- [[AI/RAG/Advanced RAG|Advanced RAG]] — 长上下文 vs RAG 的战略选择：本文面试高频问题（128K 上下文还是 RAG？）需要结合具体场景；"Lost in the Middle"问题使 RAG+精确 rerank 在某些场景仍更优
