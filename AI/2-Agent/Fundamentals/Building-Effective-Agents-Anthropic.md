---
title: "Building Effective Agents — Anthropic 智能体实践方法论"
brief: "Anthropic 一年智能体实践总结：最成功的≠最复杂的。五种工作流模式（提示链/分流/并行/领导-执行者/评估-优化）+ 智能体设计三原则（简单/透明/精心打造 ACI）"
date: 2026-02-26
source: https://www.anthropic.com/research/building-effective-agents
tags: [agent, workflow, anthropic, methodology, best-practice, ACI]
type: note
---

# Building Effective Agents — Anthropic 智能体实践方法论

> **核心主张**：最成功的智能体实现 ≠ 最复杂的。简单、可组合的模式，胜过花哨的框架。

## 1. 核心论点

### 能简单就别复杂

Anthropic 与数十个团队合作一年，发现最成功的 LLM Agent 实现都**没有用复杂框架或专用库**，而是采用简单、可组合的模式。推荐策略：

- **先从最简方案开始**，只在确实需要时才增加复杂度
- 很多场景下，**好 prompt + 检索（RAG）+ 上下文示例**就足够，不需要 agentic 系统
- Agentic 系统用**延迟和成本换任务质量**，需权衡这种 trade-off 是否值得

### 框架可以用，但必须理解底层

框架（如 Claude Agent SDK、Rivet、Vellum 等）降低了入门门槛，但：

- 额外的抽象层会**遮蔽底层 prompt 和响应**，增加调试难度
- 容易诱导开发者**过度增加复杂度**
- 建议：**从 LLM API 直接调用开始**，很多模式只需几行代码；如果用框架，务必理解底层代码

### 工作流 vs 智能体

Anthropic 的关键区分：

| | 工作流（Workflow） | 智能体（Agent） |
|---|---|---|
| **控制方式** | 预定义的代码路径编排 LLM 和工具 | LLM 动态自主决定流程和工具使用 |
| **灵活性** | 低——路径固定，可预测 | 高——根据输入和环境动态决策 |
| **适用场景** | 任务明确、步骤可预定义 | 开放性问题、步骤不可预测 |
| **成本/风险** | 较低、可控 | 较高、错误可能累积 |

---

## 2. 五种工作流模式

### 2.1 提示链（Prompt Chaining）

**模式**：任务分解为序列步骤，每步一个 LLM 调用，前一步输出 → 后一步输入。中间可加**程序化检查点（gate）**确保过程不偏。

**适用**：任务可清晰拆分为固定子任务，用延迟换精度。

**示例**：
- 先生成营销文案 → 再翻译成其他语言
- 先写文档大纲 → 检查大纲是否达标 → 基于大纲写正文

### 2.2 智能分流（Routing）

**模式**：对输入做分类 → 分发到不同的专门处理模块（各自有定制 prompt 和工具）。

**适用**：复杂任务有明确的类别划分，且分类可以准确完成。

**示例**：
- 客服系统：一般问题 / 退款请求 / 技术支持 → 分发到不同下游
- 简单问题 → 小模型（如 Haiku），复杂问题 → 强模型（如 Sonnet）

### 2.3 并行（Parallelization）

**模式**：多个 LLM 同时处理任务，输出通过程序聚合。两种变体：

- **切片（Sectioning）**：任务拆成独立子任务并行执行
- **投票（Voting）**：同一任务跑多次，多角度/多尝试取最优

**适用**：子任务相互独立可并行加速；或需要多视角提高结果置信度。

**示例**：
- Sectioning：一个模型处理用户请求，另一个并行做内容安全审查
- Voting：多个 prompt 并行审查代码漏洞，有发现就标记

### 2.4 领导-执行者（Orchestrator-Workers）

**模式**：中央 LLM（领导者）动态拆解任务 → 分派给多个 Worker LLM → 综合结果。

**关键区别**：与并行模式拓扑类似，但子任务**不是预定义的，而是由领导者根据具体输入动态决定**。

**适用**：复杂任务中子任务无法提前预测（如编码任务——需要改哪些文件、每个文件怎么改取决于具体任务）。

**示例**：
- 编码产品：每次修改涉及多个文件的复杂变更
- 搜索任务：从多个来源收集分析信息

### 2.5 评估-优化（Evaluator-Optimizer）

**模式**：一个 LLM 生成响应 → 另一个 LLM 评估并给反馈 → 循环迭代直至满意。

**适用条件**（两个信号）：
1. LLM 响应在人类反馈下可明显改善
2. LLM 本身能提供有效反馈

**示例**：
- 文学翻译：译者 LLM 初译 → 评估者 LLM 提出细节修改 → 迭代
- 复杂搜索：多轮搜索分析，评估者判断是否需要继续搜索

---

## 3. 真正的智能体（Autonomous Agent）

当 LLM 在理解复杂输入、推理规划、可靠使用工具、错误恢复方面足够成熟时，才适合使用真正的自主智能体。

### 运行机制

1. 接收用户指令或通过对话明确任务
2. **自主规划和执行**，在过程中可回到人类获取更多信息或判断
3. 每步从环境获取**真实反馈**（工具调用结果、代码执行结果）评估进展
4. 可在**检查点暂停**等待人类反馈，或遇到阻塞时求助
5. 通常在任务完成时终止，也可设最大迭代次数等**停止条件**

### 适用场景

- **开放性问题**，步骤数不可预测，无法硬编码固定路径
- 需要**信任 LLM 的决策能力**
- 在**受信环境**中可规模化执行

### 风险

- **成本更高**（多轮调用）
- **错误累积**（自主决策链中错误可能逐步放大）
- 建议：**沙箱环境充分测试 + 适当护栏**

### 实践案例（Anthropic 自己的）

- **SWE-bench 编码智能体**：根据 PR 描述自主修改多个文件解决 GitHub issue
- **Computer Use**：Claude 操作计算机完成任务

---

## 4. 智能体设计三原则

### 原则一：保持简单（Maintain Simplicity）

不要追求最精巧的系统，而是构建**对需求最合适的**系统。从简单 prompt 开始，用全面评估优化，只在简单方案不够时才引入多步 agentic 系统。

### 原则二：确保透明（Prioritize Transparency）

**明确展示智能体的规划步骤**，让用户和开发者能理解和审查智能体的决策过程。

### 原则三：精心打造 ACI（Agent-Computer Interface）

> 在 HCI（人-计算机接口）上投入多少精力，就应该在 ACI（智能体-计算机接口）上投入同等精力。

**工具设计要点**：
- **站在模型角度思考**：仅凭描述和参数，是否显然知道怎么用？如果你自己都要想半天，模型也一样
- **好的工具定义**应包含：使用示例、边界情况、输入格式要求、与其他工具的清晰边界
- **参数命名和描述要直觉化**，像给初级开发者写一个好的 docstring
- **大量测试工具使用**，在 workbench 中跑多种输入看模型犯什么错，然后迭代
- **防呆设计（Poka-yoke）**：修改参数使犯错更困难
  - 示例：SWE-bench 中发现模型用相对路径出错 → 改为强制要求绝对路径 → 错误消失

**工具格式选择**：
- 给模型足够 token 先"思考"，别让它写到一半才发现写进死角
- 格式尽量接近模型在互联网文本中自然见过的
- 避免格式开销（如需要精确计数代码行数、在 JSON 中转义代码等）

---

## 5. 两大落地场景

### 客服（Customer Support）

- 天然的对话流 + 需要访问外部信息和执行操作
- 可集成工具拉取客户数据、订单历史、知识库
- 退款、更新工单等操作可程序化处理
- 成功可通过用户确认的解决率清晰度量
- 一些公司已按**成功解决计费**，展示对 Agent 效果的信心

### 编码（Coding Agents）

- 代码方案可通过**自动化测试验证**
- Agent 可利用测试结果作为反馈迭代
- 问题空间结构化、输出质量可客观度量
- SWE-bench Verified 基准测试中，Agent 已能仅凭 PR 描述解决真实 GitHub issue
- 但**人类审查仍然关键**——确保方案符合更广泛的系统需求

---

## 6. 关键洞察总结

1. **能简单就别复杂**——好 prompt + 检索往往就够，只在确实需要时才加 agentic 系统
2. **框架是入门工具，不是生产依赖**——理解底层代码是必须的
3. **工作流 ≠ 智能体**——前者是预定义路径编排，后者是 LLM 动态自主控制
4. **五种工作流模式可灵活组合**——不是教条，是可塑造的构建块
5. **自主性 = 更高成本 + 错误累积风险**——需要护栏和沙箱测试
6. **工具集设计和文档清晰度是核心竞争力**——Anthropic 在 SWE-bench 上花在工具优化上的时间比 prompt 还多
7. **透明度和人类监督不可或缺**——即使是最强的 Agent 也需要检查点和人类审查

## See Also

- [[AI/2-Agent/Fundamentals/2025-AI-Agent-Index-MIT报告|2025 AI Agent Index（MIT 报告）]] — Agent 市场现状调研：学术实践方法论（Anthropic）的外部背景
- [[AI/2-Agent/Agentic-RL/Agentic-RL-2026前沿综合分析|Agentic RL 2026 前沿综合分析]] — 本文是 Anthropic 的实践方法论，综合分析是学术前沿——互为"工程侧 vs 研究侧"视角
- [[AI/2-Agent/Fundamentals/Agent-Tool-Use|Agent Tool Use]] — Anthropic 重点强调的 ACI 设计与工具调用深度
- [[AI/2-Agent/Multi-Agent/Multi-Agent 概述|Multi-Agent 概述]] — Anthropic 分析的 orchestrator-subagent 架构模式