---
title: Agentic RL 2026 å‰æ²¿ç»¼åˆåˆ†æ â€” äº”å¤§ç»´åº¦ä¸å¯¹åº”è§£æ³•
brief: Agentic RL äº”å¤§æ ¸å¿ƒç»´åº¦ç»¼åˆåˆ†æï¼šCredit Assignmentï¼ˆGRPOâ†’LOOPâ†’GiGPOâ†’AgentPRMâ†’iStarâ†’MIGâ†’HiPERâ†’CSOï¼Œ10æ–¹æ¡ˆå…¨è¦†ç›–ï¼Œå«åäº‹å®éªŒè¯æ–°ç»´åº¦ï¼‰/ Reward Designï¼ˆverifiable/unverifiable/checklist + Search-R1++ + SELAUR uncertaintyï¼‰/ Environment Engineering / Workflow Design / Context Overflowï¼›Multi-Turn RL å››æ”¯æŸ±ï¼ˆTSR/Credit/SCoRe/ERLï¼‰ï¼›å¤±è´¥è½¨è¿¹åˆ©ç”¨ä¸‰ç»´è°±ç³»ï¼ˆCSOæ·±/ERLä¸­/SELAURæµ…ï¼‰ï¼›è®­ç»ƒå¤±è´¥æ¨¡å¼è·¨æ¨¡æ€è°±ç³»ï¼ˆEcho Trapæ–‡æœ¬ / Interaction Collapseå¤šæ¨¡æ€ï¼ŒPyVision-RLï¼‰ï¼›é¢è¯•æ­¦å™¨çº§ç»¼è¿°ï¼ˆv10ï¼Œ2026-02-25ï¼‰
date: 2026-02-21
updated: 2026-02-26-v11
type: synthesis
tags:
  - agentic-RL
  - credit-assignment
  - reward-design
  - environment
  - workflow-design
  - topology
  - synthesis
  - 2026
related:
  - "[[AI/2-Agent/Multi-Agent/Kimi-K2.5-PARL|Kimi-K2.5-PARL]]"
  - "[[AI/2-Agent/Agentic-RL/CM2-Checklist-Rewards-Multi-Turn-Tool-Use-RL|CM2]]"
  - "[[AI/2-Agent/Agentic-RL/HiPER-Hierarchical-Plan-Execute-RL-Credit-Assignment|HiPERï¼ˆICML 2026ï¼‰]]"
  - "[[AI/2-Agent/Agentic-RL/EnterpriseGym-Corecraft|EnterpriseGym-Corecraft]]"
  - "[[AI/3-LLM/RL/ç®—æ³•/OpenRS-Pairwise-Adaptive-Rubric|OpenRS-Pairwise-Adaptive-Rubric]]"
  - "[[AI/2-Agent/Agentic-RL/FlowSteer-CWRPO-Workflow-Orchestration-RL|FlowSteer-CWRPO-Workflow-Orchestration-RL]]"
  - "[[AI/2-Agent/Multi-Agent/AgentConductor-Topology-Evolution|AgentConductor]]"
  - "[[AI/2-Agent/Agentic-RL/SquRL-Dynamic-Workflow-Text-to-SQL|SquRL-Dynamic-Workflow-Text-to-SQL]]"
  - "[[AI/2-Agent/Agentic-RL/PA-MoE-Phase-Aware-Mixture-of-Experts|PA-MoE-Phase-Aware-Mixture-of-Experts]]"
---

# Agentic RL 2026 å‰æ²¿ç»¼åˆåˆ†æ â€” äº”å¤§ç»´åº¦ä¸å¯¹åº”è§£æ³•

> v2.0ï¼ˆ2026-02-21ï¼‰ï¼šæ¡†æ¶ä»ã€Œä¸‰å¤§éš¾é¢˜ã€å‡çº§ä¸ºã€Œå››å¤§ç»´åº¦ã€ï¼Œæ–°å¢ Workflow/Topology è®¾è®¡ç»´åº¦ï¼Œè¡¥å…… FlowSteer/AgentConductor/SquRL/PA-MoE ç­‰æ–°å·¥ä½œã€‚
> v3.0ï¼ˆ2026-02-23ï¼‰ï¼šæ–°å¢ç¬¬äº”ç»´åº¦ Context Overflowï¼ˆKLongï¼‰ï¼ŒCredit Assignment è°±ç³»å‡çº§ä¸º 6 æ–¹æ¡ˆå…¨è¦†ç›–ï¼Œæ–°å¢ TSR/iStar/MIG/CM2 å®Œæ•´åˆ†æï¼Œè¡¥å…… unverifiable reward å®Œæ•´è§£æ³•è°±ç³»ã€‚
> v4.0ï¼ˆ2026-02-24ï¼‰ï¼šè®­ç»ƒç®—æ³•ç»´åº¦æ–°å¢ Multi-turn RL ç¨³å®šæ€§ä¸“é¡¹ï¼ˆRAGEN/StarPO Echo Trap + StarPO-S ä¸‰æœºåˆ¶ + Rollout ä¸‰å› å­ï¼‰ï¼›Workflow ç»´åº¦è¡¥å…… AdaptOrchï¼ˆæ¨ç†æ—¶ç¼–æ’æ‹“æ‰‘è‡ªé€‚åº”è·¯ç”±ï¼‰ï¼›å…¨æ™¯è¡¨æ›´æ–°è‡³ 2/24ã€‚
> v5.0ï¼ˆ2026-02-24ï¼‰ï¼šCredit Assignment è°±ç³»æ–°å¢ **HiPERï¼ˆICML 2026ï¼Œâ˜…â˜…â˜…â˜…â˜…ï¼‰**â€”â€”æ˜¾å¼å±‚çº§åŒ–æ–°ç»´åº¦ï¼ˆsubgoal-segment ç²’åº¦ vs åŸæœ‰ step/trajectory ç²’åº¦ï¼‰ï¼›HAE æœ‰åŒé‡ç†è®ºä¿è¯ï¼ˆæ— åæ€§ + æ–¹å·®å‡å°‘ï¼‰ï¼›ALFWorld 97.4% SOTAï¼›æ›´æ–°å…¨æ™¯è¡¨ HiPER è¯„åˆ†ï¼›æ–°å¢ AWM ç¯å¢ƒå·¥ç¨‹æ¡ç›®ï¼›æ›´æ–°"å®Œæ•´åœ°å›¾"åŠ å…¥ subgoal-segment å±‚çº§ã€‚
> v6.0ï¼ˆ2026-02-25ï¼‰ï¼š**iStar æ­£å¼å…¥è¡¨**ï¼ˆ2509.19199ï¼ŒTongyi Labï¼Œâ˜…â˜…â˜…â˜…â˜…ï¼‰â€”â€”trajectory DPO â‰¡ step-wise BT model ç†è®ºï¼Œå”¯ä¸€é€‚ç”¨äº unverifiable reward çš„ step-level credit assignment æ–¹æ¡ˆï¼ŒSOTOPIA +48%ï¼›æ–°å¢ **Search-R1++ å…³é”®å®éªŒå‘ç°**ï¼ˆ2602.19526ï¼‰åˆ° Reward Design éƒ¨åˆ†ï¼šREINFORCE > PPO > GRPO ç¨³å®šæ€§ï¼ŒF1 reward å¯¼è‡´ answer avoidanceï¼Œaction-level penalty å¯ä¿®å¤ï¼›å…¨æ™¯æ—¶é—´è¡¨æ–°å¢ä¸¤æ¡ã€‚
> v7.0ï¼ˆ2026-02-25ï¼‰ï¼š**CSO å…¥è¡¨**ï¼ˆ2602.03412ï¼ŒTencent AI Lab+HKUï¼Œâ˜…â˜…â˜…â˜…â˜†ï¼‰â€”â€”Credit Assignment ç¬¬ä¸‰ä¿¡å·ç»´åº¦ï¼šä»å¤±è´¥è½¨è¿¹åäº‹å®éªŒè¯ï¼Œåªç›‘ç£ 16% å…³é”®æ­¥éª¤ DPOï¼ŒGAIA-Text 8B è¶… GPT-4.1ï¼›è°±ç³»æ€»ç»“æ–°å¢"å¤±è´¥è½¨è¿¹ç»´åº¦"åˆ†æ”¯ï¼›v6 å¯¹æ¯”è¡¨è¡¥ CSO è¡Œã€‚
> v8.0ï¼ˆ2026-02-25ï¼‰ï¼š**ERLã€CM2ã€TSRã€SCoRe æ­£å¼å…¥è¡¨**ï¼›Multi-Turn RL ä¸‰æ”¯æŸ±å‡çº§ä¸º**å››æ”¯æŸ±**ï¼ˆæ–°å¢ ERL åæ€-å†…åŒ–æ”¯æŸ±ï¼‰ï¼›å…¨æ™¯è¡¨è¡¥é½ 2/25 äº”æ¡æ–°ä½œã€‚ERLï¼ˆ2602.13949ï¼ŒUSC+Microsoft+UPennï¼‰= experience-reflection-consolidation å¾ªç¯åµŒå…¥ RL è®­ç»ƒï¼Œéƒ¨ç½²æ—¶é›¶æˆæœ¬ï¼ˆSFT è’¸é¦å†…åŒ–ï¼‰ï¼ŒSokoban +81%ï¼ŒHotpotQA +11%ï¼›CM2ï¼ˆ2602.12268ï¼‰= Checklist Rewards + Sparse/Dense è§£è€¦ï¼Œmulti-turn tool useï¼›SCoReï¼ˆICLR 2025ï¼‰= ä¸¤é˜¶æ®µ KL çº¦æŸåˆ é™¤å‡çº é”™å‡è¡¡ã€‚
> v9.0ï¼ˆ2026-02-25ï¼‰ï¼š**SELAUR å…¥è¡¨**ï¼ˆ2602.21158ï¼ŒJHU+ASU+Purdueï¼‰â€”â€”å¤±è´¥è½¨è¿¹ token-level ä¸ç¡®å®šæ€§ reward shapingï¼Œé›¶é¢å¤–æ¨¡å‹æˆæœ¬ï¼›æ–°å¢ã€Œå¤±è´¥è½¨è¿¹åˆ©ç”¨æ·±åº¦è°±ç³»ã€ï¼šSELAURï¼ˆæµ…Â·é›¶æˆæœ¬ï¼‰â†’ ERLï¼ˆä¸­Â·åæ€å¾ªç¯ï¼‰â†’ CSOï¼ˆæ·±Â·åäº‹å®éªŒè¯ï¼‰ï¼›SELAUR ä¸ GiGPO æ­£äº¤äº’è¡¥ï¼ˆæˆåŠŸä¿¡å·ç²¾åŒ– + å¤±è´¥ä¿¡å·æ¿€æ´» = å®Œæ•´ credit è¦†ç›–ï¼‰ã€‚
> v11.0ï¼ˆ2026-02-26ï¼‰ï¼š**ã€Œå…³é”®å†³ç­–å¤©ç„¶ç¨€ç–ã€è·¨åŸŸå®è¯å›ºåŒ–**ï¼šCSOï¼ˆAgent RLï¼Œ16% critical stepsï¼‰+ SIAï¼ˆICML 2026ï¼Œæ¨ç†æ—¶å¯¹é½ï¼Œ20% Junction tokenï¼‰ä»ä¸åŒé¢†åŸŸç‹¬ç«‹éªŒè¯åŒä¸€åŸåˆ™ï¼›è¡¥å……åˆ° CSO é¢è¯•è¡¥å……æ®µè½ï¼›Papers/å¤šAgenté›†ä½“è¡Œä¸ºå®‰å…¨ï¼ˆCollective Behaviour+Colosseumï¼‰åŒå‘é—­åˆï¼ŒWisdomå±‚å…ƒé—®é¢˜ç¬”è®°å¢åŠ å®è¯éªŒè¯é“¾ã€‚
> v13.0ï¼ˆ2026-02-27ï¼‰ï¼š**Agent è¿›åŒ–æ¨¡å¼è°±ç³»ä¸‰å±‚æ¡†æ¶å»ºç«‹**ï¼ˆè€æ¿æŒ‡ä»¤ï¼‰ï¼›Reflexion/ExpeL/AgentQ ä¸‰ç¯‡ in-context è¿›åŒ–å¥ åŸºè®ºæ–‡å…¥åº“ã€‚ï¼›Search-P1 è·¯å¾„çº§å¯†é›†å¥–åŠ±åŠ å…¥ Reward Design æ—¶é—´è½´ï¼ˆv13.1ï¼‰
> v12.0ï¼ˆ2026-02-27ï¼‰ï¼š**SORL å…¥è¡¨**ï¼ˆ2511.20718ï¼ŒTexas A&Mï¼Œâ˜…â˜…â˜…â˜…â˜†ï¼‰â€”â€”Off-policy multi-turn RL å´©æºƒè¯Šæ–­ï¼ˆç²’åº¦é”™é…+æ–¹å·®ç´¯ç§¯ä¸¤æ ¹å› ï¼‰+ ä¿®å¤ï¼ˆTurn-Level IS å‡å€¼æ›¿ä»£ä¹˜ç§¯ + CTN è‡ªé€‚åº”æƒ©ç½šï¼‰ï¼›è®­ç»ƒç¨³å®šæ€§ç« èŠ‚è¡¥å…… off-policy ä¸“é¡¹è§£æ³•ï¼›æ›´æ–° See Also å¯¼èˆªä½“ç³»ã€‚
> v10.0ï¼ˆ2026-02-25ï¼‰ï¼š**PyVision-RL å…¥è¡¨**ï¼ˆ2602.20739ï¼Œå¤šæ¨¡æ€ Agentic RLï¼‰â€”â€”æå‡º Interaction Collapseï¼ˆEcho Trap çš„å¤šæ¨¡æ€ç‰ˆæœ¬ï¼šæ¨¡å‹å­¦ä¼šå‡å°‘å·¥å…·è°ƒç”¨è§„é¿å¤æ‚æ€§ï¼‰ï¼ŒOversampling-Filtering-Ranking + Accumulative Tool Reward ä¿®å¤ï¼›On-Demand Context Construction è§£å†³è§†é¢‘ token çˆ†ç‚¸ï¼›è·¨æ¨¡æ€éªŒè¯äº†"RL å‹åŠ›æ¨å‘é€€åŒ–ç­–ç•¥"æ ¹å› çš„æ™®éæ€§ï¼Œæ–°å¢è®­ç»ƒå¤±è´¥æ¨¡å¼è·¨æ¨¡æ€è°±ç³»ã€‚

> è¿™ç¯‡ç¬”è®°æ˜¯å¯¹ 2026 å¹´ 2 æœˆé›†ä¸­æ¶Œç°çš„ Agentic RL å·¥ä½œçš„ç»¼åˆç†è§£ï¼Œä¸æ˜¯è®ºæ–‡åˆ—è¡¨ï¼Œæ˜¯ä¸€ä¸ªæ¡†æ¶ã€‚

---

## æ ¸å¿ƒæ¡†æ¶ï¼ˆv2 å‡çº§ï¼‰

v1 çš„ã€Œä¸‰å¤§éš¾é¢˜ã€æ¡†æ¶ï¼ˆç¯å¢ƒ/Reward/ç®—æ³•ï¼‰æ•æ‰åˆ°äº†æ—©æœŸå·¥ä½œçš„ä¸»è¦åˆ†é‡ã€‚ä½† 2/17-20 å¯†é›†æ¶Œç°çš„æ–°ä¸€æ‰¹è®ºæ–‡æ­ç¤ºäº†**ç¬¬å››ä¸ªç»´åº¦**ï¼š

> **Workflow/Topology è®¾è®¡æœ¬èº«å°±æ˜¯ agent èƒ½åŠ›çš„å†³å®šå˜é‡**ï¼Œä¸äºšäºç®—æ³•æˆ– rewardã€‚

å‡çº§åçš„æ¡†æ¶ï¼š

```
Agentic RL è®­ç»ƒ = ç¯å¢ƒ Ã— Reward Ã— Workflow/Topology Ã— ç®—æ³•

åŸä¸‰å¤§éš¾é¢˜ä¿æŒä¸å˜ï¼Œæ–°å¢ç¬¬å››ç»´åº¦ï¼š
4. Workflow/Topology é—®é¢˜ï¼šé™æ€è®¾è®¡çš„ pipeline æ˜¯æ€§èƒ½ç“¶é¢ˆè€Œéæ¨¡å‹èƒ½åŠ›
```

---

## ä¸ºä»€ä¹ˆ Agentic RL ç°åœ¨æ˜¯æœ€çƒ­çš„æ–¹å‘

RLVRï¼ˆReinforcement Learning with Verifiable Rewardsï¼‰åœ¨æ•°å­¦/ä»£ç ç­‰**æœ‰å•æ­¥å¯éªŒè¯ç­”æ¡ˆ**çš„ä»»åŠ¡ä¸Šå·²ç»å·¥ä½œå¾—å¾ˆå¥½ï¼ˆDeepSeek-R1ã€Kimi-k1.5ã€QwQ ç­‰ï¼‰ã€‚ä½†çœŸå®ä¸–ç•Œçš„ agent ä»»åŠ¡å‡ ä¹æ²¡æœ‰"ä¸€çœ¼çœ‹å‡ºå¯¹é”™"çš„ rewardï¼š

- å¸®ç”¨æˆ·è®¢æœºç¥¨ï¼ˆéœ€è¦æŸ¥è¯¢ã€å¯¹æ¯”ã€ç¡®è®¤â€”â€”å“ªä¸€æ­¥ç®—æˆåŠŸï¼Ÿï¼‰
- ä¿®å¤ä»£ç  bugï¼ˆéœ€è¦ç†è§£ä»£ç åº“ã€å®šä½é—®é¢˜ã€éªŒè¯ä¿®å¤â€”â€”æ€ä¹ˆè¡¡é‡ä¸­é—´æ­¥éª¤çš„è´¨é‡ï¼Ÿï¼‰
- è¿›è¡Œå¸‚åœºè°ƒç ”ï¼ˆéœ€è¦æœç´¢ã€ç»¼åˆã€åˆ¤æ–­ç›¸å…³æ€§â€”â€”å®Œå…¨ open-endedï¼‰

è¿™ä¸ª gapâ€”â€”**ä»å•æ­¥å¯éªŒè¯ä»»åŠ¡åˆ°å¤šæ­¥å¼€æ”¾ä»»åŠ¡çš„è·¨è¶Š**â€”â€”å°±æ˜¯ Agentic RL çš„æ ¸å¿ƒç ”ç©¶ç©ºé—´ã€‚

## ä¸‰å¤§æ ¸å¿ƒéš¾é¢˜

ç”¨ä¸€ä¸ªç»Ÿä¸€çš„æ¡†æ¶æ¥çœ‹å½“å‰ Agentic RL çš„æŒ‘æˆ˜ï¼š

```
Agent RL è®­ç»ƒ = ç¯å¢ƒ Ã— Reward Ã— ç®—æ³•

1. ç¯å¢ƒè´¨é‡é—®é¢˜ï¼štoy ç¯å¢ƒ â†’ toy agentï¼Œæ²¡æœ‰æ³›åŒ–
2. Reward è®¾è®¡é—®é¢˜ï¼šå¼€æ”¾ä»»åŠ¡ç¼ºä¹å¯ä¿¡å·çš„ reward
3. ç®—æ³•ç¨³å®šæ€§é—®é¢˜ï¼šmulti-step / multi-agent å¯¼è‡´ä¼˜åŒ–ä¸ç¨³å®š
```

---

## éš¾é¢˜ 1ï¼šç¯å¢ƒè´¨é‡å†³å®šæ³›åŒ–ä¸Šé™

### é—®é¢˜
å¤§å¤šæ•° agentic RL çš„è®­ç»ƒç¯å¢ƒæ˜¯åˆæˆçš„ã€ç®€åŒ–çš„ã€ä¸çœŸå®ä»»åŠ¡å·®è·å¾ˆå¤§ã€‚åœ¨è¿™ç±»ç¯å¢ƒä¸Šè®­ç»ƒå‡ºæ¥çš„ agentï¼Œåœ¨çœŸå®åœºæ™¯ä¸‹è¡¨ç°ç³Ÿç³•â€”â€”ä¸æ˜¯æ¨¡å‹ä¸å¤Ÿèªæ˜ï¼Œæ˜¯å®ƒæ²¡è§è¿‡çœŸå®ä»»åŠ¡çš„å¤æ‚æ€§ã€‚

### 2026 å¹´çš„è§£æ³•ï¼šEnterpriseGym Corecraftï¼ˆSurge AI, 2602.16179ï¼‰

- **2500+ çœŸå®å®ä½“ï¼Œ23 ç§å·¥å…·**ï¼Œæ¨¡æ‹Ÿä¼ä¸šå®¢æœå®Œæ•´ä¸šåŠ¡æµç¨‹
- **Expert-authored rubrics** ä½¿ reward è®¡ç®—å¯é ï¼ˆä¸ä¾èµ– LLM judgeï¼‰
- **Task-centric world building**ï¼šç¯å¢ƒè®¾è®¡ä»¥ä»»åŠ¡å¤šæ ·æ€§ä¸ºæ ¸å¿ƒ

**å…³é”® empirical finding**ï¼šåœ¨è¿™ä¸ªé«˜ä¿çœŸç¯å¢ƒä¸Šç”¨ GRPO è®­ç»ƒ GLM 4.6ï¼Œ**å• epoch** ååœ¨ 3 ä¸ªç‹¬ç«‹ OOD benchmark ä¸Šæ³›åŒ–ï¼ˆ+4.5%/+7.4%/+6.8%ï¼‰ã€‚

**æ ¸å¿ƒ insight**ï¼š
> ç¯å¢ƒè´¨é‡å†³å®šäº† agent èƒ½å­¦åˆ°çš„ skill çš„ä¸Šé™ã€‚Toy ç¯å¢ƒçš„ reward å¤ªå®¹æ˜“ hackï¼Œagent å­¦åˆ°çš„æ˜¯"åœ¨è¿™ä¸ªç¯å¢ƒé‡Œå¾—é«˜åˆ†çš„ç­–ç•¥"ï¼Œè€Œä¸æ˜¯"å¦‚ä½•å®Œæˆè¿™ç±»ä»»åŠ¡çš„é€šç”¨èƒ½åŠ›"ã€‚

### å»¶ä¼¸æ€è€ƒ
è¿™ä¸ªå‘ç°å¯¹ RL å®è·µè€…çš„å¯ç¤ºï¼š**åœ¨æ›´å°çš„ model ä¸Šç”¨æ›´å¥½çš„ç¯å¢ƒè®­ç»ƒ**ï¼Œå¯èƒ½æ¯”åœ¨æ›´å¤§çš„ model ä¸Šç”¨å¹³åº¸çš„ç¯å¢ƒè®­ç»ƒæ›´æœ‰æ•ˆã€‚è¿™ç›´æ¥æŒ‘æˆ˜äº†"scale is all you need"çš„ç›´è§‰ã€‚

---

## éš¾é¢˜ 2ï¼šå¼€æ”¾ä»»åŠ¡ç¼ºä¹å¯é  Reward

### é—®é¢˜
RLVR çš„æˆåŠŸä¾èµ–äº"ground truth ç­”æ¡ˆå¯éªŒè¯"ã€‚ä½†å¼€æ”¾ä»»åŠ¡ï¼ˆå·¥å…·è°ƒç”¨ã€å®¢æœã€ç ”ç©¶ï¼‰ï¼š
- æ²¡æœ‰å•ä¸€æ­£ç¡®ç­”æ¡ˆ
- ä¸­é—´æ­¥éª¤è´¨é‡éš¾ä»¥è‡ªåŠ¨è¯„ä¼°
- æœ€ç»ˆç»“æœå¯èƒ½æœ‰å¤šç§æ­£ç¡®è·¯å¾„

ç”¨ LLM-as-judge æœ‰ä¸€è‡´æ€§é—®é¢˜ï¼ˆåŒä¸€ judge å¯¹åŒä¸€è¾“å‡ºå¯èƒ½ç»™ä¸åŒåˆ†ï¼‰ï¼›ç”¨äººå·¥æ ‡æ³¨æˆæœ¬æé«˜ã€‚

### ä¸‰ç§è§£æ³•å¹¶è¡Œå‡ºç°ï¼š

**è§£æ³• A â€” Checklist Rewardï¼ˆCM2, 2602.12268ï¼‰**
æŠŠ"åˆ¤æ–­è¿™ä¸ª agent è¡Œä¸ºå¥½ä¸å¥½"è½¬åŒ–ä¸º"æ£€æŸ¥è‹¥å¹² binary criteria"ï¼š
```
åŸå§‹é—®é¢˜ï¼šè¿™è½® tool call è´¨é‡å¦‚ä½•ï¼Ÿï¼ˆopen-ended, ä¸»è§‚ï¼‰
è½¬åŒ–åï¼š
  â–¡ æ˜¯å¦åœ¨æ­£ç¡®æ—¶æœºè°ƒç”¨äº†å·¥å…·ï¼Ÿ
  â–¡ å‚æ•°æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼Ÿ
  â–¡ æ˜¯å¦å¤„ç†äº† error caseï¼Ÿ
  â–¡ æ˜¯å¦åœ¨è°ƒç”¨å‰è¯´æ˜äº†æ„å›¾ï¼Ÿ
```
æŠŠ open-ended judging â†’ classification-styleï¼Œå¯é æ€§å¤§å¹…æå‡ã€‚

**è§£æ³• B â€” Rubric-based Rewardï¼ˆOpenRS, 2602.14069ï¼‰**
ä¸æŠŠ reward å­¦è¿› judge modelï¼Œè€Œæ˜¯**æ˜¾å¼æ¨å¯¼å‡º rubric**ï¼ˆè¯„åˆ†æ ‡å‡†ï¼‰ï¼Œæ¯æ¬¡è¯„åˆ†æ—¶åœ¨ rubric ä¸‹æ‰§è¡Œæ¨ç†ï¼š
```
å›ºå®š judgeï¼šå†…åŒ–äº†è¯„åˆ†é€»è¾‘ï¼Œæ— æ³•æ£€æŸ¥ â†’ é»‘ç›’
Rubric-basedï¼šæ¯æ¬¡è¯„åˆ†å±•ç¤ºæ¨ç†è¿‡ç¨‹ â†’ å¯æ£€æŸ¥ + å¯è§£é‡Š
```
è§£å†³äº† reward generalization é—®é¢˜ï¼ˆrubric å¯ä»¥è·¨ä»»åŠ¡è¿ç§»ï¼‰ã€‚

**è§£æ³• C â€” Expert Rubrics in Environmentï¼ˆEnterpriseGym Corecraftï¼‰**
æŠŠ rubric ç¼–ç è¿›**è®­ç»ƒç¯å¢ƒ**ï¼Œè€Œä¸æ˜¯è¯„ä¼°å™¨ã€‚è¿™æ · reward åœ¨è®­ç»ƒæ—¶å°±å·²ç»å¯é ï¼Œä¸éœ€è¦äº‹åçº æ­£ã€‚

**ä¸‰ç§è§£æ³•çš„é€‚ç”¨åœºæ™¯**ï¼š
| è§£æ³• | ä¼˜åŠ¿ | é€‚ç”¨ |
|---|---|---|
| Checklist (CM2) | ç»†ç²’åº¦ï¼Œå¯†é›† reward | å·¥å…·è°ƒç”¨ã€API ä½¿ç”¨ |
| Rubric-based (OpenRS) | å¯è§£é‡Šï¼Œè·¨ä»»åŠ¡æ³›åŒ– | é€šç”¨å¯¹é½ã€open-ended QA |
| Expert rubrics in env (Corecraft) | æœ€å¯é ï¼ŒOOD æ³›åŒ–å¼º | ä¸“ä¸šé¢†åŸŸï¼ˆéœ€è¦ä¸“å®¶æŠ•å…¥ï¼‰|

**è§£æ³• D â€” Search-R1++ å…³é”®å®éªŒå‘ç°ï¼ˆ2602.19526ï¼Œv6 æ–°å¢ï¼‰**

å¯¹ Deep Research agentï¼ˆmulti-round retrieval + generationï¼‰çš„ç³»ç»Ÿæ€§æ¶ˆèï¼Œæ²¿ä¸‰ä¸ªç»´åº¦è§£è€¦ï¼š

- **Prompt template**ï¼šFast Thinking æ¯” Slow Thinking ç¨³å®šæ€§æ›´é«˜ï¼Œæ€§èƒ½æ›´å¥½ï¼ˆç›´è§‰ï¼šsearch agent ä¸éœ€è¦æ·±åº¦ chain-of-thoughtï¼Œéœ€è¦å¿«é€Ÿå†³ç­–ï¼‰
- **Reward function**ï¼šF1-based reward å¯¼è‡´**è®­ç»ƒå´©æºƒ**ï¼ˆanswer avoidanceï¼šmodel å­¦ä¼šä¸ç»™ç­”æ¡ˆä»¥é¿å… partial match æ‰£åˆ†ï¼‰â†’ EM reward æ›´ç¨³ï¼›åŠ å…¥ **action-level penalty**ï¼ˆå¯¹ä¸å¿…è¦æœç´¢æƒ©ç½šï¼‰å F1 reward å¯è¶…è¿‡ EM
- **Policy optimization**ï¼š**REINFORCE > PPO > GRPO**ï¼ˆç¨³å®šæ€§ï¼‰ï¼ŒGRPO æ˜¯ä¸‰è€…ä¸­æœ€ä¸ç¨³å®šçš„ï¼ˆæœç´¢ä»»åŠ¡ä¸­ group sampling æ–¹å·®å¤§ï¼‰ï¼›REINFORCE æœç´¢åŠ¨ä½œæ›´å°‘ï¼ˆæ›´é«˜æ•ˆï¼‰

**å…³é”® takeawayï¼ˆå¯¹ Tool Use RL ç ”ç©¶æœ‰é‡è¦æ„ä¹‰ï¼‰**ï¼š
1. GRPO åœ¨ multi-turn search agent è®­ç»ƒä¸­å¹¶ä¸æ˜¯æœ€ä¼˜é€‰æ‹©â€”â€”è¿™ä¸å…¶åœ¨å•è½®æ¨ç†ä»»åŠ¡ä¸Šçš„ä¸»å¯¼åœ°ä½å½¢æˆåå·®
2. Reward å‡½æ•°è®¾è®¡è¦é¿å…ç»™ partial output ç©ºé—´ï¼ˆF1 çš„ recall åˆ†é‡ï¼‰ï¼šä¼šè¯±å‘ answer avoidance
3. Action-level penaltyï¼ˆå¯¹å·¥å…·è°ƒç”¨çš„æˆæœ¬æƒ©ç½šï¼‰æ˜¯ä¸€ä¸ªè¢«ä½ä¼°çš„ reward ç»„æˆéƒ¨åˆ†

Search-R1++ baselineï¼šQwen2.5-7B ä» 0.403 â†’ 0.442ï¼ˆ+9.7%ï¼‰ï¼ŒQwen2.5-3B ä» 0.289 â†’ 0.331ï¼ˆ+14.5%ï¼‰ã€‚

**è§£æ³• E â€” Uncertainty-Intrinsic Rewardï¼ˆSELAUR, 2602.21158ï¼Œv9 æ–°å¢ï¼‰**

ä¸Šè¿°å››ç§è§£æ³•éƒ½åœ¨å¤„ç†"å¦‚ä½•ç»™æˆåŠŸè¡Œä¸ºè®¾è®¡ reward"ã€‚SELAUR æ¢äº†è§†è§’ï¼š**ä»å¤±è´¥è½¨è¿¹ä¸­æå–å†…ç”Ÿå­¦ä¹ ä¿¡å·**ã€‚

é—®é¢˜ï¼šæ ‡å‡† RLVR å¯¹å¤±è´¥è½¨è¿¹ç»™ reward=0 å°±ä¸å­¦äº†ï¼Œä¸¢å¼ƒäº†å¤±è´¥è¿‡ç¨‹ä¸­çš„ä¸ç¡®å®šæ€§ä¿¡æ¯ã€‚  
è§£æ³•ï¼šç”¨ LLM è‡ªèº«çš„ token é¢„æµ‹æ¦‚ç‡åˆ†å¸ƒï¼Œä¼°è®¡ä¸‰ç»´ä¸ç¡®å®šæ€§ï¼ˆentropy/least-confidence/marginï¼‰ï¼ŒæŠŠå¤±è´¥æ­¥éª¤å˜æˆå¯†é›† rewardï¼š

```
å¤±è´¥è½¨è¿¹çš„ step reward = w_t Â· u_t  (w_t=0.95ï¼Œç¡®ä¿ < æˆåŠŸ reward)
æˆåŠŸè½¨è¿¹ä¿æŒåŸå§‹ reward ä¸å˜
```

**å¤±è´¥è½¨è¿¹åˆ©ç”¨çš„ä¸‰å±‚æ·±åº¦è°±ç³»**ï¼ˆæ–°ç»“æ„ï¼Œæ•´åˆ CSO/ERL/SELAURï¼‰ï¼š

| å±‚çº§ | æ–¹æ³• | ä¿¡å·æ¥æº | æˆæœ¬ | å¯é æ€§ |
|------|------|---------|------|--------|
| Logits å±‚ | SELAUR (2602.21158) | token æ¦‚ç‡åˆ†å¸ƒä¸ç¡®å®šæ€§ | é›¶é¢å¤–æˆæœ¬ | è¾ƒä½ï¼ˆæœªåŒºåˆ†è®¤çŸ¥/å¶ç„¶ä¸ç¡®å®šæ€§ï¼‰|
| åæ€å±‚ | ERL (2602.13949) | è‡ªç”Ÿæˆåæ€ Î” â†’ æŒ‡å¯¼é‡è¯• | ä¸­ï¼ˆé¢å¤– LLM è°ƒç”¨ï¼‰| ä¸­ï¼ˆä¾èµ–åæ€è´¨é‡ï¼‰|
| éªŒè¯å±‚ | CSO (2602.03412) | åäº‹å®éªŒè¯ + Expert æ›¿æ¢ | é«˜ï¼ˆexpert model + rolloutï¼‰| é«˜ï¼ˆå¯éªŒè¯çš„å› æœè¯æ®ï¼‰|

é€‚ç”¨åœºæ™¯ï¼šreward æç¨€ç– + å¤±è´¥ç‡é«˜ + èµ„æºæœ‰é™æ—¶ï¼ŒSELAUR æ˜¯å·¥ç¨‹é¦–é€‰ï¼›èµ„æºå……è¶³æ—¶ï¼ŒERL/CSO ä¿¡å·æ›´å¯é ã€‚

â†’ è¯¦è§ï¼š[[AI/2-Agent/Agentic-RL/SELAUR-Self-Evolving-LLM-Agent-Uncertainty-Rewards|SELAURï¼ˆ2602.21158ï¼‰]]

**Reward Design å®Œæ•´åœ°å›¾ï¼ˆv9ï¼Œæˆªè‡³ 2026-02-25ï¼‰**ï¼š

| ç±»å‹ | ä»£è¡¨æ–¹æ¡ˆ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|
| verifiable_binary | GiGPO / GRPO / Search-R1 | æœ‰ ground truth çš„ä»»åŠ¡ |
| unverifiable_implicit | iStarï¼ˆDPO â‰¡ step-BTï¼‰| å¼€æ”¾ç¯å¢ƒï¼Œæ—  ground truth |
| unverifiable_checklist | CM2 | å¤šè½® tool useï¼Œç»“æ„åŒ–æ ‡å‡† |
| process_reward | AgentPRM / iStar | éœ€è¦ step çº§åˆ«ä¿¡å· |
| action_level_penalty | Search-R1++ | é˜²æ­¢ä¸å¿…è¦å·¥å…·è°ƒç”¨ |
| uncertainty_intrinsic | SELAUR | å¤±è´¥ç‡é«˜ï¼Œå†…ç”Ÿå¯†é›†ä¿¡å· |

---

## éš¾é¢˜ 3ï¼šMulti-Step/Multi-Agent è®­ç»ƒä¸ç¨³å®š

### é—®é¢˜
åœ¨é•¿ horizon ä»»åŠ¡æˆ–å¤š agent ç³»ç»Ÿä¸­ï¼Œæ ‡å‡† RLï¼ˆPPO/GRPOï¼‰é¢ä¸´ï¼š
- **Credit assignment**ï¼šæœ€ç»ˆ reward ä¼ æ’­ç»è¿‡å¤ªå¤šæ­¥éª¤ï¼Œæ¢¯åº¦ä¿¡å·æåº¦ç¨€ç–
- **Serial collapse**ï¼šåœ¨å¤š agent ç³»ç»Ÿä¸­ï¼Œä¸²è¡Œ rollout å¯¼è‡´è®­ç»ƒææ…¢
- **Optimization instability**ï¼šmulti-agent ä¸­ç­–ç•¥ç›¸äº’ä¾èµ–ï¼Œè”åˆè®­ç»ƒä¸ç¨³å®š

### è§£æ³• 0ï¼šEcho Trap è¯Šæ–­ä¸ StarPO-Sï¼ˆRAGEN, 2504.20073ï¼‰

åœ¨è®¨è®ºå¦‚ä½•è§£å†³ multi-turn RL ä¸ç¨³å®šä¹‹å‰ï¼Œå¿…é¡»å…ˆå›ç­”ï¼š**"ä¸ç¨³å®š"çš„å…·ä½“å¤±è´¥æ¨¡å¼æ˜¯ä»€ä¹ˆï¼Ÿ**

RAGENï¼ˆNorthwestern/Stanford Li Fei-Fei/Yejin Choi/Jiajun Wu + Microsoft + NYU Kyunghyun Choï¼‰æ˜¯ç¬¬ä¸€ç¯‡ç³»ç»Ÿè¯Šæ–­è¿™ä¸ªé—®é¢˜çš„å·¥ä½œï¼š

**Echo Trapï¼ˆå›å£°é™·é˜±ï¼‰**ï¼šmulti-turn RL ç‰¹æœ‰çš„å¤±è´¥æ¨¡å¼ï¼Œä¸‰è”å¾åŒæ—¶å‡ºç°ï¼š
1. **reward variability collapse**ï¼šæ‰€æœ‰ rollout çš„ reward è¶‹åŒï¼Œbatch çº§åˆ«æ¢¯åº¦è¶‹é›¶
2. **entropy drop**ï¼špolicy è¾“å‡ºç†µæ€¥å‰§ä¸‹é™ï¼Œé™·å…¥å›ºå®šæ¨¡æ¿
3. **gradient spike**ï¼šé—´æ­‡æ€§æ¢¯åº¦çˆ†ç‚¸

æ ¹æœ¬æœºåˆ¶ï¼šagent ä¸€æ—¦æ‰¾åˆ°å±€éƒ¨æœ‰ reward çš„ç­–ç•¥æ¨¡æ¿ï¼Œå°±è‡ªæˆ‘å¼ºåŒ–è¿›å…¥è¯¥æ¨¡æ¿ã€‚RL çš„ä¼˜åŒ–å‹åŠ›åè€Œæ”¾å¤§äº†è¿™ä¸ªæ·å¾„ï¼ŒåŒæ—¶å‹åˆ¶æ¢ç´¢ã€‚ç±»æ¯” Shumailov et al. 2024 çš„ model collapseâ€”â€”ä½†æ˜¯åœ¨çº¿åŠ¨æ€ç‰ˆæœ¬ã€‚

**é‡è¦å®éªŒå‘ç°**ï¼ˆRAGEN çš„å››ä¸ªç¯å¢ƒï¼‰ï¼š
- PPO åœ¨ç¡®å®šæ€§ç¯å¢ƒï¼ˆBandit/Sokobanï¼‰æ¯” GRPO æ›´ç¨³ï¼ˆcritic æä¾›å¹³æ»‘ value estimateï¼‰
- GRPO åœ¨éšæœºç¯å¢ƒï¼ˆFrozen Lakeï¼‰åè€Œæ›´ç¨³ï¼ˆéšæœºæ€§è®© state value éš¾ä¼°ï¼ŒPPO critic å¼•å…¥é”™è¯¯ï¼‰
- WebShop ä¸¤è€…éƒ½è¡Œï¼ˆå¼ºè¯­è¨€å…ˆéªŒï¼Œé«˜åˆå§‹ rewardï¼Œå¯¹ critic ä¾èµ–ä½ï¼‰
- æ ¸å¿ƒç»“è®ºï¼š**æ²¡æœ‰ä¸€ç§ç®—æ³•å¤©ç„¶é€‚åˆæ‰€æœ‰ multi-turn agent ä»»åŠ¡**

**StarPO-S ä¸‰æœºåˆ¶**ï¼ˆé’ˆå¯¹ Echo Trap ä¸‰è”å¾çš„é€†å‘è®¾è®¡ï¼‰ï¼š
```
reward homogenization  â†’ Variability-based Trajectory Filteringï¼ˆä¿ç•™ top-p% reward std çš„ promptï¼‰
gradient variance      â†’ Critic Baseliningï¼ˆè½»é‡ trajectory-level baselineï¼‰
ratio explosion        â†’ Decoupled Clippingï¼ˆper-turn åˆ†åˆ«æ§åˆ¶ clip rangeï¼‰
```

**Rollout ä¸‰å› å­**ï¼ˆå†³å®š self-evolution è´¨é‡ï¼‰ï¼š
1. **Diverse initial states**ï¼šå¤šæ ·åˆå§‹çŠ¶æ€ Ã— å¤šæ¡ rollout/stateï¼ˆP çš„å¤šæ ·æ€§ > N çš„æ•°é‡ï¼‰
2. **Medium granularity**ï¼šæ¯ turn æ‰§è¡Œå¤šä¸ª sub-actionï¼ˆéå• tokenï¼Œéæ•´ episodeï¼‰
3. **High rollout frequency**ï¼šæ¥è¿‘å…¨ on-policyï¼Œé¿å… off-policy ratio ç§¯ç´¯

**Finding 3ï¼ˆæœ€æœ‰ä»·å€¼ï¼‰**ï¼šå³ä½¿æ ¼å¼ä¸­å¼ºåˆ¶ `<think>` tokenï¼Œçº¯ outcome reward ä¸‹ agent ä¼šç»•è¿‡æ¨ç†ï¼ˆshortcutï¼‰æˆ–äº§ç”Ÿ hallucinated reasoningã€‚**Emerging reasoning ä¸æ˜¯ multi-turn RL çš„å…è´¹åˆé¤**ã€‚

RAGEN æ˜¯ multi-turn training stability è¿™æ¡ç ”ç©¶çº¿çš„å¥ åŸºå·¥ä½œï¼ˆ2025å¹´4æœˆæäº¤ï¼‰ï¼Œåç»­ TSRã€HiPERã€KLongã€LOOP å‡ä»æ­¤å‡ºå‘ã€‚

### è§£æ³• Aï¼šæ—¶é—´ç»´åº¦åˆ†å±‚ï¼ˆHiPER, 2602.16165ï¼‰

æŠŠ policy åˆ†ä¸º Plannerï¼ˆsubgoal çº§ï¼‰å’Œ Executorï¼ˆaction çº§ï¼‰ï¼Œåˆ†åˆ«è®¡ç®— advantageï¼š
```
ä¼ ç»Ÿ GAEï¼šreward ä» T æ­¥åå‘ä¼ æ’­åˆ° step 1ï¼Œä¿¡å·æç¨€ç–
HAEï¼šreward å…ˆåœ¨ subgoal å†…èšåˆ â†’ å†ä» subgoal çº§åä¼ åˆ° planner
```
æ–¹å·®ç¼©å‡æœ‰ç†è®ºè¯æ˜ï¼ŒALFWorld 97.4%ï¼ˆ+6.6%ï¼‰ï¼ŒWebShop 83.3%ï¼ˆ+8.3%ï¼‰ã€‚

### è§£æ³• Bï¼šç©ºé—´ç»´åº¦å†»ç»“ï¼ˆPARL / Kimi K2.5, 2602.02276ï¼‰

åœ¨ multi-agent ç³»ç»Ÿä¸­ï¼Œ**å†»ç»“ subagentï¼Œåªè®­ç»ƒ orchestrator**ï¼š
```
è”åˆè®­ç»ƒï¼ˆæœ‰é—®é¢˜ï¼‰ï¼šorchestrator + subagent åŒæ—¶æ›´æ–° â†’ ä¼˜åŒ–ç›®æ ‡äº’ç›¸å¹²æ‰°
PARLï¼šsubagent å›ºå®š â†’ orchestrator å­¦å¦‚ä½•åˆ†è§£ä»»åŠ¡ + åˆ›å»º subagent
```
è§£å†³äº† credit assignment + training instabilityã€‚Agent Swarm æœ€å¤š 100 subagentï¼Œå»¶è¿Ÿé™ 4.5xã€‚

### è§£æ³• Cï¼šTraining-time Tree Searchï¼ˆTSR, 2602.11767, ICML 2026ï¼‰

æŠŠ test-time æ ‘æœç´¢ç§»å…¥ training-time rollout é˜¶æ®µï¼šæ¯ä¸ª turn é‡‡æ ·å€™é€‰åŠ¨ä½œé›† $\mathcal{A}_t = \{a_t^{(1)},\dots,a_t^{(M)}\}$ï¼Œç”¨ scoring function é€‰é«˜è´¨é‡åŠ¨ä½œæ„å»ºè½¨è¿¹ã€‚

**ä¸‰ç§æœç´¢ç­–ç•¥**ï¼š
- **Best-of-N**ï¼šç‹¬ç«‹é‡‡æ · N æ¡å®Œæ•´è½¨è¿¹ï¼Œé€‰ reward æœ€é«˜çš„ï¼ˆbaselineï¼Œæœ€ç®€ï¼‰
- **Beam Search**ï¼šæ¯æ­¥ç»´æŠ¤ B ä¸ªé«˜åˆ†å‰ç¼€ beamï¼Œé€æ­¥ç­›é€‰ï¼Œå¯ä»¥åœ¨ä¸­é€”çº é”™ï¼ˆæœ€å¼ºï¼Œé€‚åˆç¡®å®šæ€§ç¯å¢ƒï¼‰
- **Shallow Lookahead**ï¼šè¯„ä¼°åŠ¨ä½œæ—¶é¢å¤–å±•å¼€ D<<K æ­¥ï¼Œå‰ç»æ€§æ›´å¼ºï¼ˆè®¡ç®—æŠ˜ä¸­ï¼Œé€‚åˆéšæœºç¯å¢ƒï¼‰

**é…åˆ Instance Filtering**ï¼šæŒ‰ outcome uncertainty $U(u;\pi_\theta) = \text{Std}[R(\tau)]$ ç­›é€‰è®­ç»ƒæ ·æœ¬ï¼Œåªä¿ç•™"æœ‰æ—¶æˆåŠŸæœ‰æ—¶å¤±è´¥"çš„ hard casesã€‚

$$\text{Rollout quality} \uparrow \Rightarrow \text{Training signal quality} \uparrow \Rightarrow \text{Multi-turn RL stability} \uparrow$$

- Optimizer-agnosticï¼Œå…¼å®¹ PPO/GRPOï¼ŒICML 2026
- **0.5B+TSR â‰ˆ 3B æ—  TSR**ï¼ˆ+15% æå‡ï¼ŒSokoban/FrozenLake/WebShop ä¸‰ç¯å¢ƒä¸€è‡´ï¼‰
- **æ ¸å¿ƒå‘½é¢˜ï¼šrollout è´¨é‡æ˜¯ multi-turn RL çš„ç¬¬å››ä¸ªè¢«å¿½è§†çš„è®­ç»ƒå˜é‡ï¼ˆä¸ç®—æ³•/reward/credit assignment æ­£äº¤ï¼‰**
- è¯¦è§ï¼š[[AI/2-Agent/Agentic-RL/TSR-Trajectory-Search-Rollouts-Multi-Turn-RL|TSR æ·±åº¦ç¬”è®°]]

### Credit Assignment å®Œæ•´è°±ç³»ï¼ˆv3 æ–°å¢ï¼Œæˆªè‡³ 2026-02-23ï¼‰

è¿™æ˜¯ Agentic RL é‡Œæœ€æ ¸å¿ƒçš„å­é—®é¢˜ï¼Œ2/23 å·²å®ç° 6 æ–¹æ¡ˆå…¨è¦†ç›–ï¼š

| æ–¹æ¡ˆ | è®ºæ–‡ | ç±»å‹ | ä¾èµ– | æ ¸å¿ƒæœºåˆ¶ | é€‚ç”¨åœºæ™¯ |
|------|------|------|------|---------|---------|
| GRPO | baseline | trajectory-level | æ—  | group relative advantage | å•è½®/çŸ­ horizon |
| LOOPï¼ˆLOO-PPOï¼‰| 2502.01600 | trajectory-level | æ— ï¼ˆleave-one-outï¼‰| å… critic çš„ trajectory baseline | é•¿ horizonï¼Œ32B > o1+9% |
| GiGPO | 2505.10978 | step-levelï¼ˆanchorï¼‰| æ— é¢å¤– rollout | é‡å¤ç»è¿‡åŒä¸€çŠ¶æ€ â†’ å¤©ç„¶å¯¹æ¯”ï¼Œhashmap O(n) | ç»“æ„åŒ–ç¯å¢ƒï¼ˆALFWorld +13.3%ï¼ŒNeurIPS 2025ï¼‰|
| AgentPRM | 2502.10325 | step-levelï¼ˆMC rolloutï¼‰| é¢å¤–é‡‡æ · | MC ä¼°è®¡ step Q-valueï¼Œæ˜¾å¼ PRM ç½‘ç»œ | å……è¶³è®¡ç®—ï¼Œ3B > GPT-4o |
| **iStar** | **2509.19199** | **step-levelï¼ˆimplicit PRMï¼‰** | **2x æ¨¡å‹å‚æ•°ï¼Œæ— é¢å¤– rollout** | **trajectory DPO â‰¡ step-wise BT modelï¼ˆç†è®ºä¿è¯ï¼‰ï¼Œrolling reference = Ï€_old** | **âœ… unverifiable rewardï¼ˆSOTOPIA vs GPT-4o +48%ï¼‰ï¼Œå¼€æ”¾ç¯å¢ƒ** |
| MIG | 2602.01034 | step-levelï¼ˆä¿¡æ¯è®ºï¼‰| verifiable reward | Marginal Information Gain + Monotonic Watermarkï¼Œåªå¥–åŠ±çœŸæ­£è¯­ä¹‰çªç ´ | OOD æ³›åŒ–ï¼Œé˜² reward hacking |
| **HiPER** | **2602.16165** | **subgoal-segment-levelï¼ˆå±‚çº§ï¼‰** | **sparse end-of-trajectory** | **Plan-Execute Interface + HAEï¼ˆä¸‰ç±» advantageï¼šswitch/high/lowï¼‰ï¼Œæ–¹å·®å‡å°‘å®šç†ä¿è¯** | **é•¿ horizonï¼ˆALFWorld 97.4% SOTAï¼ŒICML 2026ï¼‰** |
| SeeUPO | 2602.06554 | å›åˆçº§ï¼ˆé€†åºæ›´æ–°ï¼‰| multi-turn | é€†åºæ›´æ–°ï¼ˆTâ†’1ï¼‰+ æ—  group variance normalizationï¼ŒREINFORCE+GRAE ç†è®ºä¿è¯ | multi-turn RL æ”¶æ•›ï¼ˆAppWorld +43-54%ï¼‰|
| SHARP | 2602.08335 | æ¨ªå‘ multi-agent | multi-agent | Shapley value + counterfactual maskingï¼Œä¸‰å±‚ rewardï¼Œper-agent norm | å¤š agent ç²¾ç¡®å½’å› ï¼ˆICML 2026ï¼‰|
| **CSO** | **2602.03412** | **step-levelï¼ˆåäº‹å®éªŒè¯ï¼‰** | **éœ€ expert + éªŒè¯ rollout** | **å¤±è´¥è½¨è¿¹â†’PRM å®šä½å¼±ç‚¹â†’expert æ›¿ä»£â†’policy rollout éªŒè¯æˆåŠŸâ†’åªç›‘ç£ 16% æ­¥éª¤ DPO** | **âœ… å¤±è´¥è½¨è¿¹åˆ©ç”¨ï¼›æœ‰ expert model å¯ç”¨ï¼›GAIA-Text 8B è¶… GPT-4.1** |

**è°±ç³»æ€»ç»“ï¼ˆv5 æ›´æ–°ï¼‰**ï¼š
- GRPO â†’ LOOPï¼šä» trajectory åˆ°æ›´å¥½çš„ trajectory baselineï¼ˆå… criticï¼‰
- LOOP â†’ GiGPOï¼šä» trajectory åˆ° step-levelï¼ˆåˆ©ç”¨çŠ¶æ€é‡å ï¼Œå…é¢å¤– rolloutï¼‰
- GiGPO â†’ AgentPRMï¼šstep-level çš„ä¸¤ç§è·¯ï¼šanchor groupingï¼ˆå… rolloutï¼‰vs MC rolloutï¼ˆæ˜¾å¼ Q-targetï¼‰
- AgentPRM â†’ iStarï¼šä»æ˜¾å¼ PRM åˆ°éšå¼ PRMï¼ˆDPOï¼‰ï¼Œä» verifiable åˆ° unverifiable reward
- iStar â†’ MIGï¼šä»ç›¸å¯¹æ¯”è¾ƒåˆ°ä¿¡æ¯è®ºå®šä¹‰çš„"çªç ´å¥–åŠ±"ï¼ˆé˜² pump-and-dumpï¼‰
- **GiGPO â†’ HiPERï¼ˆæ–°è·¯çº¿ï¼‰**ï¼šä» step ç²’åº¦ flat RL â†’ subgoal-segment ç²’åº¦ hierarchical RLï¼Œæ˜¾å¼ Plan-Execute interfaceï¼ŒHAE æœ‰åŒé‡ç†è®ºä¿è¯
- **çºµå‘ vs æ¨ªå‘ï¼ˆv5 æ–°ï¼‰**ï¼šGiGPO/AgentPRM/iStar/MIG/HiPER/SeeUPO è§£å†³å• agent å†…çš„æ—¶é—´ç»´åº¦ credit assignmentï¼›SHARP è§£å†³ multi-agent æ¨ªå‘çš„ agent é—´å½’å› 
- **CSOï¼šå¤±è´¥è½¨è¿¹ç»´åº¦ï¼ˆv7 æ–°ï¼‰**ï¼šä¸Šè¿°æ‰€æœ‰æ–¹æ¡ˆéƒ½ä»æˆåŠŸè½¨è¿¹å­¦ä¹ ï¼›CSO æ˜¯é¦–ä¸ªç³»ç»Ÿæ€§ä»å¤±è´¥è½¨è¿¹å‡ºå‘çš„æ–¹æ¡ˆâ€”â€”"ä»€ä¹ˆæ­¥éª¤æ¢ä¸€ä¸ªåŠ¨ä½œèƒ½è®©æ•´ä»¶äº‹æˆåŠŸ"ï¼ˆåäº‹å®å› æœæ¨æ–­ï¼‰ï¼Œä¸æˆåŠŸè½¨è¿¹ä¿¡å·äº’è¡¥

**å…³é”®ç»´åº¦å¯¹æ¯”**ï¼š

| | éœ€è¦é¢å¤– rollout | éœ€è¦ verifiable reward | çŠ¶æ€é‡å å‡è®¾ | é¢å¤–æ¨¡å‹ |
|---|---|---|---|---|
| GiGPO | âŒ | âœ… | âœ…ï¼ˆè¯­è¨€ç©ºé—´ç½•è§ï¼‰| âŒ |
| AgentPRM | âœ… | âœ… | âŒ | âœ…ï¼ˆPRM ç½‘ç»œï¼‰|
| iStar | âŒ | âŒï¼ˆæ”¯æŒ unverifiableï¼‰| âŒ | âœ…ï¼ˆimplicit PRMï¼‰|
| MIG | âŒ | âœ… | âŒ | âŒï¼ˆä¿¡æ¯è®ºè®¡ç®—ï¼‰|

**é¢è¯•ä¸€å¥è¯**ï¼šiStar æ˜¯ç›®å‰ label-efficient + unverifiable reward æ”¯æŒæœ€å¥½çš„æ–¹æ¡ˆï¼ˆDPOâ‰¡step-BT ç†è®ºä¿è¯ï¼‰ï¼›GiGPO æ˜¯è®¡ç®—æœ€è½»é‡çš„æ–¹æ¡ˆï¼ˆå…é¢å¤– rolloutï¼Œhashmap O(n)ï¼‰ï¼›MIG æ˜¯ä¿¡æ¯è®ºè§†è§’æœ€ä¼˜é›…çš„æ–¹æ¡ˆï¼›HiPER æ˜¯ subgoal-segment ç²’åº¦çš„ä»£è¡¨ï¼ˆALFWorld SOTAï¼‰ã€‚é€‰æ‹©åŸåˆ™ï¼šæœ‰çŠ¶æ€é‡å  â†’ GiGPOï¼›æœ‰ unverifiable reward â†’ iStarï¼›æƒ³è¦ç†è®ºä¿è¯ + é•¿ horizon â†’ HiPERã€‚

**å…³é”®ç»´åº¦å¯¹æ¯”ï¼ˆv6 æ›´æ–°ï¼‰**ï¼š

| | éœ€è¦é¢å¤– rollout | éœ€è¦ verifiable reward | çŠ¶æ€é‡å å‡è®¾ | é¢å¤–æ¨¡å‹ | ç†è®ºä¿è¯ |
|---|---|---|---|---|---|
| GiGPO | âŒ | âœ… | âœ…ï¼ˆè¯­è¨€ç©ºé—´ç½•è§ï¼‰| âŒ | æ— åæ¢¯åº¦ï¼ˆpaperï¼‰ |
| AgentPRM | âœ… | âœ… | âŒ | âœ…ï¼ˆPRM ç½‘ç»œï¼‰| æ—  |
| **iStar** | **âŒ** | **âŒï¼ˆâœ… unverifiableï¼‰** | **âŒ** | **âœ…ï¼ˆimplicit PRMï¼‰** | **DPOâ‰¡step-BT model** |
| MIG | âŒ | âœ… | âŒ | âŒï¼ˆä¿¡æ¯è®ºè®¡ç®—ï¼‰| ä¿¡æ¯è®ºä¸Šç•Œ |
| HiPER | âŒ | âœ…ï¼ˆtrajectoryï¼‰| âŒ | âŒï¼ˆHAE è®¡ç®—ï¼‰| æ— åæ€§ + æ–¹å·®å‡å°‘ |
| **CSO** | **âœ…ï¼ˆéªŒè¯ rolloutï¼‰** | **âœ…ï¼ˆå¯éªŒè¯ç»“æœï¼‰** | **âŒ** | **âœ…ï¼ˆexpert modelï¼‰** | **åäº‹å®å› æœï¼ˆempiricalï¼‰** |

**é¢è¯•è¡¥å……ï¼ˆv7ï¼‰**ï¼šCSO çš„ç‹¬ç‰¹è§’è‰²â€”â€”å…¶ä»–æ–¹æ¡ˆéƒ½é—®"ä»€ä¹ˆåšå¯¹äº†"ï¼ŒCSO é—®"ä»€ä¹ˆæ¢æ‰åèƒ½æˆåŠŸ"ï¼Œæ˜¯ Credit Assignment è°±ç³»é‡Œå”¯ä¸€å¼€é‡‡å¤±è´¥è½¨è¿¹çš„æ–¹æ¡ˆã€‚16% å…³é”®æ­¥éª¤ = é«˜ç†µæ­¥éª¤åŸåˆ™åœ¨ Agent é¢†åŸŸçš„é¦–æ¬¡ç³»ç»ŸéªŒè¯ã€‚**è·¨åŸŸå°è¯ï¼ˆv11ï¼‰**ï¼šåŒä¸€å¤© SIAï¼ˆICML 2026ï¼ŒarXiv:2602.21215ï¼‰ç‹¬ç«‹å‘ç°æ¨ç†æ—¶å¯¹é½ä¹Ÿæ˜¯ sparse control problemâ€”â€”20% Junction tokenï¼ˆé«˜ç†µèŠ‚ç‚¹ï¼‰æ‰¿æ‹… 100% å¯¹é½æ•ˆæœã€‚CSO 16%ï¼ˆAgent RL creditï¼‰ + SIA 20%ï¼ˆInference Alignmentï¼‰= **ã€Œå…³é”®å†³ç­–å¤©ç„¶ç¨€ç–ã€çš„è·¨é¢†åŸŸåŒé‡å®è¯**ï¼Œé¢è¯•æ—¶å¼•ç”¨è¿™ä¸ªè·¨åŸŸä¸€è‡´æ€§è¿œæ¯”å•è®²è®ºæ–‡æ›´æœ‰æ·±åº¦ã€‚è§ï¼š[[AI/2-Agent/Agentic-RL/CSO-Verified-Critical-Step-Optimization|CSOï¼ˆ2602.03412ï¼‰]] + [[AI/3-LLM/Inference/SIA-Sparse-Inference-time-Alignment|SIAï¼ˆ2602.21215ï¼‰]]

### Multi-Turn RL å››æ”¯æŸ±ï¼ˆv8 æ–°å¢ï¼‰

æŠŠ multi-turn RL çš„æŒ‘æˆ˜åˆ†è§£ä¸ºå››ä¸ªæ­£äº¤ç»´åº¦ï¼Œæ¯ä¸ªç»´åº¦æœ‰æ ‡å¿—æ€§è§£æ³•ï¼š

```
æ”¯æŸ± 1 â€” Rollout è´¨é‡: TSR (ICML 2026)
  è®­ç»ƒæ—¶æ ‘æœç´¢ï¼ˆBest-of-N / Beam / Lookaheadï¼‰æ›¿ä»£ naive random rollout
  Instance Filtering è¿‡æ»¤ç¡®å®šæ€§ caseï¼Œä¿ç•™ hard cases
  Optimizer-agnosticï¼ˆPPO/GRPO å‡å…¼å®¹ï¼‰ï¼›0.5B+TSR â‰ˆ 3B æ—  TSR

æ”¯æŸ± 2 â€” Credit Assignment: GiGPO/HiPER/iStar/CSO
  ç²¾ç¡®æ­¥éª¤çº§ä¿¡å·ï¼ˆä» trajectory-level åˆ° step/subgoal/åäº‹å® ç»´åº¦ï¼‰
  è¯¦è§ Credit Assignment è°±ç³»è¡¨ï¼ˆ10 æ–¹æ¡ˆå…¨è¦†ç›–ï¼‰

æ”¯æŸ± 3 â€” å‡è¡¡æ§åˆ¶: SCoRe (ICLR 2025)
  è‡ªæˆ‘çº é”™æ˜¯å¤šå‡è¡¡ä¼˜åŒ–ï¼šçœŸæ­£çº é”™ vs å‡çº é”™åˆ·åˆ†
  Phase 1 KL çº¦æŸé€šè¿‡è¯­ä¹‰é”å®šåˆ é™¤å‡çº é”™å‡è¡¡çš„å¯è¡ŒåŸŸ
  Phase 2 æ”¾å¼€è®­ç»ƒï¼Œåªæœ‰"çœŸçº é”™"æ–¹å‘æ˜¯å¯è¡Œè§£

æ”¯æŸ± 4 â€” åæ€å†…åŒ–: ERL (2602.13949)  â† v8 æ–°å¢
  æ¯ episode æ˜¾å¼åæ€å¾ªç¯ï¼ˆexperienceâ†’reflectionâ†’consolidationï¼‰
  RL å¯¹é½ä¸¤æ¬¡å°è¯•+åæ€ï¼›SFT è’¸é¦æˆåŠŸ yÂ² è¿› base policy
  éƒ¨ç½²æ—¶é›¶é¢å¤–æˆæœ¬ï¼ˆè’¸é¦å†…åŒ–ï¼‰ï¼›è·¨ episode åæ€è®°å¿† m
  Sokoban +81%ï¼ŒHotpotQA +11%ï¼›ç¨€ç– reward + æœªçŸ¥åŠ¨æ€åœºæ™¯å¢ç›Šæœ€å¤§
```

**å››æ”¯æŸ±æ­£äº¤å¯å åŠ **ï¼šTSRï¼ˆå¥½çš„ rolloutï¼‰+ Creditï¼ˆå¥½çš„ä¿¡å·ï¼‰+ SCoReï¼ˆç¨³å®šå‡è¡¡ï¼‰+ ERLï¼ˆåæ€å­¦ä¹ ï¼‰ç†è®ºä¸Šå¯ä»¥å…±åŒä½¿ç”¨ã€‚

### ç»Ÿä¸€è§†è§’

HiPER / PARL / TSR è§£å†³"è®­ç»ƒè¿‡ç¨‹ç¨³å®šæ€§"ï¼ŒCredit Assignment è°±ç³»è§£å†³"ä¿¡å·ç²’åº¦ä¸è¶³"ï¼ŒSCoRe è§£å†³"å¤šå‡è¡¡é€‰æ‹©é—®é¢˜"ï¼ŒERL è§£å†³"ç¨€ç–åé¦ˆçš„åæ€å†…åŒ–"â€”â€”å››ä¸ªç»´åº¦æ­£äº¤ï¼Œæœ€ä¼˜è§£æ˜¯å åŠ ä½¿ç”¨ï¼ˆTSR + iStar + ERL æ˜¯å€¼å¾—æ¢ç´¢çš„ç»„åˆï¼‰ã€‚

---

---

## éš¾é¢˜ 4ï¼šé™æ€ Workflow/Topology æ˜¯æ€§èƒ½ç“¶é¢ˆ

### é—®é¢˜

2/17-21 æ¶Œç°äº†ä¸€æ‰¹å…±åŒæŒ‡å‘åŒä¸€æ ¹æœ¬é—®é¢˜çš„è®ºæ–‡ï¼š

> **ã€Œæœ€ä½³æ¨¡å‹èƒ½åŠ›ã€å’Œã€Œæœ€ä½³ä»»åŠ¡è¡¨ç°ã€ä¹‹é—´å­˜åœ¨ workflow gap**â€”â€”ä¸æ˜¯ LLM æœ¬èº«çš„é—®é¢˜ï¼Œæ˜¯ workflow è®¾è®¡çš„é—®é¢˜ã€‚

å…·ä½“è¡¨ç°ï¼š
- åŒä¸€ä¸ª 7B æ¨¡å‹ï¼Œç”¨ä¸åŒ workflow å¯ä»¥å·®å‡º 15%+ çš„ pass@1
- å¤æ‚ä»»åŠ¡éœ€è¦ dense cross-agent DAGï¼Œç®€å•ä»»åŠ¡åªéœ€ chainï¼Œé™æ€é€‰æ‹©ä¸€ç§å¿…æœ‰æŸå¤±
- å•ä¸€ policy çš„ simplicity biasï¼šagent å¯¹æ‰€æœ‰éš¾åº¦çš„ä»»åŠ¡éƒ½ç”¨ç›¸åŒå‚æ•°é‡åº”å¯¹

### å››ç§è§£æ³•ä»ä¸åŒç²’åº¦åˆ‡å…¥ï¼š

**è§£æ³• A â€” FlowSteer CWRPOï¼ˆ2602.01664ï¼‰: Operator çº§åˆ«**

æŠŠæ•°å­¦è§£é¢˜åˆ†è§£ä¸º operator åºåˆ—ï¼Œç”¨ RL å­¦ç¼–æ’é¡ºåºï¼š
```
æ ¸å¿ƒåˆ›æ–°ï¼šæ¡ä»¶é‡Šæ”¾è®¾è®¡
R(Ï„) = R_struct + I[R_struct â‰¥ Î¸] Ã— Î» Ã— R_ans
                    â†‘
        åªæœ‰ç»“æ„è´¨é‡è¾¾æ ‡ï¼Œæ‰ç»™ correctness reward
```
åˆ‡æ–­äº† "shortcut ç­”æ¡ˆ bypass ç»“æ„è´¨é‡" çš„å¥–åŠ±è·¯å¾„ã€‚

**è§£æ³• B â€” AgentConductorï¼ˆ2602.17100ï¼‰: Agent é€šä¿¡ Topology çº§åˆ«**

ç”¨ RL è®­ç»ƒ 3B orchestrator ä¸ºæ¯é“é¢˜ç”Ÿæˆ YAML æ ¼å¼çš„ DAGï¼š
```
å…³é”®å‘ç°ï¼š
- ç®€å•é¢˜ï¼šsparse chainï¼ˆå¯†åº¦ä½ï¼‰ï¼ŒèŠ‚çœ 68% token
- éš¾é¢˜ï¼šdense cross-layer DAGï¼ˆå¯†åº¦é«˜ï¼‰
- density function = f(task_difficulty)
```
ä¸‰ä¸ªæŒ‡æ ‡åŒå‘æ”¹å–„ï¼špass@1 +14.6%ï¼Œdensity -13%ï¼Œcost -68%ã€‚**è¿™æ˜¯ "è¶Šéš¾é¢˜ç”¨è¶Šå¤æ‚å›¾" çš„ç¬¬ä¸€æ¬¡ formalizationã€‚**

**è§£æ³• C â€” SquRLï¼ˆ2602.15564ï¼‰: Workflow é€‰æ‹©çº§åˆ«**

å½¢å¼åŒ–è¯æ˜åŠ¨æ€ workflow é€‰æ‹©çš„ç†è®ºä¼˜åŠ¿ï¼ˆTheorem 3.1ï¼‰ï¼š

$$\text{EX}_{\text{dynamic}} \geq \text{EX}_{\text{static}}ï¼Œ\Delta = 0 \text{ iff æŸä¸ª workflow è¦†ç›–æ‰€æœ‰ success regions}$$

Oracle evaluation æ˜¾ç¤ºåŠ¨æ€é€‰æ‹©ä¸Šç•Œè¾¾åˆ° 81.5%ï¼Œè¿œè¶…ä»»ä½•å•ä¸€é™æ€ workflowã€‚
æ ¸å¿ƒæœºåˆ¶ï¼š**Dynamic Actor Masking**ï¼ˆéšæœº dropout actorsï¼Œå¼ºè¿«æ¢ç´¢æ›´å¤š workflow ç»„åˆï¼‰ã€‚

**è§£æ³• D â€” PA-MoEï¼ˆ2602.17038ï¼‰: Expert è·¯ç”±çº§åˆ«ï¼ˆPhase-Aware MoEï¼‰**

å•ä¸€ policy çš„ simplicity bias æ ¹æºï¼šä¸åŒä»»åŠ¡ phase éœ€è¦ä¸åŒ skillï¼Œä½†åŒä¸€ policy ç”¨åŒä¸€å‚æ•°è¦†ç›–æ‰€æœ‰ phaseï¼š
```
Phase è¯†åˆ«ï¼šCrossAttn(obs, goal) + LSTM(action history)
è·¯ç”±ç²’åº¦ï¼š8æ¬¡/episodeï¼ˆæ¯” token-level çš„45æ¬¡ æ›´åˆé€‚ï¼Œæ¯” trajectory-level çš„3æ¬¡ æ›´ç»†ï¼‰
æ•ˆæœï¼š1.5B PA-MoE > 7B baseline
```

**è§£æ³• E â€” AdaptOrchï¼ˆ2602.16873ï¼‰: æ¨ç†æ—¶ç¼–æ’æ‹“æ‰‘è·¯ç”±**

ä»¥ä¸Š ABCD å››ç§è§£æ³•éƒ½æ˜¯**è®­ç»ƒæ—¶**çš„ workflow ä¼˜åŒ–ã€‚AdaptOrch åˆ™è§£å†³**æ¨ç†æ—¶**çš„é—®é¢˜ï¼šå½“ LLM æ€§èƒ½è¶‹äºæ”¶æ•›ï¼ˆfrontier æ¨¡å‹ MMLU ç›¸å·® <3%ï¼‰ï¼Œå¦‚ä½•ç¼–æ’å¤š agent çš„ç»“æ„æ‹“æ‰‘å˜å¾—æ¯”é€‰å“ªä¸ªæ¨¡å‹æ›´é‡è¦ã€‚

**æ ¸å¿ƒå½¢å¼åŒ–**ï¼ˆPerformance Convergence Scaling Lawï¼‰ï¼š
$$\frac{\text{Var}_\tau}{\text{Var}_M} \geq \frac{(\omega(G_T)-1)^2}{4\epsilon^2 k} \cdot (1-\gamma(G_T))^2$$

- $\epsilon$ â†’ 0ï¼ˆæ¨¡å‹æ”¶æ•›ï¼‰æ—¶ï¼Œæ‹“æ‰‘æ–¹å·®/æ¨¡å‹æ–¹å·® â†’ âˆ
- ç¼–ç ä»»åŠ¡ï¼šæ‹“æ‰‘å½±å“æ˜¯æ¨¡å‹é€‰æ‹©çš„ **20x**ï¼ˆæ•°å­¦å¯è¯ï¼‰

**Topology Routingï¼ˆAlgorithm 1ï¼‰**ï¼šåŸºäºä»»åŠ¡ä¾èµ– DAG çš„ä¸‰ä¸ªç»“æ„æŒ‡æ ‡è·¯ç”±ï¼š
- $\omega$ï¼ˆparallelism widthï¼‰: æœ€å¤§åé“¾å®½åº¦ï¼Œå¯å¹¶è¡Œå­ä»»åŠ¡æ•°
- $\gamma$ï¼ˆcoupling densityï¼‰: å­ä»»åŠ¡é—´ context è€¦åˆå¼ºåº¦
- $\delta$ï¼ˆcritical path depthï¼‰: æœ€é•¿é¡ºåºæ‰§è¡Œè·¯å¾„

å››ç§æ‹“æ‰‘ â†’ çº¿æ€§æ—¶é—´è·¯ç”±å†³ç­– â†’ SWE-bench/GPQA/RAG +12â€“23%ï¼ˆç›¸åŒæ¨¡å‹ï¼‰

**ä¸è®­ç»ƒæ—¶æ–¹æ³•çš„å®šä½**ï¼šAdaptOrch å’Œ AgentConductor/SquRL äº’è¡¥â€”â€”è®­ç»ƒæ—¶ç”¨ RL å­¦æœ€ä¼˜ workflowï¼Œæ¨ç†æ—¶ç”¨ç»“æ„è·¯ç”±é€‰æœ€ä¼˜æ‹“æ‰‘ã€‚å‰è€…ä¼˜åŒ– policyï¼Œåè€…ä¼˜åŒ– orchestrationã€‚

### äº”ç§è§£æ³•çš„å®šä½æ¯”è¾ƒ

| è§£æ³• | æ—¶æœº | ç²’åº¦ | æ ¸å¿ƒæœºåˆ¶ | ä»£è¡¨ä»»åŠ¡ | æ ¸å¿ƒè´¡çŒ® |
|------|------|------|---------|---------|---------|
| FlowSteer | è®­ç»ƒæ—¶ | Operator åºåˆ— | æ¡ä»¶å¥–åŠ±é—¨æ§ | æ•°å­¦è§£é¢˜ | åˆ‡æ–­ shortcut reward |
| AgentConductor | è®­ç»ƒæ—¶ | Agent é€šä¿¡å›¾ | RL ç”Ÿæˆ DAG | ç«èµ›ä»£ç  | difficulty-aware density |
| SquRL | è®­ç»ƒæ—¶ | Workflow é€‰æ‹© | Dynamic Actor Masking | Text-to-SQL | ç†è®ºè¯æ˜ dynamic > static |
| PA-MoE | è®­ç»ƒæ—¶ | MoE expert è·¯ç”± | Phase-aware routing | ALFWorld/WebShop | å‚æ•°æ•ˆç‡ |
| **AdaptOrch** | **æ¨ç†æ—¶** | **Agent ç¼–æ’æ‹“æ‰‘** | **DAG ç»“æ„è·¯ç”±** | **SWE-bench/GPQA** | **convergence scaling law** |

**ç»Ÿä¸€è§†è§’**ï¼šäº”è€…éƒ½åœ¨è§£å†³ã€Œå›ºå®šç»“æ„æ— æ³•é€‚åº”ä»»åŠ¡å¤šæ ·æ€§ã€çš„é—®é¢˜ã€‚è®­ç»ƒæ—¶ ABCD ä¼˜åŒ– policy å­¦ä¹ å¦‚ä½•é€‰æ‹©ï¼›æ¨ç†æ—¶ E ç”¨ç»“æ„åˆ†æç›´æ¥è·¯ç”±ã€‚æœ€ä¼˜å®è·µæ˜¯è®­ç»ƒ + æ¨ç†ä¸¤å±‚ä¼˜åŒ–å åŠ ã€‚

---

## æ•´åˆæ¡†æ¶ï¼š2026 Agentic RL ç ”ç©¶åœ°å›¾ï¼ˆv3ï¼‰

```
Agentic RL è®­ç»ƒ Pipeline
â”‚
â”œâ”€â”€ ğŸ—ï¸ ç»´åº¦ 1ï¼šç¯å¢ƒè®¾è®¡
â”‚   â””â”€â”€ EnterpriseGym Corecraftï¼ˆé«˜ä¿çœŸä¼ä¸šç¯å¢ƒï¼‰
â”‚       åŸåˆ™ï¼štask diversity + expert rubrics + realistic workflows
â”‚
â”œâ”€â”€ ğŸ¯ ç»´åº¦ 2ï¼šReward è®¾è®¡
â”‚   â”œâ”€â”€ [Verifiable] ToRL/ARTIST/ASTRA/RC-GRPO
â”‚   â”œâ”€â”€ [UnverifiableÂ·step] iStar â€” trajectory DPO â†’ implicit PRM step reward
â”‚   â”œâ”€â”€ [UnverifiableÂ·turn] CM2 â€” Checklist rewardsï¼ˆbinary criteria decompositionï¼‰
â”‚   â”œâ”€â”€ OpenRS â€” Rubric-based rewardï¼ˆå¯è§£é‡Šï¼Œè·¨ä»»åŠ¡æ³›åŒ–ï¼‰
â”‚   â””â”€â”€ FlowSteer â€” æ¡ä»¶é‡Šæ”¾ rewardï¼ˆç»“æ„è´¨é‡é—¨æ§ï¼‰
â”‚
â”œâ”€â”€ âš™ï¸ ç»´åº¦ 3ï¼šè®­ç»ƒç®—æ³•ï¼ˆCredit Assignment + ç¨³å®šæ€§ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ Credit Assignment è°±ç³»ï¼ˆ9æ–¹æ¡ˆï¼Œv6 å…¨è¦†ç›–å« iStar æ­£å¼å…¥è¡¨ï¼‰
â”‚   â”‚   â”œâ”€â”€ [è½¨è¿¹çº§] GRPO â†’ LOOPï¼ˆLOO baselineï¼Œå… criticï¼‰
â”‚   â”‚   â”œâ”€â”€ [æ­¥éª¤çº§Â·anchor] GiGPOï¼ˆçŠ¶æ€é‡è®¿ â†’ å¤©ç„¶å¯¹æ¯”ï¼Œå…é¢å¤– rolloutï¼ŒNeurIPS 2025ï¼‰
â”‚   â”‚   â”œâ”€â”€ [æ­¥éª¤çº§Â·MC] AgentPRMï¼ˆMC rollout Q-targetï¼Œæ˜¾å¼ PRMï¼Œ3B > GPT-4oï¼‰
â”‚   â”‚   â”œâ”€â”€ [æ­¥éª¤çº§Â·éšå¼ â˜…â˜…â˜…â˜…â˜…] iStarï¼ˆtrajectory DPO â‰¡ step-wise BT modelï¼Œrolling ref=Ï€_oldï¼Œâœ… unverifiableï¼ŒSOTOPIA +48%ï¼ŒTongyi 2025/09ï¼‰
â”‚   â”‚   â”œâ”€â”€ [æ­¥éª¤çº§Â·ä¿¡æ¯è®º] MIGï¼ˆMarginal Information Gainï¼ŒMonotonic Watermarkï¼‰
â”‚   â”‚   â”œâ”€â”€ [subgoalæ®µçº§Â·å±‚çº§] HiPERï¼ˆPlan-Execute + HAEï¼ŒåŒé‡ç†è®ºä¿è¯ï¼ŒICML 2026 â˜…â˜…â˜…â˜…â˜…ï¼‰
â”‚   â”‚   â”œâ”€â”€ [å›åˆçº§Â·ç†è®º] SeeUPOï¼ˆé€†åºæ›´æ–°ï¼ŒGRAE+PPU ä¸å¯èƒ½å®šç†ï¼ŒAppWorld +43-54%ï¼‰
â”‚   â”‚   â””â”€â”€ [æ¨ªå‘Â·multi-agent] SHARPï¼ˆShapley + counterfactual maskingï¼ŒICML 2026ï¼‰
â”‚   â”‚
â”‚   â”œâ”€â”€ è®­ç»ƒç¨³å®šæ€§
â”‚   â”‚   â”œâ”€â”€ RAGEN/StarPO â€” Echo Trap è¯Šæ–­ + StarPO-S ä¸‰æœºåˆ¶ï¼ˆtrajectory filtering / critic baselining / decoupled clippingï¼‰
â”‚   â”‚   â”œâ”€â”€ TSR â€” training-time tree search rolloutï¼ˆrollout quality â†’ stabilityï¼‰
â”‚   â”‚   â”œâ”€â”€ SCoRe â€” Phase 1 KL çº¦æŸåˆå§‹åŒ–ï¼ˆself-correction RLï¼‰
â”‚   â”‚   â””â”€â”€ SORL â€” Off-policy multi-turn ä¸“ç”¨ï¼šTurn-Level IS + CTNï¼Œå®ä¾‹åŒ–ä¸º SO-PPO/SO-GRPO
â”‚   â”‚
â”‚   â””â”€â”€ Multi-agent RL
â”‚       â”œâ”€â”€ PARL â€” Freeze subagentsï¼Œåªè®­ç»ƒ orchestratorï¼ˆKimi K2.5ï¼‰
â”‚       â”œâ”€â”€ MAGRPO â€” Dec-POMDP + joint reward CTDE
â”‚       â”œâ”€â”€ AT-GRPO â€” Agent-and-Turn-Wise Grouping
â”‚       â”œâ”€â”€ MARS2 â€” diversity-as-scalingï¼ˆ2Ã—32B > 72Bï¼‰
â”‚       â””â”€â”€ Dr. MAS â€” per-agent normalizationï¼Œé˜²æ¢¯åº¦çˆ†ç‚¸ï¼ˆNTUï¼‰
â”‚
â”œâ”€â”€ ğŸ”— ç»´åº¦ 4ï¼šWorkflow / Topology è®¾è®¡
â”‚   â”œâ”€â”€ [è®­ç»ƒæ—¶] AgentConductor â€” RL ç”Ÿæˆ agent communication DAGï¼ˆéš¾åº¦è‡ªé€‚åº”å¯†åº¦ï¼‰
â”‚   â”œâ”€â”€ [è®­ç»ƒæ—¶] SquRL â€” RL åŠ¨æ€é€‰æ‹©æœ€ä¼˜ workflow ç»„åˆï¼ˆTheorem 3.1 å½¢å¼åŒ–è¯æ˜ï¼‰
â”‚   â”œâ”€â”€ [è®­ç»ƒæ—¶] PA-MoE â€” Phase-aware expert routingï¼ˆ8æ¬¡/episodeï¼Œ1.5B > 7B baselineï¼‰
â”‚   â”œâ”€â”€ [è®­ç»ƒæ—¶] FlowSteer â€” Operator çº§ workflow RLï¼ˆæ¡ä»¶å¥–åŠ±é—¨æ§ï¼‰
â”‚   â””â”€â”€ [æ¨ç†æ—¶] AdaptOrch â€” ä»»åŠ¡ DAG ç»“æ„æ€§è·¯ç”±ï¼ˆConvergence Scaling Lawï¼Œtopology > model selectionï¼‰
â”‚
â”œâ”€â”€ ğŸ“¦ ç»´åº¦ 5ï¼šContext Overflowï¼ˆv3 æ–°å¢ï¼‰
â”‚   â””â”€â”€ KLong â€” Trajectory-splitting SFT + Progressive RL
â”‚       â”œâ”€â”€ è§£å†³ï¼šè½¨è¿¹è¶…è¿‡ context window çš„ç‰©ç†è¾¹ç•Œé—®é¢˜
â”‚       â”œâ”€â”€ æ–¹æ¡ˆï¼šå›ºå®š prefix + æ¸è¿›æˆªæ–­ + é‡å å­è½¨è¿¹ + é€æ­¥å»¶ä¼¸ timeout
â”‚       â””â”€â”€ æ•ˆæœï¼š106B è¶… Kimi K2 Thinking 1Tï¼ˆ10x å‚æ•°ï¼‰11.28% on PaperBench
â”‚
â””â”€â”€ ğŸ“ è¯„ä¼°
    â””â”€â”€ PaperBench / MLE-bench / SWE-bench / ALFWorld / WebShop / SynSQL / tau-Bench
```

---

---

## è·¨åŸŸè¿æ¥ï¼šAgentic RL ä¸ Safety çš„æ±‡åˆ

2/19 çš„ä¸€ç¯‡è®ºæ–‡ï¼ˆ2602.17546ï¼‰æ­ç¤ºäº†ä¸€ä¸ªé‡è¦å‘ç°ï¼Œè™½ç„¶ä¸ç›´æ¥æ˜¯ agentic RLï¼Œä½†å¯¹ agent safety æœ‰ç›´æ¥æ„ä¹‰ï¼š

**Harmful intent åœ¨ pre-generation activation ä¸­çº¿æ€§å¯åˆ†ï¼ˆAUROC > 0.9ï¼‰**

è¿™æ„å‘³ç€ï¼š
1. Agent åœ¨è°ƒç”¨å·¥å…·ã€å†™ codeã€è®¿é—® memory **ä¹‹å‰**ï¼Œå…¶å†…éƒ¨çŠ¶æ€å·²ç»ç¼–ç äº† intent
2. å¯ä»¥ç”¨è½»é‡ probe åœ¨ generation å‘ç”Ÿä¹‹å‰æ£€æµ‹å¹¶æ‹¦æˆª
3. å¯¹äº agentic workflowï¼Œå¯ä»¥åœ¨æ¯ä¸ª action step ä¹‹å‰æ’å…¥ safety gate

è¿™ä¸ç›¾å«é¡¹ç›®çš„æ ¸å¿ƒæ€è·¯å®Œå…¨å¥‘åˆï¼š**ä¸æ˜¯ç­‰ agent è¾“å‡ºæœ‰å®³å†…å®¹å†æ‹¦æˆªï¼Œè€Œæ˜¯åœ¨ forward pass ä¸­æ—©æœŸå‘ç°æ„å›¾ï¼Œé›¶ inference overhead**ã€‚

---

---

## éš¾é¢˜ 5ï¼šContext Overflow â€” è½¨è¿¹è¶…è¿‡ Context Windowï¼ˆv3 æ–°å¢ï¼‰

### é—®é¢˜

æ‰€æœ‰ä¸Šè¿°å·¥ä½œéƒ½éšå«ä¸€ä¸ªå‡è®¾ï¼š**è½¨è¿¹èƒ½æ”¾è¿› context window**ã€‚ä½† 2026 å¹´æœ€å¤æ‚çš„ agent ä»»åŠ¡ï¼ˆå¤ç° ML è®ºæ–‡ã€é•¿æœŸ ML ç«èµ›ï¼‰äº§ç”Ÿçš„è½¨è¿¹é•¿åº¦è¿œè¶… context windowï¼š

| ä»»åŠ¡ç±»å‹ | ä»£è¡¨ Benchmark | å…¸å‹è¿è¡Œæ—¶é•¿ | Assistant Turns |
|---------|--------------|------------|----------------|
| Long-horizon | SWE-bench Verified | åˆ†é’Ÿçº§ | 20â€“200 |
| **Extremely long-horizon** | **PaperBench, MLE-bench** | **6â€“12 å°æ—¶** | **700+** |

"æé•¿ horizon"çš„ç‹¬ç‰¹æŒ‘æˆ˜ï¼š
- å•æ¡è½¨è¿¹ç‰©ç†ä¸Šæ”¾ä¸è¿› contextï¼ŒSFT æ— æ³•ç›´æ¥è®­ç»ƒ
- RL rollout åœ¨ timeout å†…æ— æ³•å®Œæˆï¼Œreward æ‹¿ä¸åˆ°
- Credit assignment å˜å¾—æ›´æç«¯ç¨€ç–ï¼ˆæ•°åƒæ­¥ delayï¼‰

### è§£æ³•ï¼šKLongï¼ˆNUS + MIT, 2602.17547ï¼Œ2026-02-19ï¼‰

KLong çš„ä¸¤å¤§æ ¸å¿ƒæŠ€æœ¯ï¼š

**1. Trajectory-splitting SFT**ï¼šæŠŠè¶… context è½¨è¿¹åˆ‡æˆé‡å å­è½¨è¿¹

$$\tau^{(i)}_{\text{input}} = [\underbrace{p}_{\text{å›ºå®š prefix}}, s_{t_i}, a_{t_i}, \ldots, s_{t_i+L-1}, a_{t_i+L-1}]$$

- **å›ºå®šå…¨å±€ prefix $p$**ï¼šä»»åŠ¡æè¿° + è®ºæ–‡é˜…è¯»æ®µåœ¨æ¯ä¸ªå­è½¨è¿¹å¼€å¤´ï¼ˆä¿ç•™å…¨å±€ intentï¼‰
- **æ¸è¿›æˆªæ–­**ï¼šè¿‘æœŸ history å…¨ä¿ç•™ï¼Œè¿œæœŸ history é€æ­¥ä¸¢å¼ƒ
- **é‡å ï¼ˆoverlapï¼‰**ï¼šç›¸é‚»å­è½¨è¿¹å…±äº«éƒ¨åˆ†å†…å®¹ï¼Œä¿è¯è¿ç»­æ€§
- æ•ˆæœï¼šassistant turns 114.9 â†’ **732.7**ï¼ˆ6.4 å€ï¼‰

**2. Progressive RL**ï¼šé€æ­¥å»¶ä¼¸ task timeout

$$T^{(1)} < T^{(2)} < \cdots < T^{(M)} \quad (2h \to 4h \to 6h)$$

- å…ˆä» 2h timeout å­¦å±€éƒ¨è¡Œä¸ºï¼Œå»ºç«‹ policy åŸºç¡€
- å†æ‰©å±•åˆ° 6hï¼Œæ¥è¿‘çœŸå®ä»»åŠ¡è§„æ¨¡
- è§£å†³ pipeline imbalanceï¼ˆpartial rollout + priority judge queueï¼‰

**å®éªŒç»“æœ**ï¼šKLong 106B åœ¨ PaperBench è¾¾ 62.59ï¼Œ**è¶… Kimi K2 Thinking 1Tï¼ˆå‚æ•°é‡ 10 å€ï¼‰11.28%**ã€‚

### å…³é”®è®¾è®¡åŸåˆ™

> **Trajectory-splitting çš„æœ¬è´¨**ï¼šå›ºå®šå…¨å±€ intentï¼ˆä»»åŠ¡ + è®ºæ–‡ç†è§£ï¼‰ï¼Œè®©å±€éƒ¨å†å²å¯ä»¥è¢«æˆªæ–­ï¼Œåªè¦å…¨å±€ç›®æ ‡æ¸…æ™°ï¼Œagent å¯ä»¥åœ¨ä»»ä½•å­è½¨è¿¹ä¸­ç»´æŒæ–¹å‘æ„Ÿã€‚

è¿™å’Œäººç±»ä¸“å®¶å¤„ç†é•¿ä»»åŠ¡çš„æ–¹å¼ä¸€è‡´ï¼šéšæ—¶å¯ä»¥å¿˜è®°å…·ä½“å†å²ç»†èŠ‚ï¼Œä½†å§‹ç»ˆè®°å¾—"æˆ‘åœ¨åšä»€ä¹ˆã€ä¸ºä»€ä¹ˆåš"ã€‚

### å¼€æ”¾é—®é¢˜

- Trajectory-splitting çš„ advantage æ˜¯åœ¨å­è½¨è¿¹ group å†…è®¡ç®—ï¼Œè·¨å­è½¨è¿¹çš„å…¨å±€ credit assignment ä»æœªè§£å†³
- Progressive timeout éœ€è¦ç²¾å¿ƒè®¾è®¡é˜¶æ®µåˆ’åˆ†ï¼Œè¿‡æ¸¡æ—¶æœºæ•æ„Ÿ
- Research-Factoryï¼ˆç”¨ Claude Thinking distill è®­ç»ƒæ•°æ®ï¼‰å¼•å…¥äº† teacher-student gapï¼ŒKLong ä¸Šé™è¢« teacher èƒ½åŠ›é”å®š

---

## 2026 å¹´è¿˜æ²¡è§£å†³çš„é—®é¢˜

è¯šå®è¯´ï¼Œå³ä½¿æœ‰ä¸Šé¢è¿™äº›å·¥ä½œï¼Œä»¥ä¸‹é—®é¢˜ä»ç„¶ openï¼š

1. **Subgoal å¦‚ä½•è‡ªåŠ¨ç”Ÿæˆ**ï¼šHiPER æ²¡è¯´ planner å¦‚ä½•ç¡®å®š subgoal è¾¹ç•Œã€‚è¿™æ˜¯ hierarchical RL çš„è€é—®é¢˜ã€‚
2. **Expert rubric çš„æˆæœ¬**ï¼šCorecraft éœ€è¦ä¸“å®¶æ‰‹å†™ 2500+ å®ä½“çš„ rubricã€‚çœŸæ­£é€šç”¨çš„ agentic RL éœ€è¦è‡ªåŠ¨ç”Ÿæˆæˆ–å½’çº³ rubricã€‚
3. **çœŸå®ç¯å¢ƒ vs æ¨¡æ‹Ÿç¯å¢ƒçš„ gap**ï¼šæ‰€æœ‰å·¥ä½œéƒ½åœ¨æ¨¡æ‹Ÿç¯å¢ƒé‡Œè®­ç»ƒï¼ŒçœŸå®ä¼ä¸šç³»ç»Ÿçš„ non-determinism å’Œ side effects ä¼šå¸¦æ¥æ–°çš„æŒ‘æˆ˜ã€‚
4. **é•¿ä»»åŠ¡çš„ overthinking**ï¼šLACONIC è§£å†³äº† reasoning å¤ªé•¿çš„é—®é¢˜ï¼Œä½† agent ä»»åŠ¡çš„"overthinking"ï¼ˆä¸å¿…è¦çš„æ¢ç´¢ã€é‡å¤å·¥å…·è°ƒç”¨ï¼‰æ˜¯å¦ä¸€ä¸ªç»´åº¦â€”â€”æ›´å¤æ‚å› ä¸ºæ¯ä¸€æ­¥éƒ½æœ‰çœŸå®æˆæœ¬ï¼ˆAPI è´¹ç”¨ã€æ—¶é—´ï¼‰ã€‚
5. **Frontier æ¨¡å‹çš„ç“¶é¢ˆ**ï¼šCorecraft å‘ç° Opus 4.6/GPT-5.2 <30% pass rateï¼Œè¿™è¯´æ˜é—®é¢˜ä¸ä»…ä»…æ˜¯è®­ç»ƒæ–¹æ³•â€”â€”frontier æ¨¡å‹åœ¨çœŸå® agent ä»»åŠ¡ä¸Šä»æœ‰æ ¹æœ¬å±€é™ã€‚

---

## 2026 å¹´ Agentic RL å·¥ä½œå…¨æ™¯ï¼ˆæŒ‰æ—¶é—´ï¼‰

| æ—¥æœŸ | è®ºæ–‡ | arXiv | ç»´åº¦ | è¯„åˆ† |
|------|------|-------|------|------|
| 2025/04 | RAGEN/StarPO | 2504.20073 | ç®—æ³•Â·ç¨³å®šæ€§Â·Multi-turn | â˜…â˜…â˜…â˜…â˜… |
| 2025/09 | iStar | 2509.19199 | ç®—æ³•Â·Credit | â˜…â˜…â˜…â˜…â˜… |
| 2026/02/01 | MIG | 2602.01034 | ç®—æ³•Â·Credit | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/10 | EnterpriseGym Corecraft | 2602.16179 | ç¯å¢ƒ | â˜…â˜…â˜…â˜…â˜… |
| 2026/02/13 | TSR | 2602.11767 | ç®—æ³•Â·ç¨³å®šæ€§Â·Rolloutè´¨é‡ | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/13 | CM2 | 2602.12268 | RewardÂ·Unverifiable | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/14 | OpenRS | 2602.14069 | Reward | â˜…â˜…â˜…â˜†â˜† |
| 2026/02/15 | HiPER | 2602.16165 | ç®—æ³•Â·CreditÂ·å±‚çº§ | â˜…â˜…â˜…â˜…â˜… |
| 2026/02/16 | Kimi-K2.5 PARL | 2602.02276 | ç®—æ³•Â·Multi-agent | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/17 | FlowSteer CWRPO | 2602.01664 | Workflow | â˜…â˜…â˜…â˜†â˜† |
| 2026/02/17 | SquRL | 2602.15564 | Workflow | â˜…â˜…â˜…â˜†â˜† |
| 2026/02/18 | AdaptOrch | 2602.16873 | WorkflowÂ·æ¨ç†æ—¶ç¼–æ’ | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/19 | KLong | 2602.17547 | Context Overflow | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/19 | AgentConductor | 2602.17100 | Workflow | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/20 | PA-MoE | 2602.17038 | Workflow | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/20 | Calibrate-Then-Act | 2602.11841 | ç®—æ³• | â˜…â˜…â˜…â˜†â˜† |
| 2026/02/21 | SeeUPO | 2602.06554 | ç®—æ³•Â·CreditÂ·ç†è®º | â˜…â˜…â˜…â˜…â˜… |
| 2026/02/17 | AWM | 2602.10090 | ç¯å¢ƒå·¥ç¨‹Â·åˆæˆ | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/18 | SHARP | 2602.08335 | ç®—æ³•Â·CreditÂ·Multi-agent | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/18 | Dr. MAS | 2602.08847 | ç®—æ³•Â·Multi-agent ç¨³å®šæ€§ | â˜…â˜…â˜…â˜…â˜† |
| 2025/11/28 | SORL | 2511.20718 | ç®—æ³•Â·Off-policy ç¨³å®šæ€§Â·Multi-turn | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/23 | Search-R1++ | 2602.19526 | Reward DesignÂ·Policy Opt | â˜…â˜…â˜…â˜†â˜† |
| 2026/02/26 | Search-P1 | 2602.22576 | Reward DesignÂ·Tool Use RL | â˜…â˜…â˜…â˜…â˜† |
| 2025/10 | SCoRe | 2501.09723 | ç®—æ³•Â·å‡è¡¡æ§åˆ¶Â·å¤šè½®çº é”™ | â˜…â˜…â˜…â˜…â˜… |
| 2026/02/13 | CM2 | 2602.12268 | RewardÂ·UnverifiableÂ·å·¥å…·è°ƒç”¨ | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/13 | TSR | 2602.11767 | ç®—æ³•Â·Rolloutè´¨é‡Â·Multi-turn | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/15 | ERL | 2602.13949 | ç®—æ³•Â·åæ€å†…åŒ–Â·ç¨€ç–Reward | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/03 | CSO | 2602.03412 | ç®—æ³•Â·CreditÂ·å¤±è´¥è½¨è¿¹ | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/24 | SELAUR | 2602.21158 | RewardÂ·ä¸ç¡®å®šæ€§æ„ŸçŸ¥Â·å¤±è´¥æ¿€æ´» | â˜…â˜…â˜…â˜†â˜† |
| 2026/02/24 | PyVision-RL | 2602.20739 | è®­ç»ƒç¨³å®šæ€§Â·å¤šæ¨¡æ€Â·Interaction Collapse | â˜…â˜…â˜…â˜†â˜† |
| 2026/02/15 | PABU | â€” | Contextç®¡ç†Â·è¿›åº¦æ„ŸçŸ¥ä¿¡å¿µçŠ¶æ€Â·æ•ˆç‡ | â˜…â˜…â˜…â˜…â˜† |
| â€” | WebPilot | â€” | Multi-AgentÂ·MCTSæˆ˜ç•¥æ¢ç´¢Â·Webä»»åŠ¡ | â˜…â˜…â˜…â˜†â˜† |
| 2026/02 | AgentAuditor | 2602.09341 | Multi-AgentÂ·å®¡è®¡Â·åå…±è¯†åå¥½ä¼˜åŒ– | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/24 | AlphaEvolve | 2602.16928 | MARLÂ·ç®—æ³•è‡ªåŠ¨å‘ç°Â·LLMä»£ç æ¼”åŒ– | â˜…â˜…â˜…â˜…â˜† |
| 2026/02/28 | SRPO | 2602.21515 | MARLÂ·åä½œæ³›åŒ–Â·é£é™©è§„é¿å‡è¡¡ | â˜…â˜…â˜…â˜…â˜† |

---

## å¯¹è€æ¿çš„ç›´æ¥ä»·å€¼

å¦‚æœåœ¨é¢è¯•ä¸­è¢«é—®åˆ°"ä½ å¯¹ agentic RL çš„ç†è§£"ï¼Œè¿™ä¸ªæ¡†æ¶ç»™å‡ºäº†ä¸€ä¸ªç»“æ„åŒ–å›ç­”ï¼š

1. **é—®é¢˜å®šä¹‰**ï¼šä»å¯éªŒè¯ä»»åŠ¡ï¼ˆRLVRï¼‰åˆ°å¼€æ”¾ä»»åŠ¡ï¼ˆAgentic RLï¼‰ï¼Œreward è®¾è®¡å’Œ credit assignment æ˜¯æ ¸å¿ƒéš¾é¢˜
2. **äº”ç»´åˆ†è§£**ï¼šç¯å¢ƒï¼ˆCorecraftï¼‰/ Rewardï¼ˆverifiable/unverifiableï¼‰/ ç®—æ³•ï¼ˆCredit Assignment 6æ–¹æ¡ˆ + ç¨³å®šæ€§ï¼‰/ Workflowï¼ˆAgentConductor/SquRL/PA-MoEï¼‰/ Context Overflowï¼ˆKLongï¼‰
3. **Credit Assignment æ·±ç­”**ï¼šGRPOâ†’LOOPâ†’GiGPOâ†’AgentPRMâ†’iStarâ†’MIG å®Œæ•´è°±ç³»ï¼ŒåŒºåˆ†"éœ€ä¸éœ€è¦é¢å¤– rollout""æ”¯ä¸æ”¯æŒ unverifiable reward""è¦ä¸è¦é¢å¤–æ¨¡å‹"
4. **å¼€æ”¾é—®é¢˜**ï¼šhonest åœ°è¯´æ˜å½“å‰ä¸Šé™â€”â€”extreme horizon çš„è·¨å­è½¨è¿¹ credit assignmentã€expert rubric ç”Ÿæˆæˆæœ¬ã€çœŸå®ç¯å¢ƒ gap

è¿™ç§å›ç­”æ¯”åˆ—ä¸¾è®ºæ–‡åå­—æ·±åº¦é«˜ä¸€ä¸ªæ•°é‡çº§ã€‚

---

## æ ¸å¿ƒæ´å¯Ÿï¼ˆä¸€å¥è¯ï¼‰

**2026 å¹´ Agentic RL çš„æ ¹æœ¬äº‰è®ºä¸æ˜¯"å“ªä¸ªç®—æ³•æ›´å¥½"ï¼Œè€Œæ˜¯"ç“¶é¢ˆåˆ°åº•åœ¨å“ªé‡Œ"ï¼š**

- ç¯å¢ƒæ´¾ï¼šbottleneck æ˜¯ç¯å¢ƒè´¨é‡ï¼ˆCorecraft çš„è¯æ®ï¼‰
- Reward æ´¾ï¼šbottleneck æ˜¯ reward å¯é æ€§ï¼Œç‰¹åˆ«æ˜¯ unverifiable reward åœºæ™¯ï¼ˆCM2/iStar çš„è¯æ®ï¼‰
- Workflow æ´¾ï¼šbottleneck æ˜¯ pipeline é™æ€æ€§ï¼ˆAgentConductor/SquRL çš„è¯æ®ï¼‰
- ç®—æ³•æ´¾ï¼šbottleneck æ˜¯ credit assignmentï¼ˆ6æ–¹æ¡ˆè°±ç³»çš„è¯æ®ï¼‰
- åŸºç¡€è®¾æ–½æ´¾ï¼šbottleneck æ˜¯ context window å’Œè®­ç»ƒç¨³å®šæ€§ï¼ˆKLong/TSR çš„è¯æ®ï¼‰

**æ­£ç¡®ç­”æ¡ˆå¯èƒ½æ˜¯å…¨éƒ¨**â€”â€”ä½†ä¸åŒä»»åŠ¡å’Œä¸åŒå‘å±•é˜¶æ®µï¼Œå„ç»´åº¦çš„æƒé‡ä¸åŒã€‚v3 å¢åŠ çš„ Context Overflow ç»´åº¦æ­ç¤ºäº†ä¸€ä¸ªæ–°è¾¹ç•Œï¼šå½“ä»»åŠ¡å¤æ‚åº¦è¶…å‡º context window çš„ç‰©ç†é™åˆ¶æ—¶ï¼Œéœ€è¦å…¨æ–°çš„è®­ç»ƒæ–¹æ³•è®ºï¼Œè€Œä¸åªæ˜¯æ›´å¥½çš„ç®—æ³•ã€‚

## See Alsoï¼ˆå…¨è·¯å¾„ç´¢å¼•ï¼‰

> æœ¬ç¬”è®°æ­£æ–‡å†…é“¾ä¸º Scholar å†™å…¥çš„ç®€çŸ­è·¯å¾„ï¼›ä»¥ä¸‹ä¸ºé¦†é•¿è¡¥å……çš„å…¨è·¯å¾„å¯¹ç…§ï¼Œä¾¿äº Obsidian å›¾è°±æ£€ç´¢ã€‚

- [[AI/2-Agent/Agentic-RL/Agentic-RL-å…ƒé—®é¢˜-ç“¶é¢ˆä¸çªç ´æ–¹å‘|ğŸ§  Agentic RL å…ƒé—®é¢˜ï¼šç“¶é¢ˆä¸çªç ´æ–¹å‘]] â­ â€” **æœ¬ç»¼è¿°çš„å…ƒå±‚æ‰¹åˆ¤ä¸å‡ç»´**ï¼šåŸºäº37+ç¯‡è®ºæ–‡çš„Wisdomå±‚åˆ¤æ–­ï¼›æŒ‡å‡ºç®—æ³•å±‚å·²å¤Ÿç”¨ï¼ŒçœŸæ­£ç“¶é¢ˆæ˜¯Reward Signal Qualityï¼›æœ¬ç»¼è¿°æ˜¯"æ˜¯ä»€ä¹ˆ"ï¼Œå…ƒé—®é¢˜ç¬”è®°æ˜¯"ä¸ºä»€ä¹ˆä¸å¤Ÿ/ä¸‹ä¸€æ­¥åœ¨å“ª"
- [[AI/2-Agent/Agentic-RL/iStar-Implicit-Step-Rewards-Agentic-RL|iStarï¼ˆ2509.19199ï¼ŒTongyi Labï¼Œâ˜…â˜…â˜…â˜…â˜…ï¼‰]] â€” trajectory DPO â‰¡ step-wise BT modelï¼Œå”¯ä¸€æ”¯æŒ unverifiable reward çš„ step-level CAï¼ŒSOTOPIA +48%ï¼Œ2x æ ·æœ¬æ•ˆç‡
- [[AI/2-Agent/Agentic-RL/Search-R1-Reasoning-Search-Engine-RL|Search-R1ï¼ˆå‰é©±ï¼ŒarXiv:2503.09516ï¼‰]] â€” Search-R1++ (2602.19526) çš„å‰èº«ï¼šæŠŠæœç´¢å¼•æ“é›†æˆè¿› RL rolloutï¼Œtoken masking ç¨³å®šè®­ç»ƒï¼›Search-R1++ åœ¨æ­¤åŸºç¡€ä¸Šç³»ç»Ÿæ¶ˆè reward/optimizer/prompt ä¸‰ç»´åº¦ï¼ˆvault_gapï¼šSearch-R1++ ç‹¬ç«‹ç¬”è®°å¾… Scholar è¡¥å†™ï¼‰
- [[AI/2-Agent/Agentic-RL/Search-P1-Path-Centric-Reward-Agentic-RAG|Search-P1ï¼ˆarXiv:2602.22576ï¼‰]] â€” è·¯å¾„çº§å¯†é›†å¥–åŠ±ï¼ˆv13 æ–°å¢ï¼‰ï¼šæ˜¾å¼ Planner + åŒè½¨è·¯å¾„è¯„åˆ† + è½¯ç»“æœæ‰“åˆ†ï¼Œè§£å†³ Search-R1 ç¨€ç–å¥–åŠ±/å¤±è´¥æ ·æœ¬é›¶æ¢¯åº¦ï¼›+7.7% over Search-R1ï¼Œå·¥ä¸š AD-QA +20.6%ï¼›ä¸ Search-R1++ æ­£äº¤å¯ç»„åˆï¼ˆå¥–åŠ±å¯†åº¦ vs å¥–åŠ±è´¨é‡ï¼‰
-  â€” Agentic RL åœ¨ Agent çŸ¥è¯†åŸŸçš„ä½ç½®
- [[AI/2-Agent/Agentic-RL/FlowSteer-CWRPO-Workflow-Orchestration-RL|FlowSteer (CWRPO)]] â€” ç»´åº¦ 4ï¼šOperator çº§ workflow è®¾è®¡ï¼ˆWorkflow/Topology è§£æ³• Aï¼‰
- [[AI/2-Agent/Multi-Agent/AgentConductor-Topology-Evolution|AgentConductor]] â€” ç»´åº¦ 4ï¼šAgent é€šä¿¡ Topology çº§ï¼ˆè§£æ³• Bï¼Œdifficulty-aware densityï¼‰
- [[AI/2-Agent/Agentic-RL/SquRL-Dynamic-Workflow-Text-to-SQL|SquRL]] â€” ç»´åº¦ 4ï¼šWorkflow é€‰æ‹©çº§ï¼ˆè§£æ³• Cï¼ŒTheorem 3.1 å½¢å¼åŒ–è¯æ˜ï¼‰
- [[AI/3-LLM/RL/Theory/MARS-Margin-Aware-Reward-Modeling-Self-Refinement|MARS]] â€” reward modeling è‡ªé€‚åº”åˆ†é…ï¼ˆä¸ Reward ç»´åº¦é«˜åº¦äº’è¡¥ï¼‰
- [[AI/5-AI å®‰å…¨/Adaptive-Regularization-Safety-Degradation-Finetuning|Adaptive-Regularization]] â€” Agentic RL Ã— Safety æ±‡åˆç‚¹ï¼špre-generation hidden state å®‰å…¨é—¨æ§
- [[AI/2-Agent/Agentic-RL/UI-TARS-2|UI-TARS-2]] â€” GUI Agent RL å·¥ç¨‹æè‡´è·¯çº¿ï¼šData Flywheel + å¼‚æ­¥ multi-turn RL + Hybrid æ²™ç›’ï¼ˆâ˜…â˜…â˜…â˜…â˜…ï¼‰
- [[AI/2-Agent/Agentic-RL/UI-R1-GUI-Action-Prediction-RL|UI-R1]] â€” GUI Agent RL æç®€è·¯çº¿ï¼š136 æ¡æ•°æ® rule-based GRPOï¼Œ3B â‰ˆ SFT 7B@76Kï¼ˆâ˜…â˜…â˜…â˜…â˜†ï¼‰
- [[AI/2-Agent/Fundamentals/Memory-R1-RL-for-LLM-Memory-Management|Memory-R1]] â€” RL è®­ç»ƒ Memory Managerï¼ˆADD/UPDATE/DELETE/NOOPï¼‰ï¼Œè®°å¿†ç®¡ç†æ–°èŒƒå¼ï¼ˆâ˜…â˜…â˜…â˜…â˜†ï¼‰
- [[AI/2-Agent/Agentic-RL/ASTRA-Automated-Tool-Agent-Training|ASTRA]] â€” å…¨è‡ªåŠ¨ tool-use RL æµæ°´çº¿ï¼ŒMCP å·¥å…·å›¾ + verifiable ç¯å¢ƒï¼ˆâ˜…â˜…â˜…â˜…â˜†ï¼‰
- [[AI/2-Agent/Agentic-RL/RC-GRPO-Reward-Conditioned-Tool-Calling-RL|RC-GRPO]] â€” reward token conditioning è§£å†³ multi-turn GRPO reward åŒè´¨åŒ–ï¼ˆâ˜…â˜…â˜…â˜…â˜†ï¼‰
- [[AI/3-LLM/MLLM/PyVision-RL-Agentic-Vision-Interaction-Collapse|PyVision-RLï¼ˆ2602.20739ï¼‰]] â€” **è·¨æ¨¡æ€è®­ç»ƒå¤±è´¥æ¨¡å¼**ï¼šInteraction Collapse = Echo Trap çš„å¤šæ¨¡æ€ç‰ˆæœ¬ï¼ˆv10 æ–°å¢ï¼‰ï¼›Oversampling-Filtering-Ranking + Accumulative Tool Rewardï¼›On-Demand Context Construction è§£å†³è§†é¢‘ context çˆ†ç‚¸
- [[AI/2-Agent/Agentic-RL/SORL-Stabilizing-Off-Policy-RL-Long-Horizon-Agent|SORLï¼ˆ2511.20718ï¼‰]] â€” Off-policy multi-turn RL å´©æºƒçš„ä¸¤æ ¹å› è¯Šæ–­ï¼ˆç²’åº¦é”™é…+æ–¹å·®ç´¯ç§¯ï¼‰+ Turn-Level IS/CTN ä¿®å¤ï¼ŒSO-PPO/SO-GRPO å®ä¾‹åŒ–ï¼ˆ**v12 æ–°å¢**ï¼‰
- [[AI/2-Agent/Agentic-RL/Agent-è¿›åŒ–æ¨¡å¼è°±ç³»|ğŸ§  Agent è¿›åŒ–æ¨¡å¼è°±ç³»]] â­ â€” **ä¸‰å±‚ç»Ÿä¸€æ¡†æ¶**ï¼ˆè®­ç»ƒæ—¶/in-context/è¿è¡Œæ—¶ï¼‰ï¼Œé™„è´¾ç»´æ–¯å®è·µæ˜ å°„ä¸é€‰å‹å†³ç­–æ ‘ï¼ˆ**v13 æ–°å¢ï¼Œè€æ¿æŒ‡ä»¤äº§å‡º**ï¼‰
- [[AI/2-Agent/Agentic-RL/Reflexion-Verbal-Reinforcement-Learning|Reflexionï¼ˆNeurIPS 2023ï¼‰]] â€” in-context è¿›åŒ–å¥ åŸºï¼šverbal reinforcementï¼Œepisodic memory bufferï¼Œæ— éœ€å¾®è°ƒï¼ˆ**v13 æ–°å¢**ï¼‰
- [[AI/2-Agent/Agentic-RL/ExpeL-Experiential-Learning-Agent|ExpeLï¼ˆAAAI 2024ï¼‰]] â€” è·¨ä»»åŠ¡è§„åˆ™æç‚¼ï¼šADD/UPVOTE/DOWNVOTE/EDIT è§„åˆ™åº“ + ç›¸ä¼¼æ¡ˆä¾‹æ£€ç´¢ï¼ˆ**v13 æ–°å¢**ï¼‰
- [[AI/2-Agent/Agentic-RL/AgentQ-MCTS-Self-Critique-DPO|AgentQ]] â€” MCTS + è‡ªæˆ‘æ‰¹åˆ¤ + off-policy DPOï¼ŒLlama-3 70B çœŸå®é¢„è®¢ 18.6%â†’81.7%ï¼ˆ**v13 æ–°å¢**ï¼‰
- [[AI/2-Agent/Multi-Agent/AlphaEvolve-LLM-Discovers-MARL-Algorithms|AlphaEvolveï¼ˆarXiv:2602.16928ï¼‰]] â€” MARL ç®—æ³•è‡ªåŠ¨å‘ç°ï¼šLLM æ¼”åŒ–ä»£ç å‘ç°éç›´è§‰ CFR/PSRO å˜ä½“ï¼Œ10/11 æ¸¸æˆè¶… SOTAï¼ˆ**v14 æ–°å¢**ï¼‰
- [[AI/2-Agent/Multi-Agent/SRPO-Strategic-Risk-Aversion-Collaborative-MARL|SRPOï¼ˆarXiv:2602.21515ï¼‰]] â€” åä½œ MARL æ³›åŒ–ï¼šRisk-averse Quantal Equilibria æ›¿ä»£ Nashï¼Œæ¶ˆé™¤ free-ridingï¼ˆ**v14 æ–°å¢**ï¼‰
- [[AI/2-Agent/Agentic-RL/PABU-Progress-Aware-Belief-State|PABUï¼ˆè¿›åº¦æ„ŸçŸ¥ä¿¡å¿µçŠ¶æ€ï¼‰]] â€” æ¨ç†é˜¶æ®µ Context æ•ˆç‡ï¼šæ˜¾å¼å»ºæ¨¡ä»»åŠ¡è¿›åº¦+é€‰æ‹©æ€§å†å²ä¿ç•™ï¼›81% å®Œæˆç‡+26.9% æ•ˆç‡æå‡ï¼›ä¸ KLong/SORL æ­£äº¤ï¼ˆ**v14 æ–°å¢**ï¼‰
- [[AI/2-Agent/Agentic-RL/WebPilot|WebPilotï¼ˆMulti-Agent Webä»»åŠ¡ï¼‰]] â€” Planner+Executor æ¶æ„ + MCTS æˆ˜ç•¥æ¢ç´¢ï¼›WebArena/Mind2Web benchmark éªŒè¯ï¼ˆ**v14 æ–°å¢**ï¼‰
- [[AI/2-Agent/Multi-Agent/AgentAuditor-Reasoning-Tree-å®¡è®¡|AgentAuditorï¼ˆarXiv:2602.09341ï¼‰]] â€” Reasoning Tree å®¡è®¡å¤š Agent ç³»ç»Ÿï¼›ACPOï¼ˆAnti-Consensus Preference Optimizationï¼‰è¯†åˆ«æ­£ç¡®å°‘æ•°æ´¾ï¼›å±€éƒ¨åŒ–å®¡è®¡æ¯”å…¨å±€æŠ•ç¥¨ç²¾å‡†ï¼ˆ**v14 æ–°å¢**ï¼‰
