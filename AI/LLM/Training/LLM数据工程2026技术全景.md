---
title: "LLM 数据工程 2026 技术全景"
date: 2026-02-20
type: landscape
domain: ai/llm/training
rating: ★★★★★
tags:
  - ai/llm/data-engineering
  - interview/weapon
  - pretraining
  - synthetic-data
  - sft
status: active
---

# LLM 数据工程 2026 技术全景

> "Data is the new oil, but raw data is crude oil — it needs refining."
> 数据是新石油，但原始数据是原油——它需要精炼。

本笔记全面覆盖 LLM 数据工程的各个环节，从预训练数据管线到合成数据生成，从 SFT 构建到偏好对齐，从质量评估到合规安全，最后展望 2026 前沿趋势。每个技术点附带代码示例或伪代码，末尾附 12 道高质量面试题。

---

## 目录

1. [预训练数据管线](#1-预训练数据管线)
   - 1.1 Common Crawl 清洗
   - 1.2 语言检测
   - 1.3 质量过滤
   - 1.4 去重（Deduplication）
   - 1.5 PII 脱敏
   - 1.6 数据配比（Data Mixing）
   - 1.7 Tokenization
2. [合成数据生成](#2-合成数据生成)
   - 2.1 Self-Instruct
   - 2.2 Evol-Instruct / WizardLM
   - 2.3 Magpie
   - 2.4 数学合成数据
   - 2.5 代码合成数据
3. [SFT 数据构建](#3-sft-数据构建)
   - 3.1 指令跟随数据
   - 3.2 多轮对话数据
   - 3.3 长文本 SFT
4. [DPO/RLHF 偏好数据构建](#4-dporlhf-偏好数据构建)
   - 4.1 RLHF 数据管线
   - 4.2 DPO 数据构建
   - 4.3 偏好数据的质量控制
5. [数据质量评估](#5-数据质量评估)
   - 5.1 Perplexity 评估
   - 5.2 Embedding 聚类分析
   - 5.3 人工审核流程
   - 5.4 自动化质量指标
6. [数据合规与安全](#6-数据合规与安全)
   - 6.1 版权与许可证
   - 6.2 PII 检测与脱敏
   - 6.3 有毒内容过滤
   - 6.4 数据溯源与审计
7. [2026 前沿趋势](#7-2026-前沿趋势)
   - 7.1 FineWeb / FineWeb-Edu
   - 7.2 DCLM（DataComp-LM）
   - 7.3 数据飞轮（Data Flywheel）
   - 7.4 合成数据 Scaling Laws
   - 7.5 多模态数据工程
   - 7.6 数据 Curation as a Service
8. [面试题库（12 题）](#8-面试题库)

---

## 1. 预训练数据管线

预训练数据管线是 LLM 训练的基石。一个典型的管线包含以下阶段：

```
Raw Web Crawl → URL Filtering → HTML Extraction → Language Detection
→ Quality Filtering → Deduplication → PII Removal → Data Mixing
→ Tokenization → Training-Ready Shards
```

### 1.1 Common Crawl 清洗

Common Crawl 是最大的公开网页爬取数据集，每月爬取约 30-40 亿网页。但原始数据极其嘈杂，需要多层清洗。

#### 1.1.1 WARC 文件解析

Common Crawl 以 WARC（Web ARChive）格式存储，每个文件约 1GB 压缩。

```python
import warcio
from warcio.archiveiterator import ArchiveIterator
from bs4 import BeautifulSoup
import trafilatura

def extract_text_from_warc(warc_path: str):
    """从 WARC 文件中提取纯文本"""
    with open(warc_path, 'rb') as stream:
        for record in ArchiveIterator(stream):
            if record.rec_type == 'response':
                url = record.rec_headers.get_header('WARC-Target-URI')
                content_type = record.http_headers.get_header('Content-Type', '')
                
                # 只处理 HTML
                if 'text/html' not in content_type:
                    continue
                
                html = record.content_stream().read().decode('utf-8', errors='ignore')
                
                # 方法1: trafilatura（推荐，效果最好）
                text = trafilatura.extract(
                    html,
                    include_comments=False,
                    include_tables=True,
                    no_fallback=False,
                    favor_precision=True
                )
                
                # 方法2: BeautifulSoup + 启发式规则
                if text is None:
                    text = fallback_extract(html)
                
                if text and len(text) > 100:
                    yield {
                        'url': url,
                        'text': text,
                        'length': len(text)
                    }

def fallback_extract(html: str) -> str:
    """BeautifulSoup 回退提取"""
    soup = BeautifulSoup(html, 'lxml')
    
    # 移除噪音标签
    for tag in soup.find_all(['script', 'style', 'nav', 'footer', 
                               'header', 'aside', 'iframe', 'noscript']):
        tag.decompose()
    
    # 提取主体文本
    main = soup.find('main') or soup.find('article') or soup.find('body')
    if main:
        text = main.get_text(separator='\n', strip=True)
        # 清理多余空行
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        return '\n'.join(lines)
    return ''
```

#### 1.1.2 URL 级别过滤

在文本提取之前，先进行 URL 级别的粗过滤，节省计算资源：

```python
import re
from urllib.parse import urlparse
from typing import Set

# 黑名单域名（成人、垃圾、恶意网站）
BLOCKLIST_DOMAINS: Set[str] = load_blocklist('ut1_blocklist.txt')

# 需要过滤的 URL 模式
SPAM_PATTERNS = [
    r'/casino', r'/gambling', r'/porn', r'/xxx',
    r'/buy-cheap', r'/click-here', r'/free-download',
    r'\.(exe|zip|rar|torrent)$',
    r'/wp-login', r'/admin', r'/cgi-bin',
]

def url_filter(url: str) -> bool:
    """URL 级别过滤，返回 True 表示保留"""
    try:
        parsed = urlparse(url)
    except Exception:
        return False
    
    domain = parsed.netloc.lower()
    path = parsed.path.lower()
    
    # 1. 域名黑名单
    if domain in BLOCKLIST_DOMAINS:
        return False
    
    # 2. 只保留 HTTP(S)
    if parsed.scheme not in ('http', 'https'):
        return False
    
    # 3. 过滤可疑路径
    for pattern in SPAM_PATTERNS:
        if re.search(pattern, path):
            return False
    
    # 4. URL 过长通常是垃圾
    if len(url) > 2000:
        return False
    
    # 5. 过滤非内容页面（纯 query string 页面）
    if len(parsed.query) > 500:
        return False
    
    return True
```

#### 1.1.3 文本级别清洗

```python
import re
from typing import Optional

def clean_extracted_text(text: str) -> Optional[str]:
    """文本级别清洗"""
    if not text:
        return None
    
    # 1. 基础清洗
    # 移除连续空白
    text = re.sub(r'[ \t]+', ' ', text)
    # 规范化换行
    text = re.sub(r'\n{3,}', '\n\n', text)
    # 移除零宽字符
    text = re.sub(r'[\u200b\u200c\u200d\ufeff]', '', text)
    
    # 2. 行级别过滤
    lines = text.split('\n')
    clean_lines = []
    for line in lines:
        line = line.strip()
        if not line:
            clean_lines.append('')
            continue
        
        # 过滤导航菜单行（短且重复出现的模式）
        if len(line) < 5 and not any(c.isalpha() for c in line):
            continue
        
        # 过滤 JavaScript 残留
        if line.startswith(('var ', 'function(', 'document.', 'window.')):
            continue
        
        # 过滤 cookie 提示等
        if any(kw in line.lower() for kw in ['cookie policy', 'accept cookies',
                                                'privacy policy', 'terms of service']):
            continue
        
        clean_lines.append(line)
    
    text = '\n'.join(clean_lines)
    
    # 3. 文档级别过滤
    words = text.split()
    
    # 过短
    if len(words) < 50:
        return None
    
    # 重复率过高（boilerplate 检测）
    unique_words = set(w.lower() for w in words)
    if len(unique_words) / len(words) < 0.1:
        return None
    
    # 特殊字符比例过高
    alpha_chars = sum(1 for c in text if c.isalpha())
    if alpha_chars / max(len(text), 1) < 0.4:
        return None
    
    return text.strip()
```

### 1.2 语言检测

语言检测是多语言数据处理的关键环节。主流工具包括 fastText lid（lid.176.bin）和 CLD3。

#### 1.2.1 基于 fastText 的语言检测

```python
import fasttext
from typing import Tuple, List
import numpy as np

class LanguageDetector:
    """基于 fastText 的高效语言检测器"""
    
    def __init__(self, model_path: str = 'lid.176.bin'):
        # fastText 的 lid.176.bin 支持 176 种语言
        self.model = fasttext.load_model(model_path)
    
    def detect(self, text: str, top_k: int = 3) -> List[Tuple[str, float]]:
        """检测文本语言，返回 top-k 结果"""
        # 预处理：移除换行（fastText 按行处理）
        text = text.replace('\n', ' ').strip()
        
        # 截断过长文本（前 1000 字符通常足够）
        text = text[:1000]
        
        predictions = self.model.predict(text, k=top_k)
        results = []
        for label, score in zip(predictions[0], predictions[1]):
            lang = label.replace('__label__', '')
            results.append((lang, float(score)))
        
        return results
    
    def detect_with_segments(self, text: str, segment_size: int = 200) -> dict:
        """
        分段检测 — 处理混合语言文本
        对长文本按段落分别检测，用于发现混合语言内容
        """
        segments = self._split_segments(text, segment_size)
        lang_votes = {}
        
        for segment in segments:
            if len(segment.strip()) < 20:
                continue
            results = self.detect(segment, top_k=1)
            if results:
                lang, score = results[0]
                if score > 0.5:
                    lang_votes[lang] = lang_votes.get(lang, 0) + 1
        
        total = sum(lang_votes.values())
        return {
            'primary_lang': max(lang_votes, key=lang_votes.get) if lang_votes else 'unknown',
            'lang_distribution': {k: v/total for k, v in lang_votes.items()} if total > 0 else {},
            'is_mixed': len([v for v in lang_votes.values() if v/max(total,1) > 0.2]) > 1
        }
    
    def _split_segments(self, text: str, size: int) -> List[str]:
        words = text.split()
        return [' '.join(words[i:i+size]) for i in range(0, len(words), size)]


# 使用示例
detector = LanguageDetector()

# 基础检测
results = detector.detect("This is an English sentence.")
# [('en', 0.99), ('de', 0.003), ...]

# 混合语言检测
mixed_result = detector.detect_with_segments(
    "This is English. 这是中文。Esto es español."
)
# {'primary_lang': 'en', 'lang_distribution': {'en': 0.33, 'zh': 0.33, 'es': 0.33}, 'is_mixed': True}
```

#### 1.2.2 语言过滤策略

```python
def language_filter(doc: dict, target_langs: set = {'en', 'zh'},
                    min_confidence: float = 0.65) -> bool:
    """
    语言过滤策略
    - 主语言必须在 target_langs 中
    - 置信度必须超过阈值
    - 混合语言文档需要特殊处理
    """
    detector = LanguageDetector()
    text = doc['text']
    
    # 短文本用整体检测
    if len(text.split()) < 100:
        results = detector.detect(text)
        if not results:
            return False
        lang, confidence = results[0]
        return lang in target_langs and confidence >= min_confidence
    
    # 长文本用分段检测
    analysis = detector.detect_with_segments(text)
    
    # 主语言必须在目标语言中
    if analysis['primary_lang'] not in target_langs:
        return False
    
    # 高度混合的文档（如翻译对照）可以保留但标记
    if analysis['is_mixed']:
        # 计算目标语言占比
        target_ratio = sum(
            analysis['lang_distribution'].get(lang, 0) 
            for lang in target_langs
        )
        return target_ratio >= 0.7  # 目标语言至少占 70%
    
    return True
```

### 1.3 质量过滤

质量过滤是预训练数据管线中最关键也最具艺术性的环节。目标是区分高质量文本（书籍、百科、学术论文、优质博客）和低质量文本（SEO 垃圾、自动生成内容、重复模板页面）。

#### 1.3.1 启发式规则过滤（Heuristic Filtering）

Gopher（DeepMind 2021）提出的经典规则集，后被 RefinedWeb、FineWeb 等广泛采用和扩展：

```python
from dataclasses import dataclass
from typing import Optional
import re

@dataclass
class QualityMetrics:
    """文档质量指标"""
    word_count: int
    avg_word_length: float
    line_count: int
    avg_line_length: float
    alpha_ratio: float          # 字母字符占比
    digit_ratio: float          # 数字字符占比
    uppercase_ratio: float      # 大写字母占比
    special_char_ratio: float   # 特殊字符占比
    stop_word_ratio: float      # 停用词占比
    type_token_ratio: float     # 词汇丰富度（unique words / total words）
    bullet_ratio: float         # 列表行占比
    ellipsis_ratio: float       # 省略号行占比
    has_end_punctuation: float  # 以句号结尾的行占比
    duplicate_line_ratio: float # 重复行占比
    short_line_ratio: float     # 短行占比（< 30 字符）
    curly_bracket_ratio: float  # 花括号比例（代码检测）


def compute_quality_metrics(text: str, lang: str = 'en') -> QualityMetrics:
    """计算文档质量指标"""
    words = text.split()
    lines = text.split('\n')
    chars = list(text)
    
    word_count = len(words)
    if word_count == 0:
        return None
    
    # 停用词列表（简化版，实际应按语言加载）
    STOP_WORDS_EN = {'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be',
                     'been', 'being', 'have', 'has', 'had', 'do', 'does',
                     'did', 'will', 'would', 'could', 'should', 'may',
                     'might', 'shall', 'can', 'to', 'of', 'in', 'for',
                     'on', 'with', 'at', 'by', 'from', 'as', 'into',
                     'through', 'during', 'before', 'after', 'and', 'but',
                     'or', 'nor', 'not', 'so', 'yet', 'both', 'either',
                     'neither', 'each', 'every', 'all', 'any', 'few',
                     'more', 'most', 'other', 'some', 'such', 'no', 'only',
                     'own', 'same', 'than', 'too', 'very', 'just'}
    
    stop_count = sum(1 for w in words if w.lower() in STOP_WORDS_EN)
    unique_words = set(w.lower() for w in words)
    
    alpha_chars = sum(1 for c in chars if c.isalpha())
    digit_chars = sum(1 for c in chars if c.isdigit())
    upper_chars = sum(1 for c in chars if c.isupper())
    total_chars = len(chars)
    
    non_empty_lines = [l for l in lines if l.strip()]
    duplicate_lines = len(non_empty_lines) - len(set(non_empty_lines))
    short_lines = sum(1 for l in non_empty_lines if len(l.strip()) < 30)
    bullet_lines = sum(1 for l in lines if l.strip().startswith(('•', '-', '*', '·')))
    ellipsis_lines = sum(1 for l in lines if l.strip().endswith('...'))
    end_punct_lines = sum(1 for l in non_empty_lines 
                          if l.strip() and l.strip()[-1] in '.!?。！？')
    curly_count = text.count('{') + text.count('}')
    
    return QualityMetrics(
        word_count=word_count,
        avg_word_length=sum(len(w) for w in words) / word_count,
        line_count=len(non_empty_lines),
        avg_line_length=sum(len(l) for l in non_empty_lines) / max(len(non_empty_lines), 1),
        alpha_ratio=alpha_chars / max(total_chars, 1),
        digit_ratio=digit_chars / max(total_chars, 1),
        uppercase_ratio=upper_chars / max(alpha_chars, 1),
        special_char_ratio=1 - (alpha_chars + digit_chars) / max(total_chars, 1),
        stop_word_ratio=stop_count / word_count,
        type_token_ratio=len(unique_words) / word_count,
        bullet_ratio=bullet_lines / max(len(non_empty_lines), 1),
        ellipsis_ratio=ellipsis_lines / max(len(non_empty_lines), 1),
        has_end_punctuation=end_punct_lines / max(len(non_empty_lines), 1),
        duplicate_line_ratio=duplicate_lines / max(len(non_empty_lines), 1),
        short_line_ratio=short_lines / max(len(non_empty_lines), 1),
        curly_bracket_ratio=curly_count / max(total_chars, 1),
    )


def gopher_quality_filter(text: str, lang: str = 'en') -> tuple[bool, str]:
    """
    Gopher 风格质量过滤规则
    返回 (是否通过, 原因)
    """
    metrics = compute_quality_metrics(text, lang)
    if metrics is None:
        return False, "empty_document"
    
    # === 长度过滤 ===
    if metrics.word_count < 50:
        return False, "too_short"
    if metrics.word_count > 100_000:
        return False, "too_long"
    
    # === 词汇质量 ===
    if metrics.avg_word_length < 3 or metrics.avg_word_length > 10:
        return False, "abnormal_word_length"
    if metrics.type_token_ratio < 0.1:
        return False, "low_vocabulary_diversity"
    
    # === 字符比例 ===
    if metrics.alpha_ratio < 0.4:
        return False, "low_alpha_ratio"
    if metrics.uppercase_ratio > 0.4:
        return False, "too_much_uppercase"  # 全大写垃圾
    if metrics.special_char_ratio > 0.5:
        return False, "too_many_special_chars"
    
    # === 停用词（自然语言检测）===
    if lang == 'en' and metrics.stop_word_ratio < 0.06:
        return False, "too_few_stop_words"  # 可能是关键词堆砌
    
    # === 行级别质量 ===
    if metrics.duplicate_line_ratio > 0.3:
        return False, "too_many_duplicate_lines"
    if metrics.short_line_ratio > 0.7:
        return False, "too_many_short_lines"  # 可能是菜单/导航
    if metrics.ellipsis_ratio > 0.3:
        return False, "too_many_ellipsis"  # 可能是列表页
    if metrics.bullet_ratio > 0.9:
        return False, "mostly_bullets"
    
    # === 结尾标点（完整性指标）===
    if metrics.has_end_punctuation < 0.05:
        return False, "no_end_punctuation"  # 句子不完整
    
    # === 代码检测（可选，按需保留代码内容）===
    if metrics.curly_bracket_ratio > 0.05:
        return True, "likely_code"  # 标记但保留
    
    return True, "passed"
```

#### 1.3.2 基于模型的质量分类（Model-based Quality Scoring）

CCNet（Meta）提出使用 KenLM 语言模型的 perplexity 来做质量分层。FineWeb 则使用了基于 fastText 的分类器来做教育价值评分。

```python
import kenlm
import numpy as np
from typing import List

class PerplexityQualityScorer:
    """
    使用 KenLM n-gram 模型计算 perplexity 进行质量评分
    核心思想：在高质量语料（如 Wikipedia）上训练的 LM，
    对高质量文本的 perplexity 低，对垃圾文本的 perplexity 高
    """
    
    def __init__(self, model_path: str):
        self.model = kenlm.Model(model_path)
    
    def score(self, text: str) -> float:
        """计算文本的 perplexity（每词困惑度）"""
        # KenLM 返回的是 log10 概率
        log_prob = self.model.score(text)
        word_count = len(text.split())
        if word_count == 0:
            return float('inf')
        
        # 转换为 perplexity
        ppl = 10 ** (-log_prob / word_count)
        return ppl
    
    def classify(self, text: str) -> str:
        """
        CCNet 风格的三分类
        - head: perplexity 最低的 1/3（高质量，类似 Wikipedia）
        - middle: 中间 1/3
        - tail: perplexity 最高的 1/3（低质量）
        """
        ppl = self.score(text)
        
        # 阈值需要根据训练集分布确定
        # 以下为英文 Wikipedia KenLM 的典型阈值
        if ppl < 230:
            return 'head'
        elif ppl < 500:
            return 'middle'
        else:
            return 'tail'


class FastTextQualityClassifier:
    """
    FineWeb-Edu 风格：用 fastText 做教育价值分类
    训练数据来自 LLM（如 Llama-3）对网页的教育价值评分
    """
    
    def __init__(self, model_path: str):
        self.model = fasttext.load_model(model_path)
    
    def predict_educational_score(self, text: str) -> int:
        """
        预测教育价值评分 0-5
        0: 无教育价值
        5: 大学教材级别
        """
        # 预处理
        text = text.replace('\n', ' ')[:5000]
        
        prediction = self.model.predict(text)
        label = prediction[0][0]  # '__label__3' -> 3
        score = int(label.replace('__label__', ''))
        
        return score
    
    def filter(self, text: str, min_score: int = 3) -> bool:
        """FineWeb-Edu 使用 score >= 3 作为过滤阈值"""
        return self.predict_educational_score(text) >= min_score


# === 训练 fastText 教育分类器的流程 ===
def prepare_educational_training_data():
    """
    伪代码：用 LLM 生成教育评分标注
    FineWeb-Edu 的核心创新：用 LLM 做 annotator
    """
    import random
    
    # 1. 从 Common Crawl 采样 500K 文档
    samples = random.sample(common_crawl_docs, 500_000)
    
    # 2. 用 Llama-3-70B-Instruct 打分
    prompt_template = """
    Rate the educational value of the following web page on a scale of 0-5:
    0: No educational value (ads, spam, adult content)
    1: Minimal educational value (news articles, social media)
    2: Some educational value (blog posts with useful info)
    3: Moderate educational value (well-written articles, tutorials)
    4: High educational value (textbook-like content, detailed explanations)
    5: Outstanding educational value (university-level, comprehensive coverage)
    
    Web page content:
    {text}
    
    Educational score (0-5):
    """
    
    training_data = []
    for doc in samples:
        score = llm_annotate(prompt_template.format(text=doc['text'][:2000]))
        training_data.append(f"__label__{score} {doc['text'][:5000]}")
    
    # 3. 训练 fastText 分类器
    # fasttext.train_supervised(input='train.txt', epoch=5, lr=0.1, wordNgrams=2)
    
    return training_data
```

### 1.4 去重（Deduplication）

去重是预训练数据处理中最消耗计算资源的环节之一，但也是提升模型质量最有效的手段之一。研究表明，去重可以减少 30-50% 的训练数据量，同时显著提升模型性能。

#### 1.4.1 精确去重（Exact Deduplication）

```python
import hashlib
from typing import Set, Iterator

class ExactDeduplicator:
    """
    精确去重：基于 hash 的完全匹配去重
    - 文档级：整篇文档的 hash
    - 段落级：每个段落的 hash（更激进）
    - URL 级：同一 URL 只保留最新版本
    """
    
    def __init__(self):
        self.seen_hashes: Set[str] = set()
    
    def document_hash(self, text: str) -> str:
        """计算文档 hash（忽略空白差异）"""
        normalized = ' '.join(text.split()).lower()
        return hashlib.sha256(normalized.encode('utf-8')).hexdigest()
    
    def is_duplicate(self, text: str) -> bool:
        h = self.document_hash(text)
        if h in self.seen_hashes:
            return True
        self.seen_hashes.add(h)
        return False
    
    def paragraph_dedup(self, text: str) -> str:
        """段落级去重：移除重复段落"""
        paragraphs = text.split('\n\n')
        seen = set()
        unique = []
        for p in paragraphs:
            p_hash = hashlib.md5(p.strip().encode()).hexdigest()
            if p_hash not in seen:
                seen.add(p_hash)
                unique.append(p)
        return '\n\n'.join(unique)


class SuffixArrayDeduplicator:
    """
    基于 Suffix Array 的子串去重（Lee et al., 2022）
    用于检测和移除跨文档的重复子串
    比 MinHash 更精确，但计算量更大
    """
    
    def __init__(self, min_dup_length: int = 200):
        self.min_dup_length = min_dup_length
    
    def find_duplicate_substrings(self, corpus: str) -> list:
        """
        伪代码：使用 suffix array 查找重复子串
        实际实现推荐使用 Rust 库（如 Google 的 deduplicate-text-datasets）
        """
        # 1. 构建 suffix array
        # sa = suffix_array(corpus)  # O(n) 或 O(n log n)
        
        # 2. 构建 LCP (Longest Common Prefix) array
        # lcp = build_lcp(corpus, sa)
        
        # 3. 找到 LCP >= min_dup_length 的位置
        # duplicates = [(sa[i], sa[i+1], lcp[i]) 
        #               for i in range(len(lcp)) 
        #               if lcp[i] >= self.min_dup_length]
        
        # 4. 移除重复子串（保留第一次出现）
        pass
```

#### 1.4.2 模糊去重（Fuzzy Deduplication）— MinHash + LSH

```python
import hashlib
import struct
import numpy as np
from typing import List, Set, Tuple
from collections import defaultdict

class MinHashDeduplicator:
    """
    MinHash + LSH (Locality-Sensitive Hashing) 模糊去重
    
    核心算法：
    1. 将文档转为 n-gram 集合
    2. 用多个 hash 函数计算 MinHash 签名
    3. 用 LSH bands 技术高效找到候选相似对
    4. 对候选对计算 Jaccard 相似度，超过阈值则去重
    
    参数选择指南（FineWeb 推荐）：
    - n-gram size: 5（5-gram 是平衡点）
    - num_hashes: 128
    - num_bands: 9, rows_per_band: 13 → 阈值约 0.75
    - Jaccard threshold: 0.8
    """
    
    def __init__(self, num_hashes: int = 128, num_bands: int = 9, 
                 ngram_size: int = 5, threshold: float = 0.8):
        self.num_hashes = num_hashes
        self.num_bands = num_bands
        self.rows_per_band = num_hashes // num_bands
        self.ngram_size = ngram_size
        self.threshold = threshold
        
        # 生成 hash 函数参数（a*x + b mod p）
        self.max_hash = 2**32 - 1
        self.prime = 4294967311  # 大于 max_hash 的质数
        np.random.seed(42)
        self.a = np.random.randint(1, self.prime, size=num_hashes).astype(np.uint64)
        self.b = np.random.randint(0, self.prime, size=num_hashes).astype(np.uint64)
        
        # LSH 桶
        self.buckets = defaultdict(set)  # band_id -> {doc_ids}
        self.signatures = {}  # doc_id -> minhash signature
    
    def _get_ngrams(self, text: str) -> Set[str]:
        """提取 n-gram 集合"""
        tokens = text.lower().split()
        if len(tokens) < self.ngram_size:
            return {' '.join(tokens)}
        return {' '.join(tokens[i:i+self.ngram_size]) 
                for i in range(len(tokens) - self.ngram_size + 1)}
    
    def _minhash(self, ngrams: Set[str]) -> np.ndarray:
        """计算 MinHash 签名"""
        signature = np.full(self.num_hashes, self.max_hash, dtype=np.uint64)
        
        for ngram in ngrams:
            # 用 MurmurHash 或 SHA 将 ngram 映射到整数
            h = int(hashlib.md5(ngram.encode()).hexdigest()[:8], 16)
            
            # 对每个 hash 函数，取最小值
            hashes = (self.a * h + self.b) % self.prime
            signature = np.minimum(signature, hashes)
        
        return signature
    
    def add_document(self, doc_id: str, text: str):
        """添加文档到 LSH 索引"""
        ngrams = self._get_ngrams(text)
        if not ngrams:
            return
        
        signature = self._minhash(ngrams)
        self.signatures[doc_id] = signature
        
        # 将签名分成 bands，每个 band hash 到一个桶
        for band_idx in range(self.num_bands):
            start = band_idx * self.rows_per_band
            end = start + self.rows_per_band
            band = signature[start:end]
            band_hash = hashlib.md5(band.tobytes()).hexdigest()
            bucket_key = f"{band_idx}_{band_hash}"
            self.buckets[bucket_key].add(doc_id)
    
    def find_duplicates(self) -> List[Tuple[str, str, float]]:
        """找到所有重复对"""
        candidates = set()
        
        # 从 LSH 桶中收集候选对
        for bucket_key, doc_ids in self.buckets.items():
            doc_list = list(doc_ids)
            for i in range(len(doc_list)):
                for j in range(i + 1, len(doc_list)):
                    candidates.add((min(doc_list[i], doc_list[j]), 
                                   max(doc_list[i], doc_list[j])))
        
        # 验证候选对的实际相似度
        duplicates = []
        for doc_a, doc_b in candidates:
            sig_a = self.signatures[doc_a]
            sig_b = self.signatures[doc_b]
            
            # 估计 Jaccard 相似度 = 签名一致的比例
            similarity = np.mean(sig_a == sig_b)
            
            if similarity >= self.threshold:
                duplicates.append((doc_a, doc_b, similarity))
        
        return duplicates
    
    def deduplicate(self, documents: dict) -> dict:
        """
        执行去重，返回去重后的文档
        使用 Union-Find 处理传递性（A≈B, B≈C → 去掉 B 和 C）
        """
        # 1. 建立索引
        for doc_id, text in documents.items():
            self.add_document(doc_id, text)
        
        # 2. 找到重复对
        duplicates = self.find_duplicates()
        
        # 3. Union-Find 聚类
        parent = {doc_id: doc_id for doc_id in documents}
        
        def find(x):
            while parent[x] != x:
                parent[x] = parent[parent[x]]
                x = parent[x]
            return x
        
        def union(x, y):
            px, py = find(x), find(y)
            if px != py:
                parent[py] = px
        
        for doc_a, doc_b, _ in duplicates:
            union(doc_a, doc_b)
        
        # 4. 每个聚类只保留一个代表
        clusters = defaultdict(list)
        for doc_id in documents:
            clusters[find(doc_id)].append(doc_id)
        
        # 保留每个聚类中最长的文档
        kept = set()
        for cluster_docs in clusters.values():
            best = max(cluster_docs, key=lambda d: len(documents[d]))
            kept.add(best)
        
        return {doc_id: text for doc_id, text in documents.items() if doc_id in kept}


# === 大规模去重的工程实践 ===
# 实际场景中，数十亿文档的去重需要分布式处理：
#
# 1. 使用 Spark/Dask 并行计算 MinHash 签名
# 2. 签名存储在分布式 KV 存储（如 Redis Cluster）
# 3. LSH 桶用 MapReduce 处理
# 4. 工具推荐：
#    - text-dedup (GitHub): 纯 Python，适合中小规模
#    - datatrove (HuggingFace): 大规模分布式管线
#    - deduplicate-text-datasets (Google): Suffix Array 方法
```

### 1.5 PII 脱敏

PII（Personally Identifiable Information）脱敏是合规要求也是伦理要求。需要检测并移除或替换：邮箱、电话号码、身份证号、银行卡号、地址、姓名等。

```python
import re
from typing import List, Tuple, Dict
from dataclasses import dataclass
from enum import Enum

class PIIType(Enum):
    EMAIL = "email"
    PHONE = "phone"
    IP_ADDRESS = "ip_address"
    SSN = "ssn"                  # 美国社会安全号
    CREDIT_CARD = "credit_card"
    ID_CARD_CN = "id_card_cn"   # 中国身份证
    URL = "url"
    PERSON_NAME = "person_name"
    ADDRESS = "address"

@dataclass
class PIIEntity:
    type: PIIType
    start: int
    end: int
    text: str
    confidence: float

class PIIDetector:
    """
    多策略 PII 检测器
    结合正则表达式 + NER 模型 + 校验算法
    """
    
    # 正则表达式规则
    PATTERNS = {
        PIIType.EMAIL: r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
        
        PIIType.PHONE: r'''(?x)
            (?:\+?1[-.\s]?)?          # 美国区号
            (?:\(?\d{3}\)?[-.\s]?)    # 区号
            \d{3}[-.\s]?\d{4}         # 号码
            |                          # 或
            (?:\+?86[-.\s]?)?         # 中国区号
            1[3-9]\d{9}               # 中国手机号
        ''',
        
        PIIType.IP_ADDRESS: r'\b(?:\d{1,3}\.){3}\d{1,3}\b',
        
        PIIType.SSN: r'\b\d{3}[-.\s]?\d{2}[-.\s]?\d{4}\b',
        
        PIIType.CREDIT_CARD: r'\b(?:\d{4}[-.\s]?){3}\d{4}\b',
        
        PIIType.ID_CARD_CN: r'\b[1-9]\d{5}(?:18|19|20)\d{2}'
                            r'(?:0[1-9]|1[0-2])(?:0[1-9]|[12]\d|3[01])'
                            r'\d{3}[\dXx]\b',
    }
    
    def __init__(self, use_ner: bool = True):
        self.use_ner = use_ner
        if use_ner:
            # 加载 NER 模型用于检测人名、地址等
            # 推荐 spaCy 或 GLiNER
            pass
    
    def detect(self, text: str) -> List[PIIEntity]:
        """检测文本中的所有 PII"""
        entities = []
        
        # 1. 正则匹配
        for pii_type, pattern in self.PATTERNS.items():
            for match in re.finditer(pattern, text):
                entity = PIIEntity(
                    type=pii_type,
                    start=match.start(),
                    end=match.end(),
                    text=match.group(),
                    confidence=0.9
                )
                
                # 额外校验
                if pii_type == PIIType.CREDIT_CARD:
                    if not self._luhn_check(entity.text):
                        entity.confidence = 0.3
                
                if pii_type == PIIType.IP_ADDRESS:
                    if not self._valid_ip(entity.text):
                        continue
                
                entities.append(entity)
        
        # 2. NER 检测人名和地址
        if self.use_ner:
            ner_entities = self._ner_detect(text)
            entities.extend(ner_entities)
        
        return entities
    
    def _luhn_check(self, number: str) -> bool:
        """Luhn 算法验证信用卡号"""
        digits = [int(d) for d in re.sub(r'\D', '', number)]
        checksum = 0
        for i, d in enumerate(reversed(digits)):
            if i % 2 == 1:
                d *= 2
                if d > 9:
                    d -= 9
            checksum += d
        return checksum % 10 == 0
    
    def _valid_ip(self, ip: str) -> bool:
        """验证 IP 地址合法性"""
        parts = ip.split('.')
        return all(0 <= int(p) <= 255 for p in parts)
    
    def _ner_detect(self, text: str) -> List[PIIEntity]:
        """使用 NER 模型检测人名、地址"""
        # 伪代码 — 实际使用 spaCy / GLiNER
        # doc = nlp(text)
        # entities = []
        # for ent in doc.ents:
        #     if ent.label_ == 'PERSON':
        #         entities.append(PIIEntity(PIIType.PERSON_NAME, ...))
        #     elif ent.label_ in ('GPE', 'LOC', 'FAC'):
        #         entities.append(PIIEntity(PIIType.ADDRESS, ...))
        return []


class PIIAnonymizer:
    """PII 脱敏器"""
    
    REPLACEMENTS = {
        PIIType.EMAIL: '[EMAIL]',
        PIIType.PHONE: '[PHONE]',
        PIIType.IP_ADDRESS: '[IP_ADDRESS]',
        PIIType.SSN: '[SSN]',
        PIIType.CREDIT_CARD: '[CREDIT_CARD]',
        PIIType.ID_CARD_CN: '[ID_CARD]',
        PIIType.PERSON_NAME: '[PERSON_NAME]',
        PIIType.ADDRESS: '[ADDRESS]',
    }
    
    def __init__(self, detector: PIIDetector, min_confidence: float = 0.7):
        self.detector = detector
        self.min_confidence = min_confidence
    
    def anonymize(self, text: str) -> Tuple[str, List[PIIEntity]]:
        """脱敏文本，返回 (脱敏后文本, 检测到的实体列表)"""
        entities = self.detector.detect(text)
        
        # 按位置降序排列，从后往前替换（避免偏移问题）
        entities = sorted(
            [e for e in entities if e.confidence >= self.min_confidence],
            key=lambda e: e.start,
            reverse=True
        )
        
        result = text
        for entity in entities:
            replacement = self.REPLACEMENTS.get(entity.type, '[PII]')
            result = result[:entity.start] + replacement + result[entity.end:]
        
        return result, entities


# 使用示例
detector = PIIDetector(use_ner=False)
anonymizer = PIIAnonymizer(detector)

text = "联系 John Smith，邮箱 john@example.com，电话 13812345678"
clean_text, found = anonymizer.anonymize(text)
# "联系 John Smith，邮箱 [EMAIL]，电话 [PHONE]"
```

### 1.6 数据配比（Data Mixing）

数据配比决定了不同来源和类型的数据在预训练中的占比，对模型能力有显著影响。

#### 1.6.1 配比策略

```python
from dataclasses import dataclass, field
from typing import Dict, List
import numpy as np

@dataclass
class DataSource:
    name: str
    path: str
    tokens: int           # 总 token 数
    quality_score: float  # 质量评分 0-1
    domain: str           # 领域标签
    natural_ratio: float  # 自然占比（按 token 数）
    weight: float = 1.0   # 采样权重

@dataclass  
class DataMixConfig:
    """
    数据配比配置
    
    典型的 2026 大模型预训练配比：
    - Web text:      50-60%
    - Code:          15-20%
    - Books:         5-10%
    - Academic:      5-8%
    - Wikipedia:     3-5%
    - Math:          3-5%
    - Multilingual:  5-10%
    - Conversation:  2-3%
    """
    sources: List[DataSource]
    total_tokens: int
    
    def compute_sampling_weights(self, strategy: str = 'sqrt') -> Dict[str, float]:
        """
        计算采样权重
        
        策略：
        - natural: 按自然比例采样
        - equal: 等比例采样
        - sqrt: 平方根采样（均衡自然比例和等比例）
        - temperature: 温度采样（Chinchilla 风格）
        - doremi: DoReMi 算法（基于参考模型的自适应配比）
        """
        if strategy == 'natural':
            return {s.name: s.natural_ratio for s in self.sources}
        
        elif strategy == 'equal':
            n = len(self.sources)
            return {s.name: 1.0 / n for s in self.sources}
        
        elif strategy == 'sqrt':
            # 对自然比例取平方根，然后归一化
            # 效果：上采样小数据集，下采样大数据集
            sqrt_ratios = {s.name: np.sqrt(s.natural_ratio) for s in self.sources}
            total = sum(sqrt_ratios.values())
            return {k: v / total for k, v in sqrt_ratios.items()}
        
        elif strategy == 'temperature':
            # 温度采样：p_i = (n_i^(1/T)) / Σ(n_j^(1/T))
            # T < 1: 更均匀，T > 1: 更极端，T = 1: 自然比例
            temperature = 0.7  # 常用值
            temp_ratios = {
                s.name: s.natural_ratio ** (1.0 / temperature) 
                for s in self.sources
            }
            total = sum(temp_ratios.values())
            return {k: v / total for k, v in temp_ratios.items()}
        
        elif strategy == 'quality_weighted':
            # 按质量加权：高质量数据上采样
            weighted = {s.name: s.natural_ratio * s.quality_score for s in self.sources}
            total = sum(weighted.values())
            return {k: v / total for k, v in weighted.items()}
        
        else:
            raise ValueError(f"Unknown strategy: {strategy}")
    
    def compute_epoch_counts(self, weights: Dict[str, float]) -> Dict[str, float]:
        """
        计算每个数据源被遍历的次数（epoch count）
        epoch > 1 意味着过采样（重复）
        epoch < 1 意味着欠采样
        """
        epochs = {}
        for source in self.sources:
            target_tokens = self.total_tokens * weights[source.name]
            epochs[source.name] = target_tokens / source.tokens
        return epochs


# === DoReMi: 基于参考模型的自适应配比 ===
def doremi_algorithm(
    proxy_model,       # 小型代理模型
    reference_model,   # 参考模型（通常是均匀配比训练的）
    domains: List[str],
    train_data: dict,
    steps: int = 10000
):
    """
    DoReMi (Domain Reweighting with Minimax Optimization)
    
    核心思想：
    1. 用均匀配比训练一个参考模型
    2. 训练代理模型时，用 DRO (Distributionally Robust Optimization) 
       动态调整各 domain 权重
    3. 权重更新原则：proxy 比 reference 差得多的 domain 应该获得更多权重
    4. 最终权重用于训练大模型
    """
    # 初始化均匀权重
    alpha = {d: 1.0 / len(domains) for d in domains}
    
    for step in range(steps):
        # 1. 从各 domain 采样 batch
        for domain in domains:
            batch = sample_batch(train_data[domain])
            
            # 2. 计算 proxy 和 reference 的 loss 差异
            proxy_loss = proxy_model.compute_loss(batch)
            ref_loss = reference_model.compute_loss(batch)
            excess_loss = max(0, proxy_loss - ref_loss)
            
            # 3. 更新权重（指数加权）
            alpha[domain] *= np.exp(0.01 * excess_loss)
        
        # 4. 归一化
        total = sum(alpha.values())
        alpha = {d: w / total for d, w in alpha.items()}
        
        # 5. 用新权重训练 proxy model
        weighted_batch = sample_with_weights(train_data, alpha)
        proxy_model.train_step(weighted_batch)
    
    return alpha  # 返回最终配比权重
```

### 1.7 Tokenization

Tokenization 是将原始文本转化为模型输入的关键步骤。现代 LLM 主要使用 BPE（Byte-Pair Encoding）及其变体。

#### 1.7.1 BPE 训练

```python
from collections import Counter, defaultdict
from typing import List, Tuple, Dict

class BPETokenizer:
    """
    Byte-Pair Encoding (BPE) 分词器
    
    主流实现：
    - SentencePiece (Google): C++ 实现，支持 BPE 和 Unigram
    - tiktoken (OpenAI): Rust 实现，极快
    - tokenizers (HuggingFace): Rust 核心，Python 绑定
    
    关键设计决策：
    1. 词表大小：32K (Llama), 100K (GPT-4), 151K (Qwen2)
    2. 字节级 vs 字符级：字节级 BPE 无 UNK，覆盖所有 Unicode
    3. 预分词：是否按空白/标点先分割
    4. 特殊 token：<|begin_of_text|>, <|end_of_text|>, <|eot_id|> 等
    """
    
    def __init__(self, vocab_size: int = 32000):
        self.vocab_size = vocab_size
        self.merges: List[Tuple[bytes, bytes]] = []
        self.vocab: Dict[int, bytes] = {}
    
    def train(self, texts: List[str], min_frequency: int = 2):
        """训练 BPE 分词器"""
        # 1. 初始化：每个字节是一个 token
        self.vocab = {i: bytes([i]) for i in range(256)}
        
        # 2. 预分词：将文本分为字节序列
        word_freqs = Counter()
        for text in texts:
            text_bytes = text.encode('utf-8')
            # 简化：按空格分词（实际实现用正则预分词）
            words = text_bytes.split(b' ')
            for word in words:
                # 每个 word 是字节 tuple
                word_freqs[tuple(bytes([b]) for b in word)] += 1
        
        # 3. 迭代合并最频繁的字节对
        num_merges = self.vocab_size - 256
        
        for i in range(num_merges):
            # 统计所有相邻 pair 的频率
            pair_freqs = defaultdict(int)
            for word, freq in word_freqs.items():
                for j in range(len(word) - 1):
                    pair = (word[j], word[j + 1])
                    pair_freqs[pair] += freq
            
            if not pair_freqs:
                break
            
            # 找到最频繁的 pair
            best_pair = max(pair_freqs, key=pair_freqs.get)
            
            if pair_freqs[best_pair] < min_frequency:
                break
            
            # 合并这个 pair
            self.merges.append(best_pair)
            new_token = best_pair[0] + best_pair[1]
            self.vocab[256 + i] = new_token
            
            # 更新所有 word 中的这个 pair
            new_word_freqs = Counter()
            for word, freq in word_freqs.items():
                new_word = self._merge_pair(word, best_pair, new_token)
                new_word_freqs[new_word] += freq
            word_freqs = new_word_freqs
            
            if (i + 1) % 1000 == 0:
                print(f"Merge {i+1}/{num_merges}: {best_pair} -> {new_token}")
    
    def _merge_pair(self, word: tuple, pair: tuple, new_token: bytes) -> tuple:
        """在 word 中合并指定的 pair"""
        new_word = []
        i = 0
        while i < len(word):
            if i < len(word) - 1 and word[i] == pair[0] and word[i+1] == pair[1]:
                new_word.append(new_token)
                i += 2
            else:
                new_word.append(word[i])
                i += 1
        return tuple(new_word)
    
    def encode(self, text: str) -> List[int]:
        """编码文本为 token ID 序列"""
        text_bytes = text.encode('utf-8')
        tokens = [bytes([b]) for b in text_bytes]
        
        # 按训练时的合并顺序应用合并
        for pair_a, pair_b in self.merges:
            new_token = pair_a + pair_b
            i = 0
            new_tokens = []
            while i < len(tokens):
                if (i < len(tokens) - 1 and 
                    tokens[i] == pair_a and tokens[i+1] == pair_b):
                    new_tokens.append(new_token)
                    i += 2
                else:
                    new_tokens.append(tokens[i])
                    i += 1
            tokens = new_tokens
        
        # 转为 ID
        token_to_id = {v: k for k, v in self.vocab.items()}
        return [token_to_id[t] for t in tokens]
    
    def decode(self, ids: List[int]) -> str:
        """解码 token ID 序列为文本"""
        byte_seq = b''.join(self.vocab[id] for id in ids)
        return byte_seq.decode('utf-8', errors='replace')


# === 多语言 Tokenization 的关键挑战 ===
# 
# 1. 中文/日文/韩文的 token 效率
#    - 纯英文 BPE 对中文效率极差：一个汉字可能需要 3-4 个 token
#    - 解决：在训练语料中保证足够的中文比例
#    - Qwen2 (151K vocab) 中文 token 效率接近 1:1
#
# 2. 代码 token 效率
#    - 空格缩进是常见的效率陷阱
#    - GPT-4 使用空格合并：4个空格 -> 1个token
#    - 关键字整合：'def ', 'class ', 'import ' 通常是单 token
#
# 3. 数字处理
#    - 原始 BPE 可能把 "12345" 切成不同粒度
#    - Llama 3 强制数字单字符切割：1,2,3,4,5
#    - 有利于数学推理
```

#### 1.7.2 SentencePiece 实战配置

```python
import sentencepiece as spm

def train_sentencepiece(
    input_files: List[str],
    model_prefix: str = 'tokenizer',
    vocab_size: int = 64000,
    model_type: str = 'bpe'  # 'bpe' 或 'unigram'
):
    """
    训练 SentencePiece 分词器
    """
    spm.SentencePieceTrainer.train(
        input=','.join(input_files),
        model_prefix=model_prefix,
        vocab_size=vocab_size,
        model_type=model_type,
        
        # === 关键参数 ===
        character_coverage=0.9999,     # 字符覆盖率（多语言设 0.9999）
        num_threads=64,                # 并行线程
        
        # 字节级回退（确保无 UNK）
        byte_fallback=True,
        
        # 预分词（可选）
        split_by_unicode_script=True,  # 按 Unicode 脚本分割（中英文分开处理）
        split_by_number=True,          # 数字单独分割
        split_by_whitespace=True,
        split_digits=True,             # 每个数字是独立 token
        
        # 特殊 token
        pad_id=0,
        unk_id=1,
        bos_id=2,
        eos_id=3,
        
        # 用户定义的特殊 token
        user_defined_symbols=[
            '<|system|>', '<|user|>', '<|assistant|>',
            '<|tool_call|>', '<|tool_result|>',
            '<|begin_of_text|>', '<|end_of_text|>',
        ],
        
        # 正则化（训练时的随机分词，提升鲁棒性）
        # Subword Regularization (Kudo, 2018)
        enable_differential_privacy=False,
        
        # 控制合并的最小频率
        min_log_freq=0,
        
        # 输入采样（避免大文件主导）
        input_sentence_size=10_000_000,
        shuffle_input_sentence=True,
        
        # 归一化
        normalization_rule_name='identity',  # 不做 NFKC 归一化（保留原始形式）
        remove_extra_whitespaces=False,
        add_dummy_prefix=True,  # 句首加空格（重要！）
    )
    
    print(f"Tokenizer trained: {model_prefix}.model")


# === 评估 Tokenizer 质量 ===
def evaluate_tokenizer(model_path: str, test_texts: Dict[str, List[str]]):
    """评估分词器在不同语言/领域的效率"""
    sp = spm.SentencePieceProcessor(model_file=model_path)
    
    results = {}
    for domain, texts in test_texts.items():
        total_chars = sum(len(t) for t in texts)
        total_tokens = sum(len(sp.encode(t)) for t in texts)
        
        # 字符/token 比率（越高越好，意味着更高效）
        char_per_token = total_chars / total_tokens
        
        # 未知 token 比例
        all_pieces = [piece for t in texts for piece in sp.encode(t, out_type=str)]
        unk_ratio = sum(1 for p in all_pieces if p == '⁇') / len(all_pieces)
        
        results[domain] = {
            'char_per_token': char_per_token,
            'unk_ratio': unk_ratio,
            'avg_tokens_per_doc': total_tokens / len(texts)
        }
        
        print(f"{domain}: {char_per_token:.2f} chars/token, "
              f"UNK: {unk_ratio:.4%}")
    
    return results
```

---

## 2. 合成数据生成

合成数据是 2024-2026 LLM 发展的核心驱动力之一。在高质量人类数据日益稀缺的背景下，用模型生成训练数据（"以模型炼模型"）成为主流范式。

### 2.1 Self-Instruct

Self-Instruct（Wang et al., 2023）是合成指令数据的开山之作，核心思想是用 LLM 自身生成指令-输入-输出三元组。

```python
import random
from typing import List, Dict

class SelfInstructPipeline:
    """
    Self-Instruct 流程：
    1. 从少量种子任务开始（175 个人工编写）
    2. 用 LLM 生成新的指令
    3. 过滤低质量和重复指令
    4. 用 LLM 生成对应的输入和输出
    5. 迭代扩充
    """
    
    def __init__(self, llm_client, seed_tasks: List[Dict]):
        self.llm = llm_client
        self.seed_tasks = seed_tasks
        self.task_pool = list(seed_tasks)
    
    def generate_instructions(self, num_instructions: int = 8) -> List[str]:
        """Step 1: 生成新指令"""
        # 从 task pool 采样 few-shot 示例
        examples = random.sample(self.task_pool, min(8, len(self.task_pool)))
        
        prompt = "Come up with a series of diverse tasks. Here are some examples:\n\n"
        for i, task in enumerate(examples, 1):
            prompt += f"Task {i}: {task['instruction']}\n"
        
        prompt += f"\nNow generate {num_instructions} new and diverse tasks:\n"
        
        response = self.llm.generate(prompt, temperature=0.7, max_tokens=2000)
        
        # 解析生成的指令
        instructions = self._parse_instructions(response)
        return instructions
    
    def filter_instructions(self, instructions: List[str]) -> List[str]:
        """Step 2: 过滤低质量指令"""
        filtered = []
        for inst in instructions:
            # 长度过滤
            if len(inst.split()) < 3 or len(inst.split()) > 150:
                continue
            
            # 去重：与已有指令的 ROUGE-L 相似度 < 0.7
            if self._is_too_similar(inst):
                continue
            
            # 关键词黑名单（避免图片/音频等无法处理的指令）
            blacklist = ['image', 'picture', 'photo', 'draw', 'video', 'audio']
            if any(kw in inst.lower() for kw in blacklist):
                continue
            
            filtered.append(inst)
        
        return filtered
    
    def generate_instances(self, instruction: str) -> Dict:
        """Step 3: 为指令生成 input-output 对"""
        # 先判断是否需要输入
        classify_prompt = f"""
        Given the instruction, determine if it requires additional input besides the instruction itself.
        
        Instruction: {instruction}
        
        Does this instruction need additional input? (yes/no):
        """
        needs_input = 'yes' in self.llm.generate(classify_prompt).lower()
        
        if needs_input:
            # 生成输入
            input_prompt = f"""
            Given the instruction below, generate an appropriate input:
            
            Instruction: {instruction}
            
            Input:
            """
            input_text = self.llm.generate(input_prompt, temperature=0.6)
        else:
            input_text = ""
        
        # 生成输出
        output_prompt = f"""
        Complete the following task.
        
        Instruction: {instruction}
        {"Input: " + input_text if input_text else ""}
        
        Output:
        """
        output_text = self.llm.generate(output_prompt, temperature=0.3)
        
        return {
            'instruction': instruction,
            'input': input_text,
            'output': output_text
        }
    
    def _is_too_similar(self, new_inst: str, threshold: float = 0.7) -> bool:
        """计算与已有指令的相似度（简化版用词重叠）"""
        new_words = set(new_inst.lower().split())
        for task in self.task_pool:
            existing_words = set(task['instruction'].lower().split())
            if not new_words or not existing_words:
                continue
            overlap = len(new_words & existing_words)
            similarity = overlap / min(len(new_words), len(existing_words))
            if similarity > threshold:
                return True
        return False
    
    def _parse_instructions(self, response: str) -> List[str]:
        """解析 LLM 输出中的指令列表"""
        lines = response.strip().split('\n')
        instructions = []
        for line in lines:
            line = line.strip()
            # 移除编号前缀
            line = re.sub(r'^\d+[\.\)]\s*', '', line)
            line = re.sub(r'^Task\s*\d+:\s*', '', line, flags=re.IGNORECASE)
            if line and len(line) > 10:
                instructions.append(line)
        return instructions
    
    def run(self, target_count: int = 50000, batch_size: int = 8):
        """运行 Self-Instruct 管线"""
        generated = []
        
        while len(generated) < target_count:
            # 生成指令
            instructions = self.generate_instructions(batch_size)
            instructions = self.filter_instructions(instructions)
            
            # 为每个指令生成实例
            for inst in instructions:
                instance = self.generate_instances(inst)
                generated.append(instance)
                self.task_pool.append(instance)
            
            print(f"Generated {len(generated)}/{target_count} instances")
        
        return generated[:target_count]
```

### 2.2 Evol-Instruct / WizardLM

Evol-Instruct（WizardLM, Xu et al., 2023）通过进化策略系统性地增加指令复杂度。核心创新是定义了多种"进化算子"。

```python
from enum import Enum
from typing import Optional

class EvolOperator(Enum):
    """进化算子类型"""
    ADD_CONSTRAINTS = "add_constraints"       # 增加约束条件
    DEEPEN = "deepen"                         # 深化问题
    CONCRETIZE = "concretize"                 # 具体化
    INCREASE_REASONING = "increase_reasoning" # 增加推理步骤
    COMPLICATE_INPUT = "complicate_input"     # 复杂化输入
    BREADTH_EVOLVE = "breadth_evolve"         # 广度进化（生成相关但不同的指令）

# 每个进化算子的 prompt 模板
EVOLUTION_PROMPTS = {
    EvolOperator.ADD_CONSTRAINTS: """
I want you to act as a Prompt Rewriter.
Your objective is to rewrite a given prompt into a more complex version by adding one more constraint/requirement.

The rewritten prompt must be reasonable, understandable, and answerable by humans.

#Given Prompt#:
{instruction}

#Rewritten Prompt#:
""",

    EvolOperator.DEEPEN: """
I want you to act as a Prompt Rewriter.
Your objective is to rewrite a given prompt into a more complex version that requires deeper thinking and multi-step reasoning.

If the original prompt can be solved with a few simple thinking processes, the rewritten prompt should require more steps of thinking.

#Given Prompt#:
{instruction}

#Rewritten Prompt#:
""",

    EvolOperator.INCREASE_REASONING: """
I want you to act as a Prompt Rewriter.
Your objective is to rewrite a given prompt to explicitly require multi-step reasoning, comparison, or analysis.

#Given Prompt#:
{instruction}

#Rewritten Prompt#:
""",

    EvolOperator.BREADTH_EVOLVE: """
I want you to act as a Prompt Creator.
Your objective is to create a brand new prompt that is inspired by but substantially different from the given prompt.
The new prompt should belong to the same domain but test different skills or knowledge.

#Given Prompt#:
{instruction}

#Created Prompt#:
""",

    EvolOperator.CONCRETIZE: """
I want you to act as a Prompt Rewriter.
Your objective is to rewrite a given prompt by replacing general concepts with more specific, concrete ones.

#Given Prompt#:
{instruction}

#Rewritten Prompt#:
""",

    EvolOperator.COMPLICATE_INPUT: """
I want you to act as a Prompt Rewriter.
Your objective is to make the input/context of the prompt more complex (e.g., longer text, more data points, nested structures).

#Given Prompt#:
{instruction}

#Rewritten Prompt#:
""",
}


class EvolInstructPipeline:
    """
    Evol-Instruct 管线
    
    流程：
    1. 从初始指令集开始
    2. 每轮随机选择进化算子
    3. 用 LLM 执行进化
    4. 质量过滤（移除失败的进化）
    5. 迭代多轮（通常 3-4 轮）
    """
    
    def __init__(self, llm_client, max_depth: int = 4):
        self.llm = llm_client
        self.max_depth = max_depth
        
        # 深度进化算子（纵向复杂化）
        self.depth_operators = [
            EvolOperator.ADD_CONSTRAINTS,
            EvolOperator.DEEPEN,
            EvolOperator.INCREASE_REASONING,
            EvolOperator.CONCRETIZE,
            EvolOperator.COMPLICATE_INPUT,
        ]
        # 广度进化算子（横向扩展）
        self.breadth_operators = [
            EvolOperator.BREADTH_EVOLVE,
        ]
    
    def evolve_instruction(self, instruction: str, 
                           operator: Optional[EvolOperator] = None) -> Optional[str]:
        """对单条指令执行一次进化"""
        if operator is None:
            # 80% 概率深度进化，20% 概率广度进化
            if random.random() < 0.8:
                operator = random.choice(self.depth_operators)
            else:
                operator = random.choice(self.breadth_operators)
        
        prompt = EVOLUTION_PROMPTS[operator].format(instruction=instruction)
        evolved = self.llm.generate(prompt, temperature=0.7, max_tokens=1024)
        
        # 质量检查
        if not self._is_valid_evolution(instruction, evolved, operator):
            return None
        
        return evolved.strip()
    
    def _is_valid_evolution(self, original: str, evolved: str, 
                            operator: EvolOperator) -> bool:
        """验证进化结果的质量"""
        if not evolved or len(evolved) < 10:
            return False
        
        # 进化后不应该比原来短太多（除非是广度进化）
        if operator != EvolOperator.BREADTH_EVOLVE:
            if len(evolved) < len(original) * 0.8:
                return False
        
        # 不应该包含 meta-instruction 痕迹
        bad_phrases = [
            'rewritten prompt', 'given prompt', 'original prompt',
            '#Rewritten', '#Given', 'I want you to act',
            'your objective is'
        ]
        if any(phrase.lower() in evolved.lower() for phrase in bad_phrases):
            return False
        
        # 与原始指令不能完全相同
        if evolved.strip().lower() == original.strip().lower():
            return False
        
        return True
    
    def run(self, seed_instructions: List[str], 
            num_rounds: int = 3) -> List[Dict]:
        """
        运行多轮进化
        返回带难度层级标注的指令集
        """
        all_instructions = []
        current_pool = [(inst, 0) for inst in seed_instructions]  # (instruction, depth)
        
        for round_idx in range(num_rounds):
            next_pool = []
            
            for instruction, depth in current_pool:
                if depth >= self.max_depth:
                    continue
                
                evolved = self.evolve_instruction(instruction)
                
                if evolved:
                    all_instructions.append({
                        'instruction': evolved,
                        'depth': depth + 1,
                        'round': round_idx + 1,
                        'parent': instruction
                    })
                    next_pool.append((evolved, depth + 1))
                
                # 保留原始指令也继续进化
                next_pool.append((instruction, depth))
            
            current_pool = next_pool
            print(f"Round {round_idx + 1}: {len(all_instructions)} total instructions")
        
        return all_instructions
```

### 2.3 Magpie

Magpie（Xu et al., 2024）是一种极简但高效的合成数据方法，核心洞察是：**预训练的 chat 模型在看到系统提示后会自然地"脑补"用户问题**。

```python
class MagpiePipeline:
    """
    Magpie: Alignment Data Synthesis from Scratch
    
    核心思想：
    1. 给 chat 模型只提供 system prompt + user turn 的开头标记
    2. 模型会自动生成一个"用户问题"
    3. 然后用生成的问题让模型生成回答
    4. 得到高质量的 (instruction, response) 对
    
    优势：
    - 无需种子数据
    - 无需复杂 prompt engineering
    - 生成的指令分布与模型的预训练分布一致
    - 可大规模并行
    """
    
    def __init__(self, model_name: str = "meta-llama/Llama-3-70B-Instruct"):
        self.model_name = model_name
        # 不同模型的 chat template 不同
        self.templates = {
            'llama3': {
                'system_prefix': '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n',
                'user_prefix': '<|eot_id|><|start_header_id|>user<|end_header_id|>\n\n',
                'assistant_prefix': '<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n',
                'end_token': '<|eot_id|>',
            },
            'qwen2': {
                'system_prefix': '<|im_start|>system\n',
                'user_prefix': '<|im_end|>\n<|im_start|>user\n',
                'assistant_prefix': '<|im_end|>\n<|im_start|>assistant\n',
                'end_token': '<|im_end|>',
            }
        }
    
    def generate_instruction(self, system_prompt: str = "You are a helpful assistant.",
                              model_family: str = 'llama3') -> str:
        """
        用 Magpie 方法生成用户指令
        
        关键：只给 system prompt + user header，让模型自己生成问题
        """
        template = self.templates[model_family]
        
        # 构造输入：system prompt + user turn 的开头
        # 模型看到 user header 后会自动填充用户问题
        input_text = (
            template['system_prefix'] + system_prompt +
            template['user_prefix']
        )
        
        # 生成（模型会产出一个"用户问题"）
        generated = self.llm.generate(
            input_text,
            temperature=1.0,        # 高温度以增加多样性
            max_tokens=512,
            stop=[template['end_token'], template['assistant_prefix']]
        )
        
        instruction = generated.strip()
        return instruction
    
    def generate_response(self, instruction: str, 
                          system_prompt: str = "You are a helpful assistant.",
                          model_family: str = 'llama3') -> str:
        """用标准方式生成回答"""
        template = self.templates[model_family]
        
        input_text = (
            template['system_prefix'] + system_prompt +
            template['user_prefix'] + instruction +
            template['assistant_prefix']
        )
        
        response = self.llm.generate(
            input_text,
            temperature=0.6,
            max_tokens=2048,
            stop=[template['end_token']]
        )
        
        return response.strip()
    
    def generate_multi_turn(self, system_prompt: str, num_turns: int = 3,
                            model_family: str = 'llama3') -> List[Dict]:
        """
        多轮对话生成
        交替生成 user 和 assistant 消息
        """
        template = self.templates[model_family]
        conversation = []
        context = template['system_prefix'] + system_prompt
        
        for turn in range(num_turns):
            # 生成 user message
            user_input = context + template['user_prefix']
            user_msg = self.llm.generate(
                user_input, temperature=1.0, max_tokens=256,
                stop=[template['end_token']]
            ).strip()
            
            conversation.append({'role': 'user', 'content': user_msg})
            context += template['user_prefix'] + user_msg
            
            # 生成 assistant message
            asst_input = context + template['assistant_prefix']
            asst_msg = self.llm.generate(
                asst_input, temperature=0.6, max_tokens=1024,
                stop=[template['end_token']]
            ).strip()
            
            conversation.append({'role': 'assistant', 'content': asst_msg})
            context += template['assistant_prefix'] + asst_msg
        
        return conversation
    
    def filter_quality(self, instruction: str, response: str) -> bool:
        """质量过滤"""
        # 1. 长度检查
        if len(instruction.split()) < 5 or len(response.split()) < 10:
            return False
        
        # 2. 指令不应该是无意义的
        if instruction.strip().endswith('?') is False and len(instruction.split()) < 3:
            return False
        
        # 3. 回答不应该是拒绝
        refusal_phrases = [
            "I cannot", "I can't", "I'm unable", "I apologize",
            "as an AI", "as a language model"
        ]
        if any(phrase.lower() in response.lower()[:100] for phrase in refusal_phrases):
            return False
        
        # 4. 回答不应该太短（相对于问题）
        if len(response) < len(instruction) * 0.5:
            return False
        
        return True
    
    def run(self, num_samples: int = 100000, 
            system_prompts: List[str] = None) -> List[Dict]:
        """大规模生成"""
        if system_prompts is None:
            system_prompts = [
                "You are a helpful assistant.",
                "You are an expert programmer.",
                "You are a creative writer.",
                "You are a knowledgeable tutor.",
                "You are a data scientist.",
            ]
        
        dataset = []
        for i in range(num_samples):
            system = random.choice(system_prompts)
            
            instruction = self.generate_instruction(system)
            response = self.generate_response(instruction, system)
            
            if self.filter_quality(instruction, response):
                dataset.append({
                    'system': system,
                    'instruction': instruction,
                    'response': response
                })
        
        return dataset
```

### 2.4 数学合成数据

数学合成数据是提升 LLM 数学推理能力的关键，覆盖从小学算术到竞赛数学。

```python
import sympy
from typing import List, Dict, Tuple
import random

class MathDataSynthesizer:
    """
    数学合成数据生成器
    
    方法论：
    1. 模板化生成（GSM8K 风格的应用题）
    2. 程序化验证（用 SymPy/Python 验证答案）
    3. 逆向生成（从答案反推问题）
    4. 进化复杂化（逐步增加推理步骤）
    5. LLM 重写（增加自然语言多样性）
    """
    
    def generate_arithmetic_problem(self, difficulty: int = 1) -> Dict:
        """生成算术应用题"""
        templates = {
            1: [  # 简单：一步运算
                {
                    'template': '{name} has {a} {item}. {name2} gives {name} {b} more {item}. How many {item} does {name} have now?',
                    'solution': lambda a, b: a + b,
                    'steps': lambda a, b, names, item: [
                        f"{names[0]} starts with {a} {item}",
                        f"{names[1]} gives {a} + {b} = {a+b} {item}",
                        f"Answer: {a + b}"
                    ]
                },
            ],
            2: [  # 中等：两步运算
                {
                    'template': '{name} bought {a} {item} at ${price} each and {b} {item2} at ${price2} each. How much did {name} spend in total?',
                    'solution': lambda a, price, b, price2: a * price + b * price2,
                    'steps': lambda a, price, b, price2, name, item, item2: [
                        f"Cost of {item}: {a} × ${price} = ${a*price}",
                        f"Cost of {item2}: {b} × ${price2} = ${b*price2}",
                        f"Total: ${a*price} + ${b*price2} = ${a*price + b*price2}",
                        f"Answer: ${a*price + b*price2}"
                    ]
                },
            ],
            3: [  # 困难：多步推理 + 百分比/比例
                {
                    'template': 'A store has {total} {item}. {pct}% are {color1} and the rest are {color2}. If {sold} {color1} {item} are sold, what percentage of remaining {item} are {color1}?',
                    'solution': lambda total, pct, sold: round(
                        (total * pct / 100 - sold) / (total - sold) * 100, 2
                    ),
                    'steps': lambda total, pct, sold, item, c1, c2: [
                        f"Number of {c1} {item}: {total} × {pct}% = {int(total*pct/100)}",
                        f"After selling {sold}: {int(total*pct/100)} - {sold} = {int(total*pct/100) - sold} {c1} {item}",
                        f"Total remaining: {total} - {sold} = {total - sold}",
                        f"Percentage: {int(total*pct/100) - sold}/{total - sold} × 100 = {round((total*pct/100 - sold)/(total - sold)*100, 2)}%",
                    ]
                },
            ],
        }
        
        template_info = random.choice(templates.get(difficulty, templates[1]))
        
        # 生成具体数值
        names = random.sample(['Alice', 'Bob', 'Charlie', 'Diana', 'Eve'], 2)
        items = random.choice(['apples', 'books', 'pencils', 'cookies', 'balls'])
        
        if difficulty == 1:
            a, b = random.randint(5, 50), random.randint(1, 30)
            problem = template_info['template'].format(
                name=names[0], name2=names[1], a=a, b=b, item=items
            )
            answer = template_info['solution'](a, b)
            steps = template_info['steps'](a, b, names, items)
        # ... 其他难度级别类似
        
        return {
            'problem': problem,
            'answer': str(answer),
            'chain_of_thought': '\n'.join(steps),
            'difficulty': difficulty,
            'verified': True  # 程序化验证
        }
    
    def generate_algebra_problem(self) -> Dict:
        """用 SymPy 生成代数题并自动求解验证"""
        x = sympy.Symbol('x')
        
        # 随机生成方程参数
        a = random.randint(1, 10)
        b = random.randint(-20, 20)
        c = random.randint(1, 10)
        d = random.randint(-20, 20)
        
        # 构造方程 ax + b = cx + d
        equation = sympy.Eq(a * x + b, c * x + d)
        solution = sympy.solve(equation, x)
        
        if not solution or not all(s.is_rational for s in solution):
            return self.generate_algebra_problem()  # 重试
        
        problem = f"Solve for x: {a}x + {b} = {c}x + {d}"
        
        steps = [
            f"Given: {a}x + {b} = {c}x + {d}",
            f"Subtract {c}x from both sides: {a-c}x + {b} = {d}",
            f"Subtract {b} from both sides: {a-c}x = {d-b}",
            f"Divide by {a-c}: x = {d-b}/{a-c} = {solution[0]}",
            f"Verification: {a}({solution[0]}) + {b} = {a*solution[0]+b}, "
            f"{c}({solution[0]}) + {d} = {c*solution[0]+d} ✓"
        ]
        
        return {
            'problem': problem,
            'answer': str(solution[0]),
            'chain_of_thought': '\n'.join(steps),
            'difficulty': 2,
            'verified': sympy.simplify(
                a * solution[0] + b - (c * solution[0] + d)
            ) == 0
        }
    
    def augment_with_llm(self, problem: Dict, llm_client) -> Dict:
        """用 LLM 重写题目，增加自然语言多样性"""
        prompt = f"""
Rewrite the following math problem in a more natural, story-like way.
Keep the same mathematical content and answer.

Original: {problem['problem']}

Rewritten (natural language):
"""
        rewritten = llm_client.generate(prompt, temperature=0.7)
        
        problem_copy = dict(problem)
        problem_copy['problem_original'] = problem['problem']
        problem_copy['problem'] = rewritten.strip()
        return problem_copy
```

### 2.5 代码合成数据

```python
import ast
import subprocess
import tempfile
from typing import Optional

class CodeDataSynthesizer:
    """
    代码合成数据生成
    
    策略：
    1. 从文档/API 生成代码（doc-to-code）
    2. 从代码生成解释（code-to-explanation）
    3. 代码优化/重构对（before-after pairs）
    4. Bug 注入 + 修复对（buggy-fixed pairs）
    5. 单元测试生成
    6. OSS-Instruct (Magicoder): 从开源代码片段生成指令
    """
    
    def __init__(self, llm_client):
        self.llm = llm_client
    
    def oss_instruct(self, code_snippet: str) -> Dict:
        """
        OSS-Instruct (Magicoder, Wei et al., 2024)
        从开源代码片段出发，生成编程指令和解决方案
        """
        prompt = f"""
Below is a code snippet from an open source repository:

```
{code_snippet}
```

Based on the concepts and patterns in this code, create a coding problem and its solution.
The problem should be self-contained and test similar programming concepts.

## Problem:
[Write a clear problem statement]

## Solution:
[Write a complete, runnable solution]
"""
        response = self.llm.generate(prompt, temperature=0.6, max_tokens=2048)
        
        # 解析问题和解答
        problem, solution = self._parse_problem_solution(response)
        
        # 验证解决方案
        is_valid = self.verify_code(solution)
        
        return {
            'seed_code': code_snippet,
            'instruction': problem,
            'response': solution,
            'verified': is_valid
        }
    
    def generate_bug_fix_pair(self, clean_code: str) -> Dict:
        """生成 bug 注入 + 修复对"""
        bug_injection_prompt = f"""
Given the following correct code, introduce a subtle but realistic bug.
The bug should be something a programmer might actually write (off-by-one, wrong variable, missing edge case, etc.)

Correct code:
```python
{clean_code}
```

Buggy code (with a subtle bug):
```python
"""
        buggy_code = self.llm.generate(bug_injection_prompt, temperature=0.5)
        buggy_code = self._extract_code_block(buggy_code)
        
        return {
            'instruction': f"Find and fix the bug in the following code:\n```python\n{buggy_code}\n```",
            'buggy_code': buggy_code,
            'fixed_code': clean_code,
            'response': f"The bug is ... Here's the fixed code:\n```python\n{clean_code}\n```"
        }
    
    def verify_code(self, code: str, test_code: str = None,
                    timeout: int = 10) -> bool:
        """验证代码是否可执行"""
        try:
            # 1. 语法检查
            ast.parse(code)
        except SyntaxError:
            return False
        
        # 2. 运行时检查（沙箱执行）
        if test_code:
            full_code = code + '\n' + test_code
        else:
            full_code = code
        
        try:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(full_code)
                f.flush()
                
                result = subprocess.run(
                    ['python', f.name],
                    capture_output=True,
                    timeout=timeout,
                    text=True
                )
                return result.returncode == 0
        except (subprocess.TimeoutExpired, Exception):
            return False
    
    def _extract_code_block(self, text: str) -> str:
        """从 LLM 输出中提取代码块"""
        import re
        pattern = r'```(?:python)?\n(.*?)```'
        matches = re.findall(pattern, text, re.DOTALL)
        return matches[0].strip() if matches else text.strip()
    
    def _parse_problem_solution(self, text: str) -> Tuple[str, str]:
        """解析问题和解决方案"""
        parts = text.split('## Solution:')
        if len(parts) == 2:
            problem = parts[0].replace('## Problem:', '').strip()
            solution = parts[1].strip()
            return problem, solution
        return text, ''
```

---

## 3. SFT 数据构建

Supervised Fine-Tuning (SFT) 是将预训练模型转化为有用助手的关键阶段。数据质量远比数据数量重要——LIMA 论文证明仅用 1000 条高质量数据就能实现优秀的指令跟随能力。

### 3.1 指令跟随数据

```python
from typing import List, Dict, Optional
from dataclasses import dataclass
import json

@dataclass
class SFTExample:
    """SFT 数据样本格式"""
    system: str                          # 系统提示
    messages: List[Dict[str, str]]       # 对话消息列表
    metadata: Optional[Dict] = None     # 元数据（来源、难度、类别等）

class SFTDataBuilder:
    """
    SFT 数据构建器
    
    关键原则（来自 LIMA、Alpaca、Tulu 等研究）：
    1. 质量 > 数量：1000 条精选 > 100K 条噪音
    2. 多样性：覆盖尽可能多的任务类型
    3. 长回答偏好：鼓励详细、结构化的回答
    4. 格式一致性：统一的对话格式
    5. 安全对齐：拒绝有害请求的示例
    """
    
    # SFT 任务类别及权重
    TASK_CATEGORIES = {
        'open_qa': 0.15,           # 开放问答
        'closed_qa': 0.10,         # 事实问答
        'brainstorming': 0.08,     # 头脑风暴
        'creative_writing': 0.10,  # 创意写作
        'rewriting': 0.05,         # 改写/润色
        'summarization': 0.08,     # 摘要
        'classification': 0.05,    # 分类
        'extraction': 0.05,        # 信息提取
        'coding': 0.12,            # 编程
        'math': 0.08,              # 数学
        'reasoning': 0.07,         # 逻辑推理
        'roleplay': 0.03,         # 角色扮演
        'safety_refusal': 0.04,   # 安全拒绝
    }
    
    def build_instruction_following_example(
        self,
        task_type: str,
        instruction: str,
        response: str,
        system_prompt: str = "You are a helpful, harmless, and honest assistant.",
        input_context: str = ""
    ) -> SFTExample:
        """构建指令跟随样本"""
        messages = []
        
        # User message
        user_content = instruction
        if input_context:
            user_content = f"{instruction}\n\n{input_context}"
        
        messages.append({'role': 'user', 'content': user_content})
        messages.append({'role': 'assistant', 'content': response})
        
        return SFTExample(
            system=system_prompt,
            messages=messages,
            metadata={
                'task_type': task_type,
                'instruction_length': len(instruction.split()),
                'response_length': len(response.split()),
            }
        )
    
    def format_for_training(self, example: SFTExample, 
                            template: str = 'chatml') -> str:
        """
        将 SFT 样本转为训练格式
        
        ChatML 格式（OpenAI/Qwen 风格）：
        <|im_start|>system
        {system}<|im_end|>
        <|im_start|>user
        {user}<|im_end|>
        <|im_start|>assistant
        {assistant}<|im_end|>
        """
        if template == 'chatml':
            parts = [f"<|im_start|>system\n{example.system}<|im_end|>"]
            for msg in example.messages:
                parts.append(f"<|im_start|>{msg['role']}\n{msg['content']}<|im_end|>")
            return '\n'.join(parts)
        
        elif template == 'llama3':
            parts = [
                f"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n"
                f"{example.system}<|eot_id|>"
            ]
            for msg in example.messages:
                parts.append(
                    f"<|start_header_id|>{msg['role']}<|end_header_id|>\n\n"
                    f"{msg['content']}<|eot_id|>"
                )
            return ''.join(parts)
        
        elif template == 'alpaca':
            user_msg = example.messages[0]['content']
            asst_msg = example.messages[1]['content']
            return (f"### Instruction:\n{user_msg}\n\n"
                    f"### Response:\n{asst_msg}")
    
    def compute_loss_mask(self, example: SFTExample, 
                          tokenizer) -> List[int]:
        """
        计算 loss mask：只在 assistant 回答部分计算 loss
        这是 SFT 训练的关键细节！
        """
        full_text = self.format_for_training(example)
        tokens = tokenizer.encode(full_text)
        mask = [0] * len(tokens)  # 默认不计算 loss
        
        # 找到 assistant 回答的 token 范围
        # 在这些位置设 mask = 1
        # 具体实现取决于 chat template
        
        # 以 ChatML 为例：
        # 找到所有 "<|im_start|>assistant\n" 后到 "<|im_end|>" 之间的 token
        asst_start_tokens = tokenizer.encode("<|im_start|>assistant\n")
        end_tokens = tokenizer.encode("<|im_end|>")
        
        in_assistant = False
        for i, token in enumerate(tokens):
            if self._matches_sequence(tokens, i, asst_start_tokens):
                in_assistant = True
                continue
            if in_assistant and self._matches_sequence(tokens, i, end_tokens):
                in_assistant = False
                continue
            if in_assistant:
                mask[i] = 1
        
        return mask
    
    def _matches_sequence(self, tokens, pos, seq) -> bool:
        """检查 tokens[pos:] 是否以 seq 开头"""
        if pos + len(seq) > len(tokens):
            return False
        return tokens[pos:pos+len(seq)] == seq


# === 高质量 SFT 数据的筛选标准 ===
def quality_check_sft(example: SFTExample) -> Tuple[bool, List[str]]:
    """
    SFT 数据质量检查
    返回 (是否通过, 问题列表)
    """
    issues = []
    
    response = example.messages[-1]['content']
    instruction = example.messages[0]['content']
    
    # 1. 回答长度
    response_words = len(response.split())
    if response_words < 20:
        issues.append("response_too_short")
    
    # 2. 回答不应该以道歉开头
    apology_starts = ['i apologize', 'sorry', "i'm sorry", 'i cannot']
    if any(response.lower().startswith(s) for s in apology_starts):
        # 除非是安全拒绝类
        if example.metadata and example.metadata.get('task_type') != 'safety_refusal':
            issues.append("unnecessary_apology")
    
    # 3. 格式一致性：不应该有残留的 prompt 模板
    template_leaks = ['### Instruction', '### Response', '[INST]', '<|im_start|>']
    if any(leak in response for leak in template_leaks):
        issues.append("template_leak")
    
    # 4. 回答应该是自包含的（不引用对话历史中不存在的内容）
    if 'as I mentioned earlier' in response and len(example.messages) <= 2:
        issues.append("hallucinated_context")
    
    # 5. 指令和回答不应该太相似（复读机检测）
    instruction_words = set(instruction.lower().split())
    response_words_set = set(response.lower().split())
    if instruction_words and len(instruction_words & response_words_set) / len(instruction_words) > 0.8:
        issues.append("response_copies_instruction")
    
    # 6. 检查是否包含模型自指（"As an AI language model"）
    self_refs = ['as an ai', 'as a language model', 'as an llm', 'i am an ai']
    if any(ref in response.lower() for ref in self_refs):
        issues.append("excessive_self_reference")
    
    return len(issues) == 0, issues
```

### 3.2 多轮对话数据

```python
class MultiTurnDialogBuilder:
    """
    多轮对话数据构建
    
    挑战：
    1. 上下文连贯性：后续轮次要引用之前的内容
    2. 主题演进：对话应该有自然的深入/转换
    3. 角色一致性：助手的风格应保持一致
    4. 长上下文：随着轮次增加，上下文窗口的管理
    """
    
    def __init__(self, llm_client):
        self.llm = llm_client
    
    def build_from_seed_topic(self, topic: str, num_turns: int = 4) -> List[Dict]:
        """从种子话题构建多轮对话"""
        conversation = []
        
        # 第一轮：初始问题
        first_question = self._generate_first_question(topic)
        first_answer = self._generate_response(first_question, [])
        conversation.extend([
            {'role': 'user', 'content': first_question},
            {'role': 'assistant', 'content': first_answer}
        ])
        
        # 后续轮次：基于上下文生成跟进问题
        for turn in range(1, num_turns):
            # 生成跟进问题的策略
            strategy = random.choice([
                'follow_up',      # 追问细节
                'clarification',  # 请求澄清
                'extension',      # 扩展到相关话题
                'challenge',      # 质疑/挑战
                'application',    # 要求应用/举例
            ])
            
            follow_up = self._generate_follow_up(conversation, strategy)
            response = self._generate_response(follow_up, conversation)
            
            conversation.extend([
                {'role': 'user', 'content': follow_up},
                {'role': 'assistant', 'content': response}
            ])
        
        return conversation
    
    def _generate_follow_up(self, history: List[Dict], 
                            strategy: str) -> str:
        """生成跟进问题"""
        strategy_prompts = {
            'follow_up': "Ask a follow-up question that digs deeper into a specific point mentioned in the assistant's last response.",
            'clarification': "Ask the assistant to clarify or elaborate on something from their previous response.",
            'extension': "Ask about a related topic that naturally extends from the current discussion.",
            'challenge': "Respectfully challenge or question one of the assistant's claims, asking for evidence or alternative perspectives.",
            'application': "Ask the assistant to provide a concrete example or practical application of what was discussed.",
        }
        
        history_text = '\n'.join([f"{m['role']}: {m['content']}" for m in history])
        
        prompt = f"""
Given this conversation:

{history_text}

{strategy_prompts[strategy]}
Generate only the user's next message:
"""
        return self.llm.generate(prompt, temperature=0.8).strip()
    
    def _generate_first_question(self, topic: str) -> str:
        prompt = f"Generate a natural, detailed question about: {topic}\nQuestion:"
        return self.llm.generate(prompt, temperature=0.8).strip()
    
    def _generate_response(self, question: str, 
                           history: List[Dict]) -> str:
        messages = list(history)
        messages.append({'role': 'user', 'content': question})
        return self.llm.chat(messages, temperature=0.6)
    
    def validate_multi_turn(self, conversation: List[Dict]) -> Tuple[bool, List[str]]:
        """验证多轮对话质量"""
        issues = []
        
        # 1. 角色交替
        for i in range(len(conversation) - 1):
            if conversation[i]['role'] == conversation[i+1]['role']:
                issues.append(f"consecutive_same_role_at_turn_{i}")
        
        # 2. 上下文引用检查
        # 后续回答应该引用之前的内容
        for i in range(2, len(conversation), 2):  # 检查每个 user 消息
            user_msg = conversation[i]['content'].lower()
            prev_asst = conversation[i-1]['content'].lower()
            
            # 跟进问题应该与上一轮回答有某种关联
            # （简化检查：至少有一些词汇重叠）
            user_words = set(user_msg.split()) - {'the', 'a', 'an', 'is', 'are', 'how', 'what', 'why'}
            asst_words = set(prev_asst.split())
            
            overlap = len(user_words & asst_words)
            if overlap < 1 and i > 2:
                issues.append(f"no_context_reference_at_turn_{i}")
        
        # 3. 回答一致性
        # 检查助手是否自相矛盾（简化版）
        assistant_msgs = [m['content'] for m in conversation if m['role'] == 'assistant']
        # ... 更复杂的一致性检查需要 NLI 模型
        
        return len(issues) == 0, issues
```

### 3.3 长文本 SFT

```python
class LongContextSFTBuilder:
    """
    长文本 SFT 数据构建
    
    目标：训练模型处理长上下文（32K-128K+ tokens）
    
    数据类型：
    1. 长文档摘要
    2. 长文档问答（needle-in-a-haystack）
    3. 多文档综合
    4. 长对话
    5. 长代码理解
    """
    
    def build_long_doc_qa(self, document: str, 
                          target_length: int = 32000) -> List[Dict]:
        """
        从长文档构建问答对
        
        策略：
        1. 在文档的不同位置放置关键信息（"针"）
        2. 问题要求模型找到并整合这些信息
        3. 变化"针"的位置：开头、中间、结尾、分散
        """
        examples = []
        
        # 将文档分成段落
        paragraphs = document.split('\n\n')
        
        # 策略1：定点问答（特定段落的信息）
        for i, para in enumerate(paragraphs):
            if len(para.split()) > 50:  # 只对内容丰富的段落提问
                position = i / len(paragraphs)  # 0.0-1.0
                
                # 生成针对该段落的问题
                qa = self._generate_qa_for_paragraph(
                    para, 
                    position_label=self._position_label(position)
                )
                
                if qa:
                    examples.append({
                        'messages': [
                            {'role': 'user', 'content': f"Based on the following document, answer the question.\n\n<document>\n{document}\n</document>\n\nQuestion: {qa['question']}"},
                            {'role': 'assistant', 'content': qa['answer']}
                        ],
                        'metadata': {
                            'task': 'long_doc_qa',
                            'doc_length': len(document.split()),
                            'answer_position': position,
                            'position_label': self._position_label(position)
                        }
                    })
        
        # 策略2：跨段落综合问答
        if len(paragraphs) > 5:
            synthesis_qa = self._generate_synthesis_qa(paragraphs)
            if synthesis_qa:
                examples.append({
                    'messages': [
                        {'role': 'user', 'content': f"Read the following document carefully and answer the question that requires synthesizing information from multiple sections.\n\n<document>\n{document}\n</document>\n\nQuestion: {synthesis_qa['question']}"},
                        {'role': 'assistant', 'content': synthesis_qa['answer']}
                    ],
                    'metadata': {'task': 'synthesis_qa'}
                })
        
        return examples
    
    def build_long_summarization(self, document: str) -> Dict:
        """长文档摘要数据"""
        # 多层级摘要
        return {
            'messages': [
                {'role': 'user', 'content': f"Please provide a comprehensive summary of the following document. Include:\n1. A one-paragraph executive summary\n2. Key points organized by section\n3. Important details and data points\n\n<document>\n{document}\n</document>"},
                {'role': 'assistant', 'content': '...'}  # 由 LLM 生成
            ],
            'metadata': {
                'task': 'long_summarization',
                'doc_length': len(document.split()),
            }
        }
    
    def build_multi_doc_qa(self, documents: List[str], 
                           question: str) -> Dict:
        """多文档问答"""
        doc_texts = '\n\n---\n\n'.join([
            f"Document {i+1}:\n{doc}" for i, doc in enumerate(documents)
        ])
        
        return {
            'messages': [
                {'role': 'user', 'content': f"Based on the following {len(documents)} documents, answer the question.\n\n{doc_texts}\n\nQuestion: {question}"},
                {'role': 'assistant', 'content': '...'}
            ],
            'metadata': {
                'task': 'multi_doc_qa',
                'num_docs': len(documents),
                'total_length': sum(len(d.split()) for d in documents)
            }
        }
    
    def _position_label(self, position: float) -> str:
        if position < 0.2:
            return 'beginning'
        elif position < 0.4:
            return 'early_middle'
        elif position < 0.6:
            return 'middle'
        elif position < 0.8:
            return 'late_middle'
        else:
            return 'end'
    
    def _generate_qa_for_paragraph(self, paragraph: str, 
                                    position_label: str) -> Optional[Dict]:
        """为特定段落生成 QA 对"""
        prompt = f"""
Generate a question and answer based on this paragraph. 
The question should require understanding the specific details in this paragraph.

Paragraph: {paragraph}

Question:
Answer:
"""
        # 调用 LLM 生成
        response = self.llm.generate(prompt)
        # 解析 question 和 answer
        return self._parse_qa(response)
    
    def _generate_synthesis_qa(self, paragraphs: List[str]) -> Optional[Dict]:
        """生成需要综合多段落信息的问题"""
        # 选择 2-3 个不相邻的段落
        indices = sorted(random.sample(range(len(paragraphs)), min(3, len(paragraphs))))
        selected = [paragraphs[i] for i in indices]
        
        prompt = f"""
Generate a question that can only be answered by combining information from multiple parts of a document.
Here are excerpts from different parts:

Part 1: {selected[0][:500]}
Part 2: {selected[1][:500]}
{"Part 3: " + selected[2][:500] if len(selected) > 2 else ""}

Generate a question that requires synthesizing info from at least 2 parts, and provide the answer.
"""
        response = self.llm.generate(prompt)
        return self._parse_qa(response)
    
    def _parse_qa(self, text: str) -> Optional[Dict]:
        """解析 QA 文本"""
        if 'Question:' in text and 'Answer:' in text:
            parts = text.split('Answer:')
            question = parts[0].replace('Question:', '').strip()
            answer = parts[1].strip()
            if question and answer:
                return {'question': question, 'answer': answer}
        return None
```

---

## 4. DPO/RLHF 偏好数据构建

偏好对齐是让模型输出符合人类价值观和偏好的关键步骤。

### 4.1 RLHF 数据管线

```python
from dataclasses import dataclass
from typing import List, Optional, Tuple
import json

@dataclass
class PreferenceExample:
    """偏好数据样本"""
    prompt: str
    chosen: str          # 被选中的（更好的）回答
    rejected: str        # 被拒绝的（更差的）回答
    chosen_rating: Optional[float] = None
    rejected_rating: Optional[float] = None
    metadata: Optional[Dict] = None

class RLHFDataPipeline:
    """
    RLHF 数据构建管线
    
    InstructGPT 论文（Ouyang et al., 2022）的经典流程：
    1. 收集 prompts
    2. 对每个 prompt 生成多个回答
    3. 人工标注偏好排序
    4. 训练 Reward Model
    5. 用 PPO 优化 Policy Model
    
    数据需求：
    - SFT 数据: ~13K 条
    - 比较数据: ~33K 条（用于 RM）
    - PPO prompts: ~31K 条
    """
    
    def __init__(self, llm_client, num_responses: int = 4):
        self.llm = llm_client
        self.num_responses = num_responses
    
    def collect_prompts(self, sources: List[str]) -> List[str]:
        """
        Prompt 来源：
        1. 用户真实请求日志（去 PII 后）
        2. 任务分类 taxonomy 生成
        3. Red-teaming 对抗提示
        """
        prompts = []
        for source in sources:
            prompts.extend(self._load_prompts(source))
        return self._deduplicate(prompts)
    
    def generate_responses(self, prompt: str) -> List[str]:
        """对每个 prompt 生成 K 个回答，用于人工比较"""
        responses = []
        for _ in range(self.num_responses):
            resp = self.llm.generate(
                prompt,
                temperature=0.8,  # 增加多样性
                top_p=0.95
            )
            responses.append(resp)
        return responses
    
    def rank_responses(self, prompt: str, responses: List[str]) -> List[Tuple[str, str]]:
        """
        人工标注偏好排序 → 转化为 pairwise 比较对
        
        标注指南要点：
        - Helpfulness > Harmlessness > Honesty（InstructGPT 优先级）
        - 不确定时选"tie"，不强制区分
        - 标注者间一致性 (Cohen's κ) 需 > 0.6
        """
        # 生成所有 C(K, 2) 对
        pairs = []
        for i in range(len(responses)):
            for j in range(i+1, len(responses)):
                pairs.append((responses[i], responses[j]))
        return pairs  # 交给人工标注平台
```

### 4.2 DPO 数据构建

DPO (Direct Preference Optimization) 直接从偏好数据学习，跳过 Reward Model 训练：

```python
class DPODataBuilder:
    """
    DPO 数据构建策略
    
    核心公式: L_DPO = -E[log σ(β(log π(y_w|x)/π_ref(y_w|x) - log π(y_l|x)/π_ref(y_l|x)))]
    
    数据格式: {"prompt": str, "chosen": str, "rejected": str}
    """
    
    def build_from_reward_model(self, prompts, policy_model, reward_model, 
                                 num_samples=8, temperature=0.7):
        """
        方法1: Best-of-N 采样 + RM 打分
        
        流程:
        1. 对每个 prompt 采样 N 个回答
        2. 用 RM 打分
        3. 最高分 = chosen，最低分 = rejected
        4. 可选: 分数差距过小的丢弃（margin filtering）
        """
        dataset = []
        for prompt in prompts:
            responses = [policy_model.generate(prompt, temp=temperature) 
                        for _ in range(num_samples)]
            scores = [reward_model.score(prompt, r) for r in responses]
            
            best_idx = max(range(len(scores)), key=lambda i: scores[i])
            worst_idx = min(range(len(scores)), key=lambda i: scores[i])
            
            # Margin filtering: 分数差距太小的对不够有信息量
            if scores[best_idx] - scores[worst_idx] > 0.5:
                dataset.append({
                    "prompt": prompt,
                    "chosen": responses[best_idx],
                    "rejected": responses[worst_idx]
                })
        return dataset
    
    def build_from_model_comparison(self, prompts, strong_model, weak_model):
        """
        方法2: 强弱模型对比
        
        用强模型(如 GPT-4)生成 chosen，弱模型生成 rejected
        简单高效，但可能引入模型偏差
        """
        dataset = []
        for prompt in prompts:
            chosen = strong_model.generate(prompt, temperature=0.3)
            rejected = weak_model.generate(prompt, temperature=0.7)
            dataset.append({
                "prompt": prompt,
                "chosen": chosen,
                "rejected": rejected
            })
        return dataset
    
    def build_iterative_dpo(self, prompts, model, num_iterations=3):
        """
        方法3: 迭代 DPO (Self-Play)
        
        SPIN (Self-Play Fine-Tuning) / 迭代 DPO:
        - Round 1: 用 SFT 模型 vs ground truth
        - Round 2: 用 DPO-1 模型 vs ground truth
        - Round N: 用 DPO-(N-1) 模型 vs ground truth
        
        每轮模型产出的回答作为 rejected，ground truth 为 chosen
        """
        for iteration in range(num_iterations):
            dataset = []
            for prompt in prompts:
                rejected = model.generate(prompt)
                chosen = self._get_ground_truth(prompt)
                dataset.append({
                    "prompt": prompt,
                    "chosen": chosen,
                    "rejected": rejected
                })
            model = self._train_dpo(model, dataset)
        return model
```

### 4.3 偏好数据质量控制

```python
class PreferenceQualityControl:
    """偏好数据质量保障"""
    
    def check_annotation_agreement(self, annotations):
        """
        标注一致性检查
        
        指标:
        - Cohen's Kappa (2人): κ > 0.6 为可接受
        - Fleiss' Kappa (多人): κ > 0.4 为中等一致
        - 标注者内部一致性: 重复标注 10% 样本
        """
        pass
    
    def detect_position_bias(self, pairs):
        """
        位置偏差检测
        
        人工标注者倾向于选第一个（primacy bias）或最后一个（recency bias）
        解决: 随机打乱 chosen/rejected 的展示顺序
        """
        first_chosen_rate = sum(1 for p in pairs if p['chosen_position'] == 0) / len(pairs)
        if abs(first_chosen_rate - 0.5) > 0.1:
            print(f"⚠️ 位置偏差: 第一个被选率 = {first_chosen_rate:.2%}")
    
    def filter_low_quality_pairs(self, dataset, min_margin=0.3):
        """
        过滤低质量偏好对
        
        - 删除 chosen 和 rejected 几乎相同的对（编辑距离过小）
        - 删除 RM 分数差距过小的对
        - 删除标注者不一致的对
        """
        filtered = []
        for item in dataset:
            edit_dist = self._normalized_edit_distance(item['chosen'], item['rejected'])
            if edit_dist > 0.1:  # 至少 10% 的差异
                filtered.append(item)
        return filtered
```

---

## 5. 数据质量评估

### 5.1 自动化质量指标

```python
class DataQualityEvaluator:
    """
    数据质量多维评估体系
    
    维度:
    1. 语言质量 (Perplexity, Grammar)
    2. 多样性 (Embedding 分布, N-gram 覆盖)
    3. 信息密度 (压缩率, 重复度)
    4. 安全性 (有毒内容, PII)
    5. 领域覆盖 (分布均衡性)
    """
    
    def perplexity_filter(self, texts, model, threshold=500):
        """
        Perplexity 过滤
        
        原理: 高质量文本在语言模型中有较低 perplexity
        
        注意事项:
        - 代码和公式的 PPL 天然较高，需分领域设阈值
        - 参考模型选择影响结果（KenLM vs GPT-2 vs domain-specific）
        - FineWeb 使用 KenLM 5-gram，CCNet 使用维基百科训练的模型
        
        典型阈值:
        - 通用文本: PPL < 500（KenLM）
        - 学术论文: PPL < 800
        - 代码: 不用 PPL 过滤
        """
        filtered = []
        for text in texts:
            ppl = model.perplexity(text)
            if ppl < threshold:
                filtered.append(text)
        return filtered
    
    def embedding_diversity(self, texts, encoder, n_clusters=50):
        """
        Embedding 空间多样性分析
        
        方法:
        1. 用 sentence-transformer 编码所有文本
        2. K-Means 聚类
        3. 计算 cluster 大小分布的熵
        4. 高熵 = 高多样性
        
        应用场景:
        - 检测数据中是否有过度集中的主题
        - 指导数据配比调整
        - 发现数据盲区
        """
        import numpy as np
        from sklearn.cluster import KMeans
        
        embeddings = encoder.encode(texts)
        kmeans = KMeans(n_clusters=n_clusters).fit(embeddings)
        
        # 计算 cluster 分布熵
        labels = kmeans.labels_
        counts = np.bincount(labels, minlength=n_clusters)
        probs = counts / counts.sum()
        entropy = -np.sum(probs * np.log(probs + 1e-10))
        max_entropy = np.log(n_clusters)
        
        diversity_score = entropy / max_entropy  # 归一化到 [0, 1]
        return diversity_score, kmeans
    
    def repetition_analysis(self, texts, ngram_sizes=[5, 10, 20]):
        """
        重复度分析
        
        检测:
        - 文档内重复: 同一文档中 N-gram 重复率
        - 跨文档重复: 不同文档间的 N-gram 重叠
        - 模板化内容: 高频出现的固定模式
        
        阈值参考（Gopher/Chinchilla）:
        - 任何 2-gram 重复率 > 20% → 过滤
        - 任何 3-gram 重复率 > 10% → 过滤
        - 任何 10-gram 重复率 > 5% → 过滤
        """
        results = {}
        for n in ngram_sizes:
            total_ngrams = 0
            duplicate_ngrams = 0
            for text in texts:
                tokens = text.split()
                ngrams = [tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)]
                total_ngrams += len(ngrams)
                duplicate_ngrams += len(ngrams) - len(set(ngrams))
            results[f"{n}-gram_dup_rate"] = duplicate_ngrams / max(total_ngrams, 1)
        return results
```

### 5.2 人工评估框架

```
人工评估维度（5分制）:
┌─────────────────────────────────────────────┐
│ 1. 准确性 (Accuracy)                         │
│    - 事实是否正确                              │
│    - 数据/数字是否可验证                        │
│                                              │
│ 2. 完整性 (Completeness)                      │
│    - 回答是否完整覆盖问题                       │
│    - 是否有遗漏关键信息                         │
│                                              │
│ 3. 相关性 (Relevance)                         │
│    - 回答是否紧扣问题                           │
│    - 是否有无关废话                             │
│                                              │
│ 4. 可读性 (Readability)                       │
│    - 语言是否流畅自然                           │
│    - 结构是否清晰                              │
│                                              │
│ 5. 安全性 (Safety)                            │
│    - 是否含有毒/偏见/不当内容                    │
│    - 是否泄露隐私                              │
└─────────────────────────────────────────────┘

评估规模参考:
- 小规模验证: 200-500 条，3 名标注者
- 中规模评估: 1000-2000 条，5+ 名标注者
- 大规模基准: 5000+ 条，众包 + 专家审核
```

### 5.3 数据飞轮（Data Flywheel）

```
数据飞轮闭环（2026 最佳实践）:

   用户交互 ──→ 日志收集 ──→ 数据清洗
       ↑                         ↓
   模型部署 ←── 模型训练 ←── 标注/合成
       │                         ↑
       └── 在线反馈 ─── 质量评估 ─┘

关键指标:
- 飞轮周期: 从用户反馈到模型更新的时间
  - 2023: 3-6 个月
  - 2024: 1-4 周
  - 2026: 1-3 天（最佳实践）

- 自动化率:
  - 数据清洗: 95%+ 自动化
  - 质量过滤: 90%+ 自动化（model-based）
  - 标注: 70-80% 合成 + 20-30% 人工验证

核心理念: 
  "数据不是一次性收集，而是持续流动的生态系统"
  — 每次模型迭代都会产生更好的合成数据
  — 更好的合成数据训练出更好的模型
  — 正反馈循环
```

---

## 6. 数据合规与安全

### 6.1 版权合规

```
版权风险矩阵:

数据源              风险等级    应对策略
─────────────────────────────────────────
Common Crawl       中-高      robots.txt 尊重 + 备案
维基百科            低         CC BY-SA，需注明来源
GitHub 代码         中         检查 License（MIT/Apache OK，GPL 需隔离）
新闻网站            高         需授权协议或 fair use 论证
书籍/出版物         高         版权保护期内不可直接使用
用户生成内容         中         检查 ToS，PII 脱敏
学术论文            低-中      开放获取可用，付费的需授权

2026 法律趋势:
- NYT vs OpenAI 案仍在审理，对整个行业有示范效应
- EU AI Act 要求训练数据透明度
- 中国《生成式AI管理办法》要求数据来源合规备案
- "Data provenance" 成为模型发布的标配
```

### 6.2 有毒内容过滤

```python
class ToxicityFilter:
    """
    多层有毒内容过滤
    
    层级:
    1. 关键词黑名单（快速但粗糙）
    2. 分类器（Perspective API / 自训练模型）
    3. LLM-as-Judge（高质量但昂贵）
    """
    
    def keyword_filter(self, text, blacklist):
        """第一层: 关键词过滤（召回率优先）"""
        text_lower = text.lower()
        for word in blacklist:
            if word in text_lower:
                return True, f"keyword: {word}"
        return False, None
    
    def classifier_filter(self, text, model, threshold=0.7):
        """
        第二层: 分类器过滤
        
        推荐模型:
        - Perspective API（Google，免费额度）
        - OpenAI Moderation API
        - HateBERT / ToxiGen 分类器
        - Llama Guard 3（开源，多语言）
        
        分类维度:
        - toxicity, severe_toxicity
        - identity_attack, insult, profanity
        - threat, sexually_explicit
        """
        scores = model.predict(text)
        flagged_categories = {k: v for k, v in scores.items() if v > threshold}
        return len(flagged_categories) > 0, flagged_categories
    
    def llm_judge_filter(self, text, judge_model):
        """
        第三层: LLM-as-Judge（精度优先）
        
        用于处理分类器不确定的边界情况
        成本高，仅用于 5-10% 的边界样本
        """
        prompt = f"""请判断以下文本是否包含有毒、偏见、不当或不安全内容。
        
文本: {text}

请从以下维度评估（0-1分）:
1. 仇恨/歧视
2. 暴力/威胁  
3. 色情/不当
4. 虚假/误导
5. 隐私泄露

输出 JSON: {{"safe": bool, "scores": dict, "reason": str}}"""
        return judge_model.generate(prompt)
```

### 6.3 PII 保护进阶

```
PII 保护策略（2026 最佳实践）:

┌─────────────────────────────────────────┐
│ 第一层: 正则匹配                         │
│  - 邮箱、电话、身份证号、信用卡           │
│  - 速度快，但只能匹配已知格式              │
│                                          │
│ 第二层: NER 模型                         │
│  - 人名、地址、组织名                     │
│  - spaCy / Presidio / GLiNER            │
│  - 多语言支持是关键                       │
│                                          │
│ 第三层: 上下文感知脱敏                    │
│  - "张三在北京工作" → 检测到人名+地点      │
│  - 需要理解语义才能判断是否为 PII          │
│  - 用 LLM 辅助判断边界情况                │
│                                          │
│ 脱敏策略:                                │
│  - 替换: [PERSON], [EMAIL], [PHONE]      │
│  - 假名化: 用假名替换真名，保持一致性      │
│  - 哈希: 不可逆但可追溯                   │
│  - 差分隐私: 添加噪声                     │
└─────────────────────────────────────────┘
```

---

## 7. 2026 前沿趋势

### 7.1 FineWeb 与开放数据集

```
开放预训练数据集（2024-2026 重要里程碑）:

数据集           发布      规模            亮点
─────────────────────────────────────────────────
RedPajama v1    2023.04   1.2T tokens     LLaMA 复现
SlimPajama      2023.06   627B tokens     RedPajama 去重版
Dolma           2024.01   3T tokens       Allen AI，开源工具链
FineWeb         2024.05   15T tokens      HuggingFace，质量飞跃
DCLM            2024.06   4T tokens       Apple+学界联合
FineWeb-Edu     2024.06   1.3T tokens     教育质量过滤
FineWeb 2       2025.02   ~56T tokens     扩展到多语言
Nemotron-CC     2025.03   6.3T tokens     NVIDIA，classifier-based

趋势:
1. 数据规模已不是瓶颈，质量过滤成为核心竞争力
2. Model-based 质量评分成为标配（FineWeb-Edu 开创）
3. 合成数据与真实数据的混合训练成为主流
4. 数据溯源（provenance）和可审计性要求日益严格
```

### 7.2 合成数据 Scaling Laws

```
合成数据的 Scaling 规律（2026 研究前沿）:

1. 合成数据比例的黄金区间:
   - 预训练: 真实:合成 ≈ 4:1（合成占 ~20%）
   - SFT: 真实:合成 ≈ 1:3（合成可占 ~75%）
   - DPO: 合成可达 100%（但需 strong teacher）

2. 模型坍塌（Model Collapse）:
   - Shumailov et al. (2024): 纯合成数据迭代训练会导致模型坍塌
   - 解决方案: 始终保留真实数据作为锚点
   - 安全比例: 合成数据不超过总训练数据的 50-60%

3. 合成数据的 Scaling Law:
   - 合成数据质量 > 数量
   - Teacher 模型越强，合成数据效用越高
   - 多样性比数量更重要（diversity > volume）
   
4. Self-Improvement 边界:
   - 弱模型很难通过自己的合成数据超越自己
   - 需要 verification signal（数学证明、代码执行、工具反馈）
   - "合成数据飞轮" 需要外部 ground truth 锚点
```

### 7.3 多模态数据工程

```
多模态数据管线（2026 趋势）:

文本-图像对:
  - LAION-5B → 清洗后 LAION-Aesthetics
  - DataComp: 标准化的数据过滤 benchmark
  - 图像-文本对齐分数（CLIP Score > 0.28 为高质量）

文本-视频对:
  - WebVid-10M, Panda-70M
  - 时序对齐是核心挑战
  - 2026: 合成 caption 质量已接近人工标注

音频-文本对:
  - LibriSpeech → GigaSpeech → WenetSpeech
  - ASR 数据 + TTS 合成数据的闭环

挑战:
  1. 跨模态对齐质量难以自动化评估
  2. 多模态数据的版权问题更复杂
  3. 存储和处理成本比纯文本高 10-100x
```

---

## 8. 面试题库（12 题）

### Q1: 请描述一个完整的 LLM 预训练数据管线，从原始数据到训练就绪数据需要哪些步骤？

**答案要点：**

```
完整管线:
1. 数据采集: Common Crawl/网页爬取 → 原始 HTML
2. 格式提取: HTML → 纯文本（trafilatura/resiliparse）
3. 语言检测: fasttext lid.176.bin 分语种
4. 启发式清洗: 
   - 行级: 去短行、去导航栏、去广告模板
   - 文档级: 最小长度、特殊字符比例、重复行比例
5. 质量过滤:
   - Perplexity 过滤（KenLM）
   - 分类器过滤（fasttext/BERT，FineWeb-Edu 模式）
6. 去重:
   - 精确去重: URL + 文档哈希
   - 模糊去重: MinHash LSH（Jaccard > 0.8 判重）
   - 子串去重: Suffix Array（学术级精度）
7. PII 脱敏: 正则 + NER + 上下文判断
8. 有毒内容过滤: 关键词 + 分类器 + LLM Judge
9. 数据配比: 按领域/语言/质量分层混合
10. Tokenization: BPE/SentencePiece → token IDs
11. 数据打包: 拼接到固定长度 + 打乱

关键经验:
- 每步都有 recall vs precision 的权衡
- 质量过滤影响最大（FineWeb-Edu 证明）
- 去重对防止过拟合至关重要
- 数据配比是黑魔法，需要大量实验
```

### Q2: MinHash LSH 去重的原理是什么？与精确去重和 Suffix Array 去重相比有什么优劣？

**答案要点：**

```
MinHash LSH 原理:
1. 将文档表示为 n-gram（通常 5-gram）集合 S
2. 用 K 个随机哈希函数 h_1...h_K 计算 MinHash 签名:
   sig_k(S) = min({h_k(x) | x ∈ S})
3. MinHash 性质: P(sig_k(A) = sig_k(B)) = J(A,B) = |A∩B|/|A∪B|
4. LSH 分桶: 将 K 个签名分成 b 个 band，每个 band r 行
   - 两文档至少一个 band 完全匹配 → 候选对
   - b 和 r 控制灵敏度曲线: P = 1-(1-s^r)^b

对比:
┌─────────┬──────────┬──────────┬───────────┐
│ 方法     │ 精度     │ 时间复杂度 │ 适用规模   │
├─────────┼──────────┼──────────┼───────────┤
│ 精确去重  │ 完美     │ O(n)     │ ∞         │
│ (哈希)   │ (完全匹配)│          │ (只能精确) │
├─────────┼──────────┼──────────┼───────────┤
│ MinHash  │ 近似     │ O(n·K)   │ TB 级     │
│ LSH     │ (可调)   │ 查询O(1) │ (工业标准) │
├─────────┼──────────┼──────────┼───────────┤
│ Suffix   │ 精确     │ O(n·L)   │ GB-TB     │
│ Array   │ (子串级) │ 构建O(n) │ (内存大)  │
└─────────┴──────────┴──────────┴───────────┘

选择建议:
- URL/哈希精确去重: 第一步，去掉完全重复
- MinHash LSH: 第二步，去掉近似重复（90%+ 相似），工业首选
- Suffix Array: 学术级精度需求，去掉长重复子串
```

### Q3: 什么是 Model-based 质量过滤？FineWeb-Edu 的方法为什么有效？

**答案要点：**

```
Model-based 质量过滤:
- 核心思想: 训练一个分类器来判断文本质量，替代人工规则
- 训练数据: 用人工标注的高/低质量样本，或用 LLM 打分的样本
- 推理: 对所有候选文本打分，按分数过滤

FineWeb-Edu 方法:
1. 用 LLaMA-3-70B-Instruct 对 500K 网页样本评分（0-5 分，教育价值维度）
2. 用这些 LLM 标注训练一个小分类器（类 BERT）
3. 用小分类器对 15T tokens 的 FineWeb 全量数据打分
4. 过滤掉教育分 < 3 的数据 → 得到 1.3T tokens 的 FineWeb-Edu

为什么有效:
- LLM-as-Judge 的质量判断能力远超启发式规则
- 小分类器蒸馏了 LLM 的判断力，推理成本极低
- "教育价值" 是一个很好的质量代理指标
  → 教育性文本通常: 结构清晰、事实准确、逻辑连贯
- 结果: FineWeb-Edu 在多个下游任务上超越 10x 大的 FineWeb

启示:
- 数据质量 >> 数据数量
- LLM 标注 + 小模型蒸馏 = 可扩展的质量过滤
- 选择好的质量代理维度是关键
```

### Q4: 如何设计 SFT 数据的配比？不同任务类型应该占多少比例？

**答案要点：**

```
SFT 数据配比设计（无统一答案，但有原则和经验值）:

任务类型配比参考（通用 Chat 模型）:
┌───────────────────┬────────┬──────────────────────┐
│ 任务类型           │ 占比   │ 说明                  │
├───────────────────┼────────┼──────────────────────┤
│ 通用对话/指令跟随   │ 30-40% │ 基础能力，不可偏科     │
│ 知识问答           │ 15-20% │ 事实准确性             │
│ 代码生成/理解       │ 15-20% │ 高价值，提升推理能力    │
│ 数学/推理          │ 10-15% │ 逻辑能力基础           │
│ 创意写作           │ 5-10%  │ 流畅度和多样性         │
│ 安全/拒绝          │ 5-10%  │ 不能太少也不能太多     │
│ 多轮对话           │ 5-10%  │ 上下文理解能力         │
│ 长文本处理         │ 3-5%   │ 长上下文利用率         │
└───────────────────┴────────┴──────────────────────┘

配比原则:
1. 能力均衡: 任何单一任务不超过 40%，防止过拟合
2. 数据质量 > 数据量: 高质量 10K 条 > 低质量 100K 条
3. 难度梯度: 简单:中等:困难 ≈ 3:5:2
4. 避免灾难性遗忘: 混入少量预训练分布数据（replay）
5. 迭代调优: 根据 eval 结果动态调整配比

调配方法:
- DoReMi (Google): 用小模型自动搜索最优配比
- 手动实验: 固定其他变量，网格搜索单一维度
- 在线调整: 训练过程中根据 loss 分布动态加权
```

### Q5: 合成数据有哪些主要方法？各自的优缺点是什么？

**答案要点：**

```
主要合成数据方法:

1. Self-Instruct (Wang et al., 2023):
   + 简单，只需种子指令 + 模型自身
   + 成本低，可大规模生成
   - 多样性受种子集限制
   - 质量受模型自身能力限制
   - 容易产生同质化输出

2. Evol-Instruct / WizardLM (Xu et al., 2024):
   + 系统化难度升级（深化/拓宽/约束/推理）
   + 多样性显著优于 Self-Instruct
   + 可控的难度梯度
   - 进化方向设计需要经验
   - 多轮进化后质量可能下降

3. Magpie (Xu et al., 2024):
   + 利用模型的对齐特性，无需种子指令
   + 生成的指令更自然（模拟真实用户）
   + 只需调用模型一次（效率高）
   - 依赖模型的 system prompt 设计
   - 可能继承模型的偏见

4. 蒸馏 (Knowledge Distillation):
   + 质量高（继承 teacher 模型能力）
   + 可针对特定能力定向提升
   - 成本取决于 teacher API 价格
   - 受 teacher 模型 ToS 限制
   - 可能学到 teacher 的错误

5. 数学/代码合成（带验证）:
   + 可以自动验证正确性（执行/证明）
   + 质量保证最强
   + 支持 rejection sampling 放大数据
   - 仅适用于可验证领域
   - 问题生成仍有多样性挑战

选择原则:
- 有 verification signal → 带验证的合成
- 有 strong teacher → 蒸馏
- 需要大规模多样性 → Evol-Instruct
- 快速启动 → Self-Instruct
- 模拟真实用户 → Magpie
```

### Q6: 什么是 Model Collapse？如何在使用合成数据时避免？

**答案要点：**

```
Model Collapse (Shumailov et al., 2024):

定义: 当模型在自己或同类模型生成的数据上迭代训练时，
      分布尾部信息逐渐丢失，模型输出趋向均值化、单调化

过程:
  Model_0 (真实数据训练) → 生成数据_1
  Model_1 (数据_1 训练) → 生成数据_2
  ...
  Model_N → 输出坍塌为低多样性分布

原因:
1. 近似误差累积: 每代模型都会丢失少量分布信息
2. 尾部截断: 低频模式最先消失
3. 模式强化: 高频模式被过度强化

避免策略:
1. 始终混合真实数据（真实数据占比 ≥ 40-50%）
2. 多源合成（不同 teacher 模型生成，增加多样性）
3. 质量过滤而非盲目扩增
4. 监控多样性指标:
   - Embedding 空间覆盖率
   - N-gram 多样性
   - 输出长度分布
5. 引入外部验证信号（代码执行、数学证明、搜索引擎）
6. 定期重新引入原始分布数据（data replay）

检测方法:
- 逐代监控模型输出的 entropy
- 逐代对比 embedding 分布（KL 散度）
- 逐代评估下游 benchmark（特别关注尾部任务）
```

### Q7: DPO 和 RLHF 在数据需求上有什么区别？各自需要什么样的数据？

**答案要点：**

```
RLHF 数据需求:
1. SFT 数据: ~10-50K 条高质量指令-回答对
2. 比较/排序数据: ~30-100K 条（用于训练 Reward Model）
   - 每条包含同一 prompt 的多个回答 + 人工排序
   - 标注成本高（需要领域专家）
3. PPO prompts: ~30-50K 条 prompt（用于 RL 训练）
   - 不需要标注答案，只需 prompt
   - 多样性要求高

DPO 数据需求:
1. 偏好对数据: ~10-100K 条 {prompt, chosen, rejected}
   - 直接从偏好数据学习，不需要 RM
   - 数据格式更简单
   - 可以复用 RLHF 的比较数据

关键区别:
┌──────────┬────────────────┬─────────────────┐
│ 维度      │ RLHF           │ DPO             │
├──────────┼────────────────┼─────────────────┤
│ 数据复杂度 │ 3 类数据       │ 1 类数据         │
│ 标注成本  │ 高（多阶段）    │ 中（单阶段）      │
│ 数据量    │ 更多（3 阶段）  │ 更少（直接学习）   │
│ 数据质量  │ RM 可容忍噪声   │ 对噪声更敏感      │
│ 扩展性    │ PPO 后可自动化  │ 需持续标注新数据   │
│ 合成友好度│ PPO 阶段高     │ 高（Best-of-N）   │
└──────────┴────────────────┴─────────────────┘

2026 趋势:
- DPO 因简单性成为主流选择
- 在线 DPO (OAIF) 进一步简化: 每步生成新数据
- KTO: 只需要 thumbs up/down，不需要成对比较
- RLHF 在最前沿仍有优势（OpenAI/Anthropic 内部用）
```

### Q8: 如何处理 SFT 数据中的长文本？长上下文 SFT 有哪些挑战和解决方案？

**答案要点：**

```
长文本 SFT 挑战:

1. 数据稀缺: 高质量长文本指令数据极少
   - 大部分 SFT 数据 < 2K tokens
   - >32K tokens 的高质量数据几乎没有

2. 训练效率: 长序列训练计算量 O(n²) 增长
   - 32K seq → 比 4K seq 慢 64 倍（朴素实现）
   - 需要 Ring Attention / Sequence Parallelism

3. 注意力稀释: 长上下文中信息利用率下降
   - "Lost in the Middle" 现象
   - 模型倾向于关注开头和结尾

解决方案:

1. 数据构建:
   - 长文档 QA: 从长文档中构建需要全文理解的问题
   - 多文档合成: 将多个相关短文档拼接为长输入
   - 渐进式训练: 4K → 16K → 64K → 128K+ 逐步增加
   - 合成长上下文数据: 用强模型基于长文档生成指令

2. 训练策略:
   - Packing: 将多个短样本打包到一个长序列中
   - 位置编码外推: RoPE ABF / NTK-aware / YaRN
   - 课程学习: 先短后长的训练顺序
   - 稀疏注意力: 训练时只计算部分注意力

3. 评估:
   - NIAH (Needle in a Haystack): 各位置检索准确率
   - LongBench: 多任务长文本理解
   - ∞-Bench: 极长上下文（100K+）评估
   - RULER: 多粒度检索和推理
```

### Q9: 什么是数据 Contamination（数据污染）？如何检测和预防？

**答案要点：**

```
数据污染定义:
  训练数据中包含了测试/评估数据集的样本，导致 benchmark 分数虚高

污染类型:
1. 直接污染: 测试集样本原样出现在训练集中
2. 间接污染: 测试集被改写/翻译后出现
3. 上游污染: Common Crawl 中包含了 benchmark 网站内容
4. 时间污染: 训练数据收集时，benchmark 已公开发布

检测方法:
1. N-gram 匹配: 
   - 计算训练集与测试集的 N-gram 重叠率
   - 通常用 13-gram（GPT-4 技术报告）
   - 重叠率 > 70% 判为污染

2. 成员推断 (Membership Inference):
   - 给模型测试集样本，看 perplexity 是否异常低
   - Min-K% Prob: 检查最低概率 token 的分布

3. Benchmark Canary:
   - 在 benchmark 中嵌入特殊标记
   - 如果模型输出这些标记 → 训练时见过

预防策略:
1. 时间切割: 训练数据截止日期 < benchmark 发布日期
2. 黑名单: 排除已知 benchmark 网站的 URL
3. 去重检查: 发布前与所有主流 benchmark 做 N-gram 比对
4. 动态 benchmark: LiveCodeBench / LiveBench — 持续更新新题目
5. 私有测试集: 不公开测试集内容
```

### Q10: 请解释 Tokenization 中 BPE 算法的原理。为什么不同模型的 tokenizer 会显著影响性能？

**答案要点：**

```
BPE (Byte Pair Encoding) 原理:

训练过程:
1. 初始化: 将文本拆分为 UTF-8 字节（或字符）
2. 统计: 计算所有相邻 token 对的频率
3. 合并: 将最高频的 token 对合并为新 token
4. 重复: 回到步骤 2，直到达到目标词表大小
5. 结果: 得到固定大小的词表（通常 32K-128K）

示例:
  "lower" → ["l", "o", "w", "e", "r"]
  最高频对 ("e", "r") → 合并为 "er"
  → ["l", "o", "w", "er"]
  最高频对 ("l", "o") → 合并为 "lo"
  → ["lo", "w", "er"]
  ...

Tokenizer 影响性能的原因:

1. 压缩率/效率:
   - 同样的文本，不同 tokenizer 产生的 token 数不同
   - 效率高的 tokenizer → 相同上下文窗口能容纳更多信息
   - 中文: 字级 vs 词级差异巨大
   
2. 罕见词处理:
   - 词表覆盖不足 → 罕见词被拆成多个 subword → 学习困难
   - 多语言: 词表中各语言占比直接影响该语言性能

3. 数值和代码:
   - 数字 tokenization 影响算术能力
   - "123456" → ["123", "456"] vs ["1", "2", "3", "4", "5", "6"]
   - 代码缩进: 空格 tokenization 策略影响代码理解

4. 词表大小权衡:
   - 大词表: 压缩率高，但 embedding 层参数多，小模型负担重
   - 小词表: 参数少，但序列变长，计算量增加
   
常见词表大小:
  GPT-4: ~100K | LLaMA: 32K | Qwen: 152K | DeepSeek: 100K
```

### Q11: 如何构建高质量的多轮对话 SFT 数据？有哪些常见陷阱？

**答案要点：**

```
多轮对话数据构建方法:

1. 基于真实对话日志:
   - 从产品日志中采样真实用户多轮交互
   - 优势: 分布最接近真实使用场景
   - 挑战: PII 清理、质量参差不齐

2. 角色扮演合成:
   - 定义 user persona + 对话目标
   - 用强模型分别扮演 user 和 assistant
   - 优势: 可控、多样性好
   - 挑战: 合成对话可能过于"完美"

3. 种子对话扩展:
   - 从单轮 QA 出发，用 LLM 生成追问和深化
   - 优势: 可复用大量单轮数据
   - 挑战: 追问的自然度

常见陷阱:
1. 上下文遗忘: 
   - 回答不引用前几轮信息 → 模型学不到上下文利用
   - 解决: 确保后续轮次必须依赖前文

2. 角色一致性:
   - assistant 人格/知识水平前后不一致
   - 解决: 明确 system prompt，审核一致性

3. 对话深度不足:
   - 大部分对话只有 2-3 轮 → 模型不会深度对话
   - 解决: 强制 5-10 轮的深度对话占比 ≥ 20%

4. Turn-level 质量参差:
   - 前几轮质量高，后面敷衍
   - 解决: 每轮独立质量评分

5. 过度顺从:
   - 合成数据中 assistant 永远同意 user
   - 解决: 添加 disagreement/correction 场景

数据格式:
  {"messages": [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "第一轮问题"},
    {"role": "assistant", "content": "第一轮回答"},
    {"role": "user", "content": "追问"},
    {"role": "assistant", "content": "深入回答"},
    ...
  ]}
```

### Q12: 2026 年 LLM 数据工程最重要的趋势是什么？数据工程师需要关注哪些新方向？

**答案要点：**

```
2026 年核心趋势:

1. 合成数据成为主流（不可逆趋势）:
   - 预训练: 高质量网页数据接近天花板（~15-60T tokens）
   - Post-training: 合成数据占比从 50% → 75%+
   - 关键: 验证信号（代码执行/数学证明/搜索）是合成质量的锚点
   - 风险: Model Collapse 需要警惕，真实数据不可完全替代

2. Model-based 数据过滤标准化:
   - FineWeb-Edu 范式: LLM 标注 → 小模型蒸馏 → 大规模过滤
   - 从启发式规则 → 学习到的质量判断
   - 多维质量评分（教育性/准确性/安全性/流畅性）

3. 数据溯源与合规:
   - EU AI Act 要求训练数据透明度
   - Data Cards / Data Provenance 成为标配
   - 版权诉讼（NYT vs OpenAI）的判决将重塑行业实践
   - 开放数据集运动（FineWeb/DCLM）的兴起

4. 多模态数据工程:
   - 文本-图像-视频-音频统一数据管线
   - 跨模态对齐质量成为核心挑战
   - 存储和处理成本是纯文本的 10-100x

5. 数据飞轮自动化:
   - 从用户反馈到模型更新: 3-6月 → 1-3天
   - 在线学习与持续训练成为可能
   - 数据质量监控自动化

数据工程师新技能:
- LLM-as-Judge 系统设计
- 合成数据管线（Evol-Instruct/Magpie/蒸馏）
- 大规模去重系统（MinHash/Bloom Filter）
- 数据合规审计（PII/版权/有毒内容）
- 多模态数据对齐
- 数据质量指标设计与监控
```

---

> **参考文献与延伸阅读：**
> - Wang et al., "Self-Instruct" (2023)
> - Xu et al., "WizardLM: Evol-Instruct" (2024)
> - Xu et al., "Magpie: Alignment Data Synthesis" (2024)
> - Penedo et al., "FineWeb" (HuggingFace, 2024)
> - Li et al., "DCLM: DataComp for Language Models" (2024)
> - Shumailov et al., "Model Collapse" (Nature, 2024)
> - Ouyang et al., "InstructGPT" (2022)
> - Rafailov et al., "DPO" (2023)
> - Soldaini et al., "Dolma" (Allen AI, 2024)

---

_本笔记覆盖 LLM 数据工程全链路：预训练管线 → 合成数据 → SFT 构建 → 偏好对齐 → 质量评估 → 合规安全 → 2026 前沿。面试场景下可作为"数据工程"方向的核心参考。_