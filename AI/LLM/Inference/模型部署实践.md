---
tags: [deployment, inference, kubernetes, docker, triton, production]
created: 2026-02-14
status: draft
---

# 模型部署实践

## 概述

[[LLM]] 模型部署是将训练好的大语言模型转化为生产服务的关键环节。现代模型部署需要考虑多种形态：实时 API 服务、边缘设备推理、批量处理任务等。本文深入探讨从容器化到 Kubernetes 集群管理的完整部署实践，涵盖 [[Triton Inference Server]] 的企业级应用和全方位监控策略。

## 部署形态分类

### 1. API 服务部署

**实时推理服务**
- **同步 API**：REST/gRPC 接口，毫秒级响应
- **异步 API**：WebSocket 流式传输，支持长文本生成
- **批量 API**：批处理多个请求，提高吞吐量

**典型架构**
```
Client → Load Balancer → API Gateway → Model Service → GPU Pool
          ↑                              ↓
      Rate Limiting              Model Server (TensorRT/vLLM)
```

### 2. 边缘部署

**移动端推理**
- **模型量化**：INT8/INT4 量化减少内存占用
- **模型剪枝**：移除冗余连接，保持核心能力  
- **知识蒸馏**：大模型教学小模型，保持性能

**IoT 设备部署**
- **ONNX Runtime**：跨平台推理引擎
- **TensorFlow Lite**：移动和嵌入式优化
- **OpenVINO**：Intel 硬件加速

### 3. 批处理部署

**大规模数据处理**
- **Spark 集成**：分布式批量推理
- **Ray Serve**：弹性批处理框架
- **Apache Beam**：流式和批处理统一

## 容器化基础

### Docker 配置

**基础镜像选择**
```dockerfile
# NVIDIA 官方 CUDA 镜像
FROM nvidia/cuda:12.1-devel-ubuntu22.04

# 系统依赖
RUN apt-get update && apt-get install -y \
    python3-pip \
    python3-dev \
    git \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Python 环境
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 模型文件
COPY model/ /app/model/
COPY src/ /app/src/

WORKDIR /app
EXPOSE 8000

CMD ["python", "serve.py"]
```

**多阶段构建优化**
```dockerfile
# 构建阶段
FROM python:3.9-slim as builder
COPY requirements.txt .
RUN pip install --user -r requirements.txt

# 运行阶段
FROM nvidia/cuda:12.1-runtime-ubuntu22.04
COPY --from=builder /root/.local /root/.local
```

### NVIDIA Container Toolkit

**安装配置**
```bash
# 安装 Docker 和 NVIDIA Container Toolkit
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

# 配置 Docker daemon
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# 验证 GPU 访问
docker run --rm --gpus all nvidia/cuda:12.1-base nvidia-smi
```

**资源限制**
```yaml
version: '3.8'
services:
  model-server:
    image: my-llm:latest
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 2
              capabilities: [gpu]
        limits:
          memory: 32G
          cpus: '8'
```

## Kubernetes 部署架构

### 集群设计

**节点规划**
```yaml
apiVersion: v1
kind: Node
metadata:
  name: gpu-node-01
  labels:
    node-type: gpu-worker
    gpu-type: a100-40gb
    zone: us-west-2a
spec:
  capacity:
    nvidia.com/gpu: "8"
    cpu: "64"
    memory: "512Gi"
```

**资源配额**
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: llm-quota
spec:
  hard:
    requests.nvidia.com/gpu: "16"
    requests.cpu: "128"
    requests.memory: "1Ti"
    persistentvolumeclaims: "10"
```

### GPU 调度策略

**Node Selector**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: llm-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: llm-service
  template:
    metadata:
      labels:
        app: llm-service
    spec:
      nodeSelector:
        gpu-type: a100-40gb
      containers:
      - name: model-server
        image: llm-server:latest
        resources:
          requests:
            nvidia.com/gpu: 2
            cpu: 8
            memory: 64Gi
          limits:
            nvidia.com/gpu: 2
            cpu: 16
            memory: 128Gi
```

**亲和性调度**
```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
      - matchExpressions:
        - key: zone
          operator: In
          values: [us-west-2a, us-west-2b]
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: app
            operator: In
            values: [llm-service]
        topologyKey: kubernetes.io/hostname
```

### 多副本管理

**水平扩展**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: llm-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Pods
    pods:
      metric:
        name: gpu_utilization
      target:
        type: AverageValue
        averageValue: "80"
```

**滚动更新**
```yaml
strategy:
  type: RollingUpdate
  rollingUpdate:
    maxUnavailable: 1
    maxSurge: 1
```

## Triton Inference Server 架构

### 核心组件

**Model Repository**
```
model_repository/
├── llama2-7b/
│   ├── config.pbtxt
│   ├── 1/
│   │   └── model.onnx
│   └── 2/
│       └── model.engine
└── ensemble_model/
    ├── config.pbtxt
    └── 1/
```

**模型配置**
```protobuf
name: "llama2-7b"
platform: "tensorrt_plan"
max_batch_size: 8
input [
  {
    name: "input_ids"
    data_type: TYPE_INT32
    dims: [-1]
  }
]
output [
  {
    name: "output"
    data_type: TYPE_INT32
    dims: [-1]
  }
]
dynamic_batching {
  max_queue_delay_microseconds: 5000
  preferred_batch_size: [4, 8]
}
```

### 性能优化

**TensorRT 优化**
```python
import tensorrt as trt

def build_engine(onnx_path, engine_path):
    builder = trt.Builder(logger)
    network = builder.create_network(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
    parser = trt.OnnxParser(network, logger)
    
    config = builder.create_builder_config()
    config.max_workspace_size = 8 * 1024 * 1024 * 1024  # 8GB
    config.set_flag(trt.BuilderFlag.FP16)
    
    # 动态 batch size
    profile = builder.create_optimization_profile()
    profile.set_shape("input_ids", (1, 1), (4, 512), (8, 1024))
    config.add_optimization_profile(profile)
    
    engine = builder.build_engine(network, config)
    with open(engine_path, "wb") as f:
        f.write(engine.serialize())
```

**并发策略**
```yaml
# triton_config.yaml
backend_config:
  python:
    shm-default-byte-size: 1073741824
    shm-growth-byte-size: 134217728
  
instance_group [
  {
    count: 2
    kind: KIND_GPU
    gpus: [0, 1]
  }
]
```

### 服务编排

**Kubernetes 部署**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: triton-server
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: triton
        image: nvcr.io/nvidia/tritonserver:23.10-py3
        command: ["tritonserver"]
        args: 
          - "--model-repository=/models"
          - "--strict-model-config=false"
          - "--log-verbose=1"
        ports:
        - containerPort: 8000  # HTTP
        - containerPort: 8001  # gRPC
        - containerPort: 8002  # Metrics
        resources:
          requests:
            nvidia.com/gpu: 1
        volumeMounts:
        - name: model-storage
          mountPath: /models
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-pvc
```

## 监控指标体系

### 性能指标

**延迟指标**
- **TTFT (Time To First Token)**：首个 token 生成时间
- **TPS (Tokens Per Second)**：平均生成速度
- **E2E Latency**：端到端响应时间
- **P99 Latency**：99% 请求的延迟上限

**吞吐量指标**
- **QPS (Queries Per Second)**：每秒处理请求数
- **Concurrent Users**：并发用户数
- **Batch Size**：平均批处理大小

### 资源监控

**GPU 监控**
```yaml
# Prometheus 配置
- job_name: 'nvidia-gpu'
  static_configs:
  - targets: ['gpu-node-01:9400', 'gpu-node-02:9400']
  scrape_interval: 30s
  metrics_path: /metrics
```

**关键指标**
```promql
# GPU 利用率
nvidia_gpu_utilization_gpu{instance=~".*"}

# 显存使用
nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes

# 温度监控
nvidia_gpu_temperature_celsius

# 功耗监控
nvidia_gpu_power_draw_watts
```

### 业务监控

**Grafana Dashboard**
```json
{
  "dashboard": {
    "title": "LLM Service Monitoring",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{status}}"
          }
        ]
      },
      {
        "title": "Response Time Distribution",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          }
        ]
      }
    ]
  }
}
```

**告警规则**
```yaml
groups:
- name: llm-alerts
  rules:
  - alert: HighLatency
    expr: histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m])) > 2
    for: 5m
    labels:
      severity: warning
    annotations:
      summary: "High latency detected"
      
  - alert: GPUMemoryHigh
    expr: nvidia_gpu_memory_used_bytes / nvidia_gpu_memory_total_bytes > 0.9
    for: 2m
    labels:
      severity: critical
```

## 面试常见问题

### Q1: LLM 模型部署的主要挑战是什么？

**答案要点：**
- **资源需求**：大模型需要大量 GPU 显存，部署成本高
- **延迟优化**：平衡模型质量和响应速度
- **并发处理**：动态批处理和负载均衡策略
- **版本管理**：模型更新的灰度发布和回滚机制

### Q2: 如何选择合适的推理引擎？

**答案要点：**
- **TensorRT**：NVIDIA GPU 优化，性能最佳，支持 FP16/INT8
- **ONNX Runtime**：跨平台支持，CPU/GPU 通用
- **vLLM**：专门针对 LLM 优化，支持 PagedAttention
- **Triton**：统一服务框架，支持多种后端引擎

### Q3: Kubernetes 部署 GPU 应用的关键配置？

**答案要点：**
- **GPU 调度**：nvidia.com/gpu 资源请求和限制
- **节点亲和性**：确保 Pod 调度到正确的 GPU 节点
- **资源隔离**：避免 GPU 资源争抢
- **监控集成**：GPU 使用率和健康状态监控

### Q4: 如何实现 LLM 服务的高可用？

**答案要点：**
- **多副本部署**：跨可用区部署，避免单点故障
- **健康检查**：定期检查服务状态和模型加载
- **滚动更新**：无中断的模型版本升级
- **降级策略**：服务异常时切换到备用模型

### Q5: LLM 推理服务的性能优化策略？

**答案要点：**
- **模型优化**：量化、剪枝、知识蒸馏
- **推理优化**：KV Cache、动态批处理、投机解码
- **硬件优化**：TensorRT 编译、混合精度
- **缓存策略**：结果缓存、预取、智能路由

## 相关概念

- [[LLM 推理优化]]
- [[容器化最佳实践]]  
- [[Kubernetes GPU 调度]]
- [[模型服务化架构]]
- [[生产环境监控]]