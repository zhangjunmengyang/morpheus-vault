---
title: "LLM é‡‡æ ·ç­–ç•¥å…¨æ™¯ï¼šä» Greedy åˆ° MCTS"
brief: "LLM æ–‡æœ¬ç”Ÿæˆçš„å®Œæ•´é‡‡æ ·ç­–ç•¥æŒ‡å—ã€‚è¦†ç›–åŸºç¡€æ–¹æ³•ï¼ˆGreedy/Beam Searchï¼‰ã€æ¦‚ç‡é‡‡æ ·ï¼ˆTemperature/Top-K/Top-P/Min-Pï¼‰ã€é«˜çº§æŠ€æœ¯ï¼ˆTypical Sampling/Mirostatï¼‰ã€é‡å¤æƒ©ç½šæœºåˆ¶å’Œä»»åŠ¡å¯¼å‘é…ç½®ã€‚æ ¸å¿ƒæ´å¯Ÿï¼šæ²¡æœ‰ä¸‡èƒ½é‡‡æ ·ç­–ç•¥ï¼ŒTemperature+Top-P ç»„åˆæ˜¯é€šç”¨èµ·ç‚¹ï¼Œä¸åŒä»»åŠ¡éœ€è¦ä¸åŒå‚æ•°â€”â€”ä»£ç  temp=0.2ã€å¯¹è¯ temp=0.7ã€åˆ›æ„ temp=0.9ã€‚"
tags:
  - ai/llm/inference
  - ai/sampling
  - ai/text-generation
  - type/tutorial
  - interview/hot
created: 2026-02-14
updated: 2026-02-22
status: complete
sources:
  - "Radford et al. Language Models are Unsupervised Multitask Learners (GPT-2). OpenAI 2019"
  - "Holtzman et al. The Curious Case of Neural Text Degeneration (Nucleus/Top-p Sampling). arXiv:1904.09751"
  - "Fan et al. Hierarchical Neural Story Generation (Top-k Sampling). arXiv:1805.04833"
  - "Zhang et al. Planning with Large Language Models for Code Generation (MCTS for LLM). arXiv:2406.07394"
  - "Basu et al. Mirostat: A Neural Text Decoding Algorithm that Directly Controls Perplexity. ICLR 2021"
  - "Meister et al. Typical Decoding for Natural Language Generation. arXiv:2202.00666"
related:
  - "[[AI/LLM/Inference/KV Cache|KV Cache]]"
  - "[[AI/LLM/Architecture/Transformeræ¶æ„æ·±åº¦è§£æ-2026æŠ€æœ¯å…¨æ™¯|Transformer æ¶æ„]]"
  - "[[AI/LLM/Inference/Continuous Batching|Continuous Batching]]"
  - "[[AI/LLM/Inference/æ¨¡å‹éƒ¨ç½²å®è·µ|æ¨¡å‹éƒ¨ç½²å®è·µ]]"
---

> [!info] å¦æœ‰é¢è¯•ç‰ˆ
> Foundations ç²¾ç®€ç‰ˆï¼š[[AI/Foundations/Inference/é‡‡æ ·ç­–ç•¥]]

# é‡‡æ ·ç­–ç•¥

é‡‡æ ·ç­–ç•¥å†³å®šäº†è¯­è¨€æ¨¡å‹å¦‚ä½•ä»è¯æ±‡è¡¨ä¸­é€‰æ‹©ä¸‹ä¸€ä¸ª tokenï¼Œç›´æ¥å½±å“ç”Ÿæˆæ–‡æœ¬çš„è´¨é‡ã€å¤šæ ·æ€§å’Œè¿è´¯æ€§ã€‚æœ¬æ–‡è¯¦ç»†åˆ†æä¸»æµé‡‡æ ·æ–¹æ³•çš„åŸç†ã€å®ç°å’Œåº”ç”¨åœºæ™¯ã€‚

## åŸºç¡€é‡‡æ ·æ–¹æ³•

### Greedy Decoding
é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ tokenï¼Œç¡®å®šæ€§ä½†ç¼ºä¹å¤šæ ·æ€§ï¼š

```python
import torch
import torch.nn.functional as F

def greedy_decode(model, input_ids, max_length=50):
    """è´ªå¿ƒè§£ç å®ç°"""
    model.eval()
    generated = input_ids.clone()
    
    with torch.no_grad():
        for _ in range(max_length):
            # è·å–ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒ
            outputs = model(generated)
            logits = outputs.logits[:, -1, :]  # [batch_size, vocab_size]
            
            # é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ token
            next_token = torch.argmax(logits, dim=-1, keepdim=True)
            
            # æ‹¼æ¥åˆ°ç”Ÿæˆåºåˆ—
            generated = torch.cat([generated, next_token], dim=1)
            
            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆç»“æŸç¬¦
            if next_token.item() == model.config.eos_token_id:
                break
    
    return generated

# ä½¿ç”¨ç¤ºä¾‹
def compare_greedy_vs_sampling():
    """å¯¹æ¯”è´ªå¿ƒè§£ç å’Œéšæœºé‡‡æ ·çš„ç»“æœ"""
    prompt = "The future of artificial intelligence"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    # è´ªå¿ƒè§£ç ï¼ˆé‡å¤è¿è¡Œç»“æœç›¸åŒï¼‰
    for i in range(3):
        greedy_output = greedy_decode(model, input_ids, max_length=20)
        greedy_text = tokenizer.decode(greedy_output[0], skip_special_tokens=True)
        print(f"Greedy {i+1}: {greedy_text}")
    
    # éšæœºé‡‡æ ·ï¼ˆæ¯æ¬¡ç»“æœä¸åŒï¼‰
    for i in range(3):
        sampling_output = model.generate(
            input_ids, 
            max_length=input_ids.shape[1] + 20,
            do_sample=True,
            temperature=1.0,
            pad_token_id=tokenizer.eos_token_id
        )
        sampling_text = tokenizer.decode(sampling_output[0], skip_special_tokens=True)
        print(f"Sampling {i+1}: {sampling_text}")
```

### Beam Search
ç»´æŠ¤å¤šä¸ªå€™é€‰åºåˆ—ï¼Œå¹³è¡¡è´¨é‡å’Œå¤šæ ·æ€§ï¼š

```python
class BeamSearchDecoder:
    def __init__(self, model, tokenizer, beam_size=4, length_penalty=1.0):
        self.model = model
        self.tokenizer = tokenizer
        self.beam_size = beam_size
        self.length_penalty = length_penalty
    
    def beam_search(self, input_ids, max_length=50, early_stopping=True):
        """Beam Search è§£ç å®ç°"""
        batch_size = input_ids.shape[0]
        vocab_size = self.model.config.vocab_size
        
        # åˆå§‹åŒ– beams: [batch_size * beam_size, seq_len]
        beams = input_ids.repeat(self.beam_size, 1)
        beam_scores = torch.zeros(batch_size * self.beam_size, device=input_ids.device)
        beam_lens = torch.full((batch_size * self.beam_size,), input_ids.shape[1], 
                              dtype=torch.long, device=input_ids.device)
        
        # å®Œæˆçš„åºåˆ—
        completed_sequences = []
        completed_scores = []
        
        for step in range(max_length):
            # è·å–å½“å‰ beams çš„æ¦‚ç‡åˆ†å¸ƒ
            with torch.no_grad():
                outputs = self.model(beams)
                logits = outputs.logits[:, -1, :]  # [batch_size * beam_size, vocab_size]
            
            # è®¡ç®—ç´¯ç§¯åˆ†æ•°
            log_probs = F.log_softmax(logits, dim=-1)  # [batch_size * beam_size, vocab_size]
            
            # å¯¹äºç¬¬ä¸€æ­¥ï¼Œåªä»ç¬¬ä¸€ä¸ª beam æ‰©å±•
            if step == 0:
                next_scores = log_probs[0].flatten()  # [vocab_size]
            else:
                # ç´¯ç§¯åˆ†æ•°ï¼šå½“å‰åˆ†æ•° + æ–° token åˆ†æ•°
                next_scores = beam_scores.unsqueeze(1) + log_probs  # [batch_size * beam_size, vocab_size]
                next_scores = next_scores.flatten()  # [batch_size * beam_size * vocab_size]
            
            # é€‰æ‹© top beam_size ä¸ªå€™é€‰
            top_scores, top_indices = torch.topk(next_scores, self.beam_size, sorted=True)
            
            # ç¡®å®šæ–°çš„ beams å’Œ tokens
            beam_indices = top_indices // vocab_size  # æ¥è‡ªå“ªä¸ª beam
            token_indices = top_indices % vocab_size  # é€‰æ‹©çš„ token
            
            # æ›´æ–° beams
            new_beams = beams[beam_indices]
            new_beams = torch.cat([new_beams, token_indices.unsqueeze(1)], dim=1)
            
            # æ£€æŸ¥å®Œæˆçš„åºåˆ—
            eos_mask = (token_indices == self.tokenizer.eos_token_id)
            
            if eos_mask.any() and early_stopping:
                # è®¡ç®—é•¿åº¦æƒ©ç½š
                completed_lens = beam_lens[beam_indices[eos_mask]] + 1
                length_penalties = ((5 + completed_lens) / 6) ** self.length_penalty
                final_scores = top_scores[eos_mask] / length_penalties
                
                completed_sequences.extend(new_beams[eos_mask].tolist())
                completed_scores.extend(final_scores.tolist())
            
            # ä¿ç•™æœªå®Œæˆçš„ beams
            continuing_mask = ~eos_mask
            beams = new_beams[continuing_mask]
            beam_scores = top_scores[continuing_mask]
            beam_lens[continuing_mask] += 1
            
            # å¦‚æœæ‰€æœ‰åºåˆ—éƒ½å®Œæˆäº†
            if beams.shape[0] == 0:
                break
        
        # å¦‚æœè¿˜æœ‰æœªå®Œæˆçš„åºåˆ—ï¼Œä¹Ÿæ·»åŠ åˆ°å®Œæˆåˆ—è¡¨
        if beams.shape[0] > 0:
            remaining_lens = beam_lens[:beams.shape[0]]
            length_penalties = ((5 + remaining_lens) / 6) ** self.length_penalty
            final_scores = beam_scores / length_penalties
            
            completed_sequences.extend(beams.tolist())
            completed_scores.extend(final_scores.tolist())
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›æœ€ä½³ç»“æœ
        if completed_sequences:
            best_idx = max(range(len(completed_scores)), key=lambda i: completed_scores[i])
            return torch.tensor([completed_sequences[best_idx]], device=input_ids.device)
        else:
            return beams[:1]  # è¿”å›ç¬¬ä¸€ä¸ªæœªå®Œæˆçš„åºåˆ—
    
    def diverse_beam_search(self, input_ids, num_groups=2, diversity_penalty=0.5):
        """å¤šæ ·åŒ– Beam Searchï¼Œç”Ÿæˆå¤šæ ·åŒ–çš„ç»“æœ"""
        group_size = self.beam_size // num_groups
        all_groups_sequences = []
        
        for group_id in range(num_groups):
            # ä¸ºæ¯ä¸ªç»„è¿è¡Œç‹¬ç«‹çš„ beam search
            group_beams = self.beam_search(input_ids, max_length=50)
            
            # åº”ç”¨å¤šæ ·æ€§æƒ©ç½šï¼ˆé¿å…ä¸ä¹‹å‰ç»„ç›¸ä¼¼ï¼‰
            if group_id > 0:
                # è®¡ç®—ä¸å·²ç”Ÿæˆåºåˆ—çš„ç›¸ä¼¼æ€§ï¼Œå¹¶æƒ©ç½šç›¸ä¼¼çš„å€™é€‰
                pass  # ç®€åŒ–å®ç°ï¼Œå®é™…ä¸­éœ€è¦è®¡ç®— n-gram é‡å¤ç­‰æŒ‡æ ‡
            
            all_groups_sequences.append(group_beams)
        
        return all_groups_sequences

# ä½¿ç”¨ç¤ºä¾‹
def demonstrate_beam_search():
    """æ¼”ç¤º Beam Search çš„æ•ˆæœ"""
    prompt = "The key to successful machine learning is"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    decoder = BeamSearchDecoder(model, tokenizer, beam_size=4)
    
    # æ ‡å‡† beam search
    result = decoder.beam_search(input_ids, max_length=30)
    text = tokenizer.decode(result[0], skip_special_tokens=True)
    print(f"Beam Search: {text}")
    
    # å¤šæ ·åŒ– beam search
    diverse_results = decoder.diverse_beam_search(input_ids, num_groups=2)
    for i, result in enumerate(diverse_results):
        text = tokenizer.decode(result[0], skip_special_tokens=True)
        print(f"Diverse Group {i+1}: {text}")
```

## æ¸©åº¦å’Œæ¦‚ç‡é‡‡æ ·

### Temperature Scaling

> æ¥æºï¼šæ ‡å‡† softmax temperature æŠ€æœ¯ï¼Œåœ¨ GPT-2ï¼ˆRadford et al., 2019ï¼‰ä¸­å¹¿æ³›é‡‡ç”¨

æ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼š

```python
def temperature_sampling(logits, temperature=1.0):
    """æ¸©åº¦é‡‡æ ·å®ç°"""
    if temperature == 0:
        # temperature=0 ç­‰ä»·äºè´ªå¿ƒè§£ç 
        return torch.argmax(logits, dim=-1)
    
    # åº”ç”¨æ¸©åº¦ç¼©æ”¾
    scaled_logits = logits / temperature
    
    # è®¡ç®—æ¦‚ç‡åˆ†å¸ƒ
    probabilities = F.softmax(scaled_logits, dim=-1)
    
    # ä»åˆ†å¸ƒä¸­é‡‡æ ·
    next_token = torch.multinomial(probabilities, num_samples=1)
    
    return next_token.squeeze(-1)

def analyze_temperature_effects():
    """åˆ†æä¸åŒæ¸©åº¦å€¼çš„æ•ˆæœ"""
    prompt = "Today is a beautiful day because"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    temperatures = [0.1, 0.7, 1.0, 1.5, 2.0]
    
    for temp in temperatures:
        print(f"\n=== Temperature: {temp} ===")
        
        # ç”Ÿæˆå¤šä¸ªæ ·æœ¬è§‚å¯Ÿå¤šæ ·æ€§
        for i in range(3):
            generated = model.generate(
                input_ids,
                max_length=input_ids.shape[1] + 15,
                do_sample=True,
                temperature=temp,
                pad_token_id=tokenizer.eos_token_id
            )
            
            text = tokenizer.decode(generated[0], skip_special_tokens=True)
            continuation = text[len(prompt):].strip()
            print(f"  Sample {i+1}: {continuation}")

# åŠ¨æ€æ¸©åº¦è°ƒæ•´
class DynamicTemperatureDecoder:
    """åŠ¨æ€è°ƒæ•´æ¸©åº¦çš„è§£ç å™¨"""
    
    def __init__(self, model, tokenizer, initial_temp=1.0):
        self.model = model
        self.tokenizer = tokenizer
        self.initial_temp = initial_temp
        
    def adaptive_temperature(self, step, total_steps, confidence_score=None):
        """æ ¹æ®ç”Ÿæˆæ­¥éª¤å’Œç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´æ¸©åº¦"""
        # ç­–ç•¥1ï¼šéšç€ç”Ÿæˆè¿›è¡Œé€æ¸é™ä½æ¸©åº¦ï¼ˆå¢åŠ ç¡®å®šæ€§ï¼‰
        progress = step / total_steps
        base_temp = self.initial_temp * (1 - 0.5 * progress)
        
        # ç­–ç•¥2ï¼šæ ¹æ®æ¨¡å‹ç½®ä¿¡åº¦è°ƒæ•´
        if confidence_score is not None:
            # ç½®ä¿¡åº¦é«˜æ—¶é™ä½æ¸©åº¦ï¼Œç½®ä¿¡åº¦ä½æ—¶æé«˜æ¸©åº¦
            confidence_factor = 1.0 + (1.0 - confidence_score) * 0.5
            base_temp *= confidence_factor
        
        return max(0.1, min(2.0, base_temp))  # é™åˆ¶åœ¨åˆç†èŒƒå›´å†…
    
    def generate_with_dynamic_temp(self, input_ids, max_length=50):
        """ä½¿ç”¨åŠ¨æ€æ¸©åº¦ç”Ÿæˆæ–‡æœ¬"""
        generated = input_ids.clone()
        
        for step in range(max_length):
            with torch.no_grad():
                outputs = self.model(generated)
                logits = outputs.logits[:, -1, :]
                
                # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆæœ€å¤§æ¦‚ç‡å€¼ï¼‰
                probs = F.softmax(logits, dim=-1)
                confidence = torch.max(probs, dim=-1)[0].item()
                
                # åŠ¨æ€è°ƒæ•´æ¸©åº¦
                current_temp = self.adaptive_temperature(step, max_length, confidence)
                
                # é‡‡æ ·ä¸‹ä¸€ä¸ª token
                next_token = temperature_sampling(logits, current_temp)
                generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)
                
                if next_token.item() == self.tokenizer.eos_token_id:
                    break
        
        return generated
```

### Top-K Sampling

> æ¥æºï¼šFan et al., "Hierarchical Neural Story Generation", arXiv:1805.04833

é™åˆ¶å€™é€‰ token æ•°é‡ï¼š

```python
def top_k_sampling(logits, k=50, temperature=1.0):
    """Top-K é‡‡æ ·å®ç°"""
    if k <= 0:
        # k=0 è¡¨ç¤ºä¸é™åˆ¶ï¼Œä½¿ç”¨æ‰€æœ‰ tokens
        return temperature_sampling(logits, temperature)
    
    # æ‰¾åˆ° top-k ä¸ªæœ€é«˜æ¦‚ç‡çš„ tokens
    top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)
    
    # å¯¹ top-k logits åº”ç”¨æ¸©åº¦å¹¶é‡‡æ ·
    if temperature != 1.0:
        top_k_logits = top_k_logits / temperature
    
    # è®¡ç®— top-k çš„æ¦‚ç‡åˆ†å¸ƒ
    top_k_probs = F.softmax(top_k_logits, dim=-1)
    
    # ä» top-k ä¸­é‡‡æ ·
    sampled_index = torch.multinomial(top_k_probs, num_samples=1)
    
    # æ˜ å°„å›åŸå§‹ token indices
    next_token = top_k_indices.gather(-1, sampled_index)
    
    return next_token.squeeze(-1)

def compare_k_values():
    """å¯¹æ¯”ä¸åŒ k å€¼çš„æ•ˆæœ"""
    prompt = "The most important aspect of deep learning is"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    k_values = [1, 5, 20, 50, 0]  # 0 è¡¨ç¤ºæ— é™åˆ¶
    
    for k in k_values:
        print(f"\n=== Top-K = {k if k > 0 else 'Unlimited'} ===")
        
        for i in range(3):
            generated = model.generate(
                input_ids,
                max_length=input_ids.shape[1] + 20,
                do_sample=True,
                top_k=k if k > 0 else None,
                temperature=0.8,
                pad_token_id=tokenizer.eos_token_id
            )
            
            text = tokenizer.decode(generated[0], skip_special_tokens=True)
            continuation = text[len(prompt):].strip()
            print(f"  Sample {i+1}: {continuation}")
```

### Top-P (Nucleus) Sampling

> æ¥æºï¼šHoltzman et al., "The Curious Case of Neural Text Degeneration", arXiv:1904.09751

åŠ¨æ€è°ƒæ•´å€™é€‰é›†å¤§å°ï¼š

```python
def top_p_sampling(logits, p=0.9, temperature=1.0):
    """Top-P (Nucleus) é‡‡æ ·å®ç°"""
    if p >= 1.0:
        return temperature_sampling(logits, temperature)
    
    # åº”ç”¨æ¸©åº¦
    if temperature != 1.0:
        logits = logits / temperature
    
    # è®¡ç®—æ¦‚ç‡å¹¶æ’åº
    probs = F.softmax(logits, dim=-1)
    sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)
    
    # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
    
    # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ p çš„ä½ç½®
    sorted_indices_to_remove = cumulative_probs > p
    
    # ä¿æŒè‡³å°‘ä¸€ä¸ª tokenï¼ˆå³ä½¿ç¬¬ä¸€ä¸ª token çš„æ¦‚ç‡å°±è¶…è¿‡äº† pï¼‰
    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
    sorted_indices_to_remove[..., 0] = 0
    
    # å°†è¦ç§»é™¤çš„ tokens çš„æ¦‚ç‡è®¾ä¸º 0
    sorted_probs[sorted_indices_to_remove] = 0.0
    
    # é‡æ–°å½’ä¸€åŒ–
    sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)
    
    # ä»è¿‡æ»¤åçš„åˆ†å¸ƒä¸­é‡‡æ ·
    sampled_sorted_index = torch.multinomial(sorted_probs, num_samples=1)
    
    # æ˜ å°„å›åŸå§‹ç´¢å¼•
    next_token = sorted_indices.gather(-1, sampled_sorted_index)
    
    return next_token.squeeze(-1)

class AdaptiveTopPSampler:
    """è‡ªé€‚åº” Top-P é‡‡æ ·å™¨"""
    
    def __init__(self, min_p=0.7, max_p=0.95):
        self.min_p = min_p
        self.max_p = max_p
    
    def adaptive_p_value(self, logits, base_p=0.9):
        """æ ¹æ®æ¦‚ç‡åˆ†å¸ƒçš„ç†µåŠ¨æ€è°ƒæ•´ p å€¼"""
        probs = F.softmax(logits, dim=-1)
        
        # è®¡ç®—åˆ†å¸ƒçš„ç†µ
        entropy = -torch.sum(probs * torch.log(probs + 1e-10), dim=-1)
        max_entropy = torch.log(torch.tensor(probs.shape[-1], dtype=torch.float))
        
        # æ ‡å‡†åŒ–ç†µ (0-1 èŒƒå›´)
        normalized_entropy = entropy / max_entropy
        
        # ç†µé«˜æ—¶å¢åŠ  p (æ›´å¤šå¤šæ ·æ€§)ï¼Œç†µä½æ—¶å‡å°‘ p (æ›´ä¿å®ˆ)
        adaptive_p = base_p + (normalized_entropy - 0.5) * 0.2
        adaptive_p = torch.clamp(adaptive_p, self.min_p, self.max_p)
        
        return adaptive_p.item()
    
    def sample(self, logits, base_p=0.9, temperature=1.0):
        """ä½¿ç”¨è‡ªé€‚åº” p å€¼è¿›è¡Œé‡‡æ ·"""
        adaptive_p = self.adaptive_p_value(logits, base_p)
        return top_p_sampling(logits, adaptive_p, temperature)

def demonstrate_top_p_effects():
    """æ¼”ç¤º Top-P é‡‡æ ·çš„æ•ˆæœ"""
    prompt = "In the near future, artificial intelligence will"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    p_values = [0.5, 0.7, 0.9, 0.95, 0.99]
    
    for p in p_values:
        print(f"\n=== Top-P = {p} ===")
        
        for i in range(3):
            generated = model.generate(
                input_ids,
                max_length=input_ids.shape[1] + 25,
                do_sample=True,
                top_p=p,
                temperature=0.8,
                pad_token_id=tokenizer.eos_token_id
            )
            
            text = tokenizer.decode(generated[0], skip_special_tokens=True)
            continuation = text[len(prompt):].strip()
            print(f"  Sample {i+1}: {continuation}")
```

## é«˜çº§é‡‡æ ·æŠ€æœ¯

### Min-P Samplingï¼ˆæ–°æ–¹æ³•ï¼‰
åŸºäºç›¸å¯¹æ¦‚ç‡é˜ˆå€¼çš„é‡‡æ ·ï¼š

```python
def min_p_sampling(logits, min_p=0.05, temperature=1.0):
    """
    Min-P é‡‡æ ·ï¼šä¿ç•™æ¦‚ç‡å¤§äºæœ€å¤§æ¦‚ç‡ * min_p çš„ tokens
    ç›¸æ¯” Top-P æ›´ç¨³å®šï¼Œä¸ä¼šå› ä¸ºåˆ†å¸ƒå¹³å¦è€ŒåŒ…å«è¿‡å¤šä½è´¨é‡ tokens
    """
    if temperature != 1.0:
        logits = logits / temperature
    
    # è®¡ç®—æ¦‚ç‡
    probs = F.softmax(logits, dim=-1)
    
    # æ‰¾åˆ°æœ€å¤§æ¦‚ç‡
    max_prob = torch.max(probs, dim=-1, keepdim=True)[0]
    
    # è®¡ç®—é˜ˆå€¼
    threshold = max_prob * min_p
    
    # è¿‡æ»¤ä½äºé˜ˆå€¼çš„ tokens
    filtered_probs = torch.where(probs >= threshold, probs, 0.0)
    
    # é‡æ–°å½’ä¸€åŒ–
    filtered_probs = filtered_probs / filtered_probs.sum(dim=-1, keepdim=True)
    
    # é‡‡æ ·
    next_token = torch.multinomial(filtered_probs, num_samples=1)
    
    return next_token.squeeze(-1)

def compare_p_methods():
    """å¯¹æ¯” Top-P å’Œ Min-P çš„å·®å¼‚"""
    prompt = "The breakthrough in quantum computing"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    print("=== Top-P vs Min-P å¯¹æ¯” ===")
    
    # Top-P é‡‡æ ·
    print("\nTop-P (p=0.9):")
    for i in range(3):
        generated = model.generate(
            input_ids,
            max_length=input_ids.shape[1] + 20,
            do_sample=True,
            top_p=0.9,
            temperature=0.8,
            pad_token_id=tokenizer.eos_token_id
        )
        text = tokenizer.decode(generated[0], skip_special_tokens=True)
        print(f"  Sample {i+1}: {text[len(prompt):].strip()}")
    
    # Min-P é‡‡æ ·ï¼ˆéœ€è¦è‡ªå®šä¹‰å®ç°ï¼‰
    print("\nMin-P (min_p=0.05):")
    # æ³¨æ„ï¼šè¿™éœ€è¦è‡ªå®šä¹‰è§£ç å¾ªç¯ï¼Œå› ä¸º transformers åº“å¯èƒ½ä¸æ”¯æŒ min-p
    for i in range(3):
        generated = input_ids.clone()
        for _ in range(20):
            with torch.no_grad():
                outputs = model(generated)
                logits = outputs.logits[:, -1, :]
                next_token = min_p_sampling(logits, min_p=0.05, temperature=0.8)
                generated = torch.cat([generated, next_token.unsqueeze(0)], dim=1)
                if next_token.item() == tokenizer.eos_token_id:
                    break
        
        text = tokenizer.decode(generated[0], skip_special_tokens=True)
        print(f"  Sample {i+1}: {text[len(prompt):].strip()}")
```

### Typical Sampling

> æ¥æºï¼šMeister et al., "Typical Decoding for Natural Language Generation", arXiv:2202.00666

åŸºäºæ¡ä»¶ç†µçš„é‡‡æ ·ï¼š

```python
def typical_sampling(logits, tau=0.95, temperature=1.0):
    """
    Typical Samplingï¼šé€‰æ‹©ä¿¡æ¯é‡æ¥è¿‘æœŸæœ›çš„ tokens
    åŸºäºè®ºæ–‡ "Typical Sampling for Natural Language Generation"
    """
    if temperature != 1.0:
        logits = logits / temperature
    
    # è®¡ç®—æ¦‚ç‡
    probs = F.softmax(logits, dim=-1)
    
    # è®¡ç®—æ¯ä¸ª token çš„ä¿¡æ¯é‡ (-log p)
    info = -torch.log(probs + 1e-10)
    
    # è®¡ç®—æœŸæœ›ä¿¡æ¯é‡ï¼ˆåˆ†å¸ƒçš„ç†µï¼‰
    expected_info = torch.sum(probs * info, dim=-1, keepdim=True)
    
    # è®¡ç®—æ¯ä¸ª token ä¿¡æ¯é‡ä¸æœŸæœ›çš„å·®å¼‚
    info_diff = torch.abs(info - expected_info)
    
    # ä¿ç•™ä¿¡æ¯é‡æ¥è¿‘æœŸæœ›çš„ tokensï¼ˆå·®å¼‚å°äºé˜ˆå€¼ï¼‰
    percentile_threshold = torch.quantile(info_diff, tau, dim=-1, keepdim=True)
    typical_mask = info_diff <= percentile_threshold
    
    # è¿‡æ»¤æ¦‚ç‡
    filtered_probs = torch.where(typical_mask, probs, 0.0)
    filtered_probs = filtered_probs / filtered_probs.sum(dim=-1, keepdim=True)
    
    # é‡‡æ ·
    next_token = torch.multinomial(filtered_probs, num_samples=1)
    
    return next_token.squeeze(-1)

def analyze_typical_sampling():
    """åˆ†æ Typical Sampling çš„ç‰¹æ€§"""
    # åˆ›å»ºä¸€ä¸ªå·²çŸ¥åˆ†å¸ƒçš„ä¾‹å­
    vocab_size = 1000
    
    # æ¨¡æ‹Ÿä¸åŒç±»å‹çš„æ¦‚ç‡åˆ†å¸ƒ
    distributions = {
        'uniform': torch.ones(vocab_size) / vocab_size,
        'peaked': F.softmax(torch.randn(vocab_size) * 3, dim=0),  # å°–é”åˆ†å¸ƒ
        'flat': F.softmax(torch.randn(vocab_size) * 0.5, dim=0)   # å¹³å¦åˆ†å¸ƒ
    }
    
    for dist_name, probs in distributions.items():
        logits = torch.log(probs + 1e-10)
        
        print(f"\n=== {dist_name.capitalize()} Distribution ===")
        
        # è®¡ç®—åˆ†å¸ƒçš„ç†µ
        entropy = -torch.sum(probs * torch.log(probs + 1e-10))
        print(f"Entropy: {entropy:.4f}")
        
        # åˆ†æä¸åŒæ–¹æ³•é€‰æ‹©çš„ tokens æ•°é‡
        methods = {
            'Top-50': lambda x: torch.topk(x, 50)[1].shape[0],
            'Top-P 0.9': lambda x: (torch.cumsum(torch.sort(F.softmax(x, dim=0), descending=True)[0], dim=0) <= 0.9).sum().item(),
            'Min-P 0.05': lambda x: (F.softmax(x, dim=0) >= F.softmax(x, dim=0).max() * 0.05).sum().item(),
            'Typical 0.95': lambda x: len(get_typical_tokens(x, 0.95))
        }
        
        for method_name, method_func in methods.items():
            try:
                count = method_func(logits)
                print(f"{method_name}: {count} tokens selected")
            except:
                print(f"{method_name}: calculation failed")

def get_typical_tokens(logits, tau=0.95):
    """è·å– typical sampling é€‰æ‹©çš„ tokensï¼ˆç”¨äºåˆ†æï¼‰"""
    probs = F.softmax(logits, dim=-1)
    info = -torch.log(probs + 1e-10)
    expected_info = torch.sum(probs * info, dim=-1, keepdim=True)
    info_diff = torch.abs(info - expected_info)
    percentile_threshold = torch.quantile(info_diff, tau, dim=-1, keepdim=True)
    return torch.where(info_diff <= percentile_threshold)[0]
```

### Mirostat Sampling

> æ¥æºï¼šBasu et al., "Mirostat: A Neural Text Decoding Algorithm that Directly Controls Perplexity", ICLR 2021

åŠ¨æ€è°ƒæ•´é‡‡æ ·ä»¥ç»´æŒç›®æ ‡å›°æƒ‘åº¦ï¼š

```python
class MirostatSampler:
    """
    Mirostat é‡‡æ ·å™¨ï¼šé€šè¿‡åŠ¨æ€è°ƒæ•´ tau ç»´æŒç›®æ ‡å›°æƒ‘åº¦
    åŸºäºè®ºæ–‡ "Mirostat: A Neural Text Decoding Algorithm that Directly Controls Perplexity"
    """
    
    def __init__(self, target_surprise=5.0, learning_rate=0.1, initial_tau=10.0):
        self.target_surprise = target_surprise  # ç›®æ ‡æƒŠå¼‚åº¦ï¼ˆæ¥è¿‘ç›®æ ‡å›°æƒ‘åº¦ï¼‰
        self.learning_rate = learning_rate
        self.tau = initial_tau  # å½“å‰æˆªæ–­é˜ˆå€¼
        
    def mirostat_v1(self, logits, temperature=1.0):
        """Mirostat v1: åŸºäºæ’åºåçš„æ¦‚ç‡è¿›è¡Œæˆªæ–­"""
        if temperature != 1.0:
            logits = logits / temperature
        
        probs = F.softmax(logits, dim=-1)
        sorted_probs, sorted_indices = torch.sort(probs, descending=True)
        
        # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
        cumsum_probs = torch.cumsum(sorted_probs, dim=-1)
        
        # æ‰¾åˆ°æˆªæ–­ç‚¹ï¼šç´¯ç§¯æ¦‚ç‡è¾¾åˆ° (1 - 1/tau)
        cutoff = 1.0 - 1.0 / self.tau
        cutoff_idx = torch.searchsorted(cumsum_probs, cutoff)
        cutoff_idx = max(1, cutoff_idx.item())  # è‡³å°‘ä¿ç•™ä¸€ä¸ª token
        
        # æˆªæ–­å¹¶é‡æ–°å½’ä¸€åŒ–
        truncated_probs = sorted_probs[:cutoff_idx]
        truncated_probs = truncated_probs / truncated_probs.sum()
        
        # é‡‡æ ·
        sampled_idx = torch.multinomial(truncated_probs, num_samples=1)
        selected_token = sorted_indices[sampled_idx]
        
        # è®¡ç®—é€‰ä¸­ token çš„æƒŠå¼‚åº¦
        selected_prob = sorted_probs[sampled_idx]
        surprise = -torch.log2(selected_prob).item()
        
        # æ›´æ–° tau
        error = surprise - self.target_surprise
        self.tau = self.tau - self.learning_rate * error
        self.tau = max(1.0, self.tau)  # tau ä¸èƒ½å°äº 1
        
        return selected_token, surprise
    
    def mirostat_v2(self, logits, temperature=1.0):
        """Mirostat v2: ç›´æ¥åŸºäºæ¦‚ç‡é˜ˆå€¼"""
        if temperature != 1.0:
            logits = logits / temperature
        
        probs = F.softmax(logits, dim=-1)
        
        # è®¡ç®—é˜ˆå€¼ï¼šæ¦‚ç‡å¿…é¡»å¤§äº 1/tau
        threshold = 1.0 / self.tau
        
        # è¿‡æ»¤ä½æ¦‚ç‡ tokens
        filtered_probs = torch.where(probs >= threshold, probs, 0.0)
        
        # å¦‚æœæ²¡æœ‰ token æ»¡è¶³æ¡ä»¶ï¼Œé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„
        if filtered_probs.sum() == 0:
            filtered_probs = torch.zeros_like(probs)
            max_idx = torch.argmax(probs)
            filtered_probs[max_idx] = 1.0
        else:
            filtered_probs = filtered_probs / filtered_probs.sum()
        
        # é‡‡æ ·
        selected_token = torch.multinomial(filtered_probs, num_samples=1)
        
        # è®¡ç®—æƒŠå¼‚åº¦
        selected_prob = probs[selected_token]
        surprise = -torch.log2(selected_prob).item()
        
        # æ›´æ–° tau
        error = surprise - self.target_surprise
        self.tau = self.tau - self.learning_rate * error
        self.tau = max(1.0, self.tau)
        
        return selected_token, surprise

def demonstrate_mirostat():
    """æ¼”ç¤º Mirostat é‡‡æ ·çš„æ•ˆæœ"""
    prompt = "The evolution of machine learning has led to"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    # åˆ›å»º Mirostat é‡‡æ ·å™¨
    sampler = MirostatSampler(target_surprise=3.0)  # ç›®æ ‡å›°æƒ‘åº¦çº¦ä¸º 8
    
    print("=== Mirostat Sampling ===")
    
    # ç”Ÿæˆæ–‡æœ¬å¹¶ç›‘æ§å›°æƒ‘åº¦
    generated = input_ids.clone()
    surprises = []
    taus = []
    
    for step in range(30):
        with torch.no_grad():
            outputs = model(generated)
            logits = outputs.logits[:, -1, :]
            
            next_token, surprise = sampler.mirostat_v2(logits, temperature=0.8)
            generated = torch.cat([generated, next_token], dim=1)
            
            surprises.append(surprise)
            taus.append(sampler.tau)
            
            if next_token.item() == tokenizer.eos_token_id:
                break
    
    # æ˜¾ç¤ºç»“æœ
    text = tokenizer.decode(generated[0], skip_special_tokens=True)
    print(f"Generated: {text}")
    print(f"Average surprise: {np.mean(surprises):.2f}")
    print(f"Final tau: {sampler.tau:.2f}")
    
    # ç»˜åˆ¶æƒŠå¼‚åº¦å˜åŒ–
    import matplotlib.pyplot as plt
    
    plt.figure(figsize=(12, 4))
    
    plt.subplot(1, 2, 1)
    plt.plot(surprises, label='Actual Surprise')
    plt.axhline(y=sampler.target_surprise, color='r', linestyle='--', label='Target Surprise')
    plt.xlabel('Generation Step')
    plt.ylabel('Surprise')
    plt.legend()
    plt.title('Surprise Control')
    
    plt.subplot(1, 2, 2)
    plt.plot(taus)
    plt.xlabel('Generation Step')
    plt.ylabel('Tau Value')
    plt.title('Tau Adaptation')
    
    plt.tight_layout()
    plt.show()
```

## é‡å¤æƒ©ç½šæœºåˆ¶

### Repetition Penalty
é¿å…ç”Ÿæˆé‡å¤å†…å®¹ï¼š

```python
def apply_repetition_penalty(logits, input_ids, penalty=1.2):
    """
    åº”ç”¨é‡å¤æƒ©ç½šï¼šé™ä½å·²å‡ºç° tokens çš„æ¦‚ç‡
    penalty > 1.0 è¡¨ç¤ºæƒ©ç½šï¼Œ< 1.0 è¡¨ç¤ºé¼“åŠ±
    """
    # è·å–å·²å‡ºç°çš„ tokens
    unique_tokens = torch.unique(input_ids)
    
    # å¯¹å·²å‡ºç°çš„ tokens åº”ç”¨æƒ©ç½š
    for token in unique_tokens:
        if logits[token] > 0:
            logits[token] = logits[token] / penalty
        else:
            logits[token] = logits[token] * penalty
    
    return logits

def apply_frequency_penalty(logits, input_ids, penalty=0.1):
    """
    é¢‘ç‡æƒ©ç½šï¼šæ ¹æ® token å‡ºç°é¢‘ç‡è¿›è¡Œæƒ©ç½š
    """
    # è®¡ç®—æ¯ä¸ª token çš„å‡ºç°æ¬¡æ•°
    token_counts = torch.bincount(input_ids.flatten(), minlength=logits.shape[-1])
    
    # åº”ç”¨é¢‘ç‡æƒ©ç½š
    frequency_penalties = token_counts.float() * penalty
    adjusted_logits = logits - frequency_penalties
    
    return adjusted_logits

def apply_presence_penalty(logits, input_ids, penalty=0.5):
    """
    å­˜åœ¨æƒ©ç½šï¼šå¯¹ä»»ä½•å·²å‡ºç°è¿‡çš„ token åº”ç”¨å›ºå®šæƒ©ç½š
    """
    # åˆ›å»ºå­˜åœ¨æ©ç 
    presence_mask = torch.zeros_like(logits, dtype=torch.bool)
    unique_tokens = torch.unique(input_ids)
    presence_mask[unique_tokens] = True
    
    # åº”ç”¨å­˜åœ¨æƒ©ç½š
    adjusted_logits = logits.clone()
    adjusted_logits[presence_mask] -= penalty
    
    return adjusted_logits

class AdaptiveRepetitionController:
    """è‡ªé€‚åº”é‡å¤æ§åˆ¶å™¨"""
    
    def __init__(self, base_penalty=1.2, max_penalty=2.0, window_size=50):
        self.base_penalty = base_penalty
        self.max_penalty = max_penalty
        self.window_size = window_size
    
    def calculate_repetition_score(self, input_ids):
        """è®¡ç®—åºåˆ—çš„é‡å¤ç¨‹åº¦"""
        sequence = input_ids.flatten()
        
        if len(sequence) < 4:
            return 0.0
        
        # è®¡ç®— n-gram é‡å¤
        repetition_scores = []
        
        for n in range(2, min(6, len(sequence)//2 + 1)):  # 2-gram åˆ° 5-gram
            ngrams = []
            for i in range(len(sequence) - n + 1):
                ngrams.append(tuple(sequence[i:i+n].tolist()))
            
            # è®¡ç®—é‡å¤ç‡
            total_ngrams = len(ngrams)
            unique_ngrams = len(set(ngrams))
            repetition_rate = 1.0 - (unique_ngrams / total_ngrams) if total_ngrams > 0 else 0.0
            repetition_scores.append(repetition_rate)
        
        return np.mean(repetition_scores) if repetition_scores else 0.0
    
    def adaptive_penalty(self, input_ids):
        """æ ¹æ®å½“å‰é‡å¤ç¨‹åº¦è‡ªé€‚åº”è°ƒæ•´æƒ©ç½šå¼ºåº¦"""
        # åªè€ƒè™‘æœ€è¿‘çš„ä¸€å®šé•¿åº¦å†…å®¹
        recent_ids = input_ids[:, -self.window_size:] if input_ids.shape[1] > self.window_size else input_ids
        
        repetition_score = self.calculate_repetition_score(recent_ids)
        
        # æ ¹æ®é‡å¤ç¨‹åº¦è°ƒæ•´æƒ©ç½š
        adaptive_strength = self.base_penalty + (self.max_penalty - self.base_penalty) * repetition_score
        
        return min(adaptive_strength, self.max_penalty)

def demonstrate_repetition_control():
    """æ¼”ç¤ºé‡å¤æ§åˆ¶çš„æ•ˆæœ"""
    prompt = "The importance of education cannot be overstated. Education is"
    input_ids = tokenizer.encode(prompt, return_tensors='pt')
    
    # ä¸åŒé‡å¤æ§åˆ¶ç­–ç•¥çš„å¯¹æ¯”
    strategies = [
        ("No Control", {}),
        ("Repetition Penalty", {"repetition_penalty": 1.2}),
        ("Frequency + Presence", {"frequency_penalty": 0.1, "presence_penalty": 0.5}),
    ]
    
    for strategy_name, params in strategies:
        print(f"\n=== {strategy_name} ===")
        
        generated = model.generate(
            input_ids,
            max_length=input_ids.shape[1] + 30,
            do_sample=True,
            temperature=0.8,
            top_p=0.9,
            pad_token_id=tokenizer.eos_token_id,
            **params
        )
        
        text = tokenizer.decode(generated[0], skip_special_tokens=True)
        print(f"Result: {text}")
        
        # åˆ†æé‡å¤ç¨‹åº¦
        controller = AdaptiveRepetitionController()
        repetition_score = controller.calculate_repetition_score(generated)
        print(f"Repetition Score: {repetition_score:.3f}")
```

## ä¸åŒä»»åŠ¡çš„é‡‡æ ·å‚æ•°æ¨è

### ä»»åŠ¡å¯¼å‘çš„é‡‡æ ·é…ç½®

```python
class TaskSpecificSampler:
    """é’ˆå¯¹ä¸åŒä»»åŠ¡çš„é‡‡æ ·é…ç½®"""
    
    @staticmethod
    def get_config(task_type, quality_priority="balanced"):
        """
        æ ¹æ®ä»»åŠ¡ç±»å‹å’Œè´¨é‡ä¼˜å…ˆçº§è¿”å›é‡‡æ ·é…ç½®
        
        Args:
            task_type: 'creative', 'factual', 'code', 'chat', 'translation'
            quality_priority: 'speed', 'quality', 'creativity', 'balanced'
        """
        
        base_configs = {
            'creative': {
                'do_sample': True,
                'temperature': 0.8,
                'top_p': 0.9,
                'top_k': None,
                'repetition_penalty': 1.1,
                'length_penalty': 1.0,
                'typical_p': 0.95
            },
            'factual': {
                'do_sample': True,
                'temperature': 0.3,
                'top_p': 0.85,
                'top_k': 40,
                'repetition_penalty': 1.15,
                'length_penalty': 1.1,
                'num_beams': 2
            },
            'code': {
                'do_sample': True,
                'temperature': 0.2,
                'top_p': 0.75,
                'top_k': 20,
                'repetition_penalty': 1.05,
                'length_penalty': 1.0,
                'typical_p': 0.9
            },
            'chat': {
                'do_sample': True,
                'temperature': 0.7,
                'top_p': 0.9,
                'top_k': None,
                'repetition_penalty': 1.1,
                'frequency_penalty': 0.05,
                'presence_penalty': 0.1
            },
            'translation': {
                'do_sample': False,
                'num_beams': 4,
                'length_penalty': 1.2,
                'early_stopping': True,
                'repetition_penalty': 1.0
            }
        }
        
        config = base_configs.get(task_type, base_configs['factual']).copy()
        
        # æ ¹æ®è´¨é‡ä¼˜å…ˆçº§è°ƒæ•´
        if quality_priority == "speed":
            config.update({
                'do_sample': False,
                'num_beams': 1,
                'temperature': None,
                'top_p': None
            })
        elif quality_priority == "quality":
            config.update({
                'num_beams': max(config.get('num_beams', 1), 4),
                'temperature': config.get('temperature', 0.7) * 0.8,
                'length_penalty': 1.2
            })
        elif quality_priority == "creativity":
            config.update({
                'temperature': min(config.get('temperature', 0.7) * 1.3, 2.0),
                'top_p': min(config.get('top_p', 0.9) + 0.05, 0.99),
                'typical_p': 0.98
            })
        
        return config
    
    @staticmethod
    def generate_with_task_config(model, tokenizer, prompt, task_type, 
                                max_length=100, quality_priority="balanced"):
        """ä½¿ç”¨ä»»åŠ¡ç‰¹å®šé…ç½®ç”Ÿæˆæ–‡æœ¬"""
        input_ids = tokenizer.encode(prompt, return_tensors='pt')
        config = TaskSpecificSampler.get_config(task_type, quality_priority)
        
        # æ·»åŠ é€šç”¨å‚æ•°
        config.update({
            'max_length': input_ids.shape[1] + max_length,
            'pad_token_id': tokenizer.eos_token_id,
            'eos_token_id': tokenizer.eos_token_id
        })
        
        # ç”Ÿæˆæ–‡æœ¬
        with torch.no_grad():
            output = model.generate(input_ids, **config)
        
        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
        return generated_text[len(prompt):].strip()

# å®é™…åº”ç”¨ç¤ºä¾‹
def demonstrate_task_specific_sampling():
    """æ¼”ç¤ºé’ˆå¯¹ä¸åŒä»»åŠ¡çš„é‡‡æ ·æ•ˆæœ"""
    
    tasks_and_prompts = {
        'creative': "Once upon a time in a magical forest,",
        'factual': "The capital of France is",
        'code': "def fibonacci(n):",
        'chat': "How are you feeling today?",
        'translation': "Translate to French: Hello, how are you?"
    }
    
    sampler = TaskSpecificSampler()
    
    for task_type, prompt in tasks_and_prompts.items():
        print(f"\n=== {task_type.upper()} TASK ===")
        print(f"Prompt: {prompt}")
        
        for priority in ['speed', 'balanced', 'quality']:
            result = sampler.generate_with_task_config(
                model, tokenizer, prompt, task_type, 
                max_length=50, quality_priority=priority
            )
            print(f"{priority.capitalize()}: {result}")

# å‚æ•°ä¼˜åŒ–å·¥å…·
class SamplingOptimizer:
    """é‡‡æ ·å‚æ•°ä¼˜åŒ–å™¨"""
    
    def __init__(self, model, tokenizer, evaluation_prompts):
        self.model = model
        self.tokenizer = tokenizer
        self.evaluation_prompts = evaluation_prompts
    
    def evaluate_config(self, config, metric='diversity'):
        """è¯„ä¼°é‡‡æ ·é…ç½®çš„æ•ˆæœ"""
        results = []
        
        for prompt in self.evaluation_prompts:
            input_ids = self.tokenizer.encode(prompt, return_tensors='pt')
            
            # ç”Ÿæˆå¤šä¸ªæ ·æœ¬
            samples = []
            for _ in range(5):
                with torch.no_grad():
                    output = self.model.generate(input_ids, **config)
                text = self.tokenizer.decode(output[0], skip_special_tokens=True)
                continuation = text[len(prompt):].strip()
                samples.append(continuation)
            
            # è®¡ç®—æŒ‡æ ‡
            if metric == 'diversity':
                # è®¡ç®—æ ·æœ¬é—´çš„å¤šæ ·æ€§
                diversity_score = self._calculate_diversity(samples)
                results.append(diversity_score)
            elif metric == 'coherence':
                # è¯„ä¼°è¿è´¯æ€§ï¼ˆç®€åŒ–å®ç°ï¼‰
                avg_length = np.mean([len(s.split()) for s in samples])
                coherence_score = min(avg_length / 20, 1.0)  # å‡è®¾åˆç†é•¿åº¦çº¦20è¯
                results.append(coherence_score)
        
        return np.mean(results)
    
    def _calculate_diversity(self, samples):
        """è®¡ç®—æ ·æœ¬å¤šæ ·æ€§"""
        if len(samples) < 2:
            return 0.0
        
        # ä½¿ç”¨ Jaccard ç›¸ä¼¼åº¦è®¡ç®—å¤šæ ·æ€§
        diversities = []
        for i in range(len(samples)):
            for j in range(i+1, len(samples)):
                words_i = set(samples[i].lower().split())
                words_j = set(samples[j].lower().split())
                
                if len(words_i | words_j) == 0:
                    similarity = 1.0
                else:
                    similarity = len(words_i & words_j) / len(words_i | words_j)
                
                diversity = 1.0 - similarity
                diversities.append(diversity)
        
        return np.mean(diversities)
    
    def grid_search(self, param_ranges):
        """ç½‘æ ¼æœç´¢æœ€ä¼˜å‚æ•°"""
        from itertools import product
        
        best_score = -1
        best_config = None
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        param_names = list(param_ranges.keys())
        param_values = [param_ranges[name] for name in param_names]
        
        for values in product(*param_values):
            config = dict(zip(param_names, values))
            config.update({
                'max_length': 80,
                'do_sample': True,
                'pad_token_id': self.tokenizer.eos_token_id
            })
            
            try:
                score = self.evaluate_config(config)
                print(f"Config: {config} -> Score: {score:.3f}")
                
                if score > best_score:
                    best_score = score
                    best_config = config.copy()
            
            except Exception as e:
                print(f"Failed config {config}: {e}")
        
        return best_config, best_score

# ä½¿ç”¨ä¼˜åŒ–å™¨
def optimize_sampling_params():
    """ä¼˜åŒ–é‡‡æ ·å‚æ•°"""
    prompts = [
        "The future of technology is",
        "In a world where AI dominates,",
        "The most important lesson I learned is"
    ]
    
    optimizer = SamplingOptimizer(model, tokenizer, prompts)
    
    # å®šä¹‰æœç´¢ç©ºé—´
    param_ranges = {
        'temperature': [0.5, 0.7, 0.9, 1.1],
        'top_p': [0.8, 0.9, 0.95],
        'top_k': [20, 40, None],
        'repetition_penalty': [1.0, 1.1, 1.2]
    }
    
    best_config, best_score = optimizer.grid_search(param_ranges)
    
    print(f"\næœ€ä¼˜é…ç½®: {best_config}")
    print(f"æœ€ä¼˜åˆ†æ•°: {best_score:.3f}")
```

## é¢è¯•å¸¸è§é—®é¢˜

### Q1: Temperature å‚æ•°å¦‚ä½•å½±å“ç”Ÿæˆè´¨é‡ï¼Ÿä»€ä¹ˆæ—¶å€™ä½¿ç”¨ä¸åŒçš„æ¸©åº¦å€¼ï¼Ÿ

**ç­”æ¡ˆï¼š**
1. **ä½œç”¨æœºåˆ¶**ï¼šTemperature é€šè¿‡é™¤æ³•æ“ä½œç¼©æ”¾ logitsï¼Œæ”¹å˜æ¦‚ç‡åˆ†å¸ƒçš„å°–é”ç¨‹åº¦
2. **å½±å“æ•ˆæœ**ï¼š
   - ä½æ¸©åº¦ï¼ˆ0.1-0.5ï¼‰ï¼šè¾“å‡ºç¡®å®šæ€§å¼ºï¼Œé‡å¤æ€§é«˜ï¼Œé€‚åˆäº‹å®æ€§ä»»åŠ¡
   - ä¸­æ¸©åº¦ï¼ˆ0.6-0.9ï¼‰ï¼šå¹³è¡¡åˆ›é€ æ€§å’Œè¿è´¯æ€§ï¼Œé€‚åˆä¸€èˆ¬å¯¹è¯å’Œå†™ä½œ
   - é«˜æ¸©åº¦ï¼ˆ1.0-2.0ï¼‰ï¼šè¾“å‡ºéšæœºæ€§å¼ºï¼Œåˆ›é€ æ€§é«˜ï¼Œé€‚åˆåˆ›æ„å†™ä½œ
3. **åŠ¨æ€è°ƒæ•´**ï¼šå¯æ ¹æ®ç”Ÿæˆè¿›åº¦ã€æ¨¡å‹ç½®ä¿¡åº¦åŠ¨æ€è°ƒæ•´æ¸©åº¦å€¼

### Q2: Top-P å’Œ Top-K é‡‡æ ·å„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿå¦‚ä½•é€‰æ‹©ï¼Ÿ

**ç­”æ¡ˆï¼š**
1. **Top-K é‡‡æ ·**ï¼š
   - ä¼˜ç‚¹ï¼šå›ºå®šå€™é€‰æ•°é‡ï¼Œè®¡ç®—ç®€å•ï¼Œé¿å…æä½è´¨é‡è¯æ±‡
   - ç¼ºç‚¹ï¼šæ— æ³•é€‚åº”æ¦‚ç‡åˆ†å¸ƒçš„å·®å¼‚ï¼Œå¯èƒ½åŒ…å«ä¸åˆé€‚çš„è¯æ±‡
2. **Top-P é‡‡æ ·**ï¼š
   - ä¼˜ç‚¹ï¼šåŠ¨æ€è°ƒæ•´å€™é€‰é›†å¤§å°ï¼Œé€‚åº”ä¸åŒçš„æ¦‚ç‡åˆ†å¸ƒ
   - ç¼ºç‚¹ï¼šåœ¨æ¦‚ç‡åˆ†å¸ƒå¹³å¦æ—¶å¯èƒ½åŒ…å«è¿‡å¤šä½è´¨é‡å€™é€‰
3. **é€‰æ‹©å»ºè®®**ï¼šé€šå¸¸ Top-P æ›´é€šç”¨ï¼Œä½†å¯ç»“åˆä½¿ç”¨ï¼ˆå…ˆ Top-K è¿‡æ»¤ï¼Œå† Top-P é€‰æ‹©ï¼‰

### Q3: ä¸ºä»€ä¹ˆéœ€è¦é‡å¤æƒ©ç½šï¼Ÿæœ‰å“ªäº›ä¸åŒçš„æƒ©ç½šç­–ç•¥ï¼Ÿ

**ç­”æ¡ˆï¼š**
1. **å¿…è¦æ€§**ï¼š
   - è¯­è¨€æ¨¡å‹å®¹æ˜“é™·å…¥é‡å¤å¾ªç¯
   - æé«˜ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§å’Œå¯è¯»æ€§
2. **æƒ©ç½šç­–ç•¥**ï¼š
   - **Repetition Penalty**ï¼šå¯¹å·²å‡ºç° token æŒ‰æ¯”ä¾‹æƒ©ç½š
   - **Frequency Penalty**ï¼šæ ¹æ®å‡ºç°é¢‘ç‡çº¿æ€§æƒ©ç½š
   - **Presence Penalty**ï¼šå¯¹ä»»ä½•å·²å‡ºç° token å›ºå®šæƒ©ç½š
3. **é€‰æ‹©åŸåˆ™**ï¼šåˆ›æ„ä»»åŠ¡ç”¨è½»åº¦æƒ©ç½šï¼Œäº‹å®æ€§ä»»åŠ¡ç”¨ä¸­åº¦æƒ©ç½š

### Q4: Beam Search ä¸é‡‡æ ·æ–¹æ³•å„æœ‰ä»€ä¹ˆé€‚ç”¨åœºæ™¯ï¼Ÿ

**ç­”æ¡ˆï¼š**
1. **Beam Search é€‚ç”¨åœºæ™¯**ï¼š
   - ç¿»è¯‘ã€æ‘˜è¦ç­‰è´¨é‡ä¼˜å…ˆçš„ä»»åŠ¡
   - éœ€è¦ç¡®å®šæ€§è¾“å‡ºçš„åº”ç”¨
   - è¾ƒçŸ­æ–‡æœ¬ç”Ÿæˆ
2. **é‡‡æ ·æ–¹æ³•é€‚ç”¨åœºæ™¯**ï¼š
   - åˆ›æ„å†™ä½œã€å¯¹è¯ç³»ç»Ÿ
   - éœ€è¦å¤šæ ·æ€§çš„é•¿æ–‡æœ¬ç”Ÿæˆ
   - æ¢ç´¢æ€§æ–‡æœ¬ç”Ÿæˆ
3. **æ··åˆç­–ç•¥**ï¼šå¯ä»¥ç”¨ Beam Search è·å¾—å¤šä¸ªå€™é€‰ï¼Œå†é€šè¿‡å…¶ä»–æŒ‡æ ‡é€‰æ‹©

### Q5: å¦‚ä½•ä¸ºæ–°çš„åº”ç”¨åœºæ™¯è®¾è®¡é‡‡æ ·ç­–ç•¥ï¼Ÿ

**ç­”æ¡ˆï¼š**
1. **åˆ†æä»»åŠ¡ç‰¹ç‚¹**ï¼š
   - ç¡®å®šæ€§ vs åˆ›é€ æ€§éœ€æ±‚
   - äº‹å®å‡†ç¡®æ€§è¦æ±‚
   - å¤šæ ·æ€§ vs è¿è´¯æ€§æƒè¡¡
2. **å®éªŒéªŒè¯**ï¼š
   - A/B æµ‹è¯•ä¸åŒå‚æ•°ç»„åˆ
   - äººå·¥è¯„ä¼°ç”Ÿæˆè´¨é‡
   - è‡ªåŠ¨æŒ‡æ ‡ï¼ˆBLEUã€ROUGE ç­‰ï¼‰è¾…åŠ©
3. **è¿­ä»£ä¼˜åŒ–**ï¼š
   - æ”¶é›†ç”¨æˆ·åé¦ˆ
   - åˆ†æå¤±è´¥æ¡ˆä¾‹
   - æŒç»­è°ƒæ•´å‚æ•°é…ç½®
4. **åŠ¨æ€ç­–ç•¥**ï¼šè€ƒè™‘æ ¹æ®ä¸Šä¸‹æ–‡ã€ç”¨æˆ·åå¥½åŠ¨æ€è°ƒæ•´é‡‡æ ·å‚æ•°

---

## é‡‡æ ·æ–¹æ³•å†³ç­–æ ‘

```mermaid
flowchart TD
    A["ä»»åŠ¡éœ€æ±‚"] -->|"ç¡®å®šæ€§è¾“å‡º"| B["Greedy / Beam Search"]
    A -->|"å¤šæ ·æ€§+è´¨é‡"| C["æ¦‚ç‡é‡‡æ ·"]
    B -->|"ç¿»è¯‘/æ‘˜è¦"| D["Beam Search\nbeam=4, length_penalty=1.2"]
    B -->|"æœ€å¿«é€Ÿåº¦"| E["Greedy\ntemp=0"]
    C -->|"é€šç”¨å¯¹è¯"| F["Top-P=0.9 + Temp=0.7"]
    C -->|"åˆ›æ„å†™ä½œ"| G["Top-P=0.95 + Temp=0.9"]
    C -->|"ä»£ç ç”Ÿæˆ"| H["Top-P=0.75 + Temp=0.2"]
    C -->|"äº‹å®é—®ç­”"| I["Top-P=0.85 + Temp=0.3"]
    F --> J["+ Repetition Penalty=1.1"]
    G --> J
```

### æ ¸å¿ƒæ•°å­¦å…¬å¼

**Temperature Scaling**ï¼š$p_i = \frac{\exp(z_i / T)}{\sum_j \exp(z_j / T)}$

- $T \to 0$ï¼šé€€åŒ–ä¸º argmaxï¼ˆgreedyï¼‰
- $T = 1$ï¼šåŸå§‹åˆ†å¸ƒ
- $T \to \infty$ï¼šè¶‹è¿‘å‡åŒ€åˆ†å¸ƒ

**Top-P (Nucleus)**ï¼šé€‰æ‹©æœ€å°é›†åˆ $V_p$ ä½¿ $\sum_{x_i \in V_p} P(x_i | x_{<i}) \geq p$

**Min-P**ï¼šä¿ç•™ $P(x_i) \geq P_{\max} \times \text{min\_p}$ çš„ tokens

---

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **API æœåŠ¡å‚æ•°è®¾ç½®**ï¼šOpenAI/Claude API çš„ `temperature`ã€`top_p` ç›´æ¥å½±å“è¾“å‡ºè´¨é‡ï¼Œé»˜è®¤æ¨è `temp=0.7, top_p=0.9`
- **ä»£ç è¡¥å…¨**ï¼š`temp=0.2, top_p=0.75`â€”â€”ä½æ¸©åº¦ç¡®ä¿è¯­æ³•æ­£ç¡®æ€§ï¼Œè¾ƒçª„ nucleus é¿å…å¥‡æ€ªè¡¥å…¨
- **è§’è‰²æ‰®æ¼”/åˆ›æ„å†™ä½œ**ï¼š`temp=0.9, top_p=0.95, presence_penalty=0.3`â€”â€”é«˜æ¸©åº¦å¢åŠ åˆ›æ„ï¼Œå­˜åœ¨æƒ©ç½šé¿å…é‡å¤

### å·¥ç¨‹å®ç°è¦ç‚¹
- Temperature å’Œ Top-P å¯ä»¥**å åŠ ä½¿ç”¨**ï¼šå…ˆ temperature scaling å† nucleus filteringï¼Œè¿™æ˜¯ HuggingFace çš„é»˜è®¤è¡Œä¸º
- Repetition Penalty çš„å®ç°ï¼šå¯¹ logit > 0 çš„å·²å‡ºç° token é™¤ä»¥ penaltyï¼Œå¯¹ logit < 0 çš„ä¹˜ä»¥ penalty
- Min-P æ¯” Top-P æ›´ç¨³å®šâ€”â€”ä¸ä¼šåœ¨åˆ†å¸ƒå¹³å¦æ—¶åŒ…å«è¿‡å¤šä½è´¨é‡ tokenï¼Œæ¨èä½œä¸º Top-P çš„æ›¿ä»£

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: Temperature=0.7 å’Œ Top-P=0.9 åŒæ—¶è®¾ç½®ï¼Œå“ªä¸ªå…ˆç”Ÿæ•ˆï¼Ÿ
  A: Temperature å…ˆ scale logitsï¼Œç„¶ååœ¨ scaled åçš„æ¦‚ç‡åˆ†å¸ƒä¸Šåš Top-P filtering
- Q: ä¸ºä»€ä¹ˆ Beam Search ä¸é€‚åˆå¼€æ”¾å¼ç”Ÿæˆï¼Ÿ
  A: Beam Search å€¾å‘ç”Ÿæˆé«˜æ¦‚ç‡ä½†å¹³åº¸çš„æ–‡æœ¬ï¼ˆ"I don't know" å‹ï¼‰ï¼Œç¼ºä¹å¤šæ ·æ€§å’Œåˆ›é€ æ€§

---

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- **é‡‡æ ·å‚æ•°æ˜¯ LLM åº”ç”¨çš„"æœ€åä¸€å…¬é‡Œ"è°ƒä¼˜**â€”â€”æ¨¡å‹å†å¥½ï¼Œé‡‡æ ·å‚æ•°ä¸å¯¹è¾“å‡ºä¹Ÿä¼šçƒ‚ã€‚è¿™æ˜¯å¼€å‘è€…æœ€å®¹æ˜“å¿½è§†ä½†å½±å“å·¨å¤§çš„ç¯èŠ‚
- **æ²¡æœ‰ä¸‡èƒ½å‚æ•°**â€”â€”å¿…é¡»æ ¹æ®å…·ä½“ä»»åŠ¡è°ƒå‚ï¼Œåˆ›æ„å†™ä½œå’Œä»£ç ç”Ÿæˆçš„æœ€ä¼˜å‚æ•°å®Œå…¨ä¸åŒ

### æœªè§£é—®é¢˜ä¸å±€é™
- åŠ¨æ€æ¸©åº¦è°ƒæ•´ï¼ˆå¦‚ Mirostatï¼‰ç†è®ºä¼˜é›…ä½†å·¥ç¨‹å®ç°å¤æ‚ï¼Œä¸»æµ API å°šæœªæ”¯æŒ
- é‡å¤æƒ©ç½šå¯èƒ½åœ¨éœ€è¦é‡å¤çš„åœºæ™¯ï¼ˆå¦‚ä»£ç ä¸­çš„å˜é‡åï¼‰äº§ç”Ÿå‰¯ä½œç”¨
- è¶…é•¿ç”Ÿæˆï¼ˆ>2000 tokensï¼‰æ—¶é‡‡æ ·è´¨é‡ä¼šé€æ¸é€€åŒ–ï¼Œç›®å‰æ²¡æœ‰å¥½çš„è§£å†³æ–¹æ¡ˆ

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- å¦‚æœæŠŠ [[AI/LLM/RL/å¼ºåŒ–å­¦ä¹ ä¸RLHFåº”ç”¨-2026å…¨æ™¯|MCTS]] æœç´¢å’Œé‡‡æ ·ç­–ç•¥ç»“åˆï¼Œå¯ä»¥åœ¨æ¨ç†æ—¶åš tree search + é‡‡æ ·çš„æ··åˆâ€”â€”å¯¹æ¯ä¸ªæ¨ç†æ­¥éª¤åš MCTS æ‰©å±•ï¼Œå¯¹æœ€ç»ˆå›å¤åš nucleus sampling
- è‡ªé€‚åº”é‡‡æ ·ï¼ˆæ ¹æ®ç”Ÿæˆå†…å®¹çš„è¯­ä¹‰ç±»å‹è‡ªåŠ¨åˆ‡æ¢å‚æ•°ï¼‰å¯èƒ½æ˜¯ä¸‹ä¸€ä¸ªçªç ´æ–¹å‘â€”â€”ä»£ç æ®µè‡ªåŠ¨é™æ¸©ï¼Œå™è¿°æ®µè‡ªåŠ¨å‡æ¸©

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [The Curious Case of Neural Text Degeneration (Top-P)](https://arxiv.org/abs/1904.09751) â€” Nucleus Sampling çš„å¥ åŸºè®ºæ–‡ï¼Œæ­ç¤ºäº† greedy/beam search å¯¼è‡´çš„é€€åŒ–é—®é¢˜
- [Hierarchical Neural Story Generation (Top-K)](https://arxiv.org/abs/1805.04833) â€” Top-K sampling çš„æ—©æœŸåº”ç”¨
- [Mirostat](https://arxiv.org/abs/2007.14966) â€” ç›´æ¥æ§åˆ¶å›°æƒ‘åº¦çš„è‡ªé€‚åº”é‡‡æ ·ï¼Œç†è®ºä¼˜é›…
- [Typical Decoding](https://arxiv.org/abs/2202.00666) â€” åŸºäºä¿¡æ¯è®ºçš„é‡‡æ ·æ–¹æ³•

### æ·±åº¦è§£è¯»
- [HuggingFace Generation Docs](https://huggingface.co/docs/transformers/generation_strategies) â€” å„ç§é‡‡æ ·ç­–ç•¥çš„ API ä½¿ç”¨æŒ‡å— â­â­â­â­â­
- [How to Generate Text (Patrick von Platen)](https://huggingface.co/blog/how-to-generate) â€” é‡‡æ ·ç­–ç•¥çš„ç›´è§‚è§£é‡Šå’Œå¯è§†åŒ– â­â­â­â­

### å®è·µèµ„æº
- [llama.cpp Sampling](https://github.com/ggerganov/llama.cpp) â€” æ”¯æŒ Min-Pã€Mirostat v1/v2 ç­‰é«˜çº§é‡‡æ ·
- [Ollama Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md) â€” æœ¬åœ°æ¨¡å‹é‡‡æ ·å‚æ•°é…ç½®æŒ‡å—

---

## See Also

> ğŸ”— See also: [[AI/LLM/Inference/KV Cache|KV Cache]] â€” é‡‡æ ·ç­–ç•¥ä¸ KV Cache ç®¡ç†çš„äº¤äº’ï¼ˆSpeculative Decoding ä¸­çš„ KV Cache å›æ»šï¼‰
> ğŸ”— See also: [[AI/LLM/RL/å¼ºåŒ–å­¦ä¹ ä¸RLHFåº”ç”¨-2026å…¨æ™¯|RL ä¸ RLHF]] â€” MCTS + LLM æ¨ç†æœç´¢æ˜¯é‡‡æ ·ç­–ç•¥çš„é«˜çº§å½¢å¼
> ğŸ”— See also: [[AI/LLM/Inference/Continuous Batching|Continuous Batching]] â€” é‡‡æ ·ç­–ç•¥å½±å“ç”Ÿæˆé•¿åº¦åˆ†å¸ƒï¼Œè¿›è€Œå½±å“ batching æ•ˆç‡
> ğŸ”— See also: [[AI/LLM/Inference/æ¨¡å‹éƒ¨ç½²å®è·µ|æ¨¡å‹éƒ¨ç½²å®è·µ]] â€” é‡‡æ ·å‚æ•°æ˜¯éƒ¨ç½²é…ç½®çš„å…³é”®éƒ¨åˆ†