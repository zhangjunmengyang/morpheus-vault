---
title: "æ··åˆç²¾åº¦è®­ç»ƒï¼šFP32/FP16/BF16/FP8 å…¨æ™¯"
brief: "æ··åˆç²¾åº¦è®­ç»ƒé€šè¿‡å¤šç§æ•°å€¼ç²¾åº¦å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œæ•°å€¼ç¨³å®šæ€§â€”â€”FP16 éœ€è¦ Loss Scaling é˜²æ¢¯åº¦ä¸‹æº¢ï¼ŒBF16 èŒƒå›´ç­‰åŒ FP32 æ— éœ€ç¼©æ”¾æˆä¸º LLM ä¸»æµï¼ŒFP8 æ˜¯ H100 æ—¶ä»£æ–°å‰æ²¿ã€‚æ ¸å¿ƒå‚è€ƒ Micikevicius et al. arXiv:1710.03740ã€‚"
type: concept
domain: ai/llm/infra
tags:
  - ai/llm/infra
  - ai/training
  - type/concept
  - interview/hot
created: 2026-02-14
updated: 2026-02-22
status: active
sources:
  - "Mixed Precision Training â€” arXiv:1710.03740 (Micikevicius et al., 2018)"
  - "BFloat16: The Secret to High Performance on Cloud TPUs â€” Google Cloud Blog"
  - "NVIDIA Transformer Engine & FP8 Training â€” NVIDIA Docs"
  - "NVIDIA Apex Mixed Precision Training â€” GitHub/NVIDIA/apex"
related:
  - "[[AI/LLM/Infra/åˆ†å¸ƒå¼è®­ç»ƒ|åˆ†å¸ƒå¼è®­ç»ƒ]]"
  - "[[AI/LLM/Infra/GPU æ˜¾å­˜è®¡ç®—æŒ‡å—|GPU æ˜¾å­˜è®¡ç®—æŒ‡å—]]"
  - "[[AI/Foundations/Training/æ··åˆç²¾åº¦è®­ç»ƒ|æ··åˆç²¾åº¦è®­ç»ƒ(Foundationsç‰ˆ)]]"
---

> [!info] å¦æœ‰é¢è¯•ç‰ˆ
> Foundations ç²¾ç®€ç‰ˆï¼š[[AI/Foundations/Training/æ··åˆç²¾åº¦è®­ç»ƒ]]

# æ··åˆç²¾åº¦è®­ç»ƒï¼šFP32/FP16/BF16/FP8 å…¨æ™¯

> æ¥æºï¼šMixed Precision Training â€” arXiv:1710.03740 (Micikevicius et al., 2018)

æ··åˆç²¾åº¦è®­ç»ƒï¼ˆMixed Precision Trainingï¼‰é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨å¤šç§æ•°å€¼ç²¾åº¦æ¥å¹³è¡¡è®¡ç®—æ•ˆç‡å’Œæ•°å€¼ç¨³å®šæ€§ã€‚æœ¬æ–‡æ·±å…¥åˆ†æå„ç§æ•°æ®æ ¼å¼çš„ç‰¹ç‚¹ã€æ··åˆç²¾åº¦çš„å®ç°åŸç†ï¼Œä»¥åŠåœ¨å¤§è¯­è¨€æ¨¡å‹è®­ç»ƒä¸­çš„å®è·µç»éªŒã€‚

## æ•°æ®æ ¼å¼å¯¹æ¯”åˆ†æ

### æµ®ç‚¹æ ¼å¼è§„èŒƒ

#### FP32ï¼ˆæ ‡å‡†å•ç²¾åº¦ï¼‰
- **æ ¼å¼**ï¼š1 ç¬¦å·ä½ + 8 æŒ‡æ•°ä½ + 23 å°¾æ•°ä½
- **èŒƒå›´**ï¼š$\pm 3.4 \times 10^{38}$
- **ç²¾åº¦**ï¼šçº¦ 7 ä½åè¿›åˆ¶æ•°
- **ç‰¹ç‚¹**ï¼šé«˜ç²¾åº¦ã€å¤§èŒƒå›´ï¼Œè®­ç»ƒç¨³å®šä½†å†…å­˜å’Œè®¡ç®—å¼€é”€å¤§

#### FP16ï¼ˆåŠç²¾åº¦ï¼‰
- **æ ¼å¼**ï¼š1 ç¬¦å·ä½ + 5 æŒ‡æ•°ä½ + 10 å°¾æ•°ä½
- **èŒƒå›´**ï¼š$\pm 6.55 \times 10^4$
- **ç²¾åº¦**ï¼šçº¦ 3-4 ä½åè¿›åˆ¶æ•°
- **ç‰¹ç‚¹**ï¼šå†…å­˜å‡åŠã€é€Ÿåº¦ç¿»å€ï¼Œä½†å®¹æ˜“æº¢å‡ºå’Œç²¾åº¦ä¸¢å¤±

#### BF16ï¼ˆBrain Float 16ï¼‰
- **æ ¼å¼**ï¼š1 ç¬¦å·ä½ + 8 æŒ‡æ•°ä½ + 7 å°¾æ•°ä½
- **èŒƒå›´**ï¼šä¸ FP32 ç›¸åŒï¼ˆ$\pm 3.4 \times 10^{38}$ï¼‰
- **ç²¾åº¦**ï¼šçº¦ 2-3 ä½åè¿›åˆ¶æ•°
- **ç‰¹ç‚¹**ï¼šGoogle è®¾è®¡ï¼Œä¸“ä¸ºæœºå™¨å­¦ä¹ ä¼˜åŒ–ï¼Œå‡å°‘æº¢å‡ºé—®é¢˜

#### FP8ï¼ˆ8ä½æµ®ç‚¹ï¼‰
- **E4M3**ï¼š1 ç¬¦å·ä½ + 4 æŒ‡æ•°ä½ + 3 å°¾æ•°ä½
- **E5M2**ï¼š1 ç¬¦å·ä½ + 5 æŒ‡æ•°ä½ + 2 å°¾æ•°ä½
- **ç‰¹ç‚¹**ï¼šH100 åŸç”Ÿæ”¯æŒï¼Œæè‡´å‹ç¼©ä½†éœ€è¦ç²¾å¿ƒè°ƒä¼˜

```python
import torch
import numpy as np

def compare_precision_formats():
    """å¯¹æ¯”ä¸åŒç²¾åº¦æ ¼å¼çš„æ•°å€¼ç‰¹æ€§"""
    
    # ç²¾åº¦èŒƒå›´å¯¹æ¯”
    formats = {
        'FP32': {'min_pos': 1.175494e-38, 'max_val': 3.402823e+38, 'eps': 1.192093e-07},
        'FP16': {'min_pos': 6.103516e-05, 'max_val': 65504.0, 'eps': 0.0009765625},
        'BF16': {'min_pos': 1.175494e-38, 'max_val': 3.389531e+38, 'eps': 0.0078125},
    }
    
    print("æ•°å€¼æ ¼å¼å¯¹æ¯”:")
    print(f"{'æ ¼å¼':<8} {'æœ€å°æ­£å€¼':<15} {'æœ€å¤§å€¼':<15} {'æœºå™¨ç²¾åº¦':<15}")
    print("-" * 60)
    for fmt, props in formats.items():
        print(f"{fmt:<8} {props['min_pos']:<15.2e} {props['max_val']:<15.2e} {props['eps']:<15.2e}")

    # æ¢¯åº¦èŒƒå›´ç¤ºä¾‹
    def analyze_gradient_distribution():
        """åˆ†æå…¸å‹ LLM è®­ç»ƒä¸­çš„æ¢¯åº¦åˆ†å¸ƒ"""
        # æ¨¡æ‹ŸçœŸå®æ¢¯åº¦åˆ†å¸ƒ
        gradients = torch.randn(1000000) * 1e-5  # å…¸å‹ LLM æ¢¯åº¦å°ºåº¦
        
        print(f"\næ¢¯åº¦åˆ†å¸ƒåˆ†æ:")
        print(f"å‡å€¼: {gradients.mean():.2e}")
        print(f"æ ‡å‡†å·®: {gradients.std():.2e}")
        print(f"æœ€å°å€¼: {gradients.min():.2e}")
        print(f"æœ€å¤§å€¼: {gradients.max():.2e}")
        
        # æ£€æŸ¥å„æ ¼å¼çš„è¡¨ç¤ºèƒ½åŠ›
        fp16_underflow = (torch.abs(gradients) < 6.103516e-05).sum()
        print(f"FP16 ä¸‹æº¢æ•°é‡: {fp16_underflow} ({fp16_underflow/len(gradients)*100:.2f}%)")
        
        return gradients

compare_precision_formats()
```

### æ•°å€¼ç¨³å®šæ€§åˆ†æ

```python
class PrecisionAnalyzer:
    """ç²¾åº¦æ ¼å¼çš„æ•°å€¼ç¨³å®šæ€§åˆ†æå·¥å…·"""
    
    def __init__(self):
        self.formats = ['float32', 'float16', 'bfloat16']
    
    def test_overflow_underflow(self):
        """æµ‹è¯•æº¢å‡ºå’Œä¸‹æº¢æƒ…å†µ"""
        test_values = [1e-10, 1e-5, 1e5, 1e10]
        
        for fmt in self.formats:
            print(f"\n{fmt.upper()} æº¢å‡ºæµ‹è¯•:")
            for val in test_values:
                try:
                    if fmt == 'bfloat16':
                        converted = val  # PyTorch è‡ªåŠ¨å¤„ç†
                        result = "æ­£å¸¸"
                    else:
                        converted = torch.tensor(val, dtype=getattr(torch, fmt))
                        result = "æ­£å¸¸" if torch.isfinite(converted) else "æº¢å‡º"
                    
                    print(f"  {val:.1e} -> {result}")
                except:
                    print(f"  {val:.1e} -> é”™è¯¯")
    
    def gradient_accumulation_error(self, steps=1000):
        """æµ‹è¯•æ¢¯åº¦ç´¯ç§¯è¯¯å·®"""
        small_grad = 1e-7
        
        # FP32 åŸºå‡†
        fp32_sum = torch.tensor(0.0, dtype=torch.float32)
        for _ in range(steps):
            fp32_sum += small_grad
        
        # FP16 ç´¯ç§¯
        fp16_sum = torch.tensor(0.0, dtype=torch.float16)
        for _ in range(steps):
            fp16_sum += small_grad
        
        # BF16 ç´¯ç§¯
        bf16_val = torch.tensor(small_grad)
        bf16_sum = torch.tensor(0.0)
        for _ in range(steps):
            bf16_sum += bf16_val.to(torch.bfloat16)
        
        print(f"\næ¢¯åº¦ç´¯ç§¯æµ‹è¯• ({steps} æ­¥, æ¯æ­¥ {small_grad:.1e}):")
        print(f"FP32 ç»“æœ: {fp32_sum:.8f}")
        print(f"FP16 ç»“æœ: {fp16_sum:.8f} (è¯¯å·®: {abs(fp32_sum - fp16_sum)/fp32_sum*100:.2f}%)")
        print(f"BF16 ç»“æœ: {bf16_sum:.8f} (è¯¯å·®: {abs(fp32_sum - bf16_sum.float())/fp32_sum*100:.2f}%)")

# è¿è¡Œåˆ†æ
analyzer = PrecisionAnalyzer()
analyzer.test_overflow_underflow()
analyzer.gradient_accumulation_error()
```

## æ··åˆç²¾åº¦è®­ç»ƒåŸç†

### æ··åˆç²¾åº¦è®­ç»ƒæ ¸å¿ƒæµç¨‹

> æ¥æºï¼šarXiv:1710.03740, Sec. 3

æ··åˆç²¾åº¦è®­ç»ƒçš„ä¸‰å¤§æ”¯æŸ±ï¼š

```mermaid
flowchart TD
    A[æ··åˆç²¾åº¦è®­ç»ƒ] --> B["FP32 Master Weights<br/>ä¼˜åŒ–å™¨åœ¨ FP32 ä¸Šæ›´æ–°"]
    A --> C["Loss Scaling<br/>æ”¾å¤§æŸå¤±é˜²æ¢¯åº¦ä¸‹æº¢"]
    A --> D["FP16/BF16 Forward/Backward<br/>å‰å‘å’Œåå‘ç”¨ä½ç²¾åº¦åŠ é€Ÿ"]
    
    B --> E["ä¿è¯å°æƒé‡æ›´æ–°ä¸ä¸¢å¤±"]
    C --> F["FP16 æ¢¯åº¦å®‰å…¨è¡¨ç¤º"]
    D --> G["2x é€Ÿåº¦ + 0.5x æ˜¾å­˜"]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
```

**Loss Scaling æ•°å­¦åŸç†**ï¼š

$$\hat{L} = L \times S \quad \text{(æ”¾å¤§æŸå¤±)}$$

$$\hat{g} = \frac{\partial \hat{L}}{\partial w} = S \cdot \frac{\partial L}{\partial w} \quad \text{(æ¢¯åº¦åŒæ­¥æ”¾å¤§)}$$

$$g = \hat{g} / S \quad \text{(ä¼˜åŒ–å™¨æ›´æ–°å‰åç¼©æ”¾)}$$

å…¶ä¸­ $S$ æ˜¯åŠ¨æ€ç¼©æ”¾å› å­ï¼ˆåˆå§‹ $2^{16}$ï¼‰ï¼Œé‡åˆ° Inf/NaN æ—¶å‡åŠï¼Œè¿ç»­æˆåŠŸ $N$ æ­¥åç¿»å€ã€‚

### è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰æ¡†æ¶

```python
import torch.cuda.amp as amp

class MixedPrecisionTrainer:
    def __init__(self, model, optimizer, loss_fn):
        self.model = model
        self.optimizer = optimizer
        self.loss_fn = loss_fn
        
        # GradScaler ç”¨äºé˜²æ­¢æ¢¯åº¦ä¸‹æº¢
        self.scaler = amp.GradScaler()
        
        # è®°å½•æŸå¤±ç¼©æ”¾å†å²
        self.scale_history = []
        
    def train_step(self, inputs, targets):
        """æ··åˆç²¾åº¦è®­ç»ƒæ­¥éª¤"""
        self.optimizer.zero_grad()
        
        # å‰å‘ä¼ æ’­ä½¿ç”¨ autocast
        with amp.autocast():
            outputs = self.model(inputs)
            loss = self.loss_fn(outputs, targets)
        
        # ç¼©æ”¾æŸå¤±å¹¶åå‘ä¼ æ’­
        self.scaler.scale(loss).backward()
        
        # è®°å½•å½“å‰ç¼©æ”¾å› å­
        current_scale = self.scaler.get_scale()
        self.scale_history.append(current_scale)
        
        # ç¼©æ”¾æ¢¯åº¦å¹¶æ›´æ–°
        self.scaler.step(self.optimizer)
        self.scaler.update()
        
        return loss.item(), current_scale
    
    def analyze_gradient_scale(self):
        """åˆ†ææ¢¯åº¦ç¼©æ”¾çš„å˜åŒ–æƒ…å†µ"""
        import matplotlib.pyplot as plt
        
        plt.figure(figsize=(12, 4))
        
        # ç¼©æ”¾å› å­å˜åŒ–
        plt.subplot(1, 2, 1)
        plt.plot(self.scale_history)
        plt.yscale('log')
        plt.title('Gradient Scale Factor')
        plt.xlabel('Step')
        plt.ylabel('Scale Factor')
        
        # ç¼©æ”¾è°ƒæ•´é¢‘ç‡
        scale_changes = np.diff(np.log2(self.scale_history))
        plt.subplot(1, 2, 2)
        plt.hist(scale_changes, bins=50, alpha=0.7)
        plt.title('Scale Change Distribution')
        plt.xlabel('Log2 Scale Change')
        plt.ylabel('Frequency')
        
        plt.tight_layout()
        plt.show()
```

### Loss Scaling æœºåˆ¶

```python
class DynamicLossScaler:
    """åŠ¨æ€æŸå¤±ç¼©æ”¾å®ç°"""
    
    def __init__(self, init_scale=2**16, scale_factor=2.0, scale_window=2000):
        self.scale = init_scale
        self.scale_factor = scale_factor
        self.scale_window = scale_window
        self.unskipped_steps = 0
        
    def scale_loss(self, loss):
        """ç¼©æ”¾æŸå¤±å€¼"""
        return loss * self.scale
    
    def unscale_gradients(self, parameters):
        """åç¼©æ”¾æ¢¯åº¦"""
        inv_scale = 1. / self.scale
        for param in parameters:
            if param.grad is not None:
                param.grad.data.mul_(inv_scale)
    
    def update_scale(self, found_inf):
        """æ ¹æ®æ˜¯å¦å‡ºç°æ— ç©·å¤§å€¼æ›´æ–°ç¼©æ”¾å› å­"""
        if found_inf:
            # å‘ç°æ— ç©·å¤§ï¼Œå‡å°ç¼©æ”¾å› å­
            self.scale /= self.scale_factor
            self.unskipped_steps = 0
            print(f"Loss scaling down to {self.scale}")
        else:
            self.unskipped_steps += 1
            
            # è¿ç»­æˆåŠŸæ­¥æ•°è¾¾åˆ°çª—å£å¤§å°ï¼Œå¢åŠ ç¼©æ”¾å› å­
            if self.unskipped_steps >= self.scale_window:
                self.scale *= self.scale_factor
                self.unskipped_steps = 0
                print(f"Loss scaling up to {self.scale}")
    
    def step(self, optimizer, parameters):
        """æ‰§è¡Œä¼˜åŒ–æ­¥éª¤"""
        # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æœ‰æ— ç©·å¤§å€¼
        found_inf = any(
            torch.isinf(param.grad).any() or torch.isnan(param.grad).any()
            for param in parameters if param.grad is not None
        )
        
        if not found_inf:
            self.unscale_gradients(parameters)
            optimizer.step()
        
        self.update_scale(found_inf)
        return not found_inf
```

### Master Weights æœºåˆ¶

```python
class MasterWeightOptimizer:
    """ä½¿ç”¨ FP32 ä¸»æƒé‡çš„ä¼˜åŒ–å™¨åŒ…è£…å™¨"""
    
    def __init__(self, optimizer, model):
        self.optimizer = optimizer
        self.model = model
        
        # åˆ›å»º FP32 ä¸»æƒé‡å‰¯æœ¬
        self.master_weights = []
        self.model_params = []
        
        for param_group in optimizer.param_groups:
            master_group = []
            model_group = []
            
            for param in param_group['params']:
                # åˆ›å»º FP32 ä¸»æƒé‡
                master_param = param.detach().clone().float()
                master_param.requires_grad = True
                master_group.append(master_param)
                model_group.append(param)
            
            self.master_weights.append(master_group)
            self.model_params.append(model_group)
    
    def zero_grad(self):
        """æ¸…é›¶ä¸»æƒé‡æ¢¯åº¦"""
        for group in self.master_weights:
            for param in group:
                if param.grad is not None:
                    param.grad.zero_()
    
    def copy_grads_to_master(self):
        """å°†æ¨¡å‹æ¢¯åº¦å¤åˆ¶åˆ°ä¸»æƒé‡"""
        for master_group, model_group in zip(self.master_weights, self.model_params):
            for master_param, model_param in zip(master_group, model_group):
                if model_param.grad is not None:
                    if master_param.grad is None:
                        master_param.grad = torch.empty_like(master_param)
                    master_param.grad.copy_(model_param.grad.float())
    
    def copy_master_to_model(self):
        """å°†ä¸»æƒé‡æ›´æ–°å¤åˆ¶å›æ¨¡å‹"""
        for master_group, model_group in zip(self.master_weights, self.model_params):
            for master_param, model_param in zip(master_group, model_group):
                model_param.data.copy_(master_param.data.to(model_param.dtype))
    
    def step(self):
        """æ‰§è¡Œä¼˜åŒ–æ­¥éª¤"""
        # å¤åˆ¶æ¢¯åº¦åˆ°ä¸»æƒé‡
        self.copy_grads_to_master()
        
        # åœ¨ FP32 ä¸»æƒé‡ä¸Šæ‰§è¡Œä¼˜åŒ–
        self.optimizer.step()
        
        # å°†æ›´æ–°çš„æƒé‡å¤åˆ¶å›æ¨¡å‹
        self.copy_master_to_model()
```

## BF16 vs FP16 é€‰æ‹©

### ä¸ºä»€ä¹ˆ LLM è®­ç»ƒåå¥½ BF16ï¼Ÿ

```python
def compare_bf16_fp16():
    """å¯¹æ¯” BF16 å’Œ FP16 åœ¨ LLM è®­ç»ƒä¸­çš„è¡¨ç°"""
    
    # 1. æ•°å€¼èŒƒå›´å¯¹æ¯”
    print("æ•°å€¼èŒƒå›´å¯¹æ¯”:")
    print(f"FP16 æœ€å¤§å€¼: {torch.finfo(torch.float16).max}")
    print(f"BF16 æœ€å¤§å€¼: {torch.finfo(torch.bfloat16).max}")
    print(f"FP16 æœ€å°æ­£å€¼: {torch.finfo(torch.float16).tiny}")
    print(f"BF16 æœ€å°æ­£å€¼: {torch.finfo(torch.bfloat16).tiny}")
    
    # 2. å…¸å‹ LLM æƒé‡åˆ†å¸ƒæµ‹è¯•
    def test_weight_distribution():
        """æµ‹è¯•å…¸å‹ LLM æƒé‡åˆ†å¸ƒä¸‹çš„è¡¨ç°"""
        
        # æ¨¡æ‹Ÿ Transformer æƒé‡åˆ†å¸ƒï¼ˆæ¥è¿‘æ­£æ€åˆ†å¸ƒï¼‰
        weights = torch.randn(1000000) * 0.02  # å…¸å‹åˆå§‹åŒ–æ ‡å‡†å·®
        
        # è½¬æ¢åˆ°ä¸åŒæ ¼å¼
        weights_fp16 = weights.half()
        weights_bf16 = weights.bfloat16()
        
        # è®¡ç®—é‡åŒ–è¯¯å·®
        fp16_error = (weights - weights_fp16.float()).abs().mean()
        bf16_error = (weights - weights_bf16.float()).abs().mean()
        
        print(f"\næƒé‡é‡åŒ–è¯¯å·®:")
        print(f"FP16 å¹³å‡è¯¯å·®: {fp16_error:.2e}")
        print(f"BF16 å¹³å‡è¯¯å·®: {bf16_error:.2e}")
        
        # æ£€æŸ¥æ¢¯åº¦ç´¯ç§¯ç¨³å®šæ€§
        small_updates = torch.randn(1000) * 1e-6
        
        fp16_acc = torch.zeros(1, dtype=torch.float16)
        bf16_acc = torch.zeros(1, dtype=torch.bfloat16)
        fp32_acc = torch.zeros(1, dtype=torch.float32)
        
        for update in small_updates:
            fp16_acc += update.half()
            bf16_acc += update.bfloat16()
            fp32_acc += update
        
        print(f"\nå°æ¢¯åº¦ç´¯ç§¯æµ‹è¯•:")
        print(f"FP32 åŸºå‡†: {fp32_acc.item():.8f}")
        print(f"FP16 ç»“æœ: {fp16_acc.float().item():.8f}")
        print(f"BF16 ç»“æœ: {bf16_acc.float().item():.8f}")
        print(f"FP16 ç›¸å¯¹è¯¯å·®: {abs(fp32_acc - fp16_acc.float())/abs(fp32_acc)*100:.2f}%")
        print(f"BF16 ç›¸å¯¹è¯¯å·®: {abs(fp32_acc - bf16_acc.float())/abs(fp32_acc)*100:.2f}%")
    
    test_weight_distribution()

# å®é™…åº”ç”¨ä¸­çš„é€‰æ‹©ç­–ç•¥
class PrecisionSelector:
    @staticmethod
    def recommend_precision(model_type, hardware, stability_priority=True):
        """æ ¹æ®æ¨¡å‹å’Œç¡¬ä»¶æ¨èç²¾åº¦æ ¼å¼"""
        
        recommendations = {}
        
        if hardware == "A100" or hardware == "H100":
            if model_type == "LLM":
                if stability_priority:
                    recommendations['forward'] = 'BF16'
                    recommendations['backward'] = 'FP32'
                    recommendations['optimizer'] = 'FP32'
                    recommendations['reason'] = 'BF16 å‰å‘ä¼ æ’­ï¼ŒFP32 æ¢¯åº¦è®¡ç®—ï¼Œæœ€ä½³ç¨³å®šæ€§'
                else:
                    recommendations['forward'] = 'FP16'
                    recommendations['backward'] = 'FP16'
                    recommendations['optimizer'] = 'FP32'
                    recommendations['reason'] = 'FP16 å‰å‘/åå‘ï¼ŒFP32 ä¼˜åŒ–å™¨ï¼Œæœ€ä½³æ€§èƒ½'
        
        elif hardware == "V100":
            recommendations['forward'] = 'FP16'
            recommendations['backward'] = 'FP16'  
            recommendations['optimizer'] = 'FP32'
            recommendations['reason'] = 'V100 ä¸æ”¯æŒ BF16ï¼Œä½¿ç”¨ FP16 + loss scaling'
        
        return recommendations

compare_bf16_fp16()
```

### BF16 å®è·µé…ç½®

```python
class BF16Trainer:
    """BF16 æ··åˆç²¾åº¦è®­ç»ƒå®ç°"""
    
    def __init__(self, model, optimizer):
        self.model = model
        self.optimizer = optimizer
        
        # è½¬æ¢æ¨¡å‹åˆ° BF16
        self.model = self.model.to(torch.bfloat16)
        
        # ä¿æŒæŸäº›å±‚ä¸º FP32ï¼ˆæ•°å€¼æ•æ„Ÿçš„å±‚ï¼‰
        self.keep_fp32_layers = ['layer_norm', 'embedding']
        self._preserve_fp32_layers()
    
    def _preserve_fp32_layers(self):
        """ä¿æŒæŸäº›å±‚ä¸º FP32 ç²¾åº¦"""
        for name, module in self.model.named_modules():
            if any(keep_type in name.lower() for keep_type in self.keep_fp32_layers):
                module.float()
                print(f"ä¿æŒ {name} ä¸º FP32 ç²¾åº¦")
    
    def train_step(self, inputs, targets):
        """BF16 è®­ç»ƒæ­¥éª¤ï¼ˆä¸éœ€è¦ loss scalingï¼‰"""
        self.optimizer.zero_grad()
        
        # BF16 å‰å‘ä¼ æ’­ï¼ˆä¸ä½¿ç”¨ autocastï¼‰
        outputs = self.model(inputs.bfloat16())
        loss = F.cross_entropy(outputs.float(), targets)
        
        # ç›´æ¥åå‘ä¼ æ’­ï¼ˆBF16 æ•°å€¼èŒƒå›´è¶³å¤Ÿï¼Œæ— éœ€ç¼©æ”¾ï¼‰
        loss.backward()
        
        # æ¢¯åº¦è£å‰ªï¼ˆåœ¨è½¬æ¢ä¸º FP32 åï¼‰
        fp32_grads = []
        for param in self.model.parameters():
            if param.grad is not None:
                fp32_grads.append(param.grad.float())
        
        torch.nn.utils.clip_grad_norm_(fp32_grads, max_norm=1.0)
        
        # ä¼˜åŒ–å™¨æ›´æ–°ï¼ˆè‡ªåŠ¨å¤„ç†ç²¾åº¦è½¬æ¢ï¼‰
        self.optimizer.step()
        
        return loss.item()
```

## FP8 è®­ç»ƒï¼ˆH100 ç‰¹æ€§ï¼‰

### FP8 æ ¼å¼è¯¦è§£

```python
# FP8 éœ€è¦ä½¿ç”¨ Transformer Engine æˆ–è€… CUDA ä¸“é—¨åº“
try:
    import transformer_engine.pytorch as te
    FP8_AVAILABLE = True
except ImportError:
    FP8_AVAILABLE = False
    print("Transformer Engine æœªå®‰è£…ï¼ŒFP8 åŠŸèƒ½ä¸å¯ç”¨")

class FP8Trainer:
    """FP8 æ··åˆç²¾åº¦è®­ç»ƒï¼ˆéœ€è¦ H100ï¼‰"""
    
    def __init__(self, model, optimizer):
        if not FP8_AVAILABLE:
            raise RuntimeError("FP8 è®­ç»ƒéœ€è¦ Transformer Engine")
        
        self.model = model
        self.optimizer = optimizer
        
        # FP8 é…ç½®
        self.fp8_recipe = te.recipe.DelayedScaling(
            margin=0,           # ç¼©æ”¾è¾¹é™…
            interval=1,         # ç¼©æ”¾é—´éš”
            fp8_format=te.recipe.Format.E4M3,  # å‰å‘ä¼ æ’­æ ¼å¼
            amax_history_len=16, # å†å²æœ€å¤§å€¼é•¿åº¦
            amax_compute_algo='max'  # æœ€å¤§å€¼è®¡ç®—ç®—æ³•
        )
    
    def convert_to_fp8(self):
        """å°†æ¨¡å‹è½¬æ¢ä¸ºæ”¯æŒ FP8 çš„ç‰ˆæœ¬"""
        # æ›¿æ¢å…³é”®å±‚ä¸º FP8 ç‰ˆæœ¬
        for name, module in self.model.named_children():
            if isinstance(module, torch.nn.Linear):
                # è½¬æ¢ä¸º FP8 Linear å±‚
                fp8_linear = te.Linear(
                    module.in_features,
                    module.out_features,
                    bias=(module.bias is not None),
                    params_dtype=torch.half  # å‚æ•°ä»ä½¿ç”¨ FP16
                )
                
                # å¤åˆ¶æƒé‡
                with torch.no_grad():
                    fp8_linear.weight.copy_(module.weight)
                    if module.bias is not None:
                        fp8_linear.bias.copy_(module.bias)
                
                setattr(self.model, name, fp8_linear)
    
    def train_step(self, inputs, targets):
        """FP8 è®­ç»ƒæ­¥éª¤"""
        self.optimizer.zero_grad()
        
        # åœ¨ FP8 ä¸Šä¸‹æ–‡ä¸­è®­ç»ƒ
        with te.fp8_autocast(enabled=True, fp8_recipe=self.fp8_recipe):
            outputs = self.model(inputs)
            loss = F.cross_entropy(outputs, targets)
        
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        self.optimizer.step()
        
        return loss.item()

# FP8 æ€§èƒ½åŸºå‡†æµ‹è¯•
def benchmark_fp8_vs_bf16():
    """å¯¹æ¯” FP8 å’Œ BF16 çš„æ€§èƒ½å·®å¼‚"""
    if not FP8_AVAILABLE:
        print("è·³è¿‡ FP8 åŸºå‡†æµ‹è¯•ï¼šç¯å¢ƒä¸æ”¯æŒ")
        return
    
    import time
    
    # æµ‹è¯•é…ç½®
    batch_size = 32
    seq_length = 512
    hidden_size = 4096
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    inputs = torch.randn(batch_size, seq_length, hidden_size, device='cuda')
    
    # BF16 åŸºå‡†
    linear_bf16 = torch.nn.Linear(hidden_size, hidden_size).cuda().bfloat16()
    
    start_time = time.time()
    for _ in range(100):
        output = linear_bf16(inputs.bfloat16())
    torch.cuda.synchronize()
    bf16_time = time.time() - start_time
    
    # FP8 åŸºå‡† (å¦‚æœå¯ç”¨)
    try:
        linear_fp8 = te.Linear(hidden_size, hidden_size).cuda()
        
        with te.fp8_autocast(enabled=True):
            start_time = time.time()
            for _ in range(100):
                output = linear_fp8(inputs.half())
            torch.cuda.synchronize()
            fp8_time = time.time() - start_time
        
        print(f"BF16 æ—¶é—´: {bf16_time:.4f}s")
        print(f"FP8 æ—¶é—´: {fp8_time:.4f}s")
        print(f"FP8 åŠ é€Ÿæ¯”: {bf16_time/fp8_time:.2f}x")
    
    except Exception as e:
        print(f"FP8 æµ‹è¯•å¤±è´¥: {e}")
```

## Nan/Inf é—®é¢˜æ’æŸ¥

### é—®é¢˜æ£€æµ‹å·¥å…·

```python
class NumericalStabilityMonitor:
    """æ•°å€¼ç¨³å®šæ€§ç›‘æ§å·¥å…·"""
    
    def __init__(self, model):
        self.model = model
        self.history = {
            'step': [],
            'loss': [],
            'grad_norm': [],
            'param_norm': [],
            'nan_count': [],
            'inf_count': []
        }
    
    def check_model_health(self, step, loss):
        """æ£€æŸ¥æ¨¡å‹æ•°å€¼å¥åº·çŠ¶å†µ"""
        health_report = {
            'step': step,
            'loss': loss.item() if torch.is_tensor(loss) else loss,
            'issues': []
        }
        
        # æ£€æŸ¥æŸå¤±å€¼
        if torch.isnan(torch.tensor(health_report['loss'])):
            health_report['issues'].append("Loss is NaN")
        if torch.isinf(torch.tensor(health_report['loss'])):
            health_report['issues'].append("Loss is Inf")
        if health_report['loss'] > 100:
            health_report['issues'].append(f"Loss unusually high: {health_report['loss']:.2f}")
        
        # æ£€æŸ¥å‚æ•°
        param_issues = self._check_parameters()
        health_report.update(param_issues)
        
        # æ£€æŸ¥æ¢¯åº¦
        grad_issues = self._check_gradients()
        health_report.update(grad_issues)
        
        # è®°å½•å†å²
        self.history['step'].append(step)
        self.history['loss'].append(health_report['loss'])
        self.history['grad_norm'].append(health_report.get('grad_norm', 0))
        self.history['param_norm'].append(health_report.get('param_norm', 0))
        self.history['nan_count'].append(health_report.get('nan_count', 0))
        self.history['inf_count'].append(health_report.get('inf_count', 0))
        
        return health_report
    
    def _check_parameters(self):
        """æ£€æŸ¥æ¨¡å‹å‚æ•°çŠ¶æ€"""
        param_norms = []
        nan_count = 0
        inf_count = 0
        
        for name, param in self.model.named_parameters():
            if param is not None:
                param_norms.append(torch.norm(param).item())
                nan_count += torch.isnan(param).sum().item()
                inf_count += torch.isinf(param).sum().item()
        
        return {
            'param_norm': np.mean(param_norms) if param_norms else 0,
            'param_nan_count': nan_count,
            'param_inf_count': inf_count
        }
    
    def _check_gradients(self):
        """æ£€æŸ¥æ¢¯åº¦çŠ¶æ€"""
        grad_norms = []
        nan_count = 0
        inf_count = 0
        
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad_norms.append(torch.norm(param.grad).item())
                nan_count += torch.isnan(param.grad).sum().item()
                inf_count += torch.isinf(param.grad).sum().item()
        
        total_grad_norm = np.sqrt(sum([norm**2 for norm in grad_norms])) if grad_norms else 0
        
        return {
            'grad_norm': total_grad_norm,
            'grad_nan_count': nan_count,
            'grad_inf_count': inf_count
        }
    
    def diagnose_instability(self):
        """è¯Šæ–­è®­ç»ƒä¸ç¨³å®šçš„åŸå› """
        if len(self.history['loss']) < 10:
            return "éœ€è¦æ›´å¤šè®­ç»ƒæ­¥éª¤è¿›è¡Œè¯Šæ–­"
        
        recent_losses = self.history['loss'][-10:]
        recent_grad_norms = self.history['grad_norm'][-10:]
        
        diagnosis = []
        
        # æ£€æŸ¥æŸå¤±å‘æ•£
        if any(loss > 1e6 for loss in recent_losses):
            diagnosis.append("æŸå¤±å€¼çˆ†ç‚¸ï¼Œå¯èƒ½æ˜¯å­¦ä¹ ç‡è¿‡é«˜")
        
        # æ£€æŸ¥æ¢¯åº¦çˆ†ç‚¸
        if any(norm > 100 for norm in recent_grad_norms):
            diagnosis.append("æ¢¯åº¦çˆ†ç‚¸ï¼Œå»ºè®®ä½¿ç”¨æ¢¯åº¦è£å‰ª")
        
        # æ£€æŸ¥æ¢¯åº¦æ¶ˆå¤±
        if all(norm < 1e-6 for norm in recent_grad_norms):
            diagnosis.append("æ¢¯åº¦æ¶ˆå¤±ï¼Œå¯èƒ½æ˜¯ç½‘ç»œå¤ªæ·±æˆ–æ¿€æ´»å‡½æ•°é—®é¢˜")
        
        # æ£€æŸ¥ NaN/Inf é¢‘ç‡
        if sum(self.history['nan_count'][-10:]) > 0:
            diagnosis.append("å‡ºç° NaN å€¼ï¼Œæ£€æŸ¥æ•°æ®é¢„å¤„ç†å’ŒæŸå¤±å‡½æ•°")
        
        return diagnosis if diagnosis else ["æ¨¡å‹è®­ç»ƒçœ‹èµ·æ¥ç¨³å®š"]

# è‡ªåŠ¨ä¿®å¤ç­–ç•¥
class NumericalStabilityFixer:
    """æ•°å€¼ç¨³å®šæ€§è‡ªåŠ¨ä¿®å¤"""
    
    def __init__(self, model, optimizer, scaler=None):
        self.model = model
        self.optimizer = optimizer
        self.scaler = scaler
        self.checkpoint = None
    
    def create_checkpoint(self):
        """åˆ›å»ºæ£€æŸ¥ç‚¹"""
        self.checkpoint = {
            'model_state': {name: param.clone() for name, param in self.model.named_parameters()},
            'optimizer_state': self.optimizer.state_dict()
        }
    
    def fix_nan_inf(self, health_report):
        """ä¿®å¤ NaN/Inf é—®é¢˜"""
        if health_report.get('grad_nan_count', 0) > 0 or health_report.get('grad_inf_count', 0) > 0:
            print("æ£€æµ‹åˆ°æ¢¯åº¦ NaN/Infï¼Œåº”ç”¨ä¿®å¤ç­–ç•¥...")
            
            # ç­–ç•¥1ï¼šæ¸…é›¶æœ‰é—®é¢˜çš„æ¢¯åº¦
            for name, param in self.model.named_parameters():
                if param.grad is not None:
                    param.grad[torch.isnan(param.grad)] = 0
                    param.grad[torch.isinf(param.grad)] = 0
            
            # ç­–ç•¥2ï¼šé™ä½å­¦ä¹ ç‡
            for param_group in self.optimizer.param_groups:
                param_group['lr'] *= 0.5
                print(f"å­¦ä¹ ç‡é™ä½åˆ°: {param_group['lr']}")
            
            # ç­–ç•¥3ï¼šå¦‚æœæœ‰ scalerï¼Œé‡ç½®ç¼©æ”¾å› å­
            if self.scaler is not None:
                self.scaler._scale = torch.tensor(2048.0)
                print("é‡ç½® loss scaling å› å­")
            
            return True
        
        return False
    
    def restore_checkpoint(self):
        """æ¢å¤åˆ°ç¨³å®šæ£€æŸ¥ç‚¹"""
        if self.checkpoint is None:
            print("æ— å¯ç”¨æ£€æŸ¥ç‚¹")
            return False
        
        print("æ¢å¤åˆ°ä¹‹å‰çš„ç¨³å®šçŠ¶æ€...")
        for name, param in self.model.named_parameters():
            param.data.copy_(self.checkpoint['model_state'][name])
        
        self.optimizer.load_state_dict(self.checkpoint['optimizer_state'])
        return True
```

## é¢è¯•å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆéœ€è¦ Loss Scalingï¼ŸåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ

**ç­”æ¡ˆï¼š**
1. **æ¢¯åº¦ä¸‹æº¢é—®é¢˜**ï¼šFP16 æœ€å°è¡¨ç¤ºå€¼æ˜¯ 6e-5ï¼Œè€Œ LLM è®­ç»ƒä¸­æ¢¯åº¦ç»å¸¸å°äºè¿™ä¸ªå€¼
2. **ç¼©æ”¾åŸç†**ï¼šå°†æŸå¤±ä¹˜ä»¥å¤§æ•°ï¼ˆå¦‚ 65536ï¼‰ï¼Œä½¿å¾—åå‘ä¼ æ’­çš„æ¢¯åº¦è¢«åŒæ¯”ä¾‹æ”¾å¤§
3. **æ¢¯åº¦æ¢å¤**ï¼šåœ¨ä¼˜åŒ–å™¨æ›´æ–°å‰å°†æ¢¯åº¦é™¤ä»¥ç¼©æ”¾å› å­ï¼Œæ¢å¤çœŸå®å¤§å°
4. **åŠ¨æ€è°ƒæ•´**ï¼šç›‘æ§æ¢¯åº¦æ˜¯å¦å‡ºç° Infï¼Œè‡ªåŠ¨è°ƒæ•´ç¼©æ”¾å› å­å¤§å°

### Q2: BF16 ç›¸æ¯” FP16 çš„ä¼˜åŠ¿ä½“ç°åœ¨å“ªé‡Œï¼Ÿ

**ç­”æ¡ˆï¼š**
1. **æ•°å€¼èŒƒå›´**ï¼šBF16 ä¸ FP32 æœ‰ç›¸åŒçš„æŒ‡æ•°èŒƒå›´ï¼Œå¤§å¹…å‡å°‘æº¢å‡ºé£é™©
2. **æ— éœ€ Loss Scaling**ï¼šç”±äºèŒƒå›´è¶³å¤Ÿå¤§ï¼Œé€šå¸¸ä¸éœ€è¦æ¢¯åº¦ç¼©æ”¾
3. **è®­ç»ƒç¨³å®šæ€§**ï¼šåœ¨å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒä¸­è¡¨ç°æ›´ç¨³å®šï¼Œæ”¶æ•›æ›´ä¸€è‡´
4. **ç¡¬ä»¶æ”¯æŒ**ï¼šA100ã€H100 ç­‰æ–°ç¡¬ä»¶åŸç”Ÿæ”¯æŒ BF16 è®¡ç®—

### Q3: Master Weights æœºåˆ¶ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

**ç­”æ¡ˆï¼š**
1. **ç²¾åº¦ä¿æŒ**ï¼šä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚ Adam çš„ momentumï¼‰éœ€è¦é«˜ç²¾åº¦ç´¯ç§¯
2. **å°æ›´æ–°ä¿ç•™**ï¼šFP16 å¯èƒ½æ— æ³•è¡¨ç¤ºå°çš„æƒé‡æ›´æ–°ï¼Œå¯¼è‡´è®­ç»ƒåœæ»
3. **æ•°å€¼ç¨³å®šæ€§**ï¼šFP32 ä¸»æƒé‡ç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹çš„æ•°å€¼ç¨³å®šæ€§
4. **æ€§èƒ½å¹³è¡¡**ï¼šå‰å‘ä¼ æ’­ä½¿ç”¨ FP16 æé€Ÿï¼Œå‚æ•°æ›´æ–°ä½¿ç”¨ FP32 ä¿ç²¾åº¦

### Q4: å¦‚ä½•è¯Šæ–­å’Œè§£å†³è®­ç»ƒä¸­çš„ NaN/Inf é—®é¢˜ï¼Ÿ

**ç­”æ¡ˆï¼š**
1. **æ£€æµ‹ç­–ç•¥**ï¼š
   - ç›‘æ§æŸå¤±å€¼ã€æ¢¯åº¦èŒƒæ•°å˜åŒ–
   - å®šæœŸæ£€æŸ¥å‚æ•°å’Œæ¢¯åº¦çš„ NaN/Inf æ•°é‡
   - ä½¿ç”¨ `torch.autograd.detect_anomaly()` è¿›è¡Œç²¾ç¡®å®šä½

2. **å¸¸è§åŸå› **ï¼š
   - å­¦ä¹ ç‡è¿‡é«˜å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸
   - Loss scaling å› å­ä¸å½“
   - æ•°æ®ä¸­å­˜åœ¨å¼‚å¸¸å€¼
   - ç½‘ç»œæ¶æ„é—®é¢˜ï¼ˆå¦‚æ²¡æœ‰æ®‹å·®è¿æ¥ï¼‰

3. **è§£å†³æ–¹æ³•**ï¼š
   - é™ä½å­¦ä¹ ç‡
   - åº”ç”¨æ¢¯åº¦è£å‰ª
   - è°ƒæ•´ loss scaling ç­–ç•¥
   - å›æ»šåˆ°ç¨³å®šæ£€æŸ¥ç‚¹

### Q5: FP8 è®­ç»ƒçš„æŒ‘æˆ˜å’Œå‰æ™¯å¦‚ä½•ï¼Ÿ

**ç­”æ¡ˆï¼š**
1. **æŠ€æœ¯æŒ‘æˆ˜**ï¼š
   - éœ€è¦ç²¾å¿ƒè®¾è®¡çš„é‡åŒ–ç­–ç•¥
   - æ›´å¤æ‚çš„æ•°å€¼ç¨³å®šæ€§é—®é¢˜
   - è½¯ä»¶ç”Ÿæ€å°šä¸æˆç†Ÿ

2. **ç¡¬ä»¶è¦æ±‚**ï¼š
   - ç›®å‰ä¸»è¦æ”¯æŒ H100ï¼Œç¡¬ä»¶æ™®åŠåº¦æœ‰é™
   - éœ€è¦ä¸“é—¨çš„è½¯ä»¶åº“ï¼ˆå¦‚ Transformer Engineï¼‰

3. **æ½œåœ¨ä¼˜åŠ¿**ï¼š
   - ç†è®ºä¸Šå¯ä»¥å¤§å¹…æå‡è®­ç»ƒé€Ÿåº¦
   - æ˜¾è‘—é™ä½å†…å­˜å ç”¨
   - ä¸ºæ›´å¤§æ¨¡å‹è®­ç»ƒæä¾›å¯èƒ½

4. **å‘å±•å‰æ™¯**ï¼š
   - éšç€ç¡¬ä»¶æ™®åŠä¼šé€æ­¥æ¨å¹¿
   - è½¯ä»¶å·¥å…·é“¾å°†é€æ­¥å®Œå–„
   - å¯èƒ½æˆä¸ºæœªæ¥è¶…å¤§æ¨¡å‹è®­ç»ƒçš„æ ‡é…

---

## ç²¾åº¦æ ¼å¼å¯¹æ¯”å›¾

```mermaid
graph LR
    subgraph FP32["FP32 (32-bit)"]
        F32S[1 sign] --- F32E[8 exp] --- F32M[23 mantissa]
    end
    subgraph FP16["FP16 (16-bit)"]
        F16S[1 sign] --- F16E[5 exp] --- F16M[10 mantissa]
    end
    subgraph BF16["BF16 (16-bit)"]
        BFS[1 sign] --- BFE[8 exp] --- BFM[7 mantissa]
    end
    subgraph FP8["FP8-E4M3 (8-bit)"]
        F8S[1 sign] --- F8E[4 exp] --- F8M[3 mantissa]
    end
    
    style FP32 fill:#8f8
    style BF16 fill:#ff8
    style FP16 fill:#8bf
    style FP8 fill:#f8f
```

> å…³é”®æ´å¯Ÿï¼šBF16 çš„ 8 ä½æŒ‡æ•°ä¸ FP32 ç›¸åŒ â†’ èŒƒå›´ä¸€è‡´ï¼ˆ$\pm 3.4 \times 10^{38}$ï¼‰ï¼Œæ— éœ€ Loss Scalingã€‚

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **BF16 è®­ç»ƒ**ï¼ˆA100/H100ï¼‰ï¼šå¤§å¤šæ•° LLM è®­ç»ƒçš„é»˜è®¤é€‰æ‹©ï¼Œæ— éœ€ Loss Scalingï¼Œä»£ç æœ€ç®€
- **FP16 + AMP**ï¼ˆV100 ç­‰è€å¡ï¼‰ï¼šå¿…é¡»ç”¨ GradScalerï¼Œæ³¨æ„ LayerNorm/Softmax ä¿æŒ FP32
- **FP8 è®­ç»ƒ**ï¼ˆH100ï¼‰ï¼šTransformer Engine æ”¯æŒï¼Œç†è®º 2x åŠ é€Ÿä½†éœ€è¦ä»”ç»†è°ƒä¼˜

### å·¥ç¨‹å®ç°è¦ç‚¹
- **å“ªäº›å±‚å¿…é¡»ä¿æŒ FP32**ï¼šLayerNormã€Embeddingã€Softmaxã€Loss è®¡ç®—ï¼ˆæ•°å€¼æ•æ„Ÿï¼‰
- **BF16 çš„å‘**ï¼šç²¾åº¦åªæœ‰ ~2-3 ä½åè¿›åˆ¶ï¼Œæ¢¯åº¦ç´¯ç§¯æ—¶è¯¯å·®å¤§â€”â€”å»ºè®® FP32 ä¼˜åŒ–å™¨çŠ¶æ€
- **NaN/Inf æ’æŸ¥æµç¨‹**ï¼šå…ˆæ£€æŸ¥ Loss Scaling æ˜¯å¦å¤ªå¤§ â†’ æ£€æŸ¥æ•°æ®å¼‚å¸¸ â†’ æ£€æŸ¥å­¦ä¹ ç‡ â†’ å¼€ `detect_anomaly()`

### é¢è¯•é«˜é¢‘é—®æ³•
- **Q: BF16 æ¯” FP16 å¥½åœ¨å“ªï¼Ÿ**
  A: èŒƒå›´ç›¸åŒï¼ˆ8ä½æŒ‡æ•° vs 5ä½æŒ‡æ•°ï¼‰ï¼ŒBF16 ä¸éœ€è¦ Loss Scalingï¼ˆå› ä¸ºä¸ä¼šæº¢å‡ºï¼‰ï¼Œè®­ç»ƒæ›´ç¨³å®šã€‚ä»£ä»·æ˜¯ç²¾åº¦æ›´ä½ï¼ˆ7ä½å°¾æ•° vs 10ä½ï¼‰ï¼Œä½†å®éªŒè¡¨æ˜å¯¹ LLM è´¨é‡å½±å“æå°ã€‚
- **Q: ä¸ºä»€ä¹ˆéœ€è¦ FP32 Master Weightsï¼Ÿ**
  A: ä¼˜åŒ–å™¨æ›´æ–°é‡é€šå¸¸å¾ˆå°ï¼ˆ$\sim 10^{-7}$ï¼‰ï¼ŒFP16 çš„ç²¾åº¦ eps=$10^{-3}$ æ— æ³•è¡¨ç¤º â†’ å°æ›´æ–°è¢«æˆªæ–­ â†’ è®­ç»ƒåœæ»ã€‚FP32 master weights åœ¨é«˜ç²¾åº¦ä¸Šç´¯ç§¯æ›´æ–°åå†æˆªæ–­å› FP16ã€‚

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- æ··åˆç²¾åº¦æ˜¯**é›¶æˆæœ¬çš„ 2x åŠ é€Ÿ**â€”â€”å‡ ä¹æ‰€æœ‰åœºæ™¯éƒ½åº”è¯¥ä½¿ç”¨ï¼Œä¸ç”¨ç­‰äºæµªè´¹ GPU
- BF16 çš„å‡ºç°ä½¿å¾—æ··åˆç²¾åº¦ä»"éœ€è¦å°å¿ƒè°ƒå‚"å˜æˆäº†"å¼€ç®±å³ç”¨"

### æœªè§£é—®é¢˜ä¸å±€é™
- FP8 è®­ç»ƒçš„è´¨é‡æŸå¤±è¾¹ç•Œå°šæœªå®Œå…¨æ¸…æ¥šâ€”â€”ä¸åŒä»»åŠ¡/æ¨¡å‹å¤§å°çš„å®¹å¿åº¦ä¸åŒ
- è¶…å¤§æ¨¡å‹ï¼ˆ>100Bï¼‰ä¸­ï¼ŒBF16 çš„ä½ç²¾åº¦åœ¨é•¿è·ç¦»æ¢¯åº¦ä¼ æ’­æ—¶å¯èƒ½ç§¯ç´¯æ˜¾è‘—è¯¯å·®
- Loss Scaling çš„åŠ¨æ€è°ƒæ•´ç­–ç•¥ä»ç„¶æ˜¯å¯å‘å¼çš„ï¼Œç¼ºä¹ç†è®ºæœ€ä¼˜è§£

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- FP8 + [[AI/LLM/Infra/åˆ†å¸ƒå¼è®­ç»ƒ|åˆ†å¸ƒå¼è®­ç»ƒ]]ï¼šé€šä¿¡é‡ä¹Ÿå¯ä»¥ç”¨ FP8ï¼ŒAllReduce å¸¦å®½éœ€æ±‚ç›´æ¥å‡åŠ
- [[AI/LLM/Infra/GPU æ˜¾å­˜è®¡ç®—æŒ‡å—|æ˜¾å­˜è®¡ç®—]]ä¸­çš„ $16\Phi$ å…¬å¼ä¼šéšç²¾åº¦ç­–ç•¥å˜åŒ–â€”â€”çº¯ FP8 è®­ç»ƒç†è®ºä¸Šå¯é™è‡³ $8\Phi$
- æ··åˆç²¾åº¦ + ç¨€ç–æ€§ï¼ˆSparse + Quantizedï¼‰å¯èƒ½æ˜¯ä¸‹ä¸€ä¸ªæ•ˆç‡çªç ´ç‚¹

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Mixed Precision Training](https://arxiv.org/abs/1710.03740) â€” Micikevicius et al.ï¼Œæ··åˆç²¾åº¦è®­ç»ƒçš„å¥ åŸºè®ºæ–‡ â­â­â­â­â­
- [8-bit Optimizers via Block-wise Quantization](https://arxiv.org/abs/2110.02861) â€” bitsandbytes çš„ç†è®ºåŸºç¡€ï¼Œ8-bit Adam

### æ·±åº¦è§£è¯»
- [NVIDIA Mixed Precision Training Guide](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/) â€” å®˜æ–¹æœ€ä½³å®è·µ â­â­â­â­â­
- [BFloat16: The Secret to High Performance on Cloud TPUs](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus) â€” Google ä»‹ç» BF16 çš„åŠ¨æœº

### å®è·µèµ„æº
- [PyTorch AMP Tutorial](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html) â€” å®˜æ–¹ AMP æ•™ç¨‹
- [NVIDIA Apex](https://github.com/NVIDIA/apex) â€” O0/O1/O2/O3 æ··åˆç²¾åº¦ç­‰çº§
- [NVIDIA Transformer Engine](https://github.com/NVIDIA/TransformerEngine) â€” H100 FP8 è®­ç»ƒå·¥å…·

---

## See Also

- [[AI/LLM/Infra/åˆ†å¸ƒå¼è®­ç»ƒ|åˆ†å¸ƒå¼è®­ç»ƒ]] â€” æ··åˆç²¾åº¦ä¸åˆ†å¸ƒå¼å¹¶è¡Œçš„ç»„åˆï¼šBF16 å‡å°‘é€šä¿¡é‡ï¼ŒZeRO çš„ä¼˜åŒ–å™¨çŠ¶æ€ä»éœ€ FP32
- [[AI/LLM/Infra/GPU æ˜¾å­˜è®¡ç®—æŒ‡å—|GPU æ˜¾å­˜è®¡ç®—æŒ‡å—]] â€” ç²¾åº¦é€‰æ‹©ç›´æ¥å½±å“æ˜¾å­˜å…¬å¼ä¸­çš„å­—èŠ‚æ•°ï¼šFP32â†’BF16 å‚æ•°æ˜¾å­˜å‡åŠ
- [[AI/Foundations/Training/æ··åˆç²¾åº¦è®­ç»ƒ|æ··åˆç²¾åº¦è®­ç»ƒ(Foundationsç‰ˆ)]] â€” æœ¬æ–‡çš„é¢è¯•ç²¾ç®€ç‰ˆ
- [[AI/LLM/Infra/DeepSpeed|DeepSpeed]] â€” ZeRO + æ··åˆç²¾åº¦çš„å·¥ç¨‹æ•´åˆ
- [[AI/LLM/Architecture/Multi-Head Latent Attention|Multi-Head Latent Attention]] â€” MLA + FP8 KV Cache æ˜¯æ¨ç†æ˜¾å­˜ä¼˜åŒ–çš„å‰æ²¿ç»„åˆ