---
title: "Ray"
brief: "UC Berkeley çš„åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œæ ¸å¿ƒæŠ½è±¡æ˜¯ Task/Actor/Object Storeâ€”â€”æŠŠåˆ†å¸ƒå¼å˜æˆå‡½æ•°è°ƒç”¨ï¼›åœ¨ LLM post-trainingï¼ˆRLHFï¼‰ä¸­çš„æ€æ‰‹çº§åº”ç”¨æ˜¯ç¼–æ’å¼‚æ„ workerï¼ˆActor/Critic/Reward åˆ†å¸ƒåœ¨ä¸åŒ GPU ç»„ï¼‰ã€‚"
type: concept
domain: ai/llm/infra
created: "2026-02-13"
updated: "2026-02-22"
tags:
  - ai/llm/infra
  - type/concept
status: complete
sources:
  - "Ray: A Distributed Framework for Emerging AI Applications â€” arXiv:1712.05889"
  - "https://docs.ray.io/en/latest/"
  - "https://github.com/ray-project/ray"
related:
  - "[[AI/LLM/Infra/Megatron-LM|Megatron-LM]]"
  - "[[AI/LLM/Frameworks/verl/HybridFlow|HybridFlow]]"
  - "[[AI/LLM/Infra/æ¨¡å‹å¹¶è¡Œç­–ç•¥|æ¨¡å‹å¹¶è¡Œç­–ç•¥]]"
---
# Ray

> UC Berkeley å‡ºå“çš„åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ï¼Œä» RL è®­ç»ƒèµ·å®¶ï¼Œç°åœ¨æ˜¯ LLM è®­ç»ƒ/æ¨ç†/serving çš„åŸºç¡€è®¾æ–½ä¹‹ä¸€ã€‚

æ–‡æ¡£ï¼šhttps://docs.ray.io/en/latest/ray-overview/index.html

## æ ¸å¿ƒæŠ½è±¡

Ray çš„è®¾è®¡å“²å­¦æ˜¯**æŠŠåˆ†å¸ƒå¼å˜æˆå‡½æ•°è°ƒç”¨**ã€‚ä¸‰ä¸ªæ ¸å¿ƒ primitiveï¼š

> æ¥æºï¼šMoritz et al., "Ray: A Distributed Framework for Emerging AI Applications" arXiv:1712.05889, Sec. 3

### Remote Functions (Tasks)

```python
import ray

@ray.remote
def train_step(batch):
    # è¿™ä¸ªå‡½æ•°ä¼šåœ¨é›†ç¾¤ä¸­æŸä¸ª worker ä¸Šæ‰§è¡Œ
    return model.forward(batch)

# å¼‚æ­¥è°ƒç”¨ï¼Œè¿”å› ObjectRefï¼ˆfutureï¼‰
futures = [train_step.remote(b) for b in batches]
results = ray.get(futures)  # é˜»å¡ç­‰å¾…ç»“æœ
```

### Remote Classes (Actors)

```python
@ray.remote
class PPOTrainer:
    def __init__(self, model_config):
        self.model = load_model(model_config)
    
    def train_step(self, batch):
        return self.model.update(batch)

# åˆ›å»º actorï¼Œæœ‰çŠ¶æ€ï¼Œç”Ÿå‘½å‘¨æœŸè·¨å¤šæ¬¡è°ƒç”¨
trainer = PPOTrainer.remote(config)
result = ray.get(trainer.train_step.remote(batch))
```

### Object Store (Plasma)

åˆ†å¸ƒå¼å…±äº«å†…å­˜ï¼Œé›¶æ‹·è´è¯»å–ã€‚`ray.put()` å†™å…¥ï¼Œ`ray.get()` è¯»å–ã€‚å¯¹äºå¤§ tensor ä¼ è¾“éå¸¸å…³é”® â€” é¿å…åºåˆ—åŒ–/ååºåˆ—åŒ–å¼€é”€ã€‚

## Ray ç”Ÿæ€ä¸ LLM

| ç»„ä»¶ | ç”¨é€” | å¤‡æ³¨ |
|------|------|------|
| **Ray Core** | åˆ†å¸ƒå¼åŸè¯­ | Task, Actor, Object Store |
| **Ray Train** | åˆ†å¸ƒå¼è®­ç»ƒ | å°è£… PyTorch DDP/FSDP/DeepSpeed |
| **Ray Serve** | æ¨¡å‹æœåŠ¡ | æ”¯æŒåŠ¨æ€ batching |
| **Ray Tune** | è¶…å‚æœç´¢ | å’Œè®­ç»ƒæ¡†æ¶æ— ç¼é›†æˆ |
| **Ray Data** | æ•°æ®å¤„ç† | æµå¼ ETLï¼Œæ›¿ä»£ Spark åœ¨ ML åœºæ™¯çš„è§’è‰² |

## åœ¨ RLHF/RL è®­ç»ƒä¸­çš„è§’è‰²

Ray åœ¨ LLM post-training ä¸­çš„æ€æ‰‹çº§åº”ç”¨æ˜¯**ç¼–æ’å¼‚æ„ worker**ã€‚ä¸€ä¸ª RLHF pipeline éœ€è¦ï¼š

```
Actor Model (ç”Ÿæˆ) â†’ Reward Model (æ‰“åˆ†) â†’ Critic Model (ä¼°å€¼) â†’ Actor Update (è®­ç»ƒ)
```

è¿™äº›æ¨¡å‹å¯èƒ½åˆ†å¸ƒåœ¨ä¸åŒçš„ GPU ç»„ä¸Šï¼ŒRay Actor å¤©ç„¶é€‚åˆè¿™ç§ç¼–æ’ï¼š

```python
@ray.remote(num_gpus=4)
class ActorWorker:
    """è´Ÿè´£ rollout ç”Ÿæˆ"""
    pass

@ray.remote(num_gpus=2) 
class CriticWorker:
    """è´Ÿè´£ value estimation"""
    pass

@ray.remote(num_gpus=1)
class RewardWorker:
    """è´Ÿè´£ reward è®¡ç®—"""
    pass
```

verl å°±æ˜¯åŸºäº Ray Actor æ¥ç¼–æ’æ•´ä¸ª RL è®­ç»ƒæµç¨‹çš„ï¼Œå‚è§ [[AI/LLM/Frameworks/verl/HybridFlow|HybridFlow]]ã€‚

> æ¥æºï¼šverl è®ºæ–‡ "HybridFlow: A Flexible and Efficient RLHF Framework" arXiv:2409.19256 â€” Ray Actor ç¼–æ’ RLHF çš„å·¥ä¸šå®è·µ

## Placement Groupï¼šèµ„æºéš”ç¦»

```python
from ray.util.placement_group import placement_group

# ç¡®ä¿ 4 å¼ å¡åœ¨åŒä¸€èŠ‚ç‚¹ï¼ˆTP éœ€è¦ NVLinkï¼‰
pg = placement_group([{"GPU": 1}] * 4, strategy="STRICT_PACK")
ray.get(pg.ready())

# åœ¨ placement group å†…åˆ›å»º actor
worker = TrainWorker.options(
    scheduling_strategy=PlacementGroupSchedulingStrategy(
        placement_group=pg
    )
).remote()
```

è¿™ä¸ªå¯¹å¤§æ¨¡å‹è®­ç»ƒç‰¹åˆ«é‡è¦ï¼šTP ç»„å¿…é¡»åœ¨åŒä¸€èŠ‚ç‚¹ï¼ŒPP ç»„å¯ä»¥è·¨èŠ‚ç‚¹ï¼Œ`STRICT_PACK` vs `SPREAD` ç›´æ¥å†³å®šäº†é€šä¿¡æ•ˆç‡ã€‚

## å®ç”¨ç»éªŒ

1. **Ray çš„ overhead ä¸å¯å¿½è§†**ï¼šæ¯æ¬¡ remote call æœ‰ ~100Î¼s çš„è°ƒåº¦å¼€é”€ã€‚çƒ­è·¯å¾„ä¸Šä¸è¦é¢‘ç¹ `.remote()`
2. **Object Store å¤§å°è¦é…å¤Ÿ**ï¼šé»˜è®¤æ˜¯ç³»ç»Ÿå†…å­˜çš„ 30%ï¼Œè®­ç»ƒå¤§æ¨¡å‹æ—¶ç»å¸¸ä¸å¤Ÿï¼Œç”¨ `--object-store-memory` è°ƒ
3. **GCS (Global Control Store) æ˜¯å•ç‚¹**ï¼šhead node æŒ‚äº†å…¨æŒ‚ã€‚ç”Ÿäº§ç¯å¢ƒè€ƒè™‘ Ray HAï¼ˆRedis-based GCSï¼‰
4. **å’Œ NCCL çš„å…³ç³»**ï¼šRay ç®¡ç¼–æ’å’Œæ•°æ®æ¬è¿ï¼ŒGPU é—´çš„ collective communication è¿˜æ˜¯èµ° NCCLã€‚Ray ä¸æ›¿ä»£ NCCL

## Ray vs çº¯ torchrun

| ç»´åº¦ | torchrun | Ray |
|------|----------|-----|
| ç¼–ç¨‹æ¨¡å‹ | SPMDï¼ˆæ‰€æœ‰è¿›ç¨‹è·‘åŒä¸€ä»½ä»£ç ï¼‰ | MPMDï¼ˆä¸åŒ actor è·‘ä¸åŒä»£ç ï¼‰ |
| é€‚ç”¨åœºæ™¯ | çº¯é¢„è®­ç»ƒ / å•æ¨¡å‹è®­ç»ƒ | å¤šæ¨¡å‹äº¤äº’ï¼ˆRLHFã€MCTSã€multi-agentï¼‰ |
| è°ƒåº¦å¼€é”€ | æä½ | æ¯æ¬¡ `.remote()` ~100Î¼s |
| å¼‚æ„ worker | ä¸æ”¯æŒ | å¤©ç„¶æ”¯æŒ |

çº¯é¢„è®­ç»ƒç”¨ torchrun å°±å¤Ÿäº†ã€‚ä¸€æ—¦æ¶‰åŠ **å¤šæ¨¡å‹äº¤äº’**ï¼ˆRLHFã€MCTSã€multi-agentï¼‰ï¼ŒRay çš„ä¼˜åŠ¿å°±ä½“ç°å‡ºæ¥äº†ã€‚

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Ray: A Distributed Framework for Emerging AI Applications](https://arxiv.org/abs/1712.05889) â€” Ray çš„æ¶æ„è®¾è®¡ä¸æ€§èƒ½åˆ†æ
- [Anyscale Blog: Scaling LLM Training with Ray](https://www.anyscale.com/blog) â€” å·¥ä¸šçº§ Ray é›†ç¾¤ç®¡ç†ç»éªŒ

### æ·±åº¦è§£è¯»
- [Ray å®˜æ–¹æ–‡æ¡£ â€” Ray Core](https://docs.ray.io/en/latest/ray-core/walkthrough.html) â€” Task/Actor/Object Store å®Œæ•´æ•™ç¨‹ â­â­â­â­
- [Ray Train æ–‡æ¡£](https://docs.ray.io/en/latest/train/train.html) â€” å°è£… PyTorch DDP/FSDP/DeepSpeed çš„åˆ†å¸ƒå¼è®­ç»ƒ

### å®è·µèµ„æº
- [ray-project/ray GitHub](https://github.com/ray-project/ray) â€” å®˜æ–¹ä»“åº“ï¼ˆ30K+ starsï¼‰
- [vllm-project/vllm](https://github.com/vllm-project/vllm) â€” åŸºäº Ray åšåˆ†å¸ƒå¼æ¨ç†çš„é«˜æ€§èƒ½æ¡†æ¶

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **RLHF å¼‚æ„ç¼–æ’**ï¼šActor/Critic/Reward æ¨¡å‹åˆ†å¸ƒåœ¨ä¸åŒ GPU ç»„ï¼ŒRay Actor å¤©ç„¶é€‚é…
- **åˆ†å¸ƒå¼æ¨ç†**ï¼švLLM åŸºäº Ray ç¼–æ’å¤š GPU æ¨ç†ï¼Œå®ç°åŠ¨æ€ batching
- **è¶…å‚æœç´¢**ï¼šRay Tune æ”¯æŒ ASHAã€PBT ç­‰é«˜æ•ˆæœç´¢ç®—æ³•

### å·¥ç¨‹å®ç°è¦ç‚¹
- **Placement Group**ï¼šTP ç»„ç”¨ `STRICT_PACK`ï¼ˆåŒä¸€èŠ‚ç‚¹ NVLinkï¼‰ï¼ŒPP ç»„å¯ç”¨ `SPREAD`
- **Object Store å¤§å°**ï¼šé»˜è®¤ç³»ç»Ÿå†…å­˜ 30%ï¼Œè®­ç»ƒå¤§æ¨¡å‹æ—¶ç”¨ `--object-store-memory` æ‰©å¤§
- **GCS å•ç‚¹é—®é¢˜**ï¼šç”Ÿäº§ç¯å¢ƒå¯ç”¨ Ray HAï¼ˆRedis-backed GCSï¼‰ï¼Œå¦åˆ™ head node æŒ‚å…¨æŒ‚
- **çƒ­è·¯å¾„é¿å… `.remote()`**ï¼šæ¯æ¬¡è°ƒåº¦ ~100Î¼s å¼€é”€ï¼Œåœ¨ inner loop ä¸­ä¼šæˆä¸ºç“¶é¢ˆ

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: Ray å’Œ NCCL æ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ
  A: Ray ç®¡ç¼–æ’å’Œæ•°æ®æ¬è¿ï¼ˆè°ƒåº¦ Task/Actorã€Object Store ä¼ è¾“ï¼‰ï¼ŒGPU é—´çš„ collective communicationï¼ˆAllReduceã€AllGatherï¼‰èµ° NCCLã€‚Ray ä¸æ›¿ä»£ NCCLï¼Œä¸¤è€…äº’è¡¥ã€‚

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- Ray æ˜¯ RLHF/RL è®­ç»ƒçš„"èƒ¶æ°´å±‚"â€”â€”ç†è§£ Actor æ¨¡å‹å’Œ Placement Group æ˜¯ä½¿ç”¨ verl/OpenRLHF çš„å‰æ
- Ray çš„ä»·å€¼ä¸åœ¨"å¿«"ï¼Œè€Œåœ¨"çµæ´»"â€”â€”èƒ½ç¼–æ’å¤æ‚çš„å¤šæ¨¡å‹å·¥ä½œæµï¼ˆè¿™æ˜¯ torchrun åšä¸åˆ°çš„ï¼‰

### æœªè§£é—®é¢˜ä¸å±€é™
- Ray çš„è°ƒåº¦å¼€é”€åœ¨è¶…å¤§è§„æ¨¡ï¼ˆ10K+ workerï¼‰ä¸‹æ˜¯å¦ä¼šæˆä¸ºç“¶é¢ˆï¼Ÿ
- Ray å¯¹æ•…éšœæ¢å¤çš„æ”¯æŒï¼ˆcheckpoint + actor restartï¼‰åœ¨é•¿æ—¶é—´ RL è®­ç»ƒä¸­çš„å¯é æ€§å¾…éªŒè¯

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- å¦‚æœæŠŠ [[AI/LLM/Frameworks/verl/HybridFlow|HybridFlow]] çš„ SPMD + MPMD æ··åˆæ€è·¯æ¨å¹¿ï¼Œèƒ½å¦æ„å»ºä¸€ä¸ª"é€šç”¨ RL ç¼–æ’æ ‡å‡†"ï¼Ÿ
- Ray Serve + vLLM çš„ç»„åˆèƒ½å¦æˆä¸º LLM Serving çš„äº‹å®æ ‡å‡†ï¼Ÿ

## ç›¸å…³

> ğŸ”— See also: [[AI/LLM/Frameworks/verl/HybridFlow|HybridFlow]] â€” åŸºäº Ray Actor çš„ RLHF æ··åˆç¼–æ’
> ğŸ”— See also: [[AI/LLM/Infra/Megatron-LM|Megatron-LM]] â€” Ray ç¼–æ’ + Megatron åšæ¨¡å‹å¹¶è¡Œæ˜¯ verl çš„æ ¸å¿ƒæ¶æ„
> ğŸ”— See also: [[AI/LLM/Infra/æ¨¡å‹å¹¶è¡Œç­–ç•¥|æ¨¡å‹å¹¶è¡Œç­–ç•¥]] â€” DP/TP/PP å¦‚ä½•ä¸ Ray é…åˆ

- [[AI/LLM/Infra/åˆ†å¸ƒå¼è®­ç»ƒ|åˆ†å¸ƒå¼è®­ç»ƒ]] â€” åˆ†å¸ƒå¼è®­ç»ƒå…¨æ™¯
- [[AI/LLM/Frameworks/verl/è®­ç»ƒåç«¯|verl è®­ç»ƒåç«¯]] â€” Ray åœ¨ verl ä¸­çš„åº”ç”¨
- [[AI/LLM/Frameworks/verl/verl æ¦‚è¿°|verl æ¦‚è¿°]]
- [[AI/LLM/Frameworks/OpenRLHF/OpenRLHF|OpenRLHF]]
