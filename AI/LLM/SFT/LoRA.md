---
title: "LoRA"
brief: "LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯å¾®è½¯æå‡ºçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œæ ¸å¿ƒå‡è®¾æ˜¯æƒé‡æ›´æ–°çŸ©é˜µæ˜¯ä½ç§©çš„ï¼Œç”¨ä¸¤ä¸ªå°çŸ©é˜µ BA è¿‘ä¼¼ Î”Wï¼Œå¯è®­ç»ƒå‚æ•°å‹ç¼© 200+ å€ä¸”æ— é¢å¤–æ¨ç†å»¶è¿Ÿï¼›æ˜¯å½“å‰å¤§æ¨¡å‹å¾®è°ƒçš„äº‹å®æ ‡å‡†ï¼Œç†è§£ rank/alpha/target modules é€‰æ‹©æ˜¯å·¥ç¨‹è½åœ°çš„å…³é”®"
type: paper
domain: ai/llm/sft
created: "2026-02-13"
updated: "2026-02-22"
tags:
  - ai/llm/sft
  - type/paper
  - interview/hot
status: complete
sources:
  - "LoRA arXiv:2106.09685 (Hu et al., 2021)"
  - "QLoRA arXiv:2305.14314 (Dettmers et al., 2023)"
  - "DoRA arXiv:2402.09353 (Liu et al., 2024)"
  - "rsLoRA arXiv:2312.03732 (Kalajdzievski, 2023)"
related:
  - "[[AI/LLM/Frameworks/Unsloth/Unsloth æ¦‚è¿°]]"
  - "[[AI/LLM/Architecture/LLaMA]]"
  - "[[AI/LLM/SFT/SFT-TRLå®è·µ]]"
  - "[[AI/LLM/Infra/åˆ†å¸ƒå¼è®­ç»ƒ]]"
  - "[[AI/LLM/Frameworks/Unsloth/é‡åŒ–]]"
  - "[[AI/LLM/SFT/SFT åŸç†]]"
  - "[[AI/Foundations/Math/çŸ©é˜µåˆ†è§£]]"
  - "[[AI/LLM/Frameworks/TRL/TRL æ¦‚è¿°]]"
---

# LoRA

> **Brief**ï¼šLoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯å¾®è½¯æå‡ºçš„å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼Œæ ¸å¿ƒå‡è®¾æ˜¯é¢„è®­ç»ƒæ¨¡å‹çš„æƒé‡æ›´æ–°çŸ©é˜µ $\Delta W$ æ˜¯ä½ç§©çš„ï¼Œç”¨ä¸¤ä¸ªå°çŸ©é˜µ $BA$ è¿‘ä¼¼ï¼Œå¯è®­ç»ƒå‚æ•°å‹ç¼© 200+ å€ä¸”æ— é¢å¤–æ¨ç†å»¶è¿Ÿã€‚æ˜¯å½“å‰å¤§æ¨¡å‹å¾®è°ƒçš„äº‹å®æ ‡å‡†ã€‚
>
> æ¥æºï¼šHu et al. arXiv:2106.09685

---

LoRAï¼ˆLow-Rank Adaptation of Large Language Modelsï¼‰æ˜¯å¾®è½¯åœ¨ 2021 å¹´æå‡ºçš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ã€‚å®ƒçš„æ ¸å¿ƒæ€æƒ³æå…¶ç®€æ´ï¼š**å¤§æ¨¡å‹çš„æƒé‡æ›´æ–°æ˜¯ä½ç§©çš„**ï¼Œå› æ­¤å¯ä»¥ç”¨ä¸¤ä¸ªå°çŸ©é˜µçš„ä¹˜ç§¯æ¥è¿‘ä¼¼å…¨é‡å¾®è°ƒçš„æ•ˆæœã€‚è¿™ä¸ªçœ‹ä¼¼ç®€å•çš„æƒ³æ³•ï¼Œå½»åº•æ”¹å˜äº†å¤§æ¨¡å‹å¾®è°ƒçš„ç»æµå­¦ã€‚

> æ¥æºï¼šLoRA arXiv:2106.09685, Abstract

## åŠ¨æœºï¼šä¸ºä»€ä¹ˆéœ€è¦ LoRA

å…¨é‡å¾®è°ƒï¼ˆFull Fine-Tuningï¼‰ä¸€ä¸ª 7B æ¨¡å‹éœ€è¦ï¼š
- å­˜å‚¨ï¼š7B Ã— 4 bytesï¼ˆfp32ï¼‰= 28GB ä»…æ¨¡å‹æƒé‡
- è®­ç»ƒï¼šåŠ ä¸Šä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdam éœ€è¦ 2 å€ï¼‰ã€æ¢¯åº¦ï¼Œè‡³å°‘éœ€è¦ ~112GB æ˜¾å­˜
- éƒ¨ç½²ï¼šæ¯ä¸ªä»»åŠ¡ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹å‰¯æœ¬

è¿™åœ¨å®é™…å·¥ç¨‹ä¸­å‡ ä¹ä¸å¯è¡Œï¼Œå°¤å…¶æ˜¯å½“ä½ éœ€è¦ä¸ºå¤šä¸ªä»»åŠ¡åˆ†åˆ«å¾®è°ƒæ—¶ã€‚

åœ¨ LoRA ä¹‹å‰ï¼Œå·²æœ‰ä¸€äº›å‚æ•°é«˜æ•ˆå¾®è°ƒæ–¹æ³•ï¼š
- **Adapter**ï¼šåœ¨æ¯ä¸ª Transformer å±‚ä¸­æ’å…¥å°çš„ç“¶é¢ˆæ¨¡å—ï¼Œä½†ä¼šå¼•å…¥æ¨ç†å»¶è¿Ÿï¼ˆæ¥æºï¼šHoulsby et al. arXiv:1902.00751ï¼‰
- **Prefix Tuning**ï¼šåœ¨è¾“å…¥å‰åŠ å¯å­¦ä¹ çš„ virtual tokensï¼Œä½†å‹ç¼©äº†æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦ï¼ˆæ¥æºï¼šLi & Liang arXiv:2101.00190ï¼‰
- **BitFit**ï¼šåªå¾®è°ƒ bias å‚æ•°ï¼Œå¤ªå—é™ï¼ˆæ¥æºï¼šZaken et al. arXiv:2106.10199ï¼‰

LoRA çš„ä¼˜åŠ¿åœ¨äºï¼š**æ²¡æœ‰é¢å¤–æ¨ç†å»¶è¿Ÿ**ï¼Œå› ä¸ºè®­ç»ƒåå¯ä»¥å°† LoRA æƒé‡åˆå¹¶å›åŸå§‹æƒé‡ã€‚

> æ¥æºï¼šLoRA arXiv:2106.09685, Sec. 1 å¯¹æ¯”äº†ä¸Šè¿°æ–¹æ³•çš„ä¼˜åŠ£

## æ ¸å¿ƒåŸç†

å¯¹äºé¢„è®­ç»ƒæƒé‡çŸ©é˜µ $W_0 \in \mathbb{R}^{d \times k}$ï¼ŒLoRA å°†æƒé‡æ›´æ–°åˆ†è§£ä¸ºä¸¤ä¸ªä½ç§©çŸ©é˜µçš„ä¹˜ç§¯ï¼š

$$W = W_0 + \Delta W = W_0 + BA$$

å…¶ä¸­ $B \in \mathbb{R}^{d \times r}$ï¼Œ$A \in \mathbb{R}^{r \times k}$ï¼Œ$r \ll \min(d, k)$ã€‚

è®­ç»ƒæ—¶å†»ç»“ $W_0$ï¼Œåªè®­ç»ƒ $A$ å’Œ $B$ã€‚å‰å‘ä¼ æ’­å˜ä¸ºï¼š

$$h = W_0 x + \frac{\alpha}{r} BAx$$

å…³é”®è®¾è®¡ï¼š
- **$A$ ç”¨ Kaiming åˆå§‹åŒ–ï¼Œ$B$ åˆå§‹åŒ–ä¸ºé›¶**ï¼šç¡®ä¿è®­ç»ƒå¼€å§‹æ—¶ $\Delta W = 0$ï¼Œä¸æ”¹å˜åŸå§‹æ¨¡å‹è¡Œä¸º
- **ç¼©æ”¾å› å­ $\alpha / r$**ï¼šæ§åˆ¶ LoRA æƒé‡çš„å½±å“ç¨‹åº¦ã€‚$\alpha$ æ˜¯ä¸€ä¸ªå¸¸æ•°ï¼Œé€šå¸¸è®¾ä¸º $r$ æˆ– $2r$

> æ¥æºï¼šLoRA arXiv:2106.09685, Sec. 4.1, Eq. (3)

### ä¸ºä»€ä¹ˆä½ç§©å‡è®¾æˆç«‹ï¼Ÿ

è®ºæ–‡çš„å…³é”®å‘ç°ï¼šé¢„è®­ç»ƒæ¨¡å‹åœ¨é€‚é…åˆ°ä¸‹æ¸¸ä»»åŠ¡æ—¶ï¼Œæƒé‡å˜åŒ–çŸ©é˜µ $\Delta W$ çš„**æœ¬å¾ç»´åº¦ï¼ˆintrinsic dimensionï¼‰** å¾ˆä½ã€‚ç›´è§‰ä¸Šï¼Œé¢„è®­ç»ƒå·²ç»å­¦åˆ°äº†ä¸°å¯Œçš„é€šç”¨è¡¨ç¤ºï¼Œå¾®è°ƒåªéœ€è¦åœ¨ä¸€ä¸ªå¾ˆå°çš„å­ç©ºé—´é‡Œåšè°ƒæ•´ã€‚

> æ¥æºï¼šLoRA arXiv:2106.09685, Sec. 7.1 åˆ†æäº† $\Delta W$ çš„ç§©åˆ†å¸ƒï¼›Aghajanyan et al. arXiv:2012.13255 é¦–å…ˆæå‡ºäº† intrinsic dimension æ¦‚å¿µ

å®éªŒè¡¨æ˜ï¼Œå³ä½¿ $r = 4$ æˆ– $r = 8$ï¼ŒLoRA å°±èƒ½è¾¾åˆ°æ¥è¿‘å…¨é‡å¾®è°ƒçš„æ•ˆæœã€‚è¿™æ„å‘³ç€å¯¹äºä¸€ä¸ª $4096 \times 4096$ çš„æƒé‡çŸ©é˜µï¼ˆçº¦ 16M å‚æ•°ï¼‰ï¼ŒLoRA åªéœ€è¦ $4096 \times 8 \times 2 = 65K$ ä¸ªå¯è®­ç»ƒå‚æ•°â€”â€”**å‹ç¼©æ¯”è¶…è¿‡ 200 å€**ã€‚

## å®è·µè¦ç‚¹

### åº”ç”¨åˆ°å“ªäº›å±‚ï¼Ÿ

åŸè®ºæ–‡ä¸»è¦åœ¨ attention çš„ $W_q$ å’Œ $W_v$ ä¸Šåº”ç”¨ LoRAã€‚åç»­å®è·µå‘ç°ï¼š

```python
# å¸¸è§çš„ target_modules é…ç½®
target_modules = [
    "q_proj", "k_proj", "v_proj", "o_proj",  # attention
    "gate_proj", "up_proj", "down_proj",       # MLP (SwiGLU)
]
```

åœ¨ MLP å±‚ä¹ŸåŠ  LoRA é€šå¸¸èƒ½è¿›ä¸€æ­¥æå‡æ•ˆæœï¼Œå°¤å…¶æ˜¯å¯¹äºéœ€è¦æ³¨å…¥æ–°çŸ¥è¯†çš„ä»»åŠ¡ã€‚

> æ¥æºï¼šLoRA åŸè®ºæ–‡åªåœ¨ Attention å±‚å®éªŒï¼ˆarXiv:2106.09685, Sec. 4.2ï¼‰ï¼›åç»­ç¤¾åŒºå®è·µï¼ˆUnslothã€LLaMA-Factory ç­‰ï¼‰å‘ç°åŠ  MLP å±‚æ•ˆæœæ›´å¥½

### rank å’Œ alpha æ€ä¹ˆé€‰ï¼Ÿ

| åœºæ™¯ | æ¨è $r$ | æ¨è $\alpha$ | è¯´æ˜ |
|------|----------|---------------|------|
| ç®€å•ä»»åŠ¡ï¼ˆé£æ ¼è¿ç§»ã€æ ¼å¼è°ƒæ•´ï¼‰ | 8 | 8~16 | ä½ç§©è¶³å¤Ÿ |
| ä¸­ç­‰ä»»åŠ¡ï¼ˆæŒ‡ä»¤è·Ÿéšã€å•é¢†åŸŸ QAï¼‰ | 16~32 | 32~64 | æœ€å¸¸ç”¨èŒƒå›´ |
| å¤æ‚ä»»åŠ¡ï¼ˆé¢†åŸŸçŸ¥è¯†æ³¨å…¥ã€å¤šä»»åŠ¡ï¼‰ | 64 | 64~128 | éœ€è¦æ›´å¤šå¯è®­ç»ƒå‚æ•° |
| æé™ä»»åŠ¡ï¼ˆCPT æ›¿ä»£æ–¹æ¡ˆï¼‰ | 128~256 | 256 | æ¥è¿‘ full fine-tune |

**ç»éªŒæ³•åˆ™**ï¼š$\alpha = r$ æˆ– $\alpha = 2r$ æ˜¯å®‰å…¨èµ·ç‚¹ã€‚$\alpha/r$ å¤ªå¤§ä¼šå¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œå¤ªå°ä¼šå¯¼è‡´ LoRA æ•ˆæœä¸æ˜æ˜¾ã€‚

> æ¥æºï¼šç¤¾åŒºæœ€ä½³å®è·µæ€»ç»“ï¼›rsLoRAï¼ˆarXiv:2312.03732ï¼‰å»ºè®®å¤§ rank æ—¶ç”¨ $\alpha/\sqrt{r}$ æ›¿ä»£ $\alpha/r$

### æ¨ç†æ—¶çš„åˆå¹¶

è®­ç»ƒå®Œæˆåï¼Œå°† LoRA æƒé‡åˆå¹¶å›åŸå§‹æƒé‡ï¼š

$$W_{\text{merged}} = W_0 + \frac{\alpha}{r} BA$$

åˆå¹¶åçš„æ¨¡å‹å’ŒåŸå§‹æ¨¡å‹ç»“æ„å®Œå…¨ä¸€è‡´ï¼Œæ²¡æœ‰ä»»ä½•é¢å¤–çš„æ¨ç†å¼€é”€ã€‚

## LoRA çš„å˜ä½“

### QLoRA

QLoRAï¼ˆ2023ï¼Œåç››é¡¿å¤§å­¦ï¼‰æ˜¯ LoRA æœ€é‡è¦çš„å®è·µæ”¹è¿›ï¼š
- å°†åŸºåº§æ¨¡å‹é‡åŒ–åˆ° **4-bit**ï¼ˆNF4 é‡åŒ–ï¼‰
- åœ¨ 4-bit æ¨¡å‹ä¸Šè®­ç»ƒ LoRA adapterï¼ˆfp16/bf16ï¼‰
- å¼•å…¥åŒé‡é‡åŒ–ï¼ˆDouble Quantizationï¼‰è¿›ä¸€æ­¥èŠ‚çœå†…å­˜
- å¼•å…¥åˆ†é¡µä¼˜åŒ–å™¨ï¼ˆPaged Optimizerï¼‰å¤„ç† GPU å†…å­˜æº¢å‡º

QLoRA ä½¿å¾—åœ¨**å•å¼  24GB æ¶ˆè´¹çº§ GPU ä¸Šå¾®è°ƒ 65B æ¨¡å‹**æˆä¸ºå¯èƒ½ã€‚

> æ¥æºï¼šQLoRA arXiv:2305.14314 (Dettmers et al., 2023)

### DoRAï¼ˆWeight-Decomposed Low-Rank Adaptationï¼‰

DoRA å°†æƒé‡åˆ†è§£ä¸º**å¹…åº¦ï¼ˆmagnitudeï¼‰å’Œæ–¹å‘ï¼ˆdirectionï¼‰** ä¸¤ä¸ªåˆ†é‡ï¼Œç„¶ååªç”¨ LoRA é€‚é…æ–¹å‘åˆ†é‡ã€‚è¿™ä¸ªç®€å•çš„ä¿®æ”¹åœ¨å¤šæ•°åŸºå‡†ä¸Šéƒ½èƒ½å¸¦æ¥ 1-2% çš„æå‡ã€‚

> æ¥æºï¼šDoRA arXiv:2402.09353 (Liu et al., 2024)

### LoRA+

å‘ç° $A$ å’Œ $B$ ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡æ•ˆæœæ›´å¥½ï¼ˆ$B$ çš„å­¦ä¹ ç‡åº”è¯¥æ˜¯ $A$ çš„ 2-8 å€ï¼‰ã€‚

> æ¥æºï¼šLoRA+ arXiv:2402.12354 (Hayou et al., 2024)

### rsLoRA

æå‡ºå°†ç¼©æ”¾å› å­ä» $\alpha/r$ æ”¹ä¸º $\alpha/\sqrt{r}$ï¼Œåœ¨æ›´å¤§çš„ rank ä¸Šè¡¨ç°æ›´ç¨³å®šã€‚

> æ¥æºï¼šrsLoRA arXiv:2312.03732 (Kalajdzievski, 2023)

---

## ğŸ”§ è½åœ°åº”ç”¨

### LoRA vs QLoRA vs Full Fine-tuning é€‰æ‹©æŒ‡å—

| ç»´åº¦ | LoRA | QLoRA | Full Fine-tuning |
|------|------|-------|------------------|
| æ˜¾å­˜éœ€æ±‚ï¼ˆ7B æ¨¡å‹ï¼‰ | ~16GB | ~6GB | ~112GB |
| è®­ç»ƒé€Ÿåº¦ | å¿« | ç¨æ…¢ï¼ˆé‡åŒ–/åé‡åŒ–å¼€é”€ï¼‰ | æœ€å¿«ï¼ˆper stepï¼‰ |
| æ•ˆæœä¸Šé™ | æ¥è¿‘ full FT | ç•¥ä½äº LoRA | æœ€é«˜ |
| å¤šä»»åŠ¡æœåŠ¡ | âœ… ä¸€ä¸ªåŸºåº§ + å¤šä¸ª adapter | âœ… åŒä¸Š | âŒ æ¯ä»»åŠ¡ä¸€ä¸ªæ¨¡å‹ |
| æ¨èåœºæ™¯ | æœ‰ 1-2 å¼  A100/H100 | åªæœ‰æ¶ˆè´¹çº§ GPUï¼ˆ3090/4090ï¼‰ | é¢„ç®—å……è¶³ã€æ•ˆæœè¦æ±‚æé«˜ |

**å†³ç­–æµç¨‹**ï¼šå…ˆç”¨ LoRA éªŒè¯å¯è¡Œæ€§ â†’ æ•ˆæœä¸å¤Ÿå†è€ƒè™‘åŠ å¤§ rank æˆ– full FTã€‚æ•°æ®è´¨é‡ >> æ–¹æ³•é€‰æ‹©ã€‚

### å·¥ç¨‹å®ç°è¦ç‚¹

```python
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=16,                          # rank
    lora_alpha=32,                 # alpha = 2r
    target_modules=[               # æ¨èï¼šAttention + MLP å…¨è¦†ç›–
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj",
    ],
    lora_dropout=0.05,             # è½»åº¦ dropout é˜²è¿‡æ‹Ÿåˆ
    bias="none",                   # ä¸è®­ç»ƒ bias
    task_type="CAUSAL_LM",
)
```

### å¸¸è§å‘
- **rank è¿‡å¤§å¯¼è‡´è¿‡æ‹Ÿåˆ**ï¼šæ•°æ®é‡å°‘æ—¶ï¼ˆ<1000æ¡ï¼‰ï¼Œ$r=8$ è¶³å¤Ÿï¼Œ$r=64$ åè€Œè¿‡æ‹Ÿåˆã€‚è§£å†³ï¼šå¢åŠ  dropout æˆ–å‡å° rank
- **alpha/r æ¯”å€¼ä¸åˆç†**ï¼š$\alpha/r > 4$ æ—¶è®­ç»ƒå®¹æ˜“å‘æ•£ã€‚è§£å†³ï¼šä» $\alpha=r$ å¼€å§‹
- **target modules ä¸å®Œæ•´**ï¼šåªåŠ  Attention ä¸åŠ  MLPï¼ŒçŸ¥è¯†æ³¨å…¥æ•ˆæœå·®ã€‚è§£å†³ï¼šåŠ ä¸Š gate_proj/up_proj/down_proj
- **åˆå¹¶åç²¾åº¦æŸå¤±**ï¼šFP16 åˆå¹¶ + INT4 é‡åŒ–å¯èƒ½ç´¯ç§¯è¯¯å·®ã€‚è§£å†³ï¼šç”¨ BF16 åˆå¹¶ï¼Œæˆ–ä¿æŒ adapter ä¸åˆå¹¶

### é¢è¯•é«˜é¢‘é—®æ³•

- Q: LoRA ä¸ºä»€ä¹ˆä¸ä¼šå¼•å…¥æ¨ç†å»¶è¿Ÿï¼Ÿ
  A: è®­ç»ƒå LoRA æƒé‡ $BA$ å¯ä»¥ç›´æ¥åˆå¹¶åˆ°åŸå§‹æƒé‡ $W_0$ é‡Œï¼ˆ$W = W_0 + \frac{\alpha}{r}BA$ï¼‰ï¼Œåˆå¹¶åæ¨¡å‹ç»“æ„ä¸å˜ï¼Œæ¨ç†è·¯å¾„å’ŒåŸæ¨¡å‹å®Œå…¨ä¸€è‡´ã€‚ï¼ˆæ¥æºï¼šLoRA arXiv:2106.09685, Sec. 4.1ï¼‰

- Q: ä¸ºä»€ä¹ˆ $B$ åˆå§‹åŒ–ä¸ºé›¶ã€$A$ ç”¨ Kaimingï¼Ÿ
  A: ä¿è¯è®­ç»ƒå¼€å§‹æ—¶ $\Delta W = BA = 0$ï¼Œæ¨¡å‹è¡Œä¸ºå’Œé¢„è®­ç»ƒä¸€è‡´ã€‚å¦‚æœä¸¤ä¸ªéƒ½éšæœºåˆå§‹åŒ–ï¼Œ$\Delta W$ ä¸€å¼€å§‹å°±ä¸ä¸ºé›¶ï¼Œä¼šç ´åé¢„è®­ç»ƒè¡¨ç¤ºï¼Œå¯¼è‡´è®­ç»ƒä¸ç¨³å®šã€‚

- Q: LoRA çš„ rank è¶Šå¤§æ•ˆæœä¸€å®šè¶Šå¥½å—ï¼Ÿ
  A: ä¸ä¸€å®šã€‚è®ºæ–‡å®éªŒæ˜¾ç¤º $r=4$ å’Œ $r=64$ çš„æ•ˆæœå·®è·å¾ˆå°ï¼ˆarXiv:2106.09685, Table 6ï¼‰ï¼Œè¯´æ˜æƒé‡æ›´æ–°ç¡®å®æ˜¯ä½ç§©çš„ã€‚rank è¿‡å¤§åè€Œå¢åŠ è¿‡æ‹Ÿåˆé£é™©ï¼Œå°¤å…¶æ˜¯æ•°æ®é‡å°‘æ—¶ã€‚

---

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿ
LoRA æ­ç¤ºäº†ä¸€ä¸ªæ·±åˆ»çš„ç°è±¡ï¼š**é¢„è®­ç»ƒæ¨¡å‹åœ¨å¾®è°ƒæ—¶çš„æƒé‡å˜åŒ–æ˜¯æåº¦ä½ç§©çš„**ã€‚è¿™æ„å‘³ç€"é€šç”¨çŸ¥è¯†"å’Œ"ä»»åŠ¡é€‚é…"å æ®äº†å‚æ•°ç©ºé—´ä¸­å‡ ä¹æ­£äº¤çš„å­ç©ºé—´ã€‚è¿™ä¸ªæ´å¯Ÿä¸ä»…æ”¹å˜äº†å¾®è°ƒçš„ç»æµå­¦ï¼Œä¹Ÿæš—ç¤ºäº†é¢„è®­ç»ƒæ¨¡å‹çš„å†…éƒ¨ç»“æ„æ¯”æˆ‘ä»¬æƒ³è±¡çš„æ›´åŠ è§„æ•´ã€‚

å¯¹è€æ¿çš„å¯ç¤ºï¼šLoRA æ˜¯"ç”¨æœ€å°ä»£ä»·è·å–æœ€å¤§ä»·å€¼"çš„å…¸èŒƒã€‚200 æ¡é«˜è´¨é‡æ•°æ® + LoRA é€šå¸¸ä¼˜äº 10000 æ¡ä½è´¨é‡æ•°æ® + å…¨é‡å¾®è°ƒã€‚**æ•°æ®è´¨é‡ >> æ–¹æ³•é€‰æ‹© >> è®¡ç®—èµ„æº**ã€‚

### å±€é™ä¸æœªè§£é—®é¢˜
- **LoRA çš„è¡¨è¾¾åŠ›ä¸Šé™**ï¼šå¯¹äºéœ€è¦å¤§å¹…ä¿®æ”¹æ¨¡å‹è¡Œä¸ºï¼ˆå¦‚å­¦ä¹ æ–°è¯­è¨€ã€CPTï¼‰çš„ä»»åŠ¡ï¼ŒLoRA å¯èƒ½ä¸å¤Ÿï¼Œrank å†å¤§ä¹Ÿæœ‰ç“¶é¢ˆ
- **æœ€ä¼˜ rank çš„ç†è®ºåˆ†æ**ï¼šç›®å‰ rank é€‰æ‹©ä¸»è¦é ç»éªŒï¼Œç¼ºä¹ç†è®ºæŒ‡å¯¼â€”â€”ç»™å®šæ•°æ®é›†å¤§å°å’Œä»»åŠ¡å¤æ‚åº¦ï¼Œæœ€ä¼˜ rank æ˜¯å¤šå°‘ï¼Ÿ
- **å¤š LoRA ç»„åˆ**ï¼šåŒæ—¶æŒ‚è½½å¤šä¸ª LoRA adapter æ—¶ï¼Œå®ƒä»¬ä¹‹é—´ä¼šå¹²æ‰°å—ï¼Ÿå¦‚ä½•åš LoRA mergingï¼Ÿï¼ˆLoRAHub arXiv:2307.13269 åšäº†åˆæ­¥æ¢ç´¢ï¼‰
- **LoRA ä¸ MoE çš„äº¤å‰**ï¼šMoLoRAâ€”â€”æ¯ä¸ªä¸“å®¶ä¸€ä¸ª LoRAï¼Œèƒ½å¦å®ç°æ›´çµæ´»çš„å¾®è°ƒï¼Ÿ

### è„‘æš´æ‹“å±•
- LoRA çš„ä½ç§©å‡è®¾å¦‚æœç”¨åœ¨æ¨ç†æ—¶ä¼šæ€æ ·ï¼Ÿâ†’ ä½ç§©è¿‘ä¼¼åŠ é€Ÿæ¨ç†ï¼ˆå·²æœ‰å·¥ä½œï¼šLoRA Inferenceï¼‰
- å¦‚æœæŠŠ LoRA çš„æ€è·¯åè¿‡æ¥â€”â€”å›ºå®š $A$, $B$ï¼Œåªè®­ç»ƒ $W_0$ çš„ä¸€ä¸ªå­ç©ºé—´ï¼Ÿâ†’ è¿™å’Œ Subspace Training æœ‰å…³
- LoRA åœ¨ RL ä¸­çš„åº”ç”¨ï¼šGRPO è®­ç»ƒæ—¶ç”¨ LoRA è€Œé full updateï¼Œèƒ½å¦é™ä½ RL çš„ä¸ç¨³å®šæ€§ï¼Ÿ

> ğŸ”— See also:
> - [[AI/LLM/SFT/SFT åŸç†]] â€” LoRA æ˜¯ SFT çš„å…·ä½“å®ç°æ–¹å¼
> - [[AI/Foundations/Math/çŸ©é˜µåˆ†è§£]] â€” LoRA çš„æ•°å­¦åŸºç¡€ï¼šä½ç§©åˆ†è§£
> - [[AI/LLM/Frameworks/Unsloth/Unsloth æ¦‚è¿°]] â€” LoRA è®­ç»ƒçš„é«˜æ•ˆæ¡†æ¶
> - [[AI/LLM/Frameworks/Unsloth/é‡åŒ–]] â€” QLoRA ä¾èµ–çš„é‡åŒ–æŠ€æœ¯
> - [[AI/LLM/Architecture/MoE æ·±åº¦è§£æ]] â€” MoE å¾®è°ƒä¸­ LoRA çš„åº”ç”¨

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) â€” åŸå§‹è®ºæ–‡ï¼Œå¿…è¯»ï¼ŒSec. 4 æ˜¯æ ¸å¿ƒåŸç†ï¼ŒSec. 7 æ˜¯æ·±åº¦åˆ†æ â­â­â­â­â­
- [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314) â€” è®© LoRA åœ¨æ¶ˆè´¹çº§ GPU ä¸Šå¯ç”¨ï¼Œå·¥ç¨‹ä»·å€¼æé«˜ â­â­â­â­â­

### æ·±åº¦è§£è¯»
- [HuggingFace PEFT æ–‡æ¡£](https://huggingface.co/docs/peft) â€” å®˜æ–¹ LoRA å®ç°ï¼Œå«å„å˜ä½“ â­â­â­â­
- [Practical Tips for Finetuning LLMs Using LoRA (Sebastian Raschka)](https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms) â€” rank/alpha/target modules çš„æœ€ä½³å®è·µ â­â­â­â­â­
- [DoRA: Weight-Decomposed Low-Rank Adaptation](https://arxiv.org/abs/2402.09353) â€” LoRA çš„æœ‰æ•ˆæ”¹è¿›ï¼Œ1-2% æå‡ â­â­â­â­

### å®è·µèµ„æº
- [Unsloth](https://github.com/unslothai/unsloth) â€” 2-5x åŠ é€Ÿ LoRA è®­ç»ƒï¼Œæ”¯æŒ QLoRA â­â­â­â­â­
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) â€” ä¸€ç«™å¼å¾®è°ƒæ¡†æ¶ï¼ŒLoRA/QLoRA/Full FT å…¨è¦†ç›– â­â­â­â­â­
- [PEFT GitHub](https://github.com/huggingface/peft) â€” HuggingFace å®˜æ–¹ PEFT åº“ â­â­â­â­
