---
title: "SFTï¼ˆSupervised Fine-Tuningï¼‰åŸç†ä¸å®è·µ"
brief: "SFT æ˜¯é¢„è®­ç»ƒåå¯¹é½äººç±»æ„å›¾çš„ç¬¬ä¸€æ­¥â€”â€”ç”¨é«˜è´¨é‡ instruction-response å¯¹å¾®è°ƒ base modelï¼Œä½¿å…¶ä»'è¡¥å…¨æ–‡æœ¬'è½¬å˜ä¸º'éµå¾ªæŒ‡ä»¤'ï¼›LIMA è®ºæ–‡ï¼ˆarXiv:2305.11206ï¼‰è¯æ˜ä»… 1000 æ¡é«˜è´¨é‡æ•°æ®å³å¯å®ç°ä¼˜ç§€å¯¹é½ï¼Œæ•°æ®è´¨é‡è¿œæ¯”æ•°é‡é‡è¦ï¼›ç†è§£ SFT æ˜¯æŒæ¡ RLHF/DPO å…¨æµç¨‹çš„å‰æ"
type: concept
domain: ai/llm/sft
created: "2026-02-13"
updated: "2026-02-22"
tags:
  - ai/llm/sft
  - type/concept
  - interview/hot
status: complete
sources:
  - "InstructGPT (Training language models to follow instructions) arXiv:2203.02155 (Ouyang et al., 2022)"
  - "LIMA: Less Is More for Alignment arXiv:2305.11206 (Zhou et al., 2023)"
  - "Stanford Alpaca https://github.com/tatsu-lab/stanford_alpaca (2023)"
  - "Self-Instruct arXiv:2212.10560 (Wang et al., 2022)"
  - "WizardLM: Empowering Large Language Models to Follow Complex Instructions arXiv:2304.12244"
  - "NEFTune arXiv:2310.05914 (Jain et al., 2023)"
related:
  - "[[AI/LLM/RL/DPO/DPO-TRLå®è·µ|DPO]]"
  - "[[AI/LLM/RL/GRPO/GRPO æ·±åº¦ç†è§£|GRPO]]"
  - "[[AI/LLM/RL/PPO/PPO åŸç†|PPO]]"
  - "[[AI/LLM/Frameworks/TRL/TRL æ¦‚è¿°|TRL]]"
  - "[[AI/LLM/SFT/LoRA|LoRA]]"
  - "[[AI/LLM/RL/RLHF-DPO-2026-æŠ€æœ¯å…¨æ™¯|RLHF-DPO å…¨æ™¯]]"
  - "[[AI/LLM/Pretraining/LLM-é¢„è®­ç»ƒä¸åˆ†å¸ƒå¼è®­ç»ƒ-2026-å…¨æ™¯|é¢„è®­ç»ƒä¸åˆ†å¸ƒå¼è®­ç»ƒ]]"
---

# SFTï¼ˆSupervised Fine-Tuningï¼‰åŸç†ä¸å®è·µ

> **Brief**ï¼šSFT æ˜¯ LLM ä¸‰é˜¶æ®µè®­ç»ƒï¼ˆPretrain â†’ SFT â†’ RLHF/DPOï¼‰çš„å…³é”®ä¸­é—´ç¯èŠ‚ã€‚é¢„è®­ç»ƒæ¨¡å‹æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ–‡æœ¬è¡¥å…¨å™¨ï¼Œä½†ä¸ä¼š"å¬è¯"â€”â€”SFT ç”¨é«˜è´¨é‡çš„ instruction-response å¯¹å¾®è°ƒï¼Œè®©æ¨¡å‹å­¦ä¼šéµå¾ªäººç±»æ„å›¾ã€‚InstructGPT è®ºæ–‡ï¼ˆarXiv:2203.02155ï¼‰å¥ å®šäº†è¿™ä¸€èŒƒå¼ã€‚
>
> æ¥æºï¼šInstructGPT arXiv:2203.02155; LIMA arXiv:2305.11206

---

## 1. SFT çš„æœ¬è´¨ï¼šä»æ–‡æœ¬è¡¥å…¨åˆ°æŒ‡ä»¤éµå¾ª

### ä¸ºä»€ä¹ˆéœ€è¦ SFTï¼Ÿ

é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚ GPT-3 baseï¼‰åœ¨æµ·é‡æ–‡æœ¬ä¸Šè®­ç»ƒï¼Œå­¦åˆ°äº†å¼ºå¤§çš„è¯­è¨€èƒ½åŠ›ï¼Œä½†å®ƒçš„è¡Œä¸ºæ˜¯**è¡¥å…¨æ–‡æœ¬**è€Œé**å›ç­”é—®é¢˜**ã€‚

> æ¥æºï¼šInstructGPT arXiv:2203.02155, Sec. 1 æ˜ç¡®æŒ‡å‡º base model çš„è¾“å‡º"ä¸æ€»æ˜¯æœ‰å¸®åŠ©çš„ã€çœŸå®çš„ã€æ— å®³çš„"

```
ç”¨æˆ·è¾“å…¥: "è¯·è§£é‡Šä»€ä¹ˆæ˜¯ Transformer"

Base Model å¯èƒ½è¾“å‡º:
  "æ¶æ„ï¼Œå®ƒæ˜¯ç”± Google åœ¨ 2017 å¹´æå‡ºçš„ã€‚Transformer æ¶æ„..."  â† è¡¥å…¨é£æ ¼

SFT Model è¾“å‡º:
  "Transformer æ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œæ¶æ„ã€‚
   å®ƒçš„æ ¸å¿ƒåˆ›æ–°æ˜¯..."  â† å›ç­”é£æ ¼
```

### SFT åœ¨ä¸‰é˜¶æ®µè®­ç»ƒä¸­çš„ä½ç½®

```mermaid
flowchart LR
    A["Pretrain<br/>æµ·é‡æ–‡æœ¬<br/>å­¦ä¹ è¯­è¨€èƒ½åŠ›"] --> B["SFT<br/>æŒ‡ä»¤æ•°æ®<br/>å­¦ä¹ éµå¾ªæ„å›¾"]
    B --> C["RLHF / DPO<br/>åå¥½æ•°æ®<br/>å­¦ä¹ äººç±»åå¥½"]
    
    style A fill:#e8e8ff,stroke:#333
    style B fill:#ffe8e8,stroke:#333,stroke-width:3px
    style C fill:#e8ffe8,stroke:#333
```

> æ¥æºï¼šInstructGPT arXiv:2203.02155, Figure 2 å±•ç¤ºäº†å®Œæ•´çš„ä¸‰é˜¶æ®µæµç¨‹
>
> ğŸ”— See also: [[AI/LLM/RL/RLHF-DPO-2026-æŠ€æœ¯å…¨æ™¯|RLHF-DPO å…¨æ™¯]] â€” SFT ä¹‹åçš„å¯¹é½é˜¶æ®µ

---

## 2. SFT çš„æ ¸å¿ƒæŠ€æœ¯

### 2.1 æ•°æ®æ ¼å¼

SFT æ•°æ®çš„åŸºæœ¬æ ¼å¼æ˜¯ **(instruction, response)** å¯¹ï¼š

```json
{
  "instruction": "è¯·ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šé‡å­çº ç¼ ",
  "input": "",
  "response": "é‡å­çº ç¼ æ˜¯é‡å­åŠ›å­¦ä¸­ä¸€ç§å¥‡ç‰¹çš„ç°è±¡..."
}
```

> æ¥æºï¼šStanford Alpaca å®šä¹‰äº†è¿™ç§ instruction-input-response ä¸‰å…ƒç»„æ ¼å¼ï¼ˆhttps://github.com/tatsu-lab/stanford_alpacaï¼‰

#### å¯¹è¯æ ¼å¼ï¼ˆChatMLï¼‰

```
<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ AI åŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
è¯·è§£é‡Šä»€ä¹ˆæ˜¯ Transformerã€‚<|im_end|>
<|im_start|>assistant
Transformer æ˜¯ä¸€ç§åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„ç¥ç»ç½‘ç»œæ¶æ„...<|im_end|>
```

### 2.2 Loss Masking

**æ ¸å¿ƒåŸåˆ™ï¼šåªå¯¹ assistant çš„å›å¤è®¡ç®— lossï¼Œä¸å¯¹ system/user è®¡ç®— lossã€‚**

```python
def create_sft_labels(input_ids, tokenizer):
    """åªå¯¹ assistant å›å¤éƒ¨åˆ†è®¡ç®— loss"""
    labels = input_ids.clone()
    
    # æ‰¾åˆ°æ‰€æœ‰ assistant å›å¤çš„ span
    assistant_spans = find_assistant_spans(input_ids, tokenizer)
    
    # é assistant éƒ¨åˆ†è®¾ä¸º -100ï¼ˆignore_indexï¼‰
    mask = torch.full_like(labels, -100)
    for start, end in assistant_spans:
        mask[start:end] = labels[start:end]
    
    return mask
```

> æ¥æºï¼šè¿™æ˜¯ InstructGPT å’Œåç»­æ‰€æœ‰ SFT æ¡†æ¶ï¼ˆTRLã€LLaMA-Factory ç­‰ï¼‰çš„æ ‡å‡†åšæ³•

**ä¸ºä»€ä¹ˆè¦åš Loss Maskingï¼Ÿ**
- å¦‚æœå¯¹ user è¾“å…¥ä¹Ÿè®¡ç®— lossï¼Œæ¨¡å‹ä¼šå­¦ä¹ "å¤è¯»"ç”¨æˆ·çš„è¯
- System prompt æ˜¯å›ºå®šæ¨¡æ¿ï¼Œä¸éœ€è¦å­¦ä¹ 
- åªå­¦ä¹ "å¦‚ä½•å›å¤"ï¼Œä¸å­¦ä¹ "å¦‚ä½•æé—®"

### 2.3 è®­ç»ƒè¶…å‚æ•°

| å‚æ•° | å…¸å‹å€¼ | è¯´æ˜ |
|------|--------|------|
| Learning Rate | 1e-5 ~ 5e-5 | æ¯”é¢„è®­ç»ƒä½ 10-100 å€ |
| Epochs | 2-5 | è¿‡å¤šä¼šè¿‡æ‹Ÿåˆ |
| Batch Size | 64-128 | è¾ƒå¤§ batch æ›´ç¨³å®š |
| Max Seq Length | 2048-4096 | è§†æ•°æ®é•¿åº¦åˆ†å¸ƒ |
| Weight Decay | 0.01-0.1 | æ­£åˆ™åŒ– |
| Warmup Ratio | 0.03-0.1 | å æ€»æ­¥æ•°çš„æ¯”ä¾‹ |

> æ¥æºï¼šAlpaca ä½¿ç”¨ lr=2e-5, epochs=3; InstructGPT ä½¿ç”¨ lr=1.3e-5

---

## 3. LIMA æ´å¯Ÿï¼šæ•°æ®è´¨é‡ >> æ•°æ®æ•°é‡

LIMAï¼ˆLess Is More for Alignmentï¼‰æ˜¯ SFT é¢†åŸŸæœ€é‡è¦çš„å®è¯ç ”ç©¶ä¹‹ä¸€ã€‚

> æ¥æºï¼šLIMA arXiv:2305.11206 (Zhou et al., Meta, 2023)

### æ ¸å¿ƒå‘ç°

**ä»… 1000 æ¡ç²¾å¿ƒç­›é€‰çš„é«˜è´¨é‡æ•°æ®ï¼Œå°±èƒ½è®© LLaMA-65B è¾¾åˆ°æ¥è¿‘ GPT-4 çº§åˆ«çš„å¯¹è¯èƒ½åŠ›ã€‚**

| å®éªŒ | æ•°æ®é‡ | æ¨¡å‹ | æ•ˆæœ |
|------|--------|------|------|
| LIMA | 1,000 æ¡ | LLaMA-65B | æ¥è¿‘ GPT-4ï¼ˆäººç±»è¯„ä¼°ï¼‰ |
| Alpaca | 52,000 æ¡ | LLaMA-7B | ä¸­ç­‰ |
| Vicuna | 70,000 æ¡ | LLaMA-13B | è¾ƒå¥½ |

> æ¥æºï¼šLIMA arXiv:2305.11206, Table 1 å’Œ Figure 3

### LIMA çš„"è¡¨é¢å¯¹é½å‡è¯´"ï¼ˆSuperficial Alignment Hypothesisï¼‰

> "A model's knowledge and capabilities are learnt almost entirely during pretraining, while alignment teaches it which subdistribution of formats should be used when interacting with users."
> â€” LIMA arXiv:2305.11206, Sec. 1

**æ ¸å¿ƒå«ä¹‰**ï¼šSFT ä¸æ˜¯åœ¨æ•™æ¨¡å‹æ–°çŸ¥è¯†ï¼Œè€Œæ˜¯åœ¨æ•™æ¨¡å‹"ç”¨ä»€ä¹ˆæ ¼å¼è¾“å‡ºå·²æœ‰çŸ¥è¯†"ã€‚è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆå°‘é‡æ•°æ®å°±å¤Ÿç”¨â€”â€”ä½ ä¸éœ€è¦è¦†ç›–æ‰€æœ‰çŸ¥è¯†ï¼Œåªéœ€è¦æ•™ä¼š"å›ç­”çš„æ¨¡å¼"ã€‚

### æ•°æ®è´¨é‡çš„å…³é”®ç»´åº¦

```
é«˜è´¨é‡ SFT æ•°æ®çš„æ ‡å‡†ï¼š
â”œâ”€â”€ å¤šæ ·æ€§ï¼šè¦†ç›–ä¸åŒä»»åŠ¡ç±»å‹ï¼ˆé—®ç­”/æ‘˜è¦/ç¿»è¯‘/åˆ›ä½œ/æ¨ç†/ä»£ç ï¼‰
â”œâ”€â”€ å›å¤è´¨é‡ï¼šè¯¦ç»†ã€æœ‰ç»“æ„ã€å‡†ç¡®ã€æœ‰å¸®åŠ©
â”œâ”€â”€ æ ¼å¼ä¸€è‡´æ€§ï¼šç»Ÿä¸€çš„å¯¹è¯é£æ ¼å’Œ markdown æ ¼å¼
â”œâ”€â”€ éš¾åº¦æ¢¯åº¦ï¼šä»ç®€å•åˆ°å¤æ‚éƒ½æœ‰è¦†ç›–
â””â”€â”€ æ— å™ªå£°ï¼šæ²¡æœ‰çŸ›ç›¾ã€é”™è¯¯ã€æˆ–ä½è´¨é‡å›å¤
```

> ğŸ”— See also: [[AI/LLM/SFT/LoRA|LoRA]] â€” å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ŒSFT æ•°æ®é‡å°‘æ—¶ LoRA æ˜¯é¦–é€‰

---

## 4. SFT æ•°æ®æ„é€ æ–¹æ³•

### 4.1 äººå·¥æ ‡æ³¨ï¼ˆæœ€è´µä½†æœ€å¥½ï¼‰

InstructGPT ä½¿ç”¨äº† 40 åæ ‡æ³¨å‘˜ï¼Œç”Ÿæˆçº¦ 13,000 æ¡é«˜è´¨é‡ SFT æ•°æ®ã€‚

> æ¥æºï¼šInstructGPT arXiv:2203.02155, Sec. 3.1

æˆæœ¬ä¼°ç®—ï¼š
- å•æ¡é«˜è´¨é‡æ ‡æ³¨ï¼šÂ¥50-200ï¼ˆå«å®¡æ ¸ï¼‰
- 5000 æ¡æ•°æ®é›†ï¼šÂ¥25ä¸‡-100ä¸‡
- å»ºè®®ï¼š20% é¢„ç®—ç”¨äºæ ‡æ³¨ï¼Œ80% ç”¨äºå®¡æ ¸å’Œè¿­ä»£

### 4.2 Self-Instructï¼ˆè‡ªåŠ¨ç”Ÿæˆï¼‰

ç”¨ LLM è‡ªå·±ç”Ÿæˆ instruction-response å¯¹ï¼Œç„¶åè¿‡æ»¤ä½è´¨é‡æ ·æœ¬ã€‚

> æ¥æºï¼šSelf-Instruct arXiv:2212.10560 (Wang et al., 2022)

æµç¨‹ï¼š
1. ç”¨ 175 æ¡ç§å­ä»»åŠ¡ï¼ˆäººå·¥ç¼–å†™ï¼‰
2. è®© GPT-3 ç”Ÿæˆæ–°çš„ instruction
3. å¯¹æ¯ä¸ª instruction ç”Ÿæˆ response
4. è¿‡æ»¤ï¼šç›¸ä¼¼åº¦å»é‡ + è´¨é‡ç­›é€‰
5. äº§å‡º 52K æ¡æ•°æ® â†’ Alpaca æ•°æ®é›†

### 4.3 Evol-Instructï¼ˆè¿›åŒ–å¼å¢å¼ºï¼‰

WizardLM æå‡ºï¼šè®© LLM é€æ­¥"è¿›åŒ–"æŒ‡ä»¤çš„å¤æ‚åº¦ã€‚

> æ¥æºï¼šWizardLM arXiv:2304.12244

```
è¿›åŒ–ç­–ç•¥ï¼š
â”œâ”€â”€ å¢åŠ çº¦æŸ (Add Constraints)
â”œâ”€â”€ åŠ æ·±æ¨ç† (Deepening)
â”œâ”€â”€ å…·ä½“åŒ– (Concretizing)
â”œâ”€â”€ å¢åŠ æ¨ç†æ­¥éª¤ (Increase Reasoning Steps)
â””â”€â”€ æ‰©å±•è¯é¢˜ (Breadth)

ç¤ºä¾‹ï¼š
åŸå§‹: "å†™ä¸€é¦–è¯—"
è¿›åŒ–: "ç”¨äº”è¨€ç»å¥çš„æ ¼å¼ï¼Œä»¥'æ˜¥é›¨'ä¸ºé¢˜ï¼Œå†™ä¸€é¦–è¡¨è¾¾æ¸¸å­æ€ä¹¡ä¹‹æƒ…çš„è¯—ï¼Œ
       è¦æ±‚æ¯å¥è‡³å°‘åŒ…å«ä¸€ä¸ªå…¸æ•…"
```

### 4.4 å¼ºæ¨¡å‹è’¸é¦

ç”¨ GPT-4/Claude ç­‰å¼ºæ¨¡å‹ç”Ÿæˆ SFT æ•°æ®ï¼Œè’¸é¦åˆ°å°æ¨¡å‹ï¼š

```python
# ç”¨å¼ºæ¨¡å‹ç”Ÿæˆå¤šä¸ªå€™é€‰ï¼Œé€‰æœ€å¥½çš„
responses = [strong_model.generate(prompt, temperature=0.7) for _ in range(3)]
best_response = quality_ranker(prompt, responses)
```

> Stanford Alpaca å°±æ˜¯ç”¨ GPT-3.5 text-davinci-003 ç”Ÿæˆçš„ 52K æ¡æ•°æ®

---

## 5. SFT å·¥ç¨‹å®è·µè¦ç‚¹

### 5.1 Packing vs Padding

```
Padding: æ¯ä¸ªæ ·æœ¬ç‹¬ç«‹ï¼ŒçŸ­æ ·æœ¬è¡¥ PAD token
  â†’ æµªè´¹è®¡ç®—ï¼ˆPAD token ä¸å‚ä¸ loss ä½†å æ˜¾å­˜å’Œè®¡ç®—ï¼‰

Packing: å¤šä¸ªæ ·æœ¬æ‹¼æ¥åˆ°ä¸€ä¸ªåºåˆ—ä¸­
  â†’ éœ€è¦ç‰¹æ®Šçš„ attention mask é˜²æ­¢è·¨æ ·æœ¬æ³¨æ„åŠ›
  â†’ è®­ç»ƒæ•ˆç‡æå‡ 1.5-3x
```

### 5.2 NEFTune æ­£åˆ™åŒ–

åœ¨ embedding ä¸ŠåŠ å™ªå£°ï¼Œç®€å•ä½†æœ‰æ•ˆçš„é˜²è¿‡æ‹Ÿåˆæ‰‹æ®µã€‚

> æ¥æºï¼šNEFTune arXiv:2310.05914 (Jain et al., 2023)

```python
def neftune_forward(model, input_ids, noise_alpha=5):
    embeddings = model.embed_tokens(input_ids)
    dims = torch.tensor(embeddings.size(1) * embeddings.size(2))
    mag_norm = noise_alpha / torch.sqrt(dims)
    noise = torch.zeros_like(embeddings).uniform_(-mag_norm, mag_norm)
    return model.forward_from_embeddings(embeddings + noise)
```

### 5.3 å…¨é‡å¾®è°ƒ vs LoRA

| æ–¹å¼ | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|------|------|------|---------|
| å…¨é‡å¾®è°ƒ | æ•ˆæœæœ€å¥½ | æ˜¾å­˜å¤§ã€æ˜“è¿‡æ‹Ÿåˆ | æ•°æ®é‡å¤§ï¼ˆ>50Kï¼‰+ æ˜¾å­˜å……è¶³ |
| LoRA | é«˜æ•ˆã€ä¸æ˜“è¿‡æ‹Ÿåˆ | æ•ˆæœç•¥é€Š | æ•°æ®é‡å°‘ï¼ˆ<10Kï¼‰ã€èµ„æºæœ‰é™ |
| QLoRA | æè‡´çœæ˜¾å­˜ | é€Ÿåº¦è¾ƒæ…¢ | å•å¡å¾®è°ƒå¤§æ¨¡å‹ |

> ğŸ”— See also: [[AI/LLM/SFT/LoRA|LoRA]] â€” å‚æ•°é«˜æ•ˆå¾®è°ƒçš„è¯¦ç»†åŸç†

---

## 6. å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•ç»¼è¿°

ä»¥ä¸‹æ–¹æ³•è§£å†³åŒä¸€ä¸ªæ ¸å¿ƒé—®é¢˜ï¼š**å¦‚ä½•åœ¨ä¸æ›´æ–°å…¨éƒ¨å‚æ•°çš„æƒ…å†µä¸‹é«˜æ•ˆå¾®è°ƒå¤§æ¨¡å‹ï¼Ÿ**

| æ–¹æ³• | æ ¸å¿ƒæ€è·¯ | å‚æ•°é‡ | æ•ˆæœ |
|------|---------|--------|------|
| LoRA | ä½ç§©çŸ©é˜µåˆ†è§£ | ~0.1% | â˜…â˜…â˜…â˜…â˜… |
| Prefix Tuning | åœ¨è¾“å…¥å‰åŠ å¯å­¦ä¹ å‰ç¼€ | ~0.1% | â˜…â˜…â˜… |
| Prompt Tuning | åœ¨ embedding å±‚åŠ è½¯æç¤º | <0.01% | â˜…â˜…â˜… |
| Adapter | åœ¨ FFN åæ’å…¥å°ç½‘ç»œ | ~1% | â˜…â˜…â˜…â˜… |
| P-Tuning v2 | æ¯å±‚åŠ  prefix | ~0.1% | â˜…â˜…â˜…â˜… |
| BitFit | åªè°ƒ bias é¡¹ | <0.1% | â˜…â˜… |

> æ¥æºï¼šä»¥ä¸Šæ–¹æ³•çš„è¯¦ç»†å¯¹æ¯”è§çŸ¥ä¹"å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°"ç³»åˆ—
> - [ç»¼è¿°ï¼ˆä¸€ï¼‰èƒŒæ™¯](https://zhuanlan.zhihu.com/p/635152813)
> - [ç»¼è¿°ï¼ˆäº”ï¼‰LoRAã€AdaLoRAã€QLoRA](https://zhuanlan.zhihu.com/p/636215898)
> - [ç»¼è¿°ï¼ˆä¸ƒï¼‰æœ€ä½³å®è·µ](https://zhuanlan.zhihu.com/p/649755252)

å®Œæ•´ç³»åˆ—é“¾æ¥ï¼š
- [ç»¼è¿°ï¼ˆäºŒï¼‰BitFitã€Prefix Tuningã€Prompt Tuning](https://zhuanlan.zhihu.com/p/635686756)
- [ç»¼è¿°ï¼ˆä¸‰ï¼‰P-Tuningã€P-Tuning v2](https://zhuanlan.zhihu.com/p/635848732)
- [ç»¼è¿°ï¼ˆå››ï¼‰Adapter Tuning åŠå…¶å˜ä½“](https://zhuanlan.zhihu.com/p/636038478)
- [ç»¼è¿°ï¼ˆå…­ï¼‰MAM Adapterã€UniPELT](https://zhuanlan.zhihu.com/p/636362246)

---

## ğŸ”§ è½åœ°åº”ç”¨

### ä»€ä¹ˆæ—¶å€™åš SFTï¼Ÿ

- **æœ‰äº† base modelï¼Œæƒ³è®©å®ƒ"å¬è¯"**ï¼šè¿™æ˜¯ SFT æœ€æ ¸å¿ƒçš„åº”ç”¨â€”â€”å°†é¢„è®­ç»ƒæ¨¡å‹è½¬åŒ–ä¸ºèƒ½éµå¾ªæŒ‡ä»¤çš„åŠ©æ‰‹
- **å‚ç›´é¢†åŸŸå®šåˆ¶**ï¼šåŒ»ç–—/æ³•å¾‹/é‡‘èç­‰é¢†åŸŸï¼Œç”¨é¢†åŸŸä¸“å®¶æ ‡æ³¨çš„ SFT æ•°æ®è®©é€šç”¨æ¨¡å‹å˜æˆé¢†åŸŸä¸“å®¶
- **æ ¼å¼æ§åˆ¶**ï¼šéœ€è¦æ¨¡å‹è¾“å‡º JSON/Markdown/ç‰¹å®šæ¨¡æ¿æ—¶ï¼ŒSFT æ˜¯æœ€ç›´æ¥çš„æ–¹å¼
- **å¤šè¯­è¨€é€‚é…**ï¼šbase model ä»¥è‹±æ–‡ä¸ºä¸»æ—¶ï¼Œç”¨ä¸­æ–‡ SFT æ•°æ®å¢å¼ºä¸­æ–‡å¯¹è¯èƒ½åŠ›

### æ•°æ®é‡è¦æ±‚

| åœºæ™¯ | æ¨èæ•°æ®é‡ | å…³é”® |
|------|-----------|------|
| é€šç”¨å¯¹è¯ | 1K-10Kï¼ˆé«˜è´¨é‡ï¼‰ | å¤šæ ·æ€§ > æ•°é‡ï¼ˆLIMA æ´å¯Ÿï¼‰ |
| å‚ç›´é¢†åŸŸ | 5K-50K | é¢†åŸŸè¦†ç›–åº¦è¦å¤Ÿ |
| æ ¼å¼æ§åˆ¶ | 500-2K | æ ¼å¼ä¸€è‡´æ€§æœ€é‡è¦ |
| å¤æ‚æ¨ç† | 10K-100K | éœ€è¦é«˜è´¨é‡ CoT æ•°æ® |

### SFT ä¸ RL çš„é…åˆ

> æ¥æºï¼šInstructGPT arXiv:2203.02155 çš„å®Œæ•´æµç¨‹

```mermaid
flowchart TD
    A["Base Model"] -->|"SFT æ•°æ® ~13K æ¡"| B["SFT Model"]
    B -->|"æ ‡æ³¨åå¥½å¯¹"| C["Reward Model"]
    B -->|"PPO è®­ç»ƒ"| D["RLHF Model"]
    C -->|"æä¾›å¥–åŠ±ä¿¡å·"| D
    
    style B fill:#ffe8e8,stroke:#333,stroke-width:2px
```

**å…³é”®è®¤çŸ¥**ï¼š
- SFT æ•™æ¨¡å‹"æ€ä¹ˆå›ç­”"ï¼ŒRLHF/DPO æ•™æ¨¡å‹"ä»€ä¹ˆæ˜¯å¥½çš„å›ç­”"
- SFT æ˜¯ RLHF çš„å¿…è¦å‰ç½®â€”â€”ä¸å…ˆåš SFT ç›´æ¥ RLHF æ•ˆæœå¾ˆå·®
- ä½† SFT ä¹Ÿä¸èƒ½æ›¿ä»£ RLHFï¼šSFT æ¨¡å‹ä¼š"è®¨å¥½å¼å›ç­”"ï¼ˆhallucinationã€è¿‡é•¿å›å¤ï¼‰ï¼Œéœ€è¦ RL çº æ­£

### é¢è¯•é«˜é¢‘é—®æ³•

- **Q: SFT é˜¶æ®µä¸ºä»€ä¹ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼Ÿå¦‚ä½•ç¼“è§£ï¼Ÿ**
  A: SFT æ•°æ®é‡é€šå¸¸åªæœ‰å‡ åƒåˆ°å‡ ä¸‡æ¡ï¼ˆvs é¢„è®­ç»ƒä¸‡äº¿ tokenï¼‰ï¼Œæ¨¡å‹å‚æ•°è¿œå¤§äºæ•°æ®é‡ã€‚ç¼“è§£ï¼šæ§åˆ¶ epochsï¼ˆ2-5ï¼‰ã€ä½å­¦ä¹ ç‡ï¼ˆ1e-5 ~ 5e-5ï¼‰ã€NEFTune æ­£åˆ™åŒ–ã€LoRA å‡å°‘å¯è®­ç»ƒå‚æ•°ã€æ—©åœã€‚

- **Q: ä¸ºä»€ä¹ˆè¦åš Loss Maskingï¼Ÿ**
  A: åªå¯¹ assistant å›å¤è®¡ç®— lossã€‚å¦åˆ™æ¨¡å‹ä¼šå­¦ä¹ å¤è¯» user è¾“å…¥ã€è¿‡æ‹Ÿåˆ system promptã€‚

- **Q: LIMA çš„æ ¸å¿ƒå‘ç°æ˜¯ä»€ä¹ˆï¼Ÿ**
  A: "è¡¨é¢å¯¹é½å‡è¯´"â€”â€”SFT ä¸æ˜¯åœ¨æ•™æ¨¡å‹æ–°çŸ¥è¯†ï¼Œè€Œæ˜¯åœ¨æ•™æ¨¡å‹ç”¨ä»€ä¹ˆæ ¼å¼è¾“å‡ºã€‚å› æ­¤ 1000 æ¡é«˜è´¨é‡æ•°æ®å°±è¶³å¤Ÿï¼Œå…³é”®æ˜¯å¤šæ ·æ€§å’Œå›å¤è´¨é‡ã€‚

---

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿ

SFT æ­ç¤ºäº†ä¸€ä¸ªæ·±åˆ»çš„è®¤çŸ¥ï¼š**å¤§æ¨¡å‹çš„èƒ½åŠ›å’Œè¡Œä¸ºæ˜¯å¯ä»¥è§£è€¦çš„**ã€‚é¢„è®­ç»ƒç»™äº†æ¨¡å‹èƒ½åŠ›ï¼ˆçŸ¥è¯†ã€æ¨ç†ã€è¯­è¨€ï¼‰ï¼ŒSFT åªéœ€è¦æå°‘æ•°æ®å°±èƒ½æ”¹å˜è¡Œä¸ºæ¨¡å¼ã€‚è¿™å°±åƒä¸€ä¸ªåšå­¦çš„äººï¼Œåªéœ€è¦å‘Šè¯‰ä»–"åœ¨è¿™ä¸ªåœºåˆåº”è¯¥æ€ä¹ˆè¯´è¯"ï¼Œè€Œä¸éœ€è¦é‡æ–°æ•™ä»–æ‰€æœ‰çŸ¥è¯†ã€‚

LIMA çš„"è¡¨é¢å¯¹é½å‡è¯´"è¿›ä¸€æ­¥è¯´æ˜ï¼š**å¯¹é½ä¸æ˜¯ä¸€ä¸ªçŸ¥è¯†é—®é¢˜ï¼Œè€Œæ˜¯ä¸€ä¸ªæ ¼å¼é—®é¢˜**ã€‚è¿™å¯¹ Agent è®¾è®¡çš„å¯ç¤ºæ˜¯â€”â€”ä¸å…¶åœ¨æ¯ä¸ª Agent é‡Œå¾®è°ƒæ‰€æœ‰çŸ¥è¯†ï¼Œä¸å¦‚è®© base model å…±äº«çŸ¥è¯†ï¼Œåªåœ¨è¡Œä¸ºå±‚åšè½»é‡å®šåˆ¶ã€‚

### å±€é™ä¸æœªè§£é—®é¢˜

- **SFT æ•°æ®çš„åè§ä¼šè¢«æ”¾å¤§**ï¼šå¦‚æœæ ‡æ³¨æ•°æ®æœ‰ç³»ç»Ÿæ€§åè§ï¼ˆå¦‚è¿‡äºå†—é•¿ã€è¿‡äº"æ”¿æ²»æ­£ç¡®"ï¼‰ï¼Œæ¨¡å‹ä¼šå¿ å®å­¦ä¹ è¿™äº›åè§
- **SFT ä¸èƒ½çº æ­£é¢„è®­ç»ƒçš„é”™è¯¯çŸ¥è¯†**ï¼šå¦‚æœ base model åœ¨é¢„è®­ç»ƒé˜¶æ®µå­¦åˆ°äº†é”™è¯¯äº‹å®ï¼ŒSFT é€šå¸¸æ— æ³•ä¿®æ­£â€”â€”å®ƒåªæ”¹å˜æ ¼å¼ï¼Œä¸æ”¹å˜çŸ¥è¯†
- **SFT ä¸ RL çš„æœ€ä¼˜åˆ‡åˆ†ç‚¹ä»ä¸æ˜ç¡®**ï¼šå“ªäº›è¡Œä¸ºè¯¥ç”¨ SFT æ•™ï¼Œå“ªäº›è¯¥ç”¨ RL æ•™ï¼Ÿç›®å‰ä¸»è¦é ç»éªŒ
- **å¤šè½®å¯¹è¯ä¸­çš„ SFT é€€åŒ–**ï¼šæ¨¡å‹åœ¨é•¿å¯¹è¯åæœŸå›å¤è´¨é‡ä¸‹é™ï¼Œç›®å‰æ²¡æœ‰å®Œç¾è§£å†³æ–¹æ¡ˆ

### è„‘æš´æ‹“å±•

- **SFT + LoRA çš„å¤šä»»åŠ¡ç»„åˆ**ï¼šæ¯ä¸ªä»»åŠ¡è®­ç»ƒä¸€ä¸ª LoRA adapterï¼Œæ¨ç†æ—¶æŒ‰éœ€åŠ è½½ â†’ "MoE-LoRA" æ€è·¯ï¼ˆå·²æœ‰ç›¸å…³å·¥ä½œï¼šMoLoRAï¼‰
- **LIMA æ´å¯Ÿçš„æé™åœ¨å“ªï¼Ÿ**ï¼šå¦‚æœæ˜¯ 100 æ¡æ•°æ®å‘¢ï¼Ÿ10 æ¡ï¼Ÿæœ‰æ²¡æœ‰ä¸€ä¸ªæ•°æ®é‡çš„ä¸‹ç•Œï¼Ÿ
- **SFT æ•°æ®èƒ½å¦å…¨è‡ªåŠ¨ç”Ÿæˆï¼Ÿ**ï¼šSelf-Instruct â†’ Evol-Instruct â†’ æœªæ¥æ˜¯å¦å¯ä»¥å®Œå…¨é—­ç¯ï¼Œäººç±»åªåšæœ€ç»ˆè´¨é‡æŠŠå…³ï¼Ÿ

> ğŸ”— See also:
> - [[AI/LLM/RL/DPO/DPO-TRLå®è·µ|DPO]] â€” SFT ä¹‹åçš„å¯¹é½æ–¹æ³•ï¼Œæ¯” PPO æ›´ç®€å•
> - [[AI/LLM/RL/GRPO/GRPO æ·±åº¦ç†è§£|GRPO]] â€” DeepSeek çš„ RL æ–¹æ³•ï¼ŒSFT æ¨¡å‹æ˜¯èµ·ç‚¹
> - [[AI/LLM/RL/PPO/PPO åŸç†|PPO]] â€” InstructGPT ä½¿ç”¨çš„ç»å…¸ RL ç®—æ³•
> - [[AI/LLM/Frameworks/TRL/TRL æ¦‚è¿°|TRL]] â€” HuggingFace çš„è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒ SFT+DPO+PPO å…¨æµç¨‹

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Training language models to follow instructions with human feedback (InstructGPT)](https://arxiv.org/abs/2203.02155) â€” SFT+RLHF èŒƒå¼çš„å¥ åŸºè®ºæ–‡ï¼Œå¿…è¯»ä¸­çš„å¿…è¯» â­â­â­â­â­
- [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206) â€” è¯æ˜ 1000 æ¡é«˜è´¨é‡æ•°æ®è¶³ä»¥å¯¹é½ï¼Œé¢ è¦†"æ•°æ®è¶Šå¤šè¶Šå¥½"çš„ç›´è§‰ â­â­â­â­â­
- [Self-Instruct: Aligning Language Models with Self-Generated Instructions](https://arxiv.org/abs/2212.10560) â€” è‡ªåŠ¨ç”Ÿæˆ SFT æ•°æ®çš„å¼€å±±ä¹‹ä½œ â­â­â­â­
- [WizardLM: Empowering Large Language Models to Follow Complex Instructions](https://arxiv.org/abs/2304.12244) â€” Evol-Instruct è¿›åŒ–å¼æ•°æ®å¢å¼º â­â­â­â­
- [NEFTune: Noisy Embeddings Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914) â€” ç®€å•æœ‰æ•ˆçš„ SFT æ­£åˆ™åŒ–æŠ€å·§ â­â­â­â­

### æ·±åº¦è§£è¯»
- [å¤§æ¨¡å‹å‚æ•°é«˜æ•ˆå¾®è°ƒæŠ€æœ¯åŸç†ç»¼è¿°ï¼ˆçŸ¥ä¹ç³»åˆ—ï¼‰](https://zhuanlan.zhihu.com/p/635152813) â€” æœ€å…¨é¢çš„ä¸­æ–‡ PEFT æŠ€æœ¯ç»¼è¿° â­â­â­â­â­
- [Stanford Alpaca GitHub](https://github.com/tatsu-lab/stanford_alpaca) â€” å¼€æº SFT çš„æ ‡æ†é¡¹ç›®ï¼Œå«å®Œæ•´æ•°æ®å’Œä»£ç  â­â­â­â­

### å®è·µèµ„æº
- [HuggingFace TRL SFTTrainer](https://huggingface.co/docs/trl/sft_trainer) â€” æœ€æ˜“ç”¨çš„ SFT è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒ Packing/NEFTune/LoRA â­â­â­â­â­
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) â€” å›½äº§å…¨æµç¨‹ LLM å¾®è°ƒæ¡†æ¶ï¼Œä¸­æ–‡æ”¯æŒæå¥½ â­â­â­â­â­
- [Axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) â€” çµæ´»çš„ SFT è®­ç»ƒé…ç½®æ¡†æ¶ â­â­â­â­
