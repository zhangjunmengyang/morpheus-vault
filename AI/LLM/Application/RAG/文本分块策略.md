---
tags: [RAG, Text-Chunking, Document-Processing, NLP, Information-Retrieval]
created: 2026-02-14
status: draft
---

# 文本分块策略

文本分块（Text Chunking）是 [[RAG]] 系统中的关键预处理步骤，直接影响检索效果和生成质量。合适的分块策略需要在语义完整性、检索粒度和计算效率之间找到最佳平衡点。不同的文档类型、检索场景和下游任务需要采用不同的分块方法。

## 核心分块策略

### 1. Fixed-size Chunking（固定长度分块）

最简单直接的分块方法，按字符数或 token 数固定切分。

```python
class FixedSizeChunker:
    def __init__(self, chunk_size=512, overlap=50, tokenizer=None):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.tokenizer = tokenizer or self._default_tokenizer
    
    def chunk_text(self, text):
        if self.tokenizer:
            # 基于 token 分块
            tokens = self.tokenizer.encode(text)
            chunks = []
            
            for i in range(0, len(tokens), self.chunk_size - self.overlap):
                chunk_tokens = tokens[i:i + self.chunk_size]
                chunk_text = self.tokenizer.decode(chunk_tokens)
                chunks.append({
                    'text': chunk_text,
                    'start_token': i,
                    'end_token': i + len(chunk_tokens),
                    'token_count': len(chunk_tokens)
                })
            
            return chunks
        else:
            # 基于字符分块
            chunks = []
            for i in range(0, len(text), self.chunk_size - self.overlap):
                chunk_text = text[i:i + self.chunk_size]
                chunks.append({
                    'text': chunk_text,
                    'start_char': i,
                    'end_char': i + len(chunk_text),
                    'char_count': len(chunk_text)
                })
            
            return chunks
    
    def _default_tokenizer(self):
        from transformers import AutoTokenizer
        return AutoTokenizer.from_pretrained('bert-base-uncased')
```

**优点**：实现简单，处理速度快，内存占用可控
**缺点**：可能破坏语义完整性，分块边界任意

### 2. Recursive Chunking（递归分块）

基于文档结构层次化分块，优先按段落、句子、词语顺序分割。

```python
import re
from typing import List, Tuple

class RecursiveChunker:
    def __init__(self, chunk_size=1000, overlap=100):
        self.chunk_size = chunk_size
        self.overlap = overlap
        
        # 分隔符优先级：段落 > 句子 > 词语 > 字符
        self.separators = [
            "\n\n",      # 段落分隔
            "\n",        # 行分隔
            ". ",        # 句子分隔（英文）
            "。",        # 句子分隔（中文）
            "! ",        # 感叹句
            "? ",        # 疑问句
            "; ",        # 分号
            ", ",        # 逗号
            " ",         # 空格
            ""           # 字符级别
        ]
    
    def chunk_text(self, text: str) -> List[dict]:
        chunks = []
        current_chunks = [text]
        
        for separator in self.separators:
            next_chunks = []
            
            for chunk in current_chunks:
                if len(chunk) <= self.chunk_size:
                    next_chunks.append(chunk)
                else:
                    # 按当前分隔符继续分割
                    sub_chunks = self._split_text(chunk, separator)
                    next_chunks.extend(sub_chunks)
            
            current_chunks = next_chunks
            
            # 检查是否所有块都满足长度要求
            if all(len(chunk) <= self.chunk_size for chunk in current_chunks):
                break
        
        # 应用重叠策略
        overlapped_chunks = self._apply_overlap(current_chunks)
        
        # 构建返回结果
        for i, chunk_text in enumerate(overlapped_chunks):
            chunks.append({
                'text': chunk_text.strip(),
                'chunk_id': i,
                'length': len(chunk_text),
                'overlap_with_previous': i > 0
            })
        
        return chunks
    
    def _split_text(self, text: str, separator: str) -> List[str]:
        if separator == "":
            # 字符级别分割
            return [text[i:i+self.chunk_size] for i in range(0, len(text), self.chunk_size)]
        
        parts = text.split(separator)
        if len(parts) == 1:
            return [text]  # 无法分割
        
        chunks = []
        current_chunk = ""
        
        for part in parts:
            if len(current_chunk + separator + part) <= self.chunk_size:
                if current_chunk:
                    current_chunk += separator + part
                else:
                    current_chunk = part
            else:
                if current_chunk:
                    chunks.append(current_chunk)
                current_chunk = part
        
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks
    
    def _apply_overlap(self, chunks: List[str]) -> List[str]:
        if len(chunks) <= 1:
            return chunks
        
        overlapped = []
        for i, chunk in enumerate(chunks):
            if i == 0:
                overlapped.append(chunk)
            else:
                # 添加与前一个块的重叠部分
                prev_chunk = chunks[i-1]
                overlap_text = prev_chunk[-self.overlap:] if len(prev_chunk) > self.overlap else prev_chunk
                overlapped_chunk = overlap_text + " " + chunk
                overlapped.append(overlapped_chunk)
        
        return overlapped
```

### 3. Semantic Chunking（语义分块）

基于语义相似度进行智能分块，保持语义连贯性。

```python
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

class SemanticChunker:
    def __init__(self, model_name='sentence-transformers/all-MiniLM-L6-v2', 
                 similarity_threshold=0.7, max_chunk_size=1000):
        self.encoder = SentenceTransformer(model_name)
        self.similarity_threshold = similarity_threshold
        self.max_chunk_size = max_chunk_size
    
    def chunk_text(self, text: str) -> List[dict]:
        # 1. 句子分割
        sentences = self._split_into_sentences(text)
        
        # 2. 计算句子嵌入
        embeddings = self.encoder.encode(sentences)
        
        # 3. 基于相似度分组
        chunks = self._group_by_similarity(sentences, embeddings)
        
        # 4. 处理过长的块
        final_chunks = self._handle_long_chunks(chunks)
        
        return [{'text': chunk, 'chunk_id': i, 'type': 'semantic'} 
                for i, chunk in enumerate(final_chunks)]
    
    def _split_into_sentences(self, text: str) -> List[str]:
        # 使用正则表达式分割句子（支持中英文）
        import re
        sentence_pattern = r'[.!?。！？]+[\s]*'
        sentences = re.split(sentence_pattern, text)
        return [s.strip() for s in sentences if s.strip()]
    
    def _group_by_similarity(self, sentences: List[str], embeddings: np.ndarray) -> List[str]:
        chunks = []
        current_chunk = [sentences[0]]
        current_embedding = embeddings[0:1]
        
        for i in range(1, len(sentences)):
            # 计算当前句子与当前块的平均相似度
            chunk_centroid = np.mean(current_embedding, axis=0, keepdims=True)
            similarity = cosine_similarity(embeddings[i:i+1], chunk_centroid)[0][0]
            
            if similarity >= self.similarity_threshold and \
               len(' '.join(current_chunk + [sentences[i]])) <= self.max_chunk_size:
                # 添加到当前块
                current_chunk.append(sentences[i])
                current_embedding = np.vstack([current_embedding, embeddings[i:i+1]])
            else:
                # 开始新块
                chunks.append(' '.join(current_chunk))
                current_chunk = [sentences[i]]
                current_embedding = embeddings[i:i+1]
        
        # 添加最后一个块
        if current_chunk:
            chunks.append(' '.join(current_chunk))
        
        return chunks
    
    def _handle_long_chunks(self, chunks: List[str]) -> List[str]:
        """处理超长块，使用递归分块"""
        result = []
        recursive_chunker = RecursiveChunker(chunk_size=self.max_chunk_size)
        
        for chunk in chunks:
            if len(chunk) <= self.max_chunk_size:
                result.append(chunk)
            else:
                # 对超长块进行递归分块
                sub_chunks = recursive_chunker.chunk_text(chunk)
                result.extend([sub_chunk['text'] for sub_chunk in sub_chunks])
        
        return result
```

## 分块大小对检索效果的影响

### 理论分析

分块大小的选择涉及**检索精度**与**上下文完整性**的权衡：

```python
def analyze_chunk_size_impact(document, queries, chunk_sizes=[128, 256, 512, 1024]):
    """分析不同分块大小对检索效果的影响"""
    results = {}
    
    for size in chunk_sizes:
        chunker = FixedSizeChunker(chunk_size=size, overlap=size//10)
        chunks = chunker.chunk_text(document)
        
        # 构建检索系统
        retriever = build_retriever(chunks)
        
        metrics = {
            'precision@k': [],
            'recall@k': [],
            'context_completeness': [],
            'retrieval_latency': []
        }
        
        for query in queries:
            start_time = time.time()
            retrieved = retriever.search(query, k=5)
            latency = time.time() - start_time
            
            # 计算指标
            precision = compute_precision_at_k(query, retrieved, k=5)
            recall = compute_recall_at_k(query, retrieved, k=5)
            completeness = assess_context_completeness(query, retrieved)
            
            metrics['precision@k'].append(precision)
            metrics['recall@k'].append(recall)
            metrics['context_completeness'].append(completeness)
            metrics['retrieval_latency'].append(latency)
        
        results[size] = {k: np.mean(v) for k, v in metrics.items()}
    
    return results

# 分块大小权衡公式
def optimal_chunk_size(doc_length, query_complexity, model_context_window):
    """
    启发式公式计算最优分块大小
    
    doc_length: 文档平均长度
    query_complexity: 查询复杂度 (1-5)
    model_context_window: 模型上下文窗口大小
    """
    base_size = min(doc_length // 4, model_context_window // 8)
    
    # 复杂查询需要更大的块保持上下文
    complexity_factor = 1 + (query_complexity - 1) * 0.2
    
    optimal_size = int(base_size * complexity_factor)
    return min(optimal_size, model_context_window // 4)
```

### 经验规律

| 场景 | 推荐块大小 | 重叠比例 | 说明 |
|------|-----------|----------|------|
| 问答系统 | 256-512 tokens | 10-20% | 平衡精确性与上下文 |
| 长文档摘要 | 1024-2048 tokens | 20-30% | 保持段落完整性 |
| 代码检索 | 100-300 tokens | 5-10% | 函数/类级别粒度 |
| 法律文档 | 512-1024 tokens | 20-25% | 保持条款完整性 |
| 对话数据 | 50-150 tokens | 15-25% | 保持对话轮次 |

## Overlap 策略

重叠策略确保重要信息不会在分块边界丢失：

```python
class AdvancedOverlapStrategy:
    def __init__(self, overlap_ratio=0.1, semantic_overlap=True):
        self.overlap_ratio = overlap_ratio
        self.semantic_overlap = semantic_overlap
        if semantic_overlap:
            self.sentence_encoder = SentenceTransformer('all-MiniLM-L6-v2')
    
    def apply_overlap(self, chunks: List[str]) -> List[str]:
        if len(chunks) <= 1:
            return chunks
        
        overlapped_chunks = []
        
        for i, current_chunk in enumerate(chunks):
            if i == 0:
                overlapped_chunks.append(current_chunk)
                continue
            
            prev_chunk = chunks[i-1]
            
            if self.semantic_overlap:
                # 智能语义重叠
                overlap_content = self._semantic_overlap(prev_chunk, current_chunk)
            else:
                # 简单尾部重叠
                overlap_size = int(len(prev_chunk) * self.overlap_ratio)
                overlap_content = prev_chunk[-overlap_size:]
            
            overlapped_chunk = overlap_content + " " + current_chunk
            overlapped_chunks.append(overlapped_chunk)
        
        return overlapped_chunks
    
    def _semantic_overlap(self, prev_chunk: str, current_chunk: str) -> str:
        """基于语义相似度的智能重叠"""
        # 获取前一个块的末尾句子
        prev_sentences = prev_chunk.split('. ')[-3:]  # 最后3个句子
        current_sentences = current_chunk.split('. ')[:3]  # 前3个句子
        
        # 计算句子间相似度
        all_sentences = prev_sentences + current_sentences
        embeddings = self.sentence_encoder.encode(all_sentences)
        
        # 找到语义边界
        similarities = []
        for i in range(len(prev_sentences)):
            for j in range(len(current_sentences)):
                sim = cosine_similarity(
                    embeddings[i:i+1], 
                    embeddings[len(prev_sentences)+j:len(prev_sentences)+j+1]
                )[0][0]
                similarities.append((i, j, sim))
        
        # 选择相似度最高的边界
        if similarities:
            best_i, best_j, best_sim = max(similarities, key=lambda x: x[2])
            if best_sim > 0.6:  # 有语义连接
                overlap_sentences = prev_sentences[best_i:]
                return '. '.join(overlap_sentences)
        
        # 回退到固定比例重叠
        overlap_size = int(len(prev_chunk) * self.overlap_ratio)
        return prev_chunk[-overlap_size:]
```

## 特殊文档处理

### 表格数据处理

```python
class TableChunker:
    def __init__(self):
        self.table_detector = self._init_table_detector()
    
    def chunk_table_document(self, text: str) -> List[dict]:
        chunks = []
        
        # 1. 检测表格
        tables = self._extract_tables(text)
        
        # 2. 处理表格
        for table in tables:
            table_chunks = self._chunk_table(table)
            chunks.extend(table_chunks)
        
        # 3. 处理非表格文本
        non_table_text = self._remove_tables(text, tables)
        text_chunks = RecursiveChunker().chunk_text(non_table_text)
        chunks.extend(text_chunks)
        
        return chunks
    
    def _chunk_table(self, table: dict) -> List[dict]:
        """表格分块策略"""
        chunks = []
        
        # 策略1：按行分块
        header = table['header']
        rows = table['rows']
        
        chunk_size = 10  # 每块包含的行数
        for i in range(0, len(rows), chunk_size):
            chunk_rows = rows[i:i+chunk_size]
            
            # 构建可搜索的表格文本
            table_text = self._table_to_text(header, chunk_rows)
            
            chunks.append({
                'text': table_text,
                'type': 'table',
                'table_id': table['id'],
                'row_range': (i, min(i+chunk_size, len(rows)))
            })
        
        return chunks
    
    def _table_to_text(self, header: List[str], rows: List[List[str]]) -> str:
        """将表格转换为可搜索的文本格式"""
        lines = []
        
        # 添加表头描述
        lines.append(f"表格包含以下列：{', '.join(header)}")
        
        # 逐行描述
        for row in rows:
            row_desc = []
            for col_name, value in zip(header, row):
                if value.strip():  # 跳过空值
                    row_desc.append(f"{col_name}为{value}")
            
            if row_desc:
                lines.append("；".join(row_desc))
        
        return "\n".join(lines)
```

### 代码文档处理

```python
import ast
import tokenize
from io import StringIO

class CodeChunker:
    def __init__(self, language='python'):
        self.language = language
        self.chunk_by_function = True
    
    def chunk_code(self, code: str) -> List[dict]:
        if self.language == 'python':
            return self._chunk_python_code(code)
        else:
            # 其他语言使用基于行的分块
            return self._chunk_by_lines(code)
    
    def _chunk_python_code(self, code: str) -> List[dict]:
        chunks = []
        
        try:
            tree = ast.parse(code)
            
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)):
                    # 提取函数/类的代码块
                    start_line = node.lineno
                    end_line = getattr(node, 'end_lineno', start_line)
                    
                    code_lines = code.split('\n')
                    chunk_code = '\n'.join(code_lines[start_line-1:end_line])
                    
                    # 添加文档字符串和注释
                    docstring = ast.get_docstring(node) or ""
                    
                    chunks.append({
                        'text': chunk_code,
                        'docstring': docstring,
                        'type': 'function' if isinstance(node, ast.FunctionDef) else 'class',
                        'name': node.name,
                        'start_line': start_line,
                        'end_line': end_line,
                        'searchable_text': f"{node.name} {docstring} {chunk_code}"
                    })
            
        except SyntaxError:
            # 语法错误时回退到行分块
            return self._chunk_by_lines(code)
        
        return chunks
    
    def _chunk_by_lines(self, code: str, lines_per_chunk=50) -> List[dict]:
        lines = code.split('\n')
        chunks = []
        
        for i in range(0, len(lines), lines_per_chunk):
            chunk_lines = lines[i:i+lines_per_chunk]
            chunk_text = '\n'.join(chunk_lines)
            
            chunks.append({
                'text': chunk_text,
                'type': 'code_block',
                'start_line': i + 1,
                'end_line': min(i + lines_per_chunk, len(lines)),
                'searchable_text': chunk_text
            })
        
        return chunks
```

### 多语言文档处理

```python
from langdetect import detect, detect_langs
import jieba  # 中文分词

class MultilingualChunker:
    def __init__(self):
        self.language_chunkers = {
            'zh': self._chunk_chinese,
            'en': self._chunk_english,
            'ja': self._chunk_japanese
        }
    
    def chunk_multilingual_text(self, text: str) -> List[dict]:
        # 1. 语言检测
        try:
            language = detect(text)
        except:
            language = 'en'  # 默认英文
        
        # 2. 选择相应的分块策略
        if language in self.language_chunkers:
            return self.language_chunkers[language](text)
        else:
            # 默认使用英文分块策略
            return self._chunk_english(text)
    
    def _chunk_chinese(self, text: str) -> List[dict]:
        """中文分块：考虑词边界和语法结构"""
        import re
        
        # 按句号、问号、感叹号分句
        sentences = re.split(r'[。！？]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            # 使用 jieba 进行分词，估算语义复杂度
            words = list(jieba.cut(sentence))
            
            if len(current_chunk + sentence) > 500:  # 中文字符数限制
                if current_chunk:
                    chunks.append({
                        'text': current_chunk.strip(),
                        'language': 'zh',
                        'word_count': len(list(jieba.cut(current_chunk)))
                    })
                current_chunk = sentence
            else:
                current_chunk += sentence + "。"
        
        if current_chunk:
            chunks.append({
                'text': current_chunk.strip(),
                'language': 'zh',
                'word_count': len(list(jieba.cut(current_chunk)))
            })
        
        return chunks
    
    def _chunk_english(self, text: str) -> List[dict]:
        """英文分块：基于句子和词汇"""
        import nltk
        from nltk.tokenize import sent_tokenize, word_tokenize
        
        sentences = sent_tokenize(text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            words = word_tokenize(sentence)
            
            if len(current_chunk.split()) + len(words) > 100:  # 词数限制
                if current_chunk:
                    chunks.append({
                        'text': current_chunk.strip(),
                        'language': 'en',
                        'word_count': len(current_chunk.split())
                    })
                current_chunk = sentence
            else:
                current_chunk += " " + sentence if current_chunk else sentence
        
        if current_chunk:
            chunks.append({
                'text': current_chunk.strip(),
                'language': 'en',
                'word_count': len(current_chunk.split())
            })
        
        return chunks
```

## Late Chunking

Late Chunking 是在嵌入生成之后进行分块的策略，保留更多上下文信息：

```python
class LateChunker:
    def __init__(self, encoder_model, max_tokens=512):
        self.encoder = encoder_model
        self.max_tokens = max_tokens
        self.tokenizer = encoder_model.tokenizer
    
    def late_chunk_and_embed(self, text: str) -> List[dict]:
        """
        Late Chunking 流程：
        1. 对完整文档进行编码
        2. 根据注意力权重确定分块边界
        3. 从全文嵌入中提取块嵌入
        """
        
        # 1. 完整文档编码（包含注意力权重）
        inputs = self.tokenizer(text, return_tensors='pt', truncation=True, 
                               max_length=self.max_tokens, return_attention_mask=True)
        
        with torch.no_grad():
            outputs = self.encoder(**inputs, output_attentions=True)
            full_embeddings = outputs.last_hidden_state[0]  # [seq_len, hidden_size]
            attention_weights = outputs.attentions  # 多层注意力权重
        
        # 2. 基于注意力权重识别语义边界
        boundaries = self._find_semantic_boundaries(attention_weights, inputs['input_ids'][0])
        
        # 3. 根据边界提取分块嵌入
        chunks = []
        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
        
        for i, (start, end) in enumerate(boundaries):
            chunk_tokens = tokens[start:end]
            chunk_text = self.tokenizer.convert_tokens_to_string(chunk_tokens)
            
            # 提取对应的嵌入向量（平均池化）
            chunk_embedding = torch.mean(full_embeddings[start:end], dim=0)
            
            chunks.append({
                'text': chunk_text,
                'embedding': chunk_embedding.numpy(),
                'token_range': (start, end),
                'chunk_id': i
            })
        
        return chunks
    
    def _find_semantic_boundaries(self, attention_weights, input_ids):
        """基于注意力权重寻找语义边界"""
        # 使用最后一层的注意力权重
        last_layer_attention = attention_weights[-1][0]  # [num_heads, seq_len, seq_len]
        
        # 计算每个位置的平均注意力连接强度
        attention_strength = torch.mean(last_layer_attention, dim=0)  # [seq_len, seq_len]
        local_attention = torch.diagonal(attention_strength, offset=1)  # 相邻位置注意力
        
        # 寻找注意力强度的局部最小值作为分块边界
        boundaries = []
        current_start = 0
        
        for i in range(1, len(local_attention)):
            # 如果注意力强度低于阈值且当前块长度合适
            if local_attention[i] < 0.1 and i - current_start > 50:
                boundaries.append((current_start, i))
                current_start = i
        
        # 添加最后一个块
        if current_start < len(input_ids):
            boundaries.append((current_start, len(input_ids)))
        
        return boundaries
```

## 面试常见问题

**Q1：为什么要设置块与块之间的重叠（Overlap）？如何确定合适的重叠比例？**

A：设置重叠的主要原因：1）**防止信息丢失**：重要信息可能跨越分块边界，重叠确保完整保留；2）**增强检索召回**：同一信息出现在多个块中，提高被检索到的概率；3）**改善上下文连贯性**：为 LLM 提供更完整的上下文。重叠比例通常设置为 10-30%：简单文档用 10-15%，复杂文档或长文档用 20-30%。过少可能丢失信息，过多会增加存储成本和检索噪音。

**Q2：Recursive Chunking 相比 Fixed-size Chunking 有什么优势？在什么场景下更适用？**

A：Recursive Chunking 的优势：1）**保持语义完整性**：优先在自然边界（段落、句子）分割，避免破坏语义单元；2）**适应文档结构**：能够处理各种文档格式和结构；3）**更好的可读性**：分块结果更符合人类阅读习惯。适用场景：结构化文档（如技术文档、学术论文）、需要保持语义完整性的场景、用户需要阅读检索结果的应用。Fixed-size 更适合处理速度要求高、文档结构不规则、或者主要用于向量检索而不需要人工阅读的场景。

**Q3：Semantic Chunking 的实现原理是什么？计算开销如何？**

A：Semantic Chunking 通过计算句子间语义相似度进行分组：1）**句子嵌入**：将文档分句后用预训练模型编码；2）**相似度计算**：计算相邻句子或句子与当前块中心的余弦相似度；3）**阈值判断**：相似度高于阈值则合并，否则开始新块。计算开销主要在句子编码阶段，时间复杂度 O(n×d)，其中 n 是句子数，d 是编码时间。可通过批处理、预计算、缓存等方式优化。适合对语义质量要求高、处理时间不敏感的场景。

**Q4：在处理代码文档时，为什么要采用特殊的分块策略？**

A：代码文档的特殊性：1）**结构化特征**：函数、类、模块有明确边界，应按逻辑单元分块；2）**语法依赖**：代码片段需要保持语法完整性，随意切分会破坏可执行性；3）**文档字符串**：代码注释和文档字符串包含重要检索信息，需要与代码关联；4）**命名空间**：函数名、变量名是重要的检索关键词。因此采用 AST 解析按函数/类分块，结合文档字符串构建搜索文本，保持代码的语义完整性和可理解性。

**Q5：Late Chunking 相比传统分块方法有什么创新？适用于哪些场景？**

A：Late Chunking 的创新：1）**全局上下文**：先对完整文档编码，保留全局语义信息；2）**注意力导向**：基于 Transformer 的注意力权重识别语义边界，而非简单的文本特征；3）**嵌入质量**：分块嵌入来源于全文编码，包含更丰富的上下文信息。适用场景：长文档处理（如学术论文、技术规范）、需要全局理解的文档、语义边界不明确的文本。缺点是计算开销大，需要完整文档能放入模型上下文窗口，适合文档数量不大但质量要求高的场景。

相关链接：[[RAG]], [[Reranker]], [[Advanced RAG]], [[向量数据库]], [[LLM 评测体系]], [[文档解析]]