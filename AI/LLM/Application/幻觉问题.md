---
tags: [llm, hallucination, safety, interview-prep]
created: 2026-02-14
status: draft
---

# 幻觉问题（成因/检测/缓解）

## 概述

大语言模型（LLM）幻觉是指模型生成看似合理但实际上错误、误导或无法验证的信息。这是当前 LLM 应用面临的核心挑战之一，直接影响模型的可信度和实用性。

## 幻觉分类

### 事实性幻觉 (Factual Hallucination)
指模型生成与客观事实不符的信息。
- **错误事实**：如错误的历史日期、地理信息、科学定律
- **虚构实体**：创造不存在的人物、机构、事件
- **数值错误**：统计数据、计算结果的错误

### 忠实性幻觉 (Faithfulness Hallucination)
指模型生成的内容与给定的输入上下文不一致。
- **上下文偏离**：回答与提供的文档内容矛盾
- **指令误解**：违背用户明确的指令要求
- **逻辑不一致**：前后回答自相矛盾

## 成因分析

### 训练数据偏差
- **数据质量问题**：训练数据中包含错误、过时或有偏见的信息
- **数据分布不均**：某些领域或语言的数据稀少，导致模型在这些领域表现不佳
- **噪声标注**：人工标注数据存在错误，影响模型学习

### 解码策略问题
- **[[采样策略]]**：高温度采样增加创造性但也增加了不确定性
- **Beam Search局限**：局部最优解可能导致全局不合理
- **长序列生成**：随着生成长度增加，累积误差导致偏离正确轨道

### 知识边界模糊
- **参数化知识限制**：模型只能依赖训练时的知识，无法获取最新信息
- **知识冲突**：不同来源的矛盾信息在训练数据中共存
- **概率性推理**：模型基于统计关联而非真实理解生成内容

### Exposure Bias
- **训练时预测**：训练时模型看到真实的前文序列
- **推理时累积误差**：推理时基于自己生成的内容继续预测，误差会放大
- **分布偏移**：训练和推理时的数据分布不一致

## 检测方法

### SelfCheckGPT
利用模型自身的不确定性进行幻觉检测。
```
核心思路：让模型多次采样生成回答，通过一致性评估检测潜在幻觉
优势：无需外部知识库
局限：计算成本较高，且模型可能在错误上保持一致
```

### G-Eval
基于 GPT 的自动化评估框架。
- 使用更强大的模型评估目标模型输出
- 支持多维度评估（事实性、相关性、连贯性）
- 可定制评估标准和提示模板

### 知识三元组验证
将生成内容转换为结构化知识三元组进行验证。
- **实体抽取**：识别文本中的关键实体
- **关系抽取**：提取实体间的关系
- **知识库查询**：与外部知识库对比验证

### FActScore
细粒度事实精度评分方法。
```
评估流程：
1. 将长文本分解为原子性事实声明
2. 逐个验证每个事实的准确性
3. 计算总体事实准确率
```

## 缓解策略

### RAG增强 ([[RAG 原理与架构]])
通过检索增强生成提供外部知识支持。
- **实时检索**：获取最新、准确的信息
- **上下文注入**：将检索结果注入生成过程
- **多源验证**：结合多个可信数据源

### Chain-of-Thought (CoT) 推理
引导模型进行逐步推理。
- **思维链提示**：要求模型展示推理步骤
- **自我验证**：让模型检查自己的推理过程
- **中间监督**：在推理链的关键节点进行人工或自动检查

### Constrained Decoding
约束生成过程以提高输出质量。
- **词汇约束**：限制可使用的词汇表
- **格式约束**：确保输出符合特定结构
- **逻辑约束**：基于规则引擎限制生成内容

### RLHF对齐 ([[RLHF]])
通过人类反馈强化学习提升模型行为。
- **奖励模型训练**：学习人类偏好
- **PPO优化**：基于奖励模型调优生成策略
- **Constitutional AI**：基于价值观对齐的训练方法

### 检索验证链
结合检索和验证的多步骤方法。
```
流程：生成 → 检索相关信息 → 交叉验证 → 修正或确认 → 最终输出
```

## 面试常见问题

### Q1: LLM幻觉的主要类型有哪些？请举例说明。

**答案**：主要分为事实性幻觉和忠实性幻觉。事实性幻觉如模型声称"拿破仑是在1815年滑铁卢战役中战胜的"（实际是被击败），忠实性幻觉如用户提供文档说"公司2023年收入1亿"，模型却回答"根据文档，公司2023年收入2亿"。

### Q2: 为什么增加模型规模不能完全解决幻觉问题？

**答案**：虽然大模型通常事实准确率更高，但幻觉问题有结构性成因：（1）训练数据本身包含错误信息；（2）模型是基于统计关联而非真实理解；（3）参数化知识存在边界，无法处理训练后的新信息；（4）Exposure bias导致推理时误差累积。因此需要架构层面的解决方案如RAG。

### Q3: SelfCheckGPT的核心原理是什么？有什么局限性？

**答案**：核心原理是利用模型的内在不确定性：对同一问题多次采样，如果模型对某个事实不确定，不同采样结果会出现不一致，据此检测潜在幻觉。局限性包括：（1）计算成本高；（2）模型可能在错误回答上保持一致性；（3）需要足够的采样次数才能有效；（4）对于模型高度自信的错误信息检测效果有限。

### Q4: 在生产环境中如何设计一个完整的幻觉缓解系统？

**答案**：采用多层防御策略：（1）**预处理层**：使用RAG提供可靠上下文；（2）**生成层**：采用CoT推理，控制采样参数；（3）**后处理层**：事实检查、一致性验证；（4）**反馈层**：收集用户反馈，持续改进；（5）**监控层**：实时检测异常输出，设置置信度阈值。同时建立人工审核机制处理高风险场景。

### Q5: RAG如何具体缓解幻觉问题？有哪些技术细节需要注意？

**答案**：RAG通过提供外部权威信息源缓解幻觉：（1）**检索质量**：确保检索到相关、准确的信息片段；（2）**上下文融合**：合理组织检索内容与用户query；（3）**来源标注**：明确信息来源，提高可追溯性；（4）**实时更新**：保证知识库内容的时效性。技术细节包括向量检索的相似度阈值设置、检索结果的重排序、多源信息的冲突处理等。

## 参考资料

- [[大语言模型可信度评估]]
- [[RAG 原理与架构]]
- [[RLHF]]
- [[采样策略]]
- [[模型对齐技术]]