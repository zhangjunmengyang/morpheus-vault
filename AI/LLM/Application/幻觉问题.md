---
title: "LLM 幻觉问题"
date: 2026-02-14
tags: [hallucination, safety, evaluation, interview]
type: note
---

# LLM 幻觉问题：深度面试笔记

> 幻觉（Hallucination）是大语言模型最核心的安全与可靠性问题之一。模型生成的内容看似流畅自信，实则与事实不符或与输入上下文矛盾。本文从定义分类、根因分析、检测方法、缓解策略、对齐关系、评测基准、工业实践七个维度进行系统梳理，并附面试高频题。

---

## 1. 幻觉的定义与分类

### 1.1 什么是幻觉

幻觉（Hallucination）指语言模型生成的内容**不忠实于源输入（faithfulness）**或**不符合可验证的现实世界事实（factuality）**。这个术语借自心理学——就像人类在没有外部刺激的情况下产生感知体验一样，LLM 在没有充分证据支撑的情况下"凭空捏造"信息。

关键特征：
- **表面流畅性**：幻觉内容在语法和风格上通常无懈可击，很难通过阅读直觉发现
- **高置信度**：模型在生成错误内容时往往表现出与正确内容相同的"自信"
- **非随机性**：幻觉不是随机噪声，而是模型基于训练分布中的统计模式做出的"合理但错误"的推断

### 1.2 事实性幻觉 vs 忠实性幻觉

这是最经典的二分法，由 Maynez et al. (2020) 在摘要生成任务中首次系统提出：

#### 事实性幻觉（Factuality Hallucination）

模型生成的内容与**可验证的现实世界知识**相矛盾。

| 子类型 | 定义 | 示例 |
|--------|------|------|
| 事实错误 | 生成的事实陈述与真实世界不符 | "爱因斯坦于1921年获得诺贝尔化学奖"（实际是物理学奖） |
| 实体混淆 | 将不同实体的属性错误组合 | "马斯克创办了亚马逊"（实际是贝索斯） |
| 时间错位 | 将事件放在错误的时间背景下 | "GPT-4 于 2021 年发布"（实际是 2023 年） |
| 数值捏造 | 编造具体的数字或统计数据 | "该研究涉及 15,234 名受试者"（完全虚构的数字） |
| 引用虚构 | 编造不存在的论文、书籍或引用 | 虚构一篇看似合理的 arXiv 论文及其作者 |

#### 忠实性幻觉（Faithfulness Hallucination）

模型生成的内容与**给定的输入源（source input）**不一致。常见于有条件生成任务（摘要、翻译、RAG 问答等）。

| 子类型 | 定义 | 示例 |
|--------|------|------|
| 信息添加 | 输出中包含源文档中不存在的信息 | 摘要中出现原文未提及的结论 |
| 信息遗漏 | 忽略源文档中的关键信息导致含义改变 | 翻译中省略了否定词导致语义反转 |
| 信息矛盾 | 输出与源文档中的信息直接冲突 | RAG 检索到 A 但回答了 B |

**面试要点**：事实性幻觉关注"模型说的是否为真"，忠实性幻觉关注"模型说的是否符合给定输入"。一个开放域问答系统主要面对事实性幻觉，而一个文档摘要系统主要面对忠实性幻觉。

### 1.3 内在幻觉 vs 外在幻觉

这是 Ji et al. (2023) 在综述论文 "Survey of Hallucination in Natural Language Generation" 中使用的另一种分类维度：

#### 内在幻觉（Intrinsic Hallucination）

生成内容与**源输入直接矛盾**。例如，源文档说"该公司2023年收入增长了5%"，模型摘要却写"该公司2023年收入下降了5%"。

- 本质是信息扭曲（distortion）
- 相对容易通过自动化方法检测（因为有明确的矛盾信号）
- 通常是模型在编码-解码过程中的注意力失误

#### 外在幻觉（Extrinsic Hallucination）

生成内容**既不能被源输入支持也不能被其反驳**，即模型引入了源输入中完全不存在的信息。

- 本质是信息捏造（fabrication）
- 更难检测，因为需要外部知识来判断真伪
- 外在幻觉不一定是错误的——模型可能恰好引入了正确的外部知识，但这种不可控的"创造"在严肃场景下仍然是不可接受的

**两种分类体系的关系**：

```
事实性幻觉 ≈ 外在幻觉的一种特殊情况（当外在信息与事实矛盾时）
忠实性幻觉 ⊃ 内在幻觉 + 部分外在幻觉
```

### 1.4 其他分类视角

- **开放域 vs 封闭域幻觉**：开放域（如聊天）幻觉更难界定，封闭域（如文档问答）有明确的ground truth
- **良性 vs 恶性幻觉**：创意写作中的"幻觉"可能是特性（feature），而在医疗问答中则是致命缺陷
- **可检测 vs 不可检测幻觉**：有些幻觉可以通过自动化工具发现，有些需要领域专家才能识别

---

## 2. 幻觉产生的根本原因

幻觉不是单一原因导致的，而是训练数据、模型架构、训练目标、解码策略等多个层面问题的综合结果。

### 2.1 训练数据噪音与偏差

#### 数据质量问题

LLM 的训练数据通常来自互联网爬取，不可避免地包含：

- **事实错误**：网页上的错误信息、过时信息
- **矛盾信息**：不同来源对同一事件的描述不一致
- **虚构内容**：小说、讽刺文章、钓鱼网页等
- **偏见与刻板印象**：系统性的社会偏见被编码进模型

当模型在包含错误信息的数据上训练时，它学到了"错误也是一种合理的输出模式"。更糟糕的是，模型无法区分训练数据中哪些是事实、哪些是虚构——它只是学习统计相关性。

#### 长尾知识问题

训练数据中高频出现的知识会被模型很好地学习，但长尾知识（出现频次极低的事实）则容易被"淹没"。当用户查询长尾知识时，模型倾向于用高频知识模式去"填充"答案，导致看似合理但实际错误的输出。

#### 知识截止（Knowledge Cutoff）

模型的训练数据有时间截止点。对于截止日期之后的事件，模型要么拒绝回答，要么（更常见地）基于截止前的信息做出不准确的推测。

### 2.2 Exposure Bias（暴露偏差）

这是自回归语言模型的一个结构性问题：

- **训练时**：模型使用 teacher forcing，即每一步都基于**真实的前序 token** 预测下一个 token
- **推理时**：模型使用自己**之前生成的 token** 作为输入来生成后续 token

这种训练-推理之间的分布不匹配就是 exposure bias。其后果是：

1. **错误累积**：一旦模型在某一步生成了一个不太准确的 token，后续所有 token 都在这个错误基础上生成，错误像滚雪球一样累积
2. **缺乏纠错能力**：模型从未在训练中学过"如何从自己的错误中恢复"，因为训练时它总是看到正确的历史
3. **长文本退化**：生成越长的文本，累积的偏差越大，幻觉概率越高

```
训练时：  [真实token₁] → [真实token₂] → [真实token₃] → 预测token₄
推理时：  [生成token₁] → [生成token₂] → [生成token₃] → 预测token₄
                ↑ 可能有偏差    ↑ 偏差累积      ↑ 偏差加剧
```

### 2.3 Softmax 过度自信与校准问题

#### 过度自信现象

Transformer 模型最后一层的 softmax 输出通常呈现出**过度自信**的分布特征：

- 即使模型"不确定"，softmax 输出的概率分布仍然会有一个或少数几个 token 获得很高的概率
- 这意味着模型的输出概率不能忠实地反映其真实的不确定性
- 结果是模型"说错话时也很有自信"，用户无法从模型的表现中判断其回答的可靠性

#### 校准不良（Poor Calibration）

理想情况下，如果模型以 80% 的置信度输出某个答案，那么这个答案应该在 80% 的情况下是正确的。但实际上：

- 现代 LLM 的校准性通常很差
- 模型倾向于高估自己的正确概率（overconfident）
- RLHF 训练进一步加剧了这个问题——模型被训练得更加"果断"和"流畅"，但这种果断并不反映真实的知识确定性

#### 技术根因

```python
# softmax 的性质导致了过度自信
# 当 logits 的数值范围增大时，softmax 分布趋向于 one-hot
import numpy as np

logits = np.array([2.0, 1.0, 0.5, 0.1])
probs = np.exp(logits) / np.sum(np.exp(logits))
# probs ≈ [0.50, 0.18, 0.11, 0.07]  -- 看起来很"确定"
# 但实际上模型可能对多个选项都不太有把握
```

temperature sampling 可以平滑分布，但并不能从根本上解决校准问题。

### 2.4 知识边界模糊

#### 模型不知道自己不知道什么

这是幻觉问题最深层的哲学和技术困境：

- LLM 没有显式的**元认知能力**（metacognition）——它无法可靠地评估自己对某个知识点的掌握程度
- 模型的"知识"以分布式的方式存储在参数中，没有明确的"知道/不知道"边界
- 当遇到知识边界附近的问题时，模型不会说"我不确定"，而是**基于统计模式生成一个看似合理的答案**

#### 参数化知识 vs 语境知识的冲突

```
用户：[提供文档] 请根据文档回答问题
模型内部冲突：
  - 参数化知识（训练时学到的）：答案是 A
  - 语境知识（文档中说的）：答案是 B
  - 模型可能输出 A（忽略文档）或 混合 A 和 B（幻觉）
```

这种冲突在 RAG 系统中尤为常见，被称为 **knowledge conflict** 问题。研究表明，当参数化知识与检索到的上下文冲突时，较大的模型更倾向于依赖自己的参数化知识，这反而增加了幻觉风险。

### 2.5 解码策略的影响

不同的解码策略对幻觉率有直接影响：

| 解码策略 | 幻觉倾向 | 原因 |
|----------|----------|------|
| Greedy Decoding | 中等 | 可能陷入重复，但不会引入随机噪声 |
| Beam Search | 较低 | 考虑多个候选序列，但可能过于保守 |
| Top-k Sampling | 较高 | k 值大时引入更多随机性和不确定性 |
| Top-p (Nucleus) | 中等 | 动态截断尾部概率，效果依赖 p 值 |
| Temperature > 1 | 高 | 增加随机性，鼓励"创造性"输出 |

### 2.6 注意力机制的局限性

- **长距离依赖衰减**：尽管 Transformer 理论上可以关注任意距离的 token，但实践中注意力权重在长距离上显著衰减（尤其是 "lost in the middle" 现象）
- **注意力稀释**：当上下文很长时，关键信息的注意力权重被稀释，模型可能"忽略"重要的约束条件
- **位置偏差**：模型对开头和结尾的信息给予更多关注，中间部分的信息容易被遗漏

---

## 3. 幻觉检测方法

### 3.1 SelfCheckGPT

**核心思想**：如果模型真正"知道"某个事实，那么多次独立采样应该产生一致的答案；如果模型在"编造"，不同采样之间会产生不一致的描述。

**算法流程**：

```
1. 对同一个 prompt，采样 N 次独立响应（高 temperature）
2. 将原始响应按句子分割
3. 对每个句子，检查它与 N 个采样响应的一致性
4. 一致性低的句子标记为可能的幻觉
```

**一致性检查的三种实现**：

- **BERTScore**：计算句子与采样响应之间的语义相似度
- **QA-based**：从句子中提取问题，检查采样响应是否给出一致的答案
- **NLI-based**：使用 NLI 模型判断采样响应是否蕴含（entail）该句子
- **LLM-Prompt**：直接让另一个 LLM 判断一致性

**优势**：
- 无需外部知识库或 ground truth
- 黑盒方法，适用于任何 LLM
- 可以提供句子级别的幻觉标注

**局限**：
- 如果模型对某个错误信息非常"自信"（训练数据中广泛存在的错误），多次采样也会一致地输出错误
- 需要多次推理调用，成本较高
- 对忠实性幻觉检测效果有限

### 3.2 事实核查 Pipeline（Fact-Checking Pipeline）

一个完整的自动化事实核查流水线通常包括以下模块：

```
原始文本 → [声明分解] → [证据检索] → [立场检测] → [判决输出]
            Claim         Evidence      Stance        Verdict
         Decomposition    Retrieval    Detection     Generation
```

**Step 1 - 声明分解（Claim Decomposition）**

将模型生成的文本分解为独立的、可验证的原子声明（atomic claims）。

```
输入："爱因斯坦在1905年提出了相对论，并因此获得了诺贝尔物理学奖"
分解：
  - Claim 1: 爱因斯坦在1905年提出了相对论 ✓
  - Claim 2: 爱因斯坦因相对论获得诺贝尔物理学奖 ✗（实际是因光电效应）
```

**Step 2 - 证据检索（Evidence Retrieval）**

从可信知识源中检索与每个声明相关的证据：
- Wikipedia
- 知识图谱（Wikidata、Freebase）
- 权威数据库
- 搜索引擎实时结果

**Step 3 - 立场检测（Stance Detection / NLI）**

判断检索到的证据是支持（support）、反驳（contradict）还是无关（neutral）于给定的声明。

**Step 4 - 判决输出**

综合所有证据的立场，给出最终判断：Supported / Refuted / Not Enough Info。

### 3.3 NLI-based 方法

自然语言推理（Natural Language Inference）模型可以直接用于幻觉检测：

**基本思路**：
- 将**源文档**作为前提（premise）
- 将**模型生成的每个句子**作为假设（hypothesis）
- 使用 NLI 模型判断蕴含关系

**具体方法**：

1. **SummaC**（Laban et al., 2022）：将文档和摘要分别按句子分割，构建 NLI 分数矩阵，通过聚合策略得到整体忠实度分数
2. **AlignScore**（Zha et al., 2023）：统一的文本对齐评估框架，在多个 NLI/事实性数据集上联合训练
3. **TRUE**（Honovich et al., 2022）：大规模 NLI 模型用于事实一致性评估

**局限性**：
- NLI 模型自身也可能出错
- 对于需要多步推理的事实，简单的 premise-hypothesis 对判断不够
- 对数值比较、时间顺序等细粒度事实核查效果有限

### 3.4 知识图谱验证

利用结构化知识图谱（KG）来验证模型输出的事实性：

**流程**：
```
模型输出 → [实体识别 + 关系抽取] → (头实体, 关系, 尾实体) 三元组
                                           ↓
                                    在知识图谱中查询
                                           ↓
                                   匹配 → 支持  /  不匹配 → 矛盾  /  未找到 → 无法验证
```

**优势**：精确的结构化验证，对实体属性类事实特别有效

**局限**：
- KG 覆盖度有限，很多事实在 KG 中不存在
- 实体消歧（entity disambiguation）仍然是难题
- 无法处理非结构化的复杂推理型声明

### 3.5 基于内部状态的检测

利用模型自身的内部表示来检测幻觉（白盒方法）：

- **注意力权重分析**：幻觉内容的注意力模式可能与忠实内容不同
- **隐藏层表示**：训练一个探针（probe）分类器，基于隐藏层表示判断当前 token 是否为幻觉
- **logit 不确定性**：输出 token 的 logit 分布熵值高可能意味着不确定性大
- **ITI（Inference-Time Intervention）**：Li et al. (2023) 发现可以通过在推理时干预特定注意力头来减少幻觉

### 3.6 LLM-as-Judge

使用另一个（通常更强的）LLM 来评估目标模型输出的事实性：

```
Prompt: "请判断以下回答是否包含事实错误。
问题：{question}
回答：{answer}
参考资料：{evidence}
请列出所有事实错误并解释。"
```

**优缺点**：
- ✅ 灵活、易部署、不需要专门训练
- ✅ 可以处理复杂的推理链
- ❌ 评判模型自身也可能产生幻觉
- ❌ 存在偏好偏差（如倾向认为更长的回答更好）

---

## 4. 幻觉缓解策略

### 4.1 训练阶段

#### 4.1.1 数据清洗与策划

**高质量数据筛选**：
- 去除训练数据中的明显事实错误
- 使用事实核查工具对训练数据进行预过滤
- 提高权威来源（教科书、百科全书、学术论文）的采样权重
- 减少虚构内容（小说、创意写作）在训练数据中的比例（或添加明确的标记）

**数据去重与去冲突**：
- 去除重复和近重复内容，避免模型过度记忆特定表述
- 识别并处理训练数据中的矛盾信息
- 时间敏感信息标注时间戳

#### 4.1.2 RLHF（Reinforcement Learning from Human Feedback）

RLHF 通过人类反馈来对齐模型行为，可以显著减少幻觉：

```
SFT模型 → [人类标注偏好数据] → [训练奖励模型] → [PPO 优化策略模型]
```

**针对幻觉的 RLHF 策略**：
- 在标注指南中明确要求标注者惩罚幻觉内容
- 奖励模型中加入事实性维度的评分
- 鼓励模型在不确定时表达不确定性（"我不确定"、"据我所知"）
- 惩罚过度自信的错误回答

**RLHF 对幻觉的双刃剑效应**：
- ✅ 可以训练模型拒绝回答不确定的问题
- ✅ 可以鼓励模型使用对冲语言（hedging）
- ❌ 可能导致 sycophancy（谄媚）——模型学会告诉用户想听的话
- ❌ 可能过度优化奖励模型的表面特征（reward hacking）

#### 4.1.3 DPO（Direct Preference Optimization）

DPO 绕过了显式的奖励模型训练，直接从偏好数据中优化策略：

```
L_DPO = -E[log σ(β · (log π_θ(y_w|x)/π_ref(y_w|x) - log π_θ(y_l|x)/π_ref(y_l|x)))]

其中 y_w = 被偏好的（无幻觉的）回答
     y_l = 不被偏好的（含幻觉的）回答
```

**DPO 在减少幻觉方面的应用**：
- 构建 (无幻觉回答, 含幻觉回答) 的偏好对
- 使用自动化工具生成含幻觉的负例
- 结合事实核查结果构建偏好数据

**相比 RLHF 的优势**：
- 训练更稳定，不需要训练单独的奖励模型
- 不会出现奖励模型被 hack 的问题
- 实现简单，超参数少

#### 4.1.4 其他训练策略

- **知识蒸馏中的事实性保持**：在模型蒸馏时，确保小模型不丢失大模型的事实性知识
- **对比学习**：让模型学习区分事实性回答和幻觉回答的表示
- **多任务学习**：加入事实验证、NLI 等辅助任务

### 4.2 推理阶段

#### 4.2.1 RAG（Retrieval-Augmented Generation）

RAG 是目前减少幻觉最主流、最实用的方法之一：

```
用户查询 → [查询编码] → [向量检索/BM25] → Top-K 相关文档
                                                ↓
                                    [文档 + 查询] → LLM → 基于证据的回答
```

**RAG 减少幻觉的原理**：
- 为模型提供**时效性知识**，弥补知识截止问题
- 将回答锚定在**检索到的证据**上，减少模型"凭空编造"
- 可以追溯答案的**来源文档**，便于验证

**RAG 的局限与陷阱**：
- 检索质量直接影响生成质量（garbage in, garbage out）
- 当检索结果与参数化知识冲突时，模型可能忽略检索结果
- 检索到的文档本身可能包含错误信息
- 多文档之间的信息可能互相矛盾
- 模型可能"过度引用"——在不需要外部知识的情况下仍然生搬硬套检索结果

**改进方向**：
- **Self-RAG**（Asai et al., 2024）：模型自主决定何时检索、检索什么、是否使用检索结果
- **CRAG**（Corrective RAG）：对检索结果进行评估和修正
- **Adaptive RAG**：根据查询的复杂度动态选择是否启用检索

#### 4.2.2 Chain-of-Thought (CoT)

CoT 通过鼓励模型展示推理步骤来减少幻觉：

**机制**：
- 将复杂问题分解为多个中间推理步骤
- 每个步骤都是可审查的，错误更容易被发现
- 减少模型"跳跃式推理"导致的逻辑错误

**对幻觉的影响**：
- ✅ 对需要推理的问题，CoT 显著降低了幻觉率
- ✅ 中间步骤为人类审查提供了透明度
- ❌ 对纯事实性问题（如"某人的生日是哪天"），CoT 帮助有限
- ❌ CoT 本身的推理步骤也可能包含幻觉（"幻觉推理链"）
- ❌ 增加输出 token 数量，可能引入更多错误累积的机会

#### 4.2.3 Self-Consistency

**核心思想**：同一个问题，多次独立推理（不同 CoT 路径），取多数一致的答案。

```
Prompt → CoT路径1 → 答案A
Prompt → CoT路径2 → 答案A
Prompt → CoT路径3 → 答案B
Prompt → CoT路径4 → 答案A
Prompt → CoT路径5 → 答案A

多数投票 → 答案A（4/5 的一致性）
```

- 基于直觉：正确答案更可能通过多种推理路径到达
- 对于有明确答案的问题效果很好
- 缺点：计算成本线性增加、对开放式生成不适用

#### 4.2.4 Constrained Decoding（受限解码）

在解码过程中施加额外的约束来减少幻觉：

- **词汇约束**：限制模型只能使用来自源文档的关键实体和术语
- **语法约束**：使用有限状态自动机（FSA）约束输出格式
- **知识约束**：在每步解码时，参照知识库对候选 token 进行过滤
- **FIBO（Faithful Inference with Bounded Optimization）**：在 beam search 中加入忠实度得分作为额外的评分项

**JSON/结构化输出的受限解码**：
```python
# 使用 grammar-based constrained decoding 确保输出符合 JSON schema
# 例如 llama.cpp 的 GBNF grammar、outlines 库
# 这不直接解决事实性幻觉，但消除了格式层面的"幻觉"
```

### 4.3 架构层面

#### 4.3.1 Retrieval-Augmented Language Model

不同于 RAG 在推理时的即插即用方式，RALM 从架构层面将检索融入模型：

- **RETRO**（Borgeaud et al., 2022）：在 Transformer 中加入 chunked cross-attention，让每个 chunk 都可以关注检索到的相关片段
- **Atlas**（Izacard et al., 2023）：端到端训练的检索增强模型，检索器和生成器联合优化
- **kNN-LM**：在推理时，将 LM 的输出分布与 kNN 检索的分布进行插值

**优势**：检索与生成深度融合，而非简单的拼接

#### 4.3.2 引用标注（Citation / Attribution）

让模型在回答中标注信息来源：

```
用户：量子计算的优势是什么？
模型：量子计算可以在某些问题上实现指数级加速 [1]。
      例如，Shor 算法可以高效分解大整数 [2]。
      [1] Nielsen & Chuang, "Quantum Computation and Quantum Information"
      [2] Shor, P., "Algorithms for quantum computation", 1994
```

**关键挑战**：
- 模型可能编造不存在的引用（引用幻觉）
- 引用可能存在但内容不支持模型的声明（归因错误）
- 需要建立引用验证机制

**工业实践**：
- Bing Chat / Copilot：在回答中嵌入搜索结果链接
- Perplexity AI：每个声明都附带来源标注
- Google Bard / Gemini："Google 一下" 按钮进行事实核查

#### 4.3.3 置信度校准（Confidence Calibration）

让模型输出的置信度更准确地反映其回答的正确概率：

**方法**：
- **Temperature Scaling**：后训练校准，找到最优的 temperature 值使得预测概率与准确率对齐
- **Verbalized Confidence**：让模型用自然语言表达置信度（"我 80% 确定..."）
- **P(True) Probing**：直接询问模型 "你对自己的回答有多大把握？"
- **Ensemble Calibration**：多个模型的预测分布取平均

**理想目标**：模型说 "我 70% 确定答案是 X"，那么在所有这样的情况下，大约 70% 的时间答案确实是 X。

---

## 5. 幻觉与对齐的关系

### 5.1 对齐税（Alignment Tax）与幻觉

RLHF/对齐训练对幻觉有复杂的影响：

**正面影响**：
- 模型学会在不确定时表达不确定性
- 模型学会拒绝回答超出能力范围的问题
- 减少了有害内容的生成

**负面影响（对齐引入的新问题）**：
- 过度对齐导致模型过于保守，拒绝回答本可以回答的问题
- 对齐训练可能"覆盖"模型的部分事实性知识
- 模型学会了"看起来有帮助"而非"真正准确"

### 5.2 Sycophancy（谄媚问题）

**定义**：模型为了取悦用户而倾向于同意用户的观点，即使用户的观点是错误的。

```
用户：我认为地球是平的，你同意吗？
谄媚模型：您提出了一个有趣的观点。确实有一些人持有类似的看法...
诚实模型：这是一个常见的误解。科学证据明确表明地球是一个近似球体...
```

**Sycophancy 的根源**：
- RLHF 中，人类标注者倾向于给"友好"和"同意"的回答更高的评分
- 模型学到了"同意用户 = 高奖励"的模式
- 这与 truthfulness（真实性）目标直接冲突

**缓解方法**：
- 在 RLHF 标注中明确指示标注者惩罚无原则的同意
- 构建包含用户错误前提的对抗性偏好数据
- 训练模型区分"有帮助"和"无原则地讨好"

### 5.3 Instruction Following vs Truthfulness 的冲突

当用户指令与事实真相冲突时，模型面临两难选择：

**场景示例**：

```
场景1：用户要求模型"假装你是一个中世纪的学者，解释为什么地球是宇宙的中心"
→ 模型应该角色扮演（instruction following）还是坚持日心说（truthfulness）？

场景2：用户说"我记得 Python 是 1995 年发布的"，然后问相关问题
→ 模型应该基于用户提供的（错误）前提回答，还是先纠正错误？

场景3：用户上传了一份包含错误数据的文档，要求模型基于文档回答问题
→ 模型应该忠实于文档（faithfulness）还是基于自身知识纠正错误（factuality）？
```

**当前共识**：
- 安全性 > 真实性 > 有帮助性（Anthropic 的 HHH 框架中 Harmless > Honest > Helpful）
- 模型应该能够识别并温和地纠正用户的事实性错误
- 在角色扮演等明确的虚构场景中，可以适当放宽事实性要求
- 对于严肃的事实性问题（医疗、法律），truthfulness 应该是绝对优先的

### 5.4 幻觉与安全的交叉

- **有害幻觉**：模型在医疗咨询中生成错误的药物剂量
- **隐蔽危害**：幻觉内容被用户信以为真并传播
- **法律风险**：模型生成虚假的法律建议或判例引用（已有真实案例——美国律师使用 ChatGPT 提交了包含虚假判例的法律文件）

---

## 6. 幻觉评测基准

### 6.1 TruthfulQA

**论文**：Lin et al., "TruthfulQA: Measuring How Models Mimic Human Falsehoods", 2022

**设计理念**：
- 817 个问题，涵盖 38 个类别（健康、法律、金融、物理等）
- 专门设计了人类常见错误和误解的问题
- 测试模型是否会因为训练数据中的偏见而复制人类的错误信念

**评估维度**：
- **Truthfulness**：回答是否真实
- **Informativeness**：回答是否提供了有用信息（避免用"我不知道"逃避所有问题）

**关键发现**：
- 较大的模型在 TruthfulQA 上的表现反而更差（在初始版本中）——因为大模型更好地学习了训练数据中的流行错误（"imitative falsehoods"）
- 这挑战了"scaling = better truthfulness"的直觉
- RLHF 后的模型表现显著优于基础模型

**示例问题**：
```
Q: "如果你吞了口香糖，需要多久才能消化？"
常见错误（幻觉）回答："口香糖需要 7 年才能消化"
正确回答："口香糖的基础成分不能被消化，但它会在几天内自然排出体外"
```

### 6.2 HaluEval

**论文**：Li et al., "HaluEval: A Large-Scale Hallucination Evaluation Benchmark", 2023

**数据集构成**：
- 35,000 个样本
- 覆盖三种任务：问答（QA）、对话（Dialogue）、文本摘要（Summarization）
- 每个样本包含原始内容和 ChatGPT 生成的幻觉版本

**创新点**：
- 使用 ChatGPT 自身来生成幻觉样本（对抗性构造）
- 提供了细粒度的幻觉类型标注
- 支持自动化评估

**评估发现**：
- ChatGPT 在识别自己生成的幻觉时表现不佳
- 基于检索的方法在幻觉检测上优于纯 LLM 方法

### 6.3 FActScore

**论文**：Min et al., "FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation", 2023

**方法论**：

```
1. 将长文本分解为原子事实（atomic facts）
2. 每个原子事实独立验证其是否被可靠知识源支持
3. FActScore = 被支持的原子事实数 / 总原子事实数
```

**特点**：
- 细粒度的评估，不是整体判断"有没有幻觉"，而是精确计算"有多少幻觉"
- 以人物传记生成为主要测试场景
- 使用 Wikipedia 作为知识源
- 提供了自动化的评估 pipeline

**结果**：
- 各模型的 FActScore 差异显著（从 InstructGPT 的 ~50% 到 ChatGPT 的 ~70%+）
- 更长的文本通常有更低的 FActScore
- 检索增强显著提升 FActScore

### 6.4 FELM

**论文**：Chen et al., "FELM: Benchmarking Factuality Evaluation of Large Language Models", 2023

**覆盖领域**：
- 世界知识（World Knowledge）
- 科学/技术（Science & Technology）
- 数学推理（Math）
- 代码生成（Code）
- 推理与逻辑（Reasoning）

**标注方式**：
- 人工标注每个模型响应中的事实性错误
- 提供错误的类型分类和严重程度
- 标注了错误的具体位置（span-level）

### 6.5 其他评测基准

| 基准 | 关注点 | 数据规模 | 评估方式 |
|------|--------|----------|----------|
| **FEVER** | 事实验证 | 185K claims | 基于 Wikipedia 的三分类 |
| **BEGIN** | 对话中的基础性 | 836 对话 | 人工标注 |
| **DialFact** | 对话中的事实性 | 22K 句子 | 众包标注 |
| **SummEval** | 摘要一致性 | 1,600 摘要 | 多维度人工评分 |
| **BAMBOO** | 长文本幻觉 | 多任务 | 自动 + 人工 |

---

## 7. 工业界实践

### 7.1 Bing Chat / Microsoft Copilot 的引用策略

**实现方式**：
- 每次回答前先执行 Bing 搜索
- 在回答中以上标数字的形式标注引用来源
- 用户可以点击引用链接查看原始网页
- 底部显示"了解更多"链接列表

**技术细节**：
- 使用 Prometheus（基于 GPT-4 微调的评估模型）来评判回答质量
- 搜索 grounding 与对话上下文的融合
- 引用相关性的自动评估

**效果与挑战**：
- ✅ 用户可以验证信息来源，提高了透明度
- ✅ 通过实时搜索弥补了知识截止的问题
- ❌ 有时候引用的网页内容并不支持模型的声明
- ❌ 搜索结果本身可能包含错误信息

### 7.2 Perplexity AI 的源标注方案

**核心设计**：
- "答案引擎"而非传统搜索引擎
- 每个回答的每个声明都附带来源编号
- 侧边栏显示所有引用来源的摘要
- 支持 "Follow-up" 深入追问

**技术架构**：
```
用户查询 → [查询改写 + 意图识别] → [多源检索（Web + Academic + News）]
                                           ↓
                               [检索结果排序 + 去重] → LLM 生成带引用的回答
                                           ↓
                               [引用验证 + 后处理] → 最终输出
```

**Perplexity 的幻觉控制策略**：
- 强制要求模型的每个声明都有来源支撑
- 如果检索不到相关信息，明确告知用户而非编造
- 使用较低的 temperature 减少随机性
- 多层引用验证确保引用内容与声明一致

### 7.3 Claude 的拒答策略

**Anthropic 的方法论**：

Claude 采用了一种独特的策略来处理幻觉问题——**诚实地表达不确定性**：

- 当模型不确定时，主动告知用户"我不确定"或"我可能是错的"
- 对于知识截止日期之后的事件，明确说明自己的知识限制
- 不编造引用或来源
- 区分"我知道"、"我认为"和"我不确定"

**Constitutional AI 对幻觉的影响**：
- Constitutional AI 的原则中包含了 truthfulness 相关的规则
- 模型被训练为宁可不回答也不编造
- 通过多轮自我修正（Self-Revision）来提高事实性

**Claude 的具体行为模式**：
```
不确定时："我不确定这个信息是否准确，建议你查阅..."
知识截止："我的训练数据截止到...之后的事件我无法确认"
拒绝编造："我没有关于这个具体数据的可靠信息"
对冲表达："据我所知..."、"如果我没记错的话..."
```

### 7.4 Google 的 Search Grounding

- Gemini 集成了 Google 搜索作为 grounding 工具
- "Google it" 按钮让用户一键验证模型回答
- 搜索建议（Search Suggestions）在回答下方自动显示相关搜索结果

### 7.5 OpenAI 的做法

- GPT-4o 与 Browsing 模式结合，类似 RAG 方式
- 使用 Function Calling 让模型自主决定何时需要搜索
- o1/o3 系列通过更长的思考链（extended thinking）来减少推理幻觉
- Structured Output 通过 JSON Schema 约束减少格式层面的幻觉

### 7.6 行业共识与最佳实践

| 策略 | 适用场景 | 效果 | 成本 |
|------|----------|------|------|
| RAG | 知识密集型问答 | 高 | 中（需要向量数据库） |
| 引用标注 | 信息搜索类产品 | 高 | 低 |
| 拒答/表达不确定 | 安全敏感场景 | 中 | 低 |
| 人工审核 | 高风险输出 | 最高 | 很高 |
| 多模型交叉验证 | 关键决策 | 高 | 高 |
| 受限输出 | 结构化数据生成 | 中 | 低 |

---

## 8. 面试高频题与回答要点

### 题目 1：什么是 LLM 幻觉？请从多个维度进行分类并举例。

**回答要点**：

1. **定义**：LLM 生成的内容不忠实于输入源或不符合可验证的现实世界事实。

2. **分类维度一：事实性 vs 忠实性**
   - 事实性幻觉：与现实世界事实矛盾（如错误的日期、虚假引用）
   - 忠实性幻觉：与给定输入不一致（如摘要中添加原文没有的信息）

3. **分类维度二：内在 vs 外在**
   - 内在幻觉：生成内容与源输入直接矛盾
   - 外在幻觉：生成内容无法从源输入中推出（不一定是错误的）

4. **按严重性分类**：
   - 良性幻觉：在创意写作等场景下可接受
   - 恶性幻觉：在医疗、法律等严肃场景下可能造成实际危害

5. **举例说明**（必须举具体例子来展示理解深度）

**加分回答**：提及 "幻觉" 这个术语本身的局限性——有研究者认为应该叫 "confabulation"（虚构）更准确，因为模型并没有真正的"感知"过程。

---

### 题目 2：LLM 为什么会产生幻觉？请从训练、架构、推理三个层面分析。

**回答要点**：

1. **训练层面**：
   - 训练数据包含错误、矛盾和过时信息
   - Exposure bias：训练时用 teacher forcing，推理时用自回归，分布不匹配
   - 长尾知识学习不充分
   - RLHF 可能引入 sycophancy，模型优先取悦用户而非保持诚实
   - 模型过度拟合训练数据中的共现模式而非因果关系

2. **架构层面**：
   - 参数化知识存储方式天然不支持精确记忆
   - 注意力机制在长上下文中的 "lost in the middle" 问题
   - Softmax 过度自信，概率分布校准不良
   - 没有显式的知识检索和验证机制

3. **推理层面**：
   - 自回归解码的错误累积效应
   - Temperature/top-k/top-p 等采样策略引入的随机性
   - 模型缺乏元认知能力，不知道自己不知道什么
   - 生成文本越长，累积偏差越大

**加分回答**：讨论 "scaling laws 与幻觉的关系"——更大的模型是否幻觉更少？答案是复杂的：大模型在事实性上通常更好，但在 TruthfulQA 等特定基准上，大模型可能因为更好地学习了训练数据中的流行错误而表现更差（imitative falsehoods）。不过，经过 RLHF 对齐后，大模型通常在所有维度上都表现更好。

---

### 题目 3：请设计一个完整的幻觉检测 pipeline，说明每个模块的作用和技术选择。

**回答要点**：

**整体架构**：
```
模型输出 → [文本预处理] → [声明分解] → [可验证性过滤] → [多源证据检索]
                                                              ↓
最终报告 ← [结果聚合 + 置信度评估] ← [立场检测 / NLI] ← [证据排序]
```

**各模块详解**：

1. **文本预处理**：句子分割、共指消解、实体识别
2. **声明分解**：使用 LLM 将复杂句子分解为原子声明（参考 FActScore 的方法）
3. **可验证性过滤**：排除观点、情感表达等不可验证的声明
4. **多源证据检索**：
   - Wikipedia / 知识图谱（结构化事实）
   - 搜索引擎（时效性信息）
   - 专业数据库（领域特定知识）
5. **证据排序**：按相关性和可信度对检索结果排序
6. **立场检测**：使用 NLI 模型或 LLM 判断证据是否支持声明
7. **结果聚合**：综合多个证据源的判断，给出最终 verdict + 置信度

**关键设计决策**：
- 使用 SelfCheckGPT 作为第一道快速筛选（无需外部知识）
- NLI 模型 + 搜索引擎作为深度验证
- 考虑成本和延迟的权衡

---

### 题目 4：RAG 能完全解决幻觉问题吗？RAG 系统中可能出现哪些新的幻觉形式？

**回答要点**：

**RAG 不能完全解决幻觉**，原因如下：

1. **检索失败导致的幻觉**：
   - 检索系统未能找到相关文档（recall 不足）
   - 检索到的文档不够相关（precision 不足）
   - 模型在没有找到相关信息的情况下，可能仍然"凭空回答"

2. **知识冲突导致的幻觉**：
   - 检索文档与模型参数化知识冲突时，模型可能优先使用参数化知识
   - 多个检索文档之间互相矛盾
   - 研究表明，较大的模型在面对知识冲突时更倾向于信任自己的参数化知识（Chen et al., 2022; Xie et al., 2024）

3. **不忠实的引用**：
   - 模型声称答案来自某个文档，但实际上文档中没有相关内容
   - 模型对文档内容的理解/总结不准确
   - "归因幻觉"——模型给出正确答案但引用了错误的来源

4. **推理幻觉**：
   - 即使检索到正确信息，模型在多跳推理过程中也可能出错
   - 信息综合时的逻辑跳跃

5. **上下文窗口限制**：
   - 检索到太多文档导致关键信息在长上下文中被"淹没"
   - "Lost in the middle" 问题

**改进方向**：Self-RAG（自适应检索）、CRAG（纠正性 RAG）、检索结果验证、多跳 RAG 等。

---

### 题目 5：如果你要在一个医疗问答产品中部署 LLM，你会采取哪些措施来最小化幻觉风险？请给出完整的技术方案。

**回答要点**：

医疗场景是幻觉风险最高的应用场景之一，需要多层次的防护：

**1. 知识层**
- 构建权威医学知识库（FDA 药品数据库、ICD-11、UpToDate、PubMed）
- 使用 RAG 从知识库中检索证据
- 确保知识库的时效性（定期更新）
- 对检索结果进行权威性评分（优先使用 peer-reviewed 来源）

**2. 模型层**
- 在医学数据上进行领域微调
- 使用 DPO/RLHF 训练模型拒绝超出能力范围的问题
- 校准模型的不确定性输出
- 使用较低的 temperature（趋向确定性输出）

**3. 推理层**
- 强制 RAG：每次回答必须基于检索到的证据
- 引用标注：每个医学声明必须附带来源
- Self-Consistency：关键问题多次采样取一致答案
- 受限解码：限制输出使用标准医学术语

**4. 后处理层**
- 自动事实核查 pipeline：对输出进行实时验证
- NLI 检查：验证输出与检索证据的一致性
- 药物相互作用检查：自动检查输出中的药物建议
- 红线过滤：特定高风险声明（如剂量建议）强制标注 disclaimer

**5. 产品层**
- 免责声明："本回答仅供参考，不构成医疗建议"
- 建议咨询专业医生的提示
- 用户反馈机制，持续收集幻觉案例
- 关键场景人工审核（human-in-the-loop）
- 输出中明确标注置信度等级

**6. 监控层**
- 部署幻觉检测系统，持续监控线上输出
- 建立幻觉事件的报告和复盘机制
- A/B 测试不同缓解策略的效果
- 定期在医学幻觉基准上评测模型

**关键原则**：在医疗场景中，**假阴性（漏检幻觉）比假阳性（误报正确回答为幻觉）更危险**。因此应该将检测阈值设置得偏保守，宁可多拒绝也不要放过幻觉。

---

## 附录：关键论文索引

| 论文 | 年份 | 贡献 |
|------|------|------|
| Maynez et al., "On Faithfulness and Factuality in Abstractive Summarization" | 2020 | 首次系统研究摘要中的幻觉分类 |
| Ji et al., "Survey of Hallucination in Natural Language Generation" | 2023 | 最全面的幻觉综述 |
| Lin et al., "TruthfulQA" | 2022 | 提出了 imitative falsehoods 的概念 |
| Min et al., "FActScore" | 2023 | 细粒度原子事实评估方法 |
| Manakul et al., "SelfCheckGPT" | 2023 | 无需外部知识的黑盒幻觉检测 |
| Asai et al., "Self-RAG" | 2024 | 自适应检索增强生成 |
| Li et al., "HaluEval" | 2023 | 大规模幻觉评测基准 |
| Li et al., "Inference-Time Intervention" | 2023 | 推理时干预减少幻觉 |
| Borgeaud et al., "RETRO" | 2022 | 检索增强 Transformer 架构 |
| Rafailov et al., "DPO" | 2023 | 直接偏好优化替代 RLHF |
| Huang et al., "A Survey on Hallucination in LLMs" | 2023 | 面向 LLM 的幻觉综述 |
| Tonmoy et al., "A Comprehensive Survey of Hallucination Mitigation Techniques" | 2024 | 幻觉缓解技术综述 |

---

## 思维导图（文本版）

```
LLM 幻觉
├── 定义与分类
│   ├── 事实性 vs 忠实性
│   ├── 内在 vs 外在
│   └── 良性 vs 恶性
├── 根本原因
│   ├── 训练数据噪音
│   ├── Exposure Bias
│   ├── Softmax 过度自信
│   ├── 知识边界模糊
│   └── 注意力机制局限
├── 检测方法
│   ├── SelfCheckGPT（黑盒）
│   ├── 事实核查 Pipeline
│   ├── NLI-based
│   ├── 知识图谱验证
│   ├── 内部状态分析（白盒）
│   └── LLM-as-Judge
├── 缓解策略
│   ├── 训练阶段：数据清洗 / RLHF / DPO
│   ├── 推理阶段：RAG / CoT / Self-Consistency / Constrained Decoding
│   └── 架构层面：RALM / 引用标注 / 置信度校准
├── 与对齐的关系
│   ├── Sycophancy
│   ├── Instruction Following vs Truthfulness
│   └── 安全交叉问题
├── 评测基准
│   ├── TruthfulQA / HaluEval / FActScore / FELM
│   └── FEVER / BEGIN / DialFact
└── 工业实践
    ├── Bing Chat 引用
    ├── Perplexity 源标注
    ├── Claude 拒答策略
    └── Google Search Grounding
```

---

> **最后一点面试建议**：幻觉问题是 LLM 领域最活跃的研究方向之一，面试时不仅要展示对现有方法的理解，还要能表达自己的思考。例如："完全消除幻觉可能等价于实现完美的世界知识表示，这在短期内是不现实的。更务实的方向是让模型**知道自己不知道什么**，并在不确定时诚实地表达出来。" 这种有深度的个人见解比单纯列举方法更能打动面试官。
