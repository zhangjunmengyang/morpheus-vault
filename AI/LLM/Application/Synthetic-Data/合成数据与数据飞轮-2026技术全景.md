---
brief: "合成数据与数据飞轮 2026 技术全景——Self-Instruct/RLAIF/Model-Collapse 完整谱系；数据飞轮的工程实现（模型蒸馏→质量过滤→迭代微调）；面试武器库，解释为什么 GPT-4 可以蒸馏 Llama 的标准论证。"
title: "合成数据与数据飞轮 — 2026 技术全景"
date: 2026-02-21
tags: [合成数据, 数据飞轮, Self-Instruct, RLAIF, Model-Collapse, 面试武器库]
domain: AI/LLM/Application/Synthetic-Data
rating: 5
status: permanent
dikw: K
---

# 合成数据与数据飞轮 — 2026 技术全景

> 面试武器库 #25 | 最后更新：2026-02-21

---

## 目录

1. [概述与动机](#1-概述与动机)
2. [合成数据生成方法论](#2-合成数据生成方法论)
3. [数据飞轮架构](#3-数据飞轮架构)
4. [质量控制](#4-质量控制)
5. [领域应用](#5-领域应用)
6. [指令调优数据](#6-指令调优数据)
7. [偏好数据生成](#7-偏好数据生成)
8. [前沿方向](#8-前沿方向)
9. [面试高频问题](#9-面试高频问题)
10. [常见误区与陷阱](#10-常见误区与陷阱)
11. [参考文献](#11-参考文献)

---

## 1. 概述与动机

### 1.1 为什么合成数据是 2026 年最重要的趋势之一

2024-2025 年，合成数据从"辅助手段"升级为 LLM 训练的**核心方法论**。从 Phi 系列的 "Textbooks Are All You Need" 到 DeepSeek-R1 的蒸馏数据，再到 Meta 的 Llama 3 系列在 mid-training 阶段大量使用合成数据——行业共识已经形成：**下一代模型的核心竞争力不再是参数量或算力，而是数据工程**。

三个结构性力量同时在推动：

**力量一：数据墙（Data Wall）**

Epoch AI 在 2024 年的研究估算，到 2026 年互联网高质量自然语言数据将基本被主流模型"训练殆尽"。Llama 3 405B 的预训练使用了 15T tokens，GPT-4 据估计超过 13T tokens——这已经接近公开互联网高质量文本的上限。

```
互联网高质量文本存量估算（Epoch AI 2024）：
┌─────────────────┬────────────────┐
│ 类别            │ 估计 tokens    │
├─────────────────┼────────────────┤
│ 图书            │ ~600B          │
│ 学术论文        │ ~300B          │
│ 网页（去重后）  │ ~10-15T        │
│ 代码            │ ~1-2T          │
│ 社交媒体        │ ~5T（低质量）  │
├─────────────────┼────────────────┤
│ 高质量总计      │ ~15-20T        │
└─────────────────┴────────────────┘
当前头部模型训练需求：13-15T tokens
→ 高质量数据即将或已经耗尽
```

**力量二：隐私法规与数据合规**

GDPR、中国《个人信息保护法》、美国 CCPA/CPRA 对训练数据的使用限制日益严格。2024 年 New York Times v. OpenAI 诉讼进一步加大了版权风险。合成数据绕过了这些限制——你不需要爬取用户数据，而是让模型生成等价分布的训练信号。

**力量三：领域特定数据稀缺**

医疗诊断记录、法律判例推理、科学实验数据、金融交易序列——这些领域数据不仅稀少，还受严格监管。传统方法（人工标注）成本高、速度慢、一致性差。合成数据可以在数小时内生成百万级领域样本。

### 1.2 合成数据的本质：知识蒸馏 + 数据增强

从信息论视角看，合成数据的核心价值有两层：

1. **格式转换**：将隐式知识（模型权重中的知识）转化为显式知识（训练样本），这是一种知识蒸馏
2. **分布扩展**：在已有数据分布的基础上，通过受控变换扩展到未覆盖的区域

关键认识：**合成数据不是"凭空创造知识"，而是"重新组织已有知识"**。模型不能生成超越自身能力上限的数据——但它可以用更适合训练的格式呈现已有知识，或者通过组合推理生成新的知识组合。

### 1.3 合成数据全景地图

```
┌────────────────────────────────────────────────────────────┐
│                  合成数据全景（2026）                       │
├────────────┬───────────┬───────────┬──────────────────────┤
│  生成方法  │ 质量控制   │ 应用场景  │ 飞轮架构            │
├────────────┼───────────┼───────────┼──────────────────────┤
│Self-Instruct│RM 过滤   │ 代码      │ 生成→过滤→训练→评估 │
│Evol-Instruct│去污染    │ 数学      │ RLHF 数据飞轮       │
│Magpie      │多样性度量 │ 科学/医疗 │ On-policy 循环      │
│模型蒸馏    │Model Collapse│指令调优│ 课程学习飞轮        │
│对抗生成    │人工验证   │ 偏好数据  │ Rejection Sampling  │
│规则/模板   │一致性校验 │ 多模态    │ Self-Play           │
│世界模型    │          │ Agent 训练│                      │
└────────────┴───────────┴───────────┴──────────────────────┘
```

### 1.4 成本对比

| 方式 | 单条成本 | 质量 | 规模上限 | 速度 |
|------|----------|------|----------|------|
| 专家标注 | $5-50 | 最高但方差大 | 千级 | 慢（周级） |
| 众包标注 | $0.5-5 | 中等 | 万级 | 中（天级） |
| LLM 生成（GPT-4o） | $0.01-0.1 | 中高 | 百万级 | 快（小时级） |
| LLM 生成（开源 70B） | $0.001-0.01 | 中 | 千万级 | 快（小时级） |
| 规则/模板生成 | ~$0 | 低-中 | 无限 | 最快 |

**关键 trade-off**：成本下降 100 倍，但质量方差也增大。合成数据的核心挑战不在"生成"，而在"过滤"。

---

## 2. 合成数据生成方法论

### 2.1 LLM 自生成（Self-Instruct 家族）

#### Self-Instruct（Wang et al. 2023）

最早的系统化方法。核心思想：用一小组人工种子指令引导 LLM 生成大量新指令和回答。

```
流程：
175 条种子指令 → LLM 生成新指令 → LLM 生成输入 → LLM 生成输出 → 过滤 → 循环
                                                                     ↑              │
                                                                     └──────────────┘

过滤规则：
- ROUGE-L > 0.7 与已有指令的去重
- 格式校验（指令长度、是否包含关键词等）
- 分类平衡（避免某类指令占比过高）
```

**局限性**：
- 多样性退化：多轮生成后指令趋同，"请写一篇关于…的文章"类占比过高
- 质量天花板：受限于种子指令的质量和 LLM 的能力
- 格式偏差：模型倾向于生成结构化/形式化的指令，偏离真实用户的口语化表达

#### Evol-Instruct（WizardLM, Xu et al. 2023）

在 Self-Instruct 基础上引入**指令进化**：通过系统化的复杂化操作，将简单指令逐步升级。

```python
# Evol-Instruct 的五种进化操作
evolution_operators = {
    "add_constraints": "在指令中增加限制条件",      # "写排序" → "写不用额外空间的排序"
    "deepen":         "要求更深层次的推理",          # "解释X" → "从Y角度对比分析X和Z"
    "concretize":     "用具体场景替代抽象描述",      # "写ML代码" → "用PyTorch实现GAN训练MNIST"
    "increase_steps": "增加解题所需的步骤数",        # "计算面积" → "先推导公式再计算"
    "breadth":        "生成全新但同等难度的指令",    # 拓展覆盖领域
}

# 多轮进化示例
round_0 = "Write a Python function to sort a list"
round_1 = "Write a Python function to sort a list of dictionaries by multiple keys, supporting both ascending and descending order"
round_2 = "Write a Python function that implements a stable, in-place sort for a list of dictionaries by multiple keys with custom comparators, handling None values gracefully"
```

**关键贡献**：解决了 Self-Instruct 的多样性退化问题，生成的指令复杂度分布更接近真实用户需求。

WizardLM-70B 在 MT-Bench 上一度与 GPT-3.5 持平，证明了 Evol-Instruct 的有效性。

#### Magpie（Xu et al. 2024）

2024 年的创新方法，核心思想令人惊讶的简单：**不给 LLM 任何输入，只给系统提示，让它自动生成用户指令**。

```python
# Magpie 的核心机制
# 利用 LLM 的自回归特性：给定 system prompt + 空的 user turn，
# 模型会自动"补全"一个合理的用户指令

prompt = """<|system|>You are a helpful assistant.<|end|>
<|user|>"""
# 模型自动生成：
# "Can you explain the difference between supervised and unsupervised learning?"

# 然后用这个生成的指令让模型回答
response = model.generate(prompt + generated_instruction + "\n<|assistant|>")
```

**为什么有效？**

LLM 在 chat fine-tuning 阶段看过海量真实对话，已经学会了"用户通常会问什么"的分布。当你给它一个空的 user turn，它会从这个学到的分布中采样——这比人工设计种子指令更接近真实用户分布。

**Magpie 的优势**：
- 零种子指令，完全自动化
- 生成的指令分布更接近真实用户（因为利用了模型训练时见过的真实对话分布）
- 速度极快：无需多轮进化或复杂管线

**Magpie 数据集规模**：Magpie-Air-3M（300 万条），Magpie-Pro-1M（100 万条高质量）

### 2.2 模型蒸馏（Knowledge Distillation）

用强模型（teacher）的输出训练弱模型（student）。这是最直接有效的合成数据方法。

#### 经典蒸馏管线

```
┌──────────────┐     ┌─────────────┐     ┌──────────────┐
│ 收集/生成问题 │────▶│ Teacher 回答 │────▶│ 过滤 + 格式化 │
│（种子数据集） │     │ (GPT-4o等)   │     │              │
└──────────────┘     └─────────────┘     └──────┬───────┘
                                                 │
                     ┌─────────────┐     ┌───────▼───────┐
                     │ 评估 Student │◀────│ SFT Student   │
                     │ 能力提升     │     │ (Qwen-7B等)   │
                     └─────────────┘     └───────────────┘
```

#### 代表性蒸馏数据集

| 数据集 | Teacher | Student | 规模 | 年份 | 特点 |
|--------|---------|---------|------|------|------|
| Alpaca | GPT-3.5 | LLaMA-7B | 52K | 2023 | 开创性工作，证明可行性 |
| Orca | GPT-4 | LLaMA-13B | 5M | 2023 | 系统提示引导详细推理 |
| Orca 2 | GPT-4 | LLaMA-13B | 800K | 2023 | 教 student 用不同策略 |
| Phi-1.5 | GPT-3.5 | 1.3B | 30B tokens | 2023 | "Textbooks" 质量数据 |
| OpenHermes 2.5 | GPT-4 | Mistral-7B | 1M | 2023 | 多源混合 |
| DeepSeek-R1-Distill | DeepSeek-R1 | Qwen/Llama | 800K | 2025 | 推理链蒸馏 |

#### Orca 的 System Message 技巧

Orca 的关键创新不只是"用 GPT-4 生成回答"，而是通过 system message 引导 teacher 输出**详细推理过程**：

```python
system_messages = [
    "You are an AI assistant. Explain your reasoning step by step.",
    "You are an AI assistant that helps people find information. "
    "Provide a detailed answer so user don't need to search outside.",
    "You are an AI assistant. User will give you a task. "
    "Your goal is to complete the task as faithfully as possible. "
    "While performing the task, think step-by-step and justify your steps.",
]
# 不同的 system message → teacher 输出不同"推理风格"的回答
# Student 学到的不只是答案，还有"如何推理"
```

这个洞察后来被 DeepSeek-R1 的蒸馏管线大规模应用——通过蒸馏 R1 的长推理链（CoT），让小模型也获得 reasoning 能力。

#### 蒸馏的理论边界

**核心限制**：Student 不可能超越 Teacher 在**同一任务**上的表现上限（除非任务分布发生变化）。

但实践中有几个"绕过"机制：
1. **Format transfer**：Teacher 的知识以更适合 Student 的格式呈现，降低学习难度
2. **Ensemble effect**：多个 Teacher 的输出融合，相当于模型集成
3. **Distribution matching**：Student 在 Teacher 擅长的子分布上接近 Teacher，同时在 Teacher 不擅长的子分布上保持自身优势
4. **Weak-to-Strong**（Burns et al. 2023）：弱 supervisor 在某些条件下可以引导强模型发挥超越 supervisor 的能力

### 2.3 对抗生成

#### 红队对抗数据

为安全对齐生成"攻击性"输入，用于训练模型的拒绝能力：

```python
# Red-teaming 数据生成管线
pipeline = [
    # 1. 种子攻击：人工编写的初始攻击prompt
    seed_attacks,
    # 2. LLM 变体生成：用 LLM 改写/组合攻击
    llm_rephrase(seed_attacks, n_variants=10),
    # 3. 对抗过滤：只保留能成功"突破"目标模型的攻击
    filter_by_success(target_model, threshold=0.5),
    # 4. 正确拒绝标注：为每个攻击生成标准拒绝回答
    generate_refusal(attacks, refusal_model),
]
```

#### Adversarial Filtering

用对抗过程筛选高质量训练样本：

```
生成大量候选样本 → 用当前模型评估难度 → 保留"刚好做不对"的样本 → 训练 → 循环
```

这本质上是 **curriculum learning + active learning** 的合成数据版本。

### 2.4 基于规则/模板的生成

不依赖 LLM，而是用确定性规则生成训练数据。在特定领域极其有效。

#### 数学数据生成

```python
import random
import sympy

def generate_math_problem():
    """规则生成代数方程题"""
    # 随机参数
    a = random.randint(1, 20)
    b = random.randint(-10, 10)
    c = random.randint(1, 50)

    x = sympy.Symbol('x')
    equation = a * x + b - c  # ax + b = c

    solution = sympy.solve(equation, x)[0]

    problem = f"Solve for x: {a}x + {b} = {c}"
    answer = f"x = {solution}"
    steps = f"Step 1: {a}x = {c} - ({b}) = {c - b}\n"
    steps += f"Step 2: x = {c - b}/{a} = {solution}"

    return {"problem": problem, "solution": steps + f"\nAnswer: {answer}"}
```

**优势**：答案 100% 正确（可验证），可以精确控制难度分布。

**局限**：只能覆盖结构化程度高的任务，自然语言表达多样性差。

**实战组合**：规则生成问题+答案骨架 → LLM 改写为自然语言 → 验证改写后的答案仍正确。

#### 代码数据生成

```python
# 生成单元测试 + 代码对
def generate_coding_problem():
    # 1. 随机选择算法类型
    algo_type = random.choice(["sorting", "searching", "graph", "dp"])
    # 2. 随机生成测试用例
    test_cases = generate_test_cases(algo_type, n=10)
    # 3. 用 LLM 生成解法
    solution = llm.generate(f"Write a Python solution for: {problem_description}")
    # 4. 执行验证
    if all(run_test(solution, tc) for tc in test_cases):
        return {"problem": problem_description, "solution": solution, "tests": test_cases}
    return None  # 过滤掉错误解法
```

### 2.5 世界模型模拟

2025-2026 年兴起的前沿方向：用世界模型（world model）生成具身智能、机器人、自动驾驶等领域的合成数据。

#### 核心思想

```
┌──────────────┐     ┌──────────────┐     ┌──────────────────┐
│ 世界模型      │────▶│ 模拟环境数据  │────▶│ 训练 Agent/策略  │
│ (UniSim/Genie│     │（视频/状态序列│     │                  │
│  /WorldDreamer)    │  /奖励信号）  │     │                  │
└──────────────┘     └──────────────┘     └──────────────────┘
```

**代表工作**：
- **UniSim**（Google 2024）：通过视频预测模型模拟物理世界，为机器人策略生成训练数据
- **Genie 2**（DeepMind 2024）：从单张图片生成可交互的 3D 世界
- **DayDreamer**：用世界模型在"梦境"中训练机器人策略，减少 10 倍真实环境交互

**与 LLM 合成数据的联系**：World model 本质上也是"用模型生成训练数据"——只是从文本域扩展到了物理世界域。

---

## 3. 数据飞轮架构

### 3.1 核心飞轮：生成→过滤→训练→评估→反馈→再生成

数据飞轮（Data Flywheel）是合成数据的高级组织形式：不是一次性生成，而是持续的**闭环迭代**。

```
                   ┌─────────────┐
                   │  1. 生成     │ ← 用当前最优模型/规则生成候选数据
                   │  (Generate)  │
                   └──────┬──────┘
                          │
                   ┌──────▼──────┐
                   │  2. 过滤     │ ← RM/规则/人工验证筛选高质量数据
                   │  (Filter)    │
                   └──────┬──────┘
                          │
                   ┌──────▼──────┐
                   │  3. 训练     │ ← 用筛选后的数据更新模型
                   │  (Train)     │
                   └──────┬──────┘
                          │
                   ┌──────▼──────┐
                   │  4. 评估     │ ← 在 benchmark + held-out set 上评估
                   │  (Evaluate)  │
                   └──────┬──────┘
                          │
                   ┌──────▼──────┐
                   │  5. 反馈     │ ← 分析弱点，调整生成策略
                   │  (Feedback)  │
                   └──────┬──────┘
                          │
                          └──────────→ 回到 1. 生成
```

#### 实际案例：DeepSeek-Coder 的飞轮

```
Round 1: 用 GPT-4 生成 100K 代码指令-回答对
         → 过滤（执行测试 + 格式检查）→ 剩余 60K
         → SFT DeepSeek-Coder-Base → v0.1

Round 2: 用 v0.1 自己生成新数据 + 用 GPT-4 对 v0.1 的弱点领域补充
         → 过滤（更严格的测试 + RM 打分）→ 剩余 80K
         → SFT → v0.2（能力提升 → 生成质量也提升）

Round 3: v0.2 + 更多领域（算法、系统设计、调试）
         → 执行验证 + 人工抽检 5%
         → SFT → v1.0
```

每一轮，模型变强 → 生成质量提升 → 下一轮训练数据更好 → 模型更强。这就是飞轮效应。

### 3.2 RLHF 数据飞轮

RLHF 场景下的飞轮有独特的结构：

```
┌─────────────────────────────────────────────────────────┐
│                   RLHF 数据飞轮                          │
│                                                          │
│  Policy Model ──生成回答──▶ 人工/AI 偏好标注             │
│       ▲                         │                        │
│       │                         ▼                        │
│       │                  Reward Model 训练/更新           │
│       │                         │                        │
│       │                         ▼                        │
│       └──── RL 训练 ◀── RM 提供奖励信号                  │
│                                                          │
│  关键：每一轮 RL 训练后，Policy 分布变化                   │
│  → 需要在新分布上重新采样 + 标注 → On-policy 数据         │
└─────────────────────────────────────────────────────────┘
```

### 3.3 On-policy vs Off-policy 数据

这是数据飞轮中最关键的技术决策之一。

| 维度 | On-policy 数据 | Off-policy 数据 |
|------|---------------|-----------------|
| 来源 | 当前策略模型自己生成 | 其他模型或历史版本生成 |
| 分布匹配 | 完美匹配当前模型 | 存在分布偏移 |
| 新鲜度 | 每轮 RL 后需重新采样 | 可以复用 |
| 成本 | 高（每轮都要推理+标注） | 低（一次生成多次使用） |
| RL 效率 | 高（梯度方向准确） | 低（需要 importance sampling 修正） |

**实战选择**：

```
2024-2025 主流做法：
├── DPO/SimPO/KTO → Off-policy（用固定偏好数据集训练）
│   优势：简单、稳定、可复现
│   劣势：不能适应模型能力变化
│
├── PPO/GRPO → On-policy（每批采样来自当前策略）
│   优势：持续改进，无分布偏移
│   劣势：计算成本高，需要同时维护多个模型
│
└── Hybrid → 混合
    70% on-policy（当前模型生成）+ 30% off-policy（高质量历史数据）
    → 平衡效率和成本
```

**On-policy 的关键优势（以 InstructGPT 为例）**：

PPO 训练时，模型在第 N 轮的输出分布已经与第 1 轮大不相同。如果 RM 始终在第 1 轮数据上训练，它对第 N 轮的输出打分可能不准确——这就是 reward hacking 的根源之一。On-policy 采样 + RM 更新的飞轮可以缓解这个问题。

### 3.4 Rejection Sampling（拒绝采样飞轮）

最简单但极其有效的飞轮变体：

```python
def rejection_sampling_flywheel(model, rm, questions, k=16, top_p=0.25):
    """
    对每个问题生成 k 个回答，用 RM 打分，保留 top 25%
    """
    selected = []
    for q in questions:
        candidates = [model.generate(q, temperature=0.8) for _ in range(k)]
        scores = [rm.score(q, c) for c in candidates]
        # 保留得分最高的 top_p 比例
        threshold = sorted(scores, reverse=True)[int(k * top_p)]
        selected.extend([(q, c) for c, s in zip(candidates, scores) if s >= threshold])
    return selected

# 飞轮循环
for round in range(num_rounds):
    data = rejection_sampling_flywheel(model, rm, questions)
    model = sft(model, data)
    # 可选：也更新 RM
```

**为什么有效？**
- k=16, top_p=0.25 意味着只保留最好的 4 个回答
- 相当于用 RM 做了隐式的 RL：没有梯度计算，但筛选的效果类似
- Llama 2 论文报告：5 轮 rejection sampling 后质量持续提升

**与 Best-of-N 的关系**：Rejection sampling 是 Best-of-N 的训练版本。Best-of-N 在推理时选最优，Rejection sampling 在训练时选最优。

### 3.5 Self-Play 飞轮

受博弈论启发的数据生成方式：让模型与自己对弈，从对弈中产生训练数据。

```
┌────────────────┐
│  Model v_n      │
│  (作为出题者)   │──生成挑战性问题──┐
└────────────────┘                    │
       ▲                              ▼
       │                    ┌────────────────┐
       │                    │  Model v_n      │
       │                    │  (作为回答者)   │
       │                    └───────┬────────┘
       │                            │
       │                    ┌───────▼────────┐
       │                    │  验证/评估      │
       │                    │  (执行/RM/规则) │
       │                    └───────┬────────┘
       │                            │
       └──── 训练 v_{n+1} ◀────────┘
```

**代表工作**：
- **SPIN**（Self-Play fINe-tuning，Chen et al. 2024）：模型自己扮演"人类"和"AI"两个角色
- **Self-Rewarding**（Meta 2024）：模型同时做生成者和评判者，迭代提升

**核心风险**：自我循环可能放大偏见——如果模型在某个方向上有系统性错误，Self-Play 会进一步强化这个错误。必须引入外部信号（人类验证/形式化验证/执行反馈）来锚定。

---

## 4. 质量控制

### 4.1 Model Collapse：合成数据的最大敌人

**Shumailov et al. 2024**（Nature 论文）的核心发现：如果模型在**纯合成数据**上持续训练多代，模型能力会逐渐退化——分布的尾部被截断，输出越来越趋同。

```
Model Collapse 机制：

Generation 0 (真实数据训练): 学到真实分布 P_real
         ↓
Generation 1 (用 Gen-0 的输出训练): 学到 P_1 ≈ P_real（近似）
         ↓
Generation 2 (用 Gen-1 的输出训练): P_2 = 近似的近似
         ↓
...
         ↓
Generation N: P_N → 退化为高斯/均匀分布，失去多样性

每一代的近似误差会累积：
- 低概率事件（分布尾部）被低估 → 下一代的训练数据中更少出现 → 进一步被低估
- 高概率事件被过度表示 → 输出趋同
```

**关键数据点**：
- 在 wikitext-103 上，纯合成训练 9 代后 perplexity 从 16.1 上升到 54.3
- 文本多样性（distinct n-gram ratio）下降 60%+
- 退化速度与模型大小无关——大模型也会 collapse，只是慢一些

#### 防止 Model Collapse 的实战策略

```python
# 策略 1：始终混合真实数据（最重要！）
training_data = concat([
    real_data,      # 占比 30-50%，锚定真实分布
    synthetic_data, # 占比 50-70%，扩展能力
])

# 策略 2：新鲜度控制——只用当前模型生成的数据
# 不要用 Gen-1 模型生成的数据训练 Gen-3 模型
synthetic_data = current_model.generate(prompts)  # ✅ 当代生成
# synthetic_data = old_model_output  # ❌ 避免跨代累积误差

# 策略 3：多样性约束
from collections import Counter

def diversity_filter(samples, min_distinct_ratio=0.5):
    """确保 n-gram 多样性"""
    ngrams = []
    for s in samples:
        ngrams.extend(get_ngrams(s, n=4))
    distinct_ratio = len(set(ngrams)) / len(ngrams)
    if distinct_ratio < min_distinct_ratio:
        # 增加采样温度或更换种子
        raise LowDiversityWarning(f"Distinct 4-gram ratio: {distinct_ratio}")
```

**2025 年新发现**：Alemohammad et al. 2024 证明，如果每代训练都保留原始真实数据（accumulate 而非 replace），Model Collapse 可以被显著延缓甚至避免。这与实际做法一致——没有人会只用合成数据训练。

### 4.2 去污染（Decontamination）

合成数据可能"泄露"评估集——尤其当 Teacher 模型在评估集数据上训练过时。

```
污染路径：
GPT-4 训练数据 ⊇ MMLU 题目 → GPT-4 生成的合成数据可能包含 MMLU 变体
→ Student 在合成数据上训练 → Student 在 MMLU 上分数虚高
```

**去污染方法**：

```python
# 方法 1：n-gram 匹配（粗粒度但快速）
def ngram_decontaminate(synthetic_data, eval_sets, n=13):
    """移除与评估集有 13-gram 重叠的样本"""
    eval_ngrams = set()
    for eval_item in eval_sets:
        eval_ngrams.update(get_ngrams(eval_item, n=13))

    clean_data = []
    for item in synthetic_data:
        item_ngrams = set(get_ngrams(item, n=13))
        if len(item_ngrams & eval_ngrams) == 0:
            clean_data.append(item)
    return clean_data

# 方法 2：语义相似度（更精确但慢）
def semantic_decontaminate(synthetic_data, eval_sets, threshold=0.95):
    eval_embeddings = embed(eval_sets)
    clean_data = []
    for item in synthetic_data:
        item_emb = embed(item)
        max_sim = max(cosine_similarity(item_emb, e) for e in eval_embeddings)
        if max_sim < threshold:
            clean_data.append(item)
    return clean_data
```

**GPT-4 Technical Report** 中提到：他们使用 substring matching 对主要 benchmark 进行去污染，移除了训练数据中与评估集有 50 字符以上连续匹配的样本。

### 4.3 多样性度量

多样性是合成数据质量的核心指标之一。

| 指标 | 计算方式 | 衡量维度 |
|------|----------|----------|
| Distinct-N | unique n-grams / total n-grams | 词汇多样性 |
| Self-BLEU ↓ | 数据集内样本间的 BLEU | 表达多样性（越低越好） |
| Embedding 覆盖率 | embedding 空间中的 coverage | 语义多样性 |
| 类别均衡度 | 各任务/主题的分布熵 | 任务多样性 |
| IFD (Instruction Following Difficulty) | 模型对指令的困惑度 | 难度多样性 |

```python
# 实战：多维度多样性监控
def diversity_report(dataset):
    """生成数据集多样性报告"""
    report = {}

    # 1. 词汇多样性
    all_tokens = [tokenize(d["text"]) for d in dataset]
    for n in [1, 2, 3, 4]:
        ngrams_all = [ng for tokens in all_tokens for ng in get_ngrams(tokens, n)]
        report[f"distinct_{n}"] = len(set(ngrams_all)) / len(ngrams_all)

    # 2. 语义多样性
    embeddings = embed([d["text"] for d in dataset])
    pairwise_sims = cosine_similarity_matrix(embeddings)
    report["mean_pairwise_sim"] = pairwise_sims.mean()  # 越低越多样
    report["self_bleu_4"] = compute_self_bleu(dataset, n=4)

    # 3. 长度分布
    lengths = [len(d["text"]) for d in dataset]
    report["length_std"] = np.std(lengths)
    report["length_entropy"] = entropy(np.histogram(lengths, bins=20)[0])

    # 4. 主题分布
    topics = classify_topics([d["text"] for d in dataset])
    report["topic_entropy"] = entropy(Counter(topics).values())

    return report
```

### 4.4 奖励模型过滤

用 Reward Model（RM）作为自动质量筛选器，是当前最主流的过滤方法。

```python
# RM 过滤管线
def rm_filter(dataset, rm, strategy="absolute", threshold=None, top_k=None):
    """
    strategy:
    - "absolute": 绝对分数过滤，保留 score > threshold
    - "top_k": 保留得分最高的 top_k%
    - "stratified": 在每个难度层级中保留 top_k%
    """
    scored = [(item, rm.score(item["prompt"], item["response"])) for item in dataset]

    if strategy == "absolute":
        return [item for item, score in scored if score > threshold]

    elif strategy == "top_k":
        scored.sort(key=lambda x: x[1], reverse=True)
        cutoff = int(len(scored) * top_k / 100)
        return [item for item, _ in scored[:cutoff]]

    elif strategy == "stratified":
        # 按难度分层，每层保留 top_k%
        by_difficulty = defaultdict(list)
        for item, score in scored:
            diff = classify_difficulty(item["prompt"])
            by_difficulty[diff].append((item, score))

        result = []
        for diff, items in by_difficulty.items():
            items.sort(key=lambda x: x[1], reverse=True)
            cutoff = int(len(items) * top_k / 100)
            result.extend([item for item, _ in items[:cutoff]])
        return result
```

**RM 过滤的风险**：

1. **RM 偏见放大**：RM 可能偏好"冗长但空洞"的回答（length bias），过滤后的数据集可能加剧这个偏见
2. **多样性损失**：RM 高分的回答往往风格相似，过度过滤会降低多样性
3. **难度偏移**：如果 RM 对"简单但正确"的回答打高分，过滤后的数据集会偏向简单任务

**缓解**：使用 stratified filtering + 多个 RM 交叉验证 + 多样性约束。

### 4.5 人工验证占比

**理想比例（2026 最佳实践）**：

| 数据规模 | 人工验证比例 | 方式 |
|----------|-------------|------|
| < 10K（高价值） | 30-100% | 全量或高比例审核 |
| 10K-100K | 5-10% | 分层抽样 + 重点领域全审 |
| 100K-1M | 1-3% | 随机抽检 + 异常检测 |
| > 1M | 0.1-0.5% | 自动化为主，人工验证 RM 边界 case |

**关键原则**：人工验证不是为了逐条审核，而是为了**校准自动化管线**。如果人工抽检发现 RM 系统性地给某类错误回答打高分，应该修正 RM 而不是增加人工审核量。

---

## 5. 领域应用

### 5.1 代码领域

代码是合成数据最成功的应用场景——因为代码有天然的可验证性（编译/执行/测试）。

#### StarCoder / StarCoder 2

```
数据管线：
The Stack v2 (67.5TB) → 语言检测 → 去重(MinHash) → 质量过滤
→ PII 移除 → 许可证过滤 → 15.6B token 训练集

合成数据增强：
- 生成 docstring → code 和 code → docstring 训练对
- 用强模型生成 code review / bug fix 样本
- 竞赛题目的多解生成
```

#### DeepSeek-Coder / DeepSeek-Coder-V2

DeepSeek 的代码模型系列是合成数据飞轮的典范：

```
Phase 1 - 预训练：
  2T tokens (87% 代码 + 13% 自然语言)
  → 代码与自然语言交替训练，保持两种能力

Phase 2 - SFT 数据合成：
  指令数据来源：
  ├── 开源数据集（Code Alpaca, Magicoder-OSS）
  ├── GPT-4 蒸馏（复杂算法/系统设计）
  └── 自生成 + 执行验证（简单到中等难度）

Phase 3 - RL 阶段（V2）：
  GRPO + 代码执行反馈
  → 正确执行 = 正奖励，错误/超时 = 负奖励
  → 无需人工标注，纯自动化飞轮
```

**关键数据点**：DeepSeek-Coder-V2（236B MoE）在 HumanEval 上达到 90.2%，与 GPT-4o 持平。

#### Magicoder / OSS-Instruct

Magicoder 提出了 **OSS-Instruct**：从开源代码片段中提取灵感，生成编程指令。

```python
# OSS-Instruct 流程
code_snippet = """
# From GitHub: pandas groupby with multiple aggregations
df.groupby(['city', 'year']).agg({
    'revenue': ['sum', 'mean'],
    'customers': 'count'
}).reset_index()
"""

prompt = f"""Inspired by the following code snippet, create a coding problem 
that requires similar skills but with a different context:

{code_snippet}

Generate:
1. A clear problem description
2. Example input/output
3. A reference solution"""
```

**优势**：生成的指令更贴近真实编程场景（而非算法竞赛），多样性更好。

### 5.2 数学领域

数学是合成数据的第二大成功领域——因为数学有形式化的验证手段。

#### NuminaMath

NuminaMath 的数据飞轮：

```
Phase 1: 收集全球数学竞赛题目（AMC/AIME/IMO/CMO 等）
         → 人工编写 CoT 解答 → 种子数据集

Phase 2: GPT-4 + Claude 扩写解答
         → 每题生成 5 种不同解法
         → SymPy/SageMath 验证最终答案
         → 只保留验证通过的解法

Phase 3: 难度分层 + 课程学习
         → 简单 → 中等 → 困难 逐步训练
         → 模型在每个难度层级都达到阈值后再进入下一层
```

#### DeepSeek-Math

```
数据收集策略：
├── 数学网页（StackExchange, MathOverflow, Khan Academy 等）
│   → 专门训练 fasttext 分类器识别"数学相关"网页
│   → 从 Common Crawl 中提取 120B tokens 数学语料
│
├── 合成数据增强
│   → 从题目生成多种解法
│   → 变换问题参数生成新题（parametric variation）
│   → 为每个解法生成详细的 step-by-step 推理
│
└── GRPO 强化学习
    → 奖励 = 答案是否正确（math_verify 工具检验）
    → 无需人工标注偏好
```

**DeepSeek-Math-7B** 在 MATH 数据集上达到 51.7%（超越了许多 70B 模型），主要归功于数据质量而非模型规模。

#### 数学合成数据的特殊挑战

```
1. 推理链的正确性验证
   - 最终答案正确 ≠ 推理链正确
   - "凑答案"现象：模型可能通过错误推理得到正确答案
   - 缓解：Process Reward Model (PRM) 逐步验证 + 形式化验证

2. 符号/数值精度
   - LLM 生成的数学表达式可能有微妙错误
   - 例如：(x+1)² = x²+2x+1 vs x²+x+1（少了系数）
   - 缓解：用 SymPy 解析并验证等价性

3. 难度标定
   - AMC 8 (中学) vs AMC 10 vs AIME vs IMO 的难度差距巨大
   - 合成数据的难度分布需要有意识地控制
```

### 5.3 科学领域

#### SciQ / SciInstruct

科学领域合成数据的挑战：**需要领域专家级的准确性**。

```
管线（以生物医学为例）：
1. 从 PubMed 摘要中提取 claim
2. 用 LLM 将 claim 转化为 QA 对
3. 领域专家验证（关键步骤！）
4. 用验证后的数据训练领域模型
5. 用领域模型生成更多数据（但仍需抽检验证）
```

**Galactica 的教训**：Meta 的 Galactica（2022）因为在科学问答中生成"令人信服但错误"的回答而被紧急下架。这说明在科学领域，**合成数据的错误比其他领域更危险**——因为非专家无法识别。

### 5.4 医疗领域

```
医疗合成数据的特殊约束：
├── HIPAA/GDPR 严格限制真实患者数据
├── 错误可能导致实际伤害
├── 需要多层验证（AI 生成 → 医生审核 → 伦理委员会）
└── 数据分布高度不均（常见病 vs 罕见病）

实践做法：
1. 匿名化真实病历 → 作为种子
2. LLM 生成变体（改变年龄/性别/症状组合/lab values）
3. 临床医生验证"临床合理性"
4. 保持罕见病的比例（oversampling）
```

**代表项目**：PMC-LLaMA（用 PubMed Central 文献训练）、Med-PaLM 2（Google，用合成 QA 微调）

### 5.5 法律领域

```
法律合成数据面临独特挑战：
- 管辖区差异巨大（中国法 vs 美国法 vs EU 法差异极大）
- 法条引用必须精确（不存在的法条 = 严重错误）
- 法律推理需要多步论证 + 判例引用

管线：
1. 从真实判决书/法规中提取知识
2. 用 LLM 生成假设情境 + 法律分析
3. 律师审核法条引用准确性（必须！）
4. 训练领域模型
```

### 5.6 多模态图文对

#### 图文合成数据

```
方法 1：Caption 增强
  原始图片 + 简短 caption → VLM (GPT-4V) 生成详细描述
  → 生成多粒度 caption (简短/中等/详细)
  → 作为 VLM 训练数据

方法 2：文本到图像反向生成
  高质量文本描述 → Diffusion Model 生成图片
  → (文本, 图片) 对作为训练数据
  → 用于训练图像理解模型

方法 3：视觉问答合成
  图片 → VLM 生成关于图片的 QA 对
  → 用于训练 VQA 能力
```

**LLaVA 的数据合成**：LLaVA 使用 GPT-4 为 COCO 图片生成了 158K 视觉指令数据，包括 conversation、detail description、complex reasoning 三种类型。仅用这些合成数据，LLaVA 就在多模态理解上达到了令人惊讶的效果。

**ShareGPT4V**：扩展了 LLaVA 的思路，用 GPT-4V 为 100K 图片生成极其详细的描述（平均 500+ words），质量远超 COCO 原始 caption。

---

## 6. 指令调优数据

### 6.1 里程碑数据集

| 数据集 | 规模 | 来源 | 年份 | 核心贡献 |
|--------|------|------|------|----------|
| **LIMA** | 1,000 | 精选人工 | 2023 | "Less is More"——1K 高质量 > 100K 低质量 |
| **Alpaca** | 52K | GPT-3.5 蒸馏 | 2023 | 首个大规模合成指令数据集 |
| **ShareGPT** | ~90K | 用户分享 | 2023 | 真实用户对话，分布最自然 |
| **WizardLM** | 250K | Evol-Instruct | 2023 | 指令进化方法论 |
| **OpenHermes 2.5** | 1M | 多源合成 | 2023 | 混合多个来源，Mistral-7B 最佳配方 |
| **UltraChat** | 1.47M | 多轮对话合成 | 2023 | 大规模多轮对话 |
| **Magpie** | 3M | 自生成 | 2024 | 无种子指令，分布最接近真实用户 |
| **Tulu 3** | ~1M | 混合优化 | 2024 | Allen AI 的系统化数据混合方法论 |

### 6.2 质量 vs 数量的辩论

**LIMA（Zhou et al. 2023）的核心论点**：

> "Alignment can be a simple process where the model learns the style or format 
> for interacting with users, to expose the knowledge and capabilities that were 
> already acquired during pretraining."

用 1,000 条精心挑选的数据 SFT LLaMA-65B，效果接近 GPT-4（人类评估）。这引爆了"质量 vs 数量"的辩论。

**支持"质量优先"的证据**：
1. LIMA：1K 精选 ≈ 52K Alpaca（在人类偏好评估中）
2. Phi-1.5：高质量 "textbook" 数据让 1.3B 模型超越 7B 模型
3. **Allen AI Tulu 3** 的消融实验：去掉 50% 低质量数据后，模型效果反而提升 2-3%

**支持"数量有益"的证据**：
1. Llama 3：15T tokens 预训练，Scaling Laws 仍然有效
2. OpenHermes 2.5：1M 条数据比 250K 有明显提升（前提是质量过滤）
3. DeepSeek-R1 蒸馏：800K 条 reasoning 数据让小模型获得强推理能力

**2026 共识**：

```
不是"质量 OR 数量"，而是"质量 × 数量"

最优策略：
1. 先做质量——用少量精选数据建立 baseline（LIMA 路线）
2. 再做规模——在质量有保证的前提下扩大数据量
3. 过滤是关键——大规模生成 + 严格过滤 > 小规模精选

实际配方（7B 模型 SFT）：
├── 高质量种子数据：1-5K（人工精选）
├── RM 过滤后的合成数据：100K-500K
├── 安全/格式数据：5-10K
└── 总计：~100K-500K（过滤前可能生成 2-5M）
```

### 6.3 Alignment Tax

**定义**：SFT/RLHF 对齐过程中，模型在某些维度上性能下降的现象。

```
典型表现：
├── 对齐后模型"太听话"，失去创造性和探索性
├── 过度拒绝：对模糊请求默认拒绝（"I can't help with that"）
├── 冗长性增加：回答变长但信息密度下降
├── 多语言能力退化：英文对齐数据太多，中文/其他语言变差
└── 专业能力稀释：通用对齐数据"冲淡"了领域专业知识
```

**合成数据如何减轻 Alignment Tax**：

```python
# 策略 1：保留预训练分布的 replay data
sft_data = {
    "instruction_following": 60%,  # 对齐数据
    "pretrain_replay": 20%,        # 预训练格式数据（防遗忘）
    "domain_specific": 15%,        # 领域数据（防专业能力退化）
    "safety": 5%,                  # 安全数据
}

# 策略 2：LIMA-style "表面对齐"
# 不改变模型的知识，只教它"用什么格式回答"
# → 对齐税最小化

# 策略 3：DPO 替代 RLHF
# DPO 的 alignment tax 比 PPO 小（因为更新幅度受 β 控制）
```

### 6.4 数据混合的科学

**Tulu 3 的系统化方法论**（Allen AI 2024）：

```
数据混合不是拍脑袋，而是需要实验的：

Step 1: 确定目标能力维度
  [指令跟随, 推理, 代码, 数学, 创意写作, 安全, 多语言]

Step 2: 为每个维度准备候选数据
  指令跟随 → ShareGPT + Magpie
  推理 → CoT 数据
  代码 → CodeAlpaca + Magicoder
  数学 → NuminaMath
  ...

Step 3: 小规模消融实验（10% 数据量）
  测试不同混合比例 → 在每个维度的 benchmark 上评估

Step 4: 找到帕累托最优混合
  目标：没有任何一个维度显著退化的前提下，整体最优

Step 5: 全量训练 + 验证
```

---

## 7. 偏好数据生成

### 7.1 AI 反馈（RLAIF）

**Constitutional AI（Bai et al. 2022）** 开创了"用 AI 替代人类做偏好判断"的范式。

```
传统 RLHF：
  人类标注 (response_A > response_B) → 训练 Reward Model → PPO

RLAIF / Constitutional AI：
  AI Judge 评估 (response_A vs response_B) → 训练 Reward Model → PPO

Constitutional AI 的独特之处：
  用一组"宪法原则"（Constitutional Principles）指导 AI Judge：
  - "Choose the response that is less harmful"
  - "Choose the response that is more helpful"
  - "Choose the response that is more honest"
  → AI 不是自由发挥，而是根据明确原则做判断
```

**RLAIF 的有效性**（Lee et al. 2023）：

Google 的研究表明，在 helpfulness 维度上，RLAIF 与 RLHF 的对齐效果**统计上不可区分**。在 harmlessness 维度上，RLAIF 甚至略优（因为 AI 标注一致性更高）。

```
RLAIF vs RLHF 人类评估胜率：
  RLAIF win: 48%
  Tie: 4%
  RLHF win: 48%
→ 基本持平
```

#### RLAIF 管线实现

```python
def generate_preference_pair(prompt, model, judge, principles):
    """生成一组偏好对"""
    # 1. 用被训练模型生成两个回答
    response_a = model.generate(prompt, temperature=0.9)
    response_b = model.generate(prompt, temperature=0.9)

    # 2. AI Judge 根据原则评判
    judge_prompt = f"""Based on the following principles:
{principles}

Compare these two responses to: "{prompt}"

Response A: {response_a}
Response B: {response_b}

Which response better follows the principles? Answer A or B, and explain."""

    judgment = judge.generate(judge_prompt)

    # 3. 解析判断结果
    chosen, rejected = parse_judgment(judgment, response_a, response_b)

    # 4. 位置偏见修正：交换 A/B 顺序再判断一次
    judge_prompt_swapped = swap_ab(judge_prompt)
    judgment_swapped = judge.generate(judge_prompt_swapped)
    # 只保留两次判断一致的样本
    if consistent(judgment, judgment_swapped):
        return {"prompt": prompt, "chosen": chosen, "rejected": rejected}
    return None  # 丢弃不一致的
```

### 7.2 合成偏好对的构造方法

#### 方法 1：Same Model, Different Temperature

```python
# 同一模型，不同采样参数
response_good = model.generate(prompt, temperature=0.3)   # 低温 = 更确定/保守
response_bad  = model.generate(prompt, temperature=1.2)   # 高温 = 更随机/可能出错
# 用 RM 或 AI Judge 确认 good > bad
```

#### 方法 2：Strong vs Weak Model

```python
# 强模型的回答 > 弱模型的回答
response_chosen  = gpt4o.generate(prompt)
response_rejected = llama_7b.generate(prompt)
# 假设：强模型回答通常更好（但需要验证！）
```

#### 方法 3：Targeted Perturbation

```python
# 故意"弄坏"好的回答，构造 chosen-rejected 对
good_response = model.generate(prompt)
bad_response = perturb(good_response, type="inject_error")
# 扰动类型：
# - 插入事实错误
# - 降低逻辑连贯性
# - 增加不必要的冗长
# - 加入有害/偏见内容
```

#### 方法 4：Revision-based

```python
# 先生成初始回答，再让 AI 改进
initial = model.generate(prompt)
revised = judge.generate(f"Improve this response: {initial}")
# chosen = revised, rejected = initial
```

### 7.3 UltraFeedback

**UltraFeedback**（Cui et al. 2024）是 2024 年最具影响力的合成偏好数据集，直接推动了 DPO 方法的大规模应用。

```
构建流程：
1. 收集 64K 指令（来自多个开源数据集）
2. 用 17 个不同的 LLM 生成回答（GPT-4, Claude, LLaMA 等）
3. GPT-4 对每个回答在 4 个维度上打分（1-5）：
   - Instruction Following
   - Truthfulness
   - Honesty
   - Helpfulness
4. 从每个指令的多个回答中构造偏好对：
   chosen = 最高分回答
   rejected = 较低分回答（不一定是最低，保证难度）

最终规模：~64K instructions × ~4 responses each = ~256K 评估
→ 构造 ~64K 偏好对
```

**UltraFeedback 的影响**：
- Zephyr-7B-β（使用 UltraFeedback + DPO）超越了 Llama 2-70B-Chat
- 证明了"合成偏好数据 + DPO"是一条可行且高效的对齐路径
- 几乎所有 2024 年的开源 chat 模型都直接或间接使用了 UltraFeedback

### 7.4 偏好数据的质量陷阱

```
常见问题：
1. 长度偏见 — AI Judge（尤其是 GPT-4）倾向于选择更长的回答
   缓解：在评判原则中明确"简洁也是优点" + 长度归一化

2. 位置偏见 — 放在前面的回答更容易被选为"更好"
   缓解：交换位置做两次判断，只保留一致结果

3. 自我偏好 — GPT-4 作为 Judge 会偏好 GPT-4 生成的回答
   缓解：使用不同系列的模型做 Judge（如用 Claude 评 GPT 的输出）

4. 标注者协议度低 — 不同 AI Judge 对同一对的判断可能不一致
   缓解：多个 Judge 投票 + 只保留高一致性样本

5. 能力天花板 — AI Judge 无法评估超越自身能力的回答
   缓解：对高难度任务引入领域专家验证
```

---

## 8. 前沿方向

### 8.1 个性化合成数据

**核心想法**：不是为所有人生成通用数据，而是根据目标用户群体的特征定制合成数据。

```
应用场景：
├── 企业微调：根据企业内部文档风格生成训练数据
├── 领域适配：根据医生/律师/工程师的专业用语习惯生成
├── 文化适配：考虑文化背景的语言和行为规范
└── 个人助手：根据用户历史偏好生成个性化训练数据

技术路径：
User Profile → Prompt Conditioning → LLM 生成 → 个性化过滤
→ 用个性化数据微调（或 RAG 检索）
```

### 8.2 课程学习（Curriculum Learning）

按照"由易到难"的顺序组织合成数据，模拟人类学习过程。

```python
def curriculum_synthetic_data(model, difficulty_levels=5):
    """课程学习式数据飞轮"""
    for level in range(1, difficulty_levels + 1):
        # 1. 生成当前难度的数据
        data = generate_at_difficulty(level)

        # 2. 过滤：保留模型"刚好做不对"的数据（最大信息增益）
        data = filter_by_model_difficulty(data, model, target_accuracy=0.3)

        # 3. 训练
        model = train(model, data)

        # 4. 评估：当前难度通过率 > 阈值才进入下一级
        accuracy = evaluate(model, level)
        if accuracy < 0.7:
            # 重新生成更多当前难度数据
            continue

    return model
```

**实际效果**：在数学推理任务上，课程学习比随机顺序训练提升 5-10%（DeepSeek-Math 的训练管线中有类似做法）。

### 8.3 数据溯源与归属

随着合成数据大规模使用，**谁拥有合成数据的权利？** 成为法律和伦理难题。

```
核心问题：
1. 如果 GPT-4 生成的数据用于训练竞品模型，OpenAI 有权阻止吗？
   → OpenAI ToS 禁止用输出训练竞品（但执行困难）

2. 如果合成数据中包含受版权保护的内容的"变体"，算侵权吗？
   → 2024 NYT v. OpenAI 诉讼未解决这个问题

3. 如果模型 A 蒸馏模型 B，模型 B 又蒸馏了模型 C，权利链条怎么追溯？
   → 尚无法律先例

技术方案：
- 数据水印（见 8.4）
- 模型指纹（Model Fingerprinting）
- 训练数据成员推断（Membership Inference）
```

### 8.4 合成数据检测与水印

#### 文本水印（Kirchenbauer et al. 2023）

```
原理：
在生成过程中，将 token 词表分为"绿色"和"红色"两组（基于前文 hash）
→ 偏向选择"绿色" token
→ 对生成质量影响极小（人类几乎无法察觉）
→ 但可以通过统计检验高可信度地检测

检测：
统计文本中"绿色 token"的比例
→ 如果显著高于 50%（z-test），则判定为水印文本

挑战：
- 对生成长度敏感（短文本检测困难）
- 改写攻击可以破坏水印
- 多个水印系统共存时可能互相干扰
```

#### 合成数据检测（无水印）

```
检测方法：
1. Log-likelihood 方法：合成文本通常有更高的 log-likelihood（因为是模型自然输出）
2. 统计特征：合成文本的 perplexity 分布与人类文本不同
3. 训练检测器：用已知的真实/合成文本对训练二分类器

2025 前沿：
- DetectGPT（Mitchell et al. 2023）：基于曲率的零样本检测
- DNA-GPT：基于发散重生成的检测
- RADAR（Hu et al. 2024）：对抗训练的鲁棒检测器
```

**合成数据检测用于训练数据审计**：随着 EU AI Act 要求披露训练数据来源，检测训练数据中合成数据的比例将成为合规需求。

### 8.5 Agent 训练数据的合成

2025-2026 年的新兴方向：为 Agent（工具调用、多步规划）生成合成训练数据。

```
挑战：
- Agent 数据需要 environment interaction traces，不能纯文本生成
- 工具调用的结果需要模拟（或用真实环境运行）
- 多步规划的正确性验证比单步 QA 复杂得多

方案：
1. 模拟环境：用规则引擎模拟工具返回值
2. 真实环境 + 过滤：让 Agent 在沙箱中运行，只保留成功轨迹
3. 轨迹改写：将成功轨迹改写为不同变体
4. Hindsight 重标注：将失败轨迹也利用起来（标注为 "rejected"）

代表工作：
- AgentInstruct（Microsoft 2024）：用多 Agent 协作生成高质量训练数据
- Agent-FLAN（Google 2024）：为 Agent 任务定制的指令调优数据
- FireAct（Chen et al. 2023）：用 GPT-4 轨迹微调开源模型做 Agent
```

### 8.6 2026 前沿趋势总结

```
┌─────────────────────────────────────────────────────┐
│              2026 合成数据前沿趋势                    │
├─────────────────────────────────────────────────────┤
│ 1. 预训练阶段也大量使用合成数据（Mid-training）      │
│    → Phi-4, Llama 3 已在做                          │
│                                                      │
│ 2. 端到端自动化飞轮（全流程无人工）                  │
│    → 生成→执行验证→RM过滤→训练→评估→再生成           │
│                                                      │
│ 3. 合成数据的合成数据（Meta-Synthetic）              │
│    → 用合成数据训练的模型再生成合成数据               │
│    → 需要严格的 collapse 监控                        │
│                                                      │
│ 4. 跨模态合成（Text→Image→Video→3D）                │
│    → 一种模态的合成数据用于训练另一种模态             │
│                                                      │
│ 5. 合成数据市场化                                    │
│    → Scale AI, Surge AI 等公司提供"合成数据即服务"    │
│    → 数据质量标准和审计框架正在建立                   │
│                                                      │
│ 6. 合规驱动                                          │
│    → EU AI Act 要求训练数据披露                      │
│    → 合成数据作为合规路径受到企业青睐                 │
└─────────────────────────────────────────────────────┘
```

---

## 9. 面试高频问题

### Q1: 合成数据的核心价值是什么？它能"创造新知识"吗？

**参考答案**：

合成数据的核心价值有三层：

1. **格式转换**：将模型权重中的隐式知识转化为显式训练样本。例如 GPT-4 "知道"如何写好代码，通过生成代码样本将这个知识变成可用于训练的显式数据。

2. **分布扩展**：通过受控变换（参数变化、上下文替换、难度调节）在已有数据分布基础上扩展到更多场景。

3. **成本降低**：相比人工标注降低 100-1000 倍成本，同时支持百万级规模。

**能否创造新知识？** 严格来说不能——模型不能生成超越自身能力的数据。但有两个"近似创造"的机制：
- **组合创新**：模型将不同领域的知识组合，产生原始训练数据中没有的"新组合"
- **涌现能力外化**：模型在训练中获得的涌现能力（如 CoT 推理）可以通过合成数据转移给更小的模型

**如果面试官追问**："那 DeepSeek-R1 蒸馏不就是让 7B 模型获得了它本身不具备的推理能力吗？" 回答：是的，但这个推理能力来自 R1（671B）——蒸馏的上限是 Teacher 的能力。7B 模型不是"创造"了推理能力，而是从 R1 "继承"了推理能力的一个子集。

### Q2: 如何判断合成数据的质量是否足够好？

**参考答案**：

多维度评估框架：

1. **Correctness（正确性）**：回答是否事实正确。对于代码和数学，用执行/验证；对于自然语言，用 RM 或人工抽检。

2. **Diversity（多样性）**：用 Distinct-N、Self-BLEU、embedding 覆盖率度量。Self-BLEU > 0.7 说明数据太同质。

3. **Difficulty Distribution（难度分布）**：数据集的难度分布是否与目标任务匹配。如果全是简单题，模型学不到高难度推理。

4. **Contamination（污染度）**：与评估集的重叠度。必须做 n-gram + 语义去污染。

5. **Downstream Performance（下游表现）**：最终判据——在 held-out 评估集上的表现。

**实战优先级**：先看下游表现（最终标准），如果下游表现差，再逐一排查上述维度。

**如果面试官追问**："你怎么区分'数据质量问题'和'训练配置问题'？" 回答：控制变量法——先用已知高质量数据（如 LIMA 1K）训练，确认训练管线无误；然后替换为合成数据，差异即归因于数据质量。

### Q3: 解释 Model Collapse，以及在工程实践中如何避免？

**参考答案**：

**Model Collapse**（Shumailov et al. 2024, Nature）：当模型在纯合成数据上迭代训练多代时，模型能力逐渐退化。机制是每代近似误差累积——分布尾部（低频事件）被逐步截断，输出趋向同质化。

工程实践中的防范：

1. **永远混合真实数据**：30-50% 的真实数据作为"锚"——这是最重要的单一措施
2. **当代生成**：只用当前模型生成数据，避免跨代误差累积
3. **多样性监控**：持续追踪 Distinct-N 和 Self-BLEU，发现退化及时干预
4. **多源生成**：用多个不同的模型生成，而非单一模型
5. **保留原始数据**：每代训练都包含所有历史真实数据（accumulate 策略）

**如果面试官追问**："大模型也会 collapse 吗？" 回答：会，只是慢一些。模型越大，近似能力越强，每代的误差越小——但误差仍然在累积，只是需要更多代才会显现。

### Q4: Self-Instruct、Evol-Instruct、Magpie 三种方法的核心区别和适用场景？

**参考答案**：

| | Self-Instruct | Evol-Instruct | Magpie |
|---|---|---|---|
| 核心机制 | 从种子指令生成新指令 | 对指令做复杂化进化 | 利用空 user turn 自动生成 |
| 种子需求 | 175 条种子指令 | 需要初始指令集 | 零种子 |
| 多样性 | 中等，有退化风险 | 高（多种进化策略） | 最高（接近真实分布） |
| 难度控制 | 弱 | 强（可控进化轮数） | 中等（取决于模型） |
| 适用场景 | 通用指令生成 | 需要高难度/复杂指令 | 大规模通用数据生成 |

**选择建议**：
- 需要快速生成百万级通用数据 → **Magpie**
- 需要特定难度/领域的复杂指令 → **Evol-Instruct**
- 需要从少量种子快速启动 → **Self-Instruct**
- 生产中通常混合使用

### Q5: RLAIF 和 RLHF 的核心区别？什么时候 RLAIF 更好？

**参考答案**：

**核心区别**：偏好标注者不同——RLHF 用人类，RLAIF 用 AI（通常是更强的 LLM）。

**RLAIF 的优势**：
- 成本低 100-1000 倍
- 标注一致性更高（同一 AI Judge 不会自相矛盾）
- 可扩展到百万级偏好对
- 在 harmlessness 维度上甚至略优（AI 更不会被情绪影响）

**RLAIF 更好的场景**：
- 预算有限，无法雇佣大量标注人员
- 任务的评判标准可以明确用原则描述
- 需要大规模偏好数据（>100K 对）

**RLHF 仍然必要的场景**：
- 涉及微妙的文化/社会判断（AI 可能有盲区）
- 评估超越 AI Judge 能力的输出（如前沿研究质量）
- 法规要求人类参与决策的场景
- 作为 RLAIF 的校准锚——用少量人类标注验证 AI Judge 的偏好是否合理

**如果面试官追问**："RLAIF 的 AI Judge 有偏见怎么办？" 回答：用多个不同系列的 Judge（GPT-4 + Claude + Gemini）交叉验证；对 Judge 的判断做一致性检测（Cohen's Kappa）；定期用人类标注样本校准 Judge。

### Q6: 如何为一个新领域（比如法律）构建合成数据管线？

**参考答案**：

完整管线：

```
Step 1: 领域知识注入
  → 收集领域文档（法规、判例、教材）
  → RAG 索引或 continue pretrain（如果量大）

Step 2: 种子数据构建（关键！）
  → 领域专家编写 200-500 条高质量 QA
  → 覆盖核心场景：法律咨询/条文解读/案例分析/文书撰写

Step 3: 合成扩展
  → Evol-Instruct 增加复杂度
  → 参数变换：改变案件细节/法律适用
  → 多角度：原告/被告/法官视角

Step 4: 质量过滤
  → 法条引用准确性验证（规则检查）
  → 领域 RM 或 GPT-4 打分
  → 5% 律师抽检

Step 5: 迭代
  → 训练模型 → 发现弱点 → 补充对应数据 → 再训练
```

**如果面试官追问**："法律领域最大的合成数据风险是什么？" 回答：**幻觉法条**——模型引用不存在的法律条文。这在法律场景比其他领域严重得多，因为虚假法条可能直接导致当事人败诉。必须用法律数据库做实体级验证。

### Q7: On-policy 和 Off-policy 数据在 RL 训练中的区别？为什么 On-policy 更有效？

**参考答案**：

**On-policy**：训练数据来自当前策略模型。PPO 每步都用当前模型采样，保证梯度方向准确。

**Off-policy**：训练数据来自其他模型或历史版本。DPO 使用固定偏好数据集，是典型的 off-policy。

**On-policy 更有效的原因**：

1. **无分布偏移**：当前模型在第 N 步的输出分布已经与第 1 步大不相同。Off-policy 数据对应的是"旧"的输出分布，梯度方向可能不准确。

2. **RM 校准**：如果 RM 在旧分布上训练，对新分布的评分可能不准——这是 reward hacking 的根源之一。

3. **自适应难度**：On-policy 采样天然聚焦在模型当前"犹豫"的区域——即信息增益最大的区域。

**但 Off-policy 也有优势**：
- 数据可复用，计算成本低
- 训练更稳定（数据不随训练变化）
- DPO/SimPO 的简单性在大多数场景够用

**如果面试官追问**："怎么缓解 Off-policy 的分布偏移？" 回答：importance sampling 修正（但方差大）；定期用当前模型重新采样一批数据混入；或者用 Iterative DPO（每几步重新采样偏好对）。

### Q8: Rejection Sampling 为什么有效？它和 Best-of-N 的关系？

**参考答案**：

**Rejection Sampling** 对每个问题生成 K 个回答，用 RM 选最好的 N 个（N < K）作为训练数据。

**为什么有效**：
- 相当于隐式 RL：不直接优化 RM 梯度，但通过选择实现了类似效果
- 当 K=16, N=4 时，选出的回答质量显著高于随机——这就是"用推理计算换训练质量"
- Llama 2 论文证明 5 轮 rejection sampling 后质量持续提升

**与 Best-of-N 的关系**：
- Best-of-N 在**推理时**选最优：生成 N 个，选 RM 分数最高的返回
- Rejection Sampling 在**训练时**选最优：生成 K 个，选最好的 N 个做 SFT

**本质区别**：Best-of-N 是推理成本 ×N，Rejection Sampling 是训练数据质量 ×N。后者的成本只发生一次（训练时），推理时无额外成本。

### Q9: UltraFeedback 为什么如此成功？它对 DPO 生态的影响？

**参考答案**：

**成功原因**：
1. **多模型覆盖**：用 17 个不同 LLM 生成回答，确保 chosen-rejected 对有足够的质量差异
2. **多维度评分**：4 个维度（instruction following、truthfulness、honesty、helpfulness），而非简单的好/坏二分
3. **GPT-4 做 Judge 的一致性**：比人类标注者间一致性更高
4. **规模恰当**：64K 指令不算大，但质量极高——完美体现"质量 > 数量"

**对 DPO 生态的影响**：
- Zephyr-7B-β = Mistral-7B + UltraFeedback + DPO → 超越 Llama 2-70B-Chat
- 这个结果证明了"合成偏好数据 + DPO"这条路径的可行性
- 几乎所有 2024 年开源 chat 模型都直接或间接使用了 UltraFeedback
- 催生了更多类似数据集：Nectar、HelpSteer 2、Skywork-Reward-Preference-80K

### Q10: 如何设计一个端到端的代码合成数据飞轮？

**参考答案**：

```
完整设计：

数据来源层：
├── 种子数据：开源编程题库（LeetCode/Codeforces 公开题）
├── 增量数据：OSS-Instruct（从 GitHub 代码中提取灵感）
└── 用户反馈：生产环境中用户标记的好/坏回答

生成层：
├── 当前模型生成解答（on-policy）
├── 多解生成（同一题 K=8 个解法）
└── 多语言/多框架变体

验证层（关键差异化！）：
├── 编译检查
├── 单元测试执行
├── 性能测试（时间/空间复杂度）
├── 代码风格检查（linter）
└── RM 评分（代码质量维度）

过滤层：
├── 只保留全部测试通过的解答
├── 在通过的解答中用 RM 选最优
├── 多样性采样（避免所有解法雷同）
└── 难度平衡（简单:中等:困难 = 3:5:2）

训练层：
├── SFT 一个 epoch
├── 评估（HumanEval/MBPP/LiveCodeBench）
└── 如果提升 → 用新模型进入下一轮生成

循环频率：每周一轮（工程化后可自动运行）
```

### Q11: 合成数据在多模态训练中如何应用？

**参考答案**：

三大应用模式：

1. **Caption 增强**：用 VLM（GPT-4V）为图片生成高质量描述。LLaVA 用 158K 合成视觉指令数据就达到了很好效果。ShareGPT4V 进一步扩展了这个思路。

2. **图文对生成**：文本描述 → Diffusion 模型 → 图片。用于数据稀缺的场景（如医学影像描述对）。

3. **视觉 QA 合成**：图片 → VLM 生成 QA 对。包括物体识别、空间关系、常识推理、图表理解等多种 QA 类型。

**核心挑战**：多模态合成数据的"幻觉"更难检测——VLM 可能描述图片中不存在的物体或错误的空间关系，而自动验证比纯文本困难得多。

### Q12: 什么是 Alignment Tax？合成数据如何帮助减少它？

**参考答案**：

**Alignment Tax** 是对齐（SFT/RLHF）过程中模型某些能力退化的代价。具体表现：
- 创造性/探索性下降（模型变得"太安全"）
- 过度拒绝（对模糊请求默认拒绝）
- 冗长化（回答变长但信息密度降低）
- 多语言/领域能力退化

**合成数据减轻 Alignment Tax 的方法**：

1. **数据混合**：在对齐数据中混入 20-30% 预训练格式数据（replay buffer），防止遗忘
2. **领域保持**：为需要保持的能力（代码、数学、多语言）生成等量的对齐格式数据
3. **LIMA 式轻量对齐**：只教格式，不改知识——用少量高质量数据做"表面对齐"
4. **DPO 替代 PPO**：DPO 的更新幅度受 β 控制，alignment tax 更小

**如果面试官追问**："能量化 alignment tax 吗？" 回答：可以。在 SFT 前后分别跑 MMLU/HumanEval/MATH 等 benchmark，性能下降的幅度就是 tax。典型值：MMLU 下降 1-3%，代码能力下降 2-5%（如果不做 replay）。

### Q13: 如何检测训练数据中是否包含合成数据？

**参考答案**：

三类检测方法：

1. **水印检测**：如果合成数据有 watermark（如 Kirchenbauer 2023），可以通过统计"绿色 token"比例检测。准确率高但前提是必须有水印。

2. **统计特征检测**：合成文本的 perplexity 分布与人类文本不同——通常更低且更集中。DetectGPT 利用 log-probability 的局部曲率做零样本检测。

3. **分类器检测**：用已知的真实/合成文本对训练二分类器。RoBERTa 在此任务上可达 95%+ 准确率——但对改写后的合成文本效果下降。

**2026 前沿**：EU AI Act 可能要求大模型披露训练数据中合成数据的比例，推动了合成数据检测技术的发展。

### Q14: 什么场景下合成数据**不应该**使用？

**参考答案**：

1. **安全关键决策**：医疗诊断、法律判决、金融风控——合成数据的错误可能导致实际伤害
2. **需要极高事实准确性**：百科知识、科学研究——合成数据的幻觉率无法降到零
3. **评估和测试**：用合成数据做评估集会污染结果——评估集必须用真实/手工数据
4. **模型已经在目标分布上有充足真实数据**：此时合成数据的边际收益很小
5. **涉及用户隐私/个人信息**：合成数据可能"泄露"真实数据的模式

**核心原则**：合成数据是"扩展器"不是"替代品"——它扩展真实数据的覆盖范围和规模，但不能替代真实数据的锚定作用。

---

## 10. 常见误区与陷阱

### 误区 1：合成数据量越大越好

**真相**：没有质量过滤的合成数据，规模扩大反而有害。Phi 系列证明：高质量 30B tokens > 低质量 300B tokens。数据管线的核心瓶颈不是"生成"而是"过滤"。

### 误区 2：强模型生成的数据一定是高质量的

**真相**：即使 GPT-4 也有 5-10% 的事实性错误率。在数学推理中，GPT-4 的 CoT 正确率在 MATH hard 上只有 ~60%。**模型越自信地犯错，数据质量危害越大**——因为看起来正确的错误答案比明显的错误更难被过滤掉。

### 误区 3：DPO 不需要高质量偏好数据

**真相**：DPO 对数据质量**极其敏感**。如果 chosen 和 rejected 的质量差异不够大（或标注有噪声），DPO 训练会退化甚至有害。UltraFeedback 的成功恰恰因为它有清晰的质量分层。

### 误区 4：合成数据可以完全替代真实数据

**真相**：Model Collapse 研究明确表明，纯合成数据训练会导致不可逆退化。**真实数据是锚**，合成数据是扩展。最佳实践是 30-50% 真实数据 + 50-70% 合成数据。

### 误区 5：去重就等于去污染

**真相**：去重（deduplication）移除重复样本；去污染（decontamination）移除与评估集重叠的样本。二者是不同的操作。一个去重完美的数据集仍然可能被评估集污染——如果每个样本都独特但都和评估集的某道题相似。

### 误区 6：RM 分数越高的数据越好

**真相**：RM 有系统性偏见（length bias、style bias）。过度依赖 RM 过滤会导致数据集偏向 RM 的偏好而非真正的质量。应该使用 stratified filtering + 多 RM 交叉验证 + 多样性约束。

### 误区 7：一次性生成就足够了

**真相**：合成数据的最大价值在于**迭代飞轮**——模型变强 → 生成质量提升 → 训练数据更好 → 模型更强。一次性生成只利用了价值链的一小部分。

### 误区 8：Magpie/Self-Instruct 可以生成任何领域的数据

**真相**：这些方法生成的数据分布受限于模型的训练分布。如果模型在法律/医疗等专业领域的训练不足，自动生成的指令也会集中在模型"会"的通用主题上，无法覆盖真正的专业需求。领域专精需要专门设计的种子 + 领域知识注入。

### 误区 9：合成数据的版权没有风险

**真相**：虽然合成数据不直接复制版权内容，但如果 Teacher 模型在版权材料上训练，生成的合成数据可能包含受保护内容的"实质性相似"变体。2024 年的 NYT v. OpenAI 案还在审理中，法律前景不明确。

### 误区 10：AI Judge 的评判结果可以直接信任

**真相**：AI Judge 有多种偏见——位置偏见（偏好 A 位置）、长度偏见（偏好长回答）、自我偏好（偏好同系列模型的输出）。必须做位置交换验证、多 Judge 交叉验证、定期人工抽检校准。

---

## 11. 参考文献

1. Wang et al. "Self-Instruct: Aligning Language Models with Self-Generated Instructions" (2023) — Self-Instruct 方法论
2. Xu et al. "WizardLM: Empowering Large Language Models to Follow Complex Instructions" (2023) — Evol-Instruct
3. Xu et al. "Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing" (2024) — Magpie
4. Taori et al. "Stanford Alpaca: An Instruction-following LLaMA Model" (2023) — 首个大规模合成指令数据集
5. Mukherjee et al. "Orca: Progressive Learning from Complex Explanation Traces of GPT-4" (2023) — 系统提示引导蒸馏
6. Zhou et al. "LIMA: Less Is More for Alignment" (2023) — 1K 数据对齐
7. Gunasekar et al. "Textbooks Are All You Need" (2023) — Phi-1，高质量合成数据
8. Li et al. "Textbooks Are All You Need II: phi-1.5 Technical Report" (2023) — Phi-1.5
9. Shumailov et al. "AI Models Collapse When Trained on Recursively Generated Data" (2024, Nature) — Model Collapse
10. Alemohammad et al. "Self-Consuming Generative Models Go MAD" (2024) — Model Collapse 分析
11. Bai et al. "Constitutional AI: Harmlessness from AI Feedback" (2022) — Constitutional AI / RLAIF
12. Lee et al. "RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback" (2023) — RLAIF 有效性验证
13. Cui et al. "UltraFeedback: Boosting Language Models with High-quality Feedback" (2024) — UltraFeedback
14. Tunstall et al. "Zephyr: Direct Distillation of LM Alignment" (2023) — Zephyr + DPO
15. Wei et al. "Magicoder: Source Code Is All You Need" (2023) — OSS-Instruct
16. Guo et al. "DeepSeek-Coder: When the Large Language Model Meets Programming" (2024)
17. Zhu et al. "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence" (2024)
18. Shao et al. "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" (2024)
19. Li et al. "NuminaMath: The Largest Public Math Competition Dataset" (2024)
20. Lozhkov et al. "StarCoder 2 and The Stack v2: The Next Generation" (2024) — StarCoder 2
21. DeepSeek-AI. "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning" (2025) — R1 蒸馏
22. Burns et al. "Weak-to-Strong Generalization: Eliciting Strong Capabilities with Weak Supervision" (2023)
23. Chen et al. "Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models" (2024) — SPIN
24. Yuan et al. "Self-Rewarding Language Models" (2024) — 自我奖励
25. Kirchenbauer et al. "A Watermark for Large Language Models" (2023) — 文本水印
26. Mitchell et al. "DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature" (2023)
27. Liu et al. "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning" (2024)
28. Ivison et al. "Tulu 3: Pushing Frontiers in Open Language Model Post-Training" (2024) — 数据混合方法论
29. Liu et al. "LLaVA: Visual Instruction Tuning" (2024) — 多模态合成数据
30. Chen et al. "ShareGPT4V: Improving Large Multi-Modal Models with Better Captions" (2024)
31. Ding et al. "UltraChat: An Informative and Diverse Multi-Turn Dialogue Dataset" (2023)
32. Zeng et al. "AgentInstruct: Toward Generative Teaching with Agentic Flows" (2024) — Agent 训练数据合成
33. Touvron et al. "Llama 2: Open Foundation and Fine-Tuned Chat Models" (2023) — Rejection Sampling 飞轮
34. Abdin et al. "Phi-4 Technical Report" (2024) — 合成数据在预训练中的应用
35. Hu et al. "RADAR: Robust AI-Text Detection via Adversarial Learning" (2024) — 合成文本检测

---

_面试武器库 #25 — 合成数据与数据飞轮方向完整覆盖_
_涵盖：生成方法论 / 数据飞轮 / 质量控制 / 领域应用 / 指令调优 / 偏好数据 / 前沿方向_

---

## See Also

- [[AI/LLM/Application/Synthetic-Data/Synthetic Data|Synthetic Data 综述]] — 合成数据基础概念速查
- [[AI/LLM/Application/Synthetic-Data/DataFlow|DataFlow]] — 字节跳动数据质量评估框架
- [[AI/LLM/RL/RLHF-DPO-2026-技术全景|RLHF DPO 2026 技术全景]] — 对齐技术深度版
- [[AI/LLM/Efficiency/知识蒸馏与模型压缩-2026技术全景|知识蒸馏与模型压缩 2026]] — 蒸馏方向深度
- [[AI/LLM/Application/LLM代码生成-2026技术全景|LLM 代码生成 2026]] — 代码合成数据的应用场景
- [[AI/LLM/目录|LLM MOC]] — 大语言模型知识全图谱
- [[AI/Agent/Agentic-RL/AWM-Agent-World-Model-Synthetic-Environments|AWM（合成环境生成）]] ⭐ — 合成数据的Agentic RL应用：AWM用code-driven POMDP合成1000个训练环境（而非文本数据），是"数据飞轮"思想在Agent领域的延伸——环境合成≡环境数据飞轮
