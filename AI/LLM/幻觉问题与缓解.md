---
title: "LLM 幻觉问题与缓解策略"
date: 2026-02-13
tags:
  - ai/llm/hallucination
  - ai/llm/safety
  - type/concept
  - interview/hot
status: active
---

# LLM 幻觉问题与缓解策略

> 幻觉的产生原因、分类、检测方法与缓解策略——从 RAG 到 Self-Consistency 到 Grounding

## 1. 什么是幻觉（Hallucination）？

**定义**：LLM 生成的内容看起来流畅合理，但与事实不符或无法被输入/已知知识验证。

### 幻觉分类

```
LLM 幻觉
├── Intrinsic Hallucination（内在幻觉）
│   └── 与输入/上下文矛盾的输出
│   └── 例: 文档说"2024年"，模型说"2023年"
│
├── Extrinsic Hallucination（外在幻觉）
│   └── 输出无法被输入验证（编造信息）
│   └── 例: 模型编造不存在的论文引用
│
└── Confabulation（混淆/杜撰）
    └── 模型"自信地"编造细节
    └── 例: 给一个真实人物编造虚假经历
```

### 幻觉的严重性

| 场景 | 影响 | 容忍度 |
|------|------|--------|
| 闲聊 | 低 | 高 |
| 内容创作 | 中 | 中 |
| 医疗问答 | 极高 | 极低 |
| 法律文书 | 极高 | 零 |
| 代码生成 | 高（bug） | 低 |
| 金融分析 | 极高 | 极低 |

## 2. 产生原因

### 2.1 训练数据层面

```
1. 数据噪声: 训练语料中存在错误/过时信息
2. 知识截止: 模型无法获取训练截止后的信息
3. 长尾知识: 低频实体/事件在训练中见得少，容易编造
4. 重复模式: 模型倾向于复制训练中的高频模式（而非推理）
```

### 2.2 模型架构层面

```
1. Next-token prediction: 优化"最可能的下一个词"，不等于优化"正确性"
2. 注意力漂移: 长文本中注意力可能偏离关键信息
3. 位置编码限制: 超出训练长度的位置外推不稳定
4. Softmax 瓶颈: 概率分布的表达能力有限
```

### 2.3 解码策略层面

```python
# Temperature 和采样策略影响幻觉率
# Temperature 越高 → 多样性越大 → 幻觉风险越高

# 低温度（确定性强，幻觉少）
response = model.generate(temperature=0.1, top_p=0.9)

# 高温度（创造性强，幻觉多）
response = model.generate(temperature=1.0, top_p=0.95)

# Beam search 通常比 sampling 幻觉更少
# 但也更无聊（多样性低）
```

### 2.4 Prompt 层面

- 模糊/歧义 prompt → 模型"脑补"
- 引导性 prompt → 模型顺着错误前提走
- 超出模型能力范围的问题 → 硬答不如不答

## 3. 检测方法

### 3.1 基于模型自身

**Self-Evaluation（自我评估）**：

```python
def detect_hallucination_self_eval(question, answer, model):
    """让模型评估自己的回答是否有幻觉"""
    prompt = f"""Given the question and answer below, identify any statements 
that might be factually incorrect or unverifiable.

Question: {question}
Answer: {answer}

For each statement, rate confidence (1-5) and flag potential hallucinations:"""
    
    evaluation = model.generate(prompt)
    return parse_evaluation(evaluation)
```

**Logprob 分析**：低置信度 token 序列更可能是幻觉。

```python
def detect_low_confidence_spans(logprobs, threshold=-2.0):
    """标记低置信度片段"""
    suspicious = []
    for i, lp in enumerate(logprobs):
        if lp < threshold:
            suspicious.append(i)
    return suspicious
```

### 3.2 基于一致性（Self-Consistency）

```python
import collections

def consistency_check(question, model, n_samples=5):
    """多次采样检查答案一致性"""
    answers = []
    for _ in range(n_samples):
        response = model.generate(
            question, 
            temperature=0.7
        )
        answer = extract_key_claims(response)
        answers.append(answer)
    
    # 不一致的回答更可能包含幻觉
    claim_counts = collections.Counter(
        claim for ans in answers for claim in ans
    )
    
    inconsistent_claims = [
        claim for claim, count in claim_counts.items()
        if count < n_samples * 0.5  # 不到一半采样出现
    ]
    return inconsistent_claims
```

### 3.3 基于外部知识（Fact-Checking）

```python
def rag_based_fact_check(claims, knowledge_base):
    """基于 RAG 的事实核查"""
    results = []
    for claim in claims:
        # 1. 分解为可验证的原子事实
        atomic_facts = decompose_claim(claim)
        
        for fact in atomic_facts:
            # 2. 检索相关证据
            evidence = knowledge_base.search(fact, top_k=3)
            
            # 3. 判断支持/反驳/无关
            verdict = nli_model.predict(
                premise=evidence,
                hypothesis=fact
            )  # SUPPORT / CONTRADICT / NEUTRAL
            
            results.append({
                "fact": fact,
                "evidence": evidence,
                "verdict": verdict
            })
    return results
```

### 3.4 专用检测工具

| 工具 | 方法 | 特点 |
|------|------|------|
| **SelfCheckGPT** | 采样一致性 | 无需外部知识 |
| **FActScore** | 原子事实 + 维基百科验证 | 细粒度评分 |
| **HHEM** (Vectara) | NLI-based 检测 | 快速，支持 RAG 场景 |
| **Chainpoll** | 多次 LLM 投票 | 灵活，可自定义 |
| **TruLens** | RAG 三角评估 | 与 RAG 管线集成 |

## 4. 缓解策略

### 4.1 检索增强生成（RAG）

最成熟、最广泛使用的策略。通过外部检索为 LLM 提供"事实基础"。

```python
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

# 构建知识库
documents = SimpleDirectoryReader("./knowledge_base").load_data()
index = VectorStoreIndex.from_documents(documents)

# RAG 查询
query_engine = index.as_query_engine(
    similarity_top_k=5,
    response_mode="compact",  # 压缩上下文减少噪声
)

# System prompt 强化引用
system_prompt = """Answer based ONLY on the provided context. 
If the context doesn't contain enough information, say "I don't have enough information."
Always cite the source for each claim."""

response = query_engine.query("What is the revenue of company X in 2025?")
```

**RAG 自身的幻觉问题**：
- 检索到错误/无关文档 → "garbage in, garbage out"
- 上下文窗口塞太多 → 模型忽略关键片段（"lost in the middle"）
- 生成时脱离检索内容 → 仍然编造

参见 [[RAG 工程实践]] 中的详细方案。

### 4.2 Self-Consistency 采样

多次采样 + 多数投票，幻觉作为"随机噪声"在投票中被过滤。

```python
def answer_with_self_consistency(question, model, n=5):
    """多次采样 + 投票"""
    answers = []
    for _ in range(n):
        resp = model.generate(question, temperature=0.7)
        answers.append(extract_answer(resp))
    
    # 多数投票
    most_common = collections.Counter(answers).most_common(1)[0]
    confidence = most_common[1] / n
    
    if confidence < 0.6:
        return "I'm not confident enough to answer this question."
    return most_common[0]
```

### 4.3 Grounding（接地）

将模型输出锚定到可验证的来源：

```python
# 1. 引用标注（Citation）
system_prompt = """For every factual claim, include a citation [1], [2], etc.
At the end, list all sources. If you cannot cite a source, prefix with 
"Based on my training data (may be inaccurate):"."""

# 2. 工具调用验证
def grounded_answer(question):
    # 先让模型规划需要验证的事实
    plan = model.generate(f"What facts need verification for: {question}")
    
    # 调用搜索/数据库验证
    verified_facts = []
    for fact in parse_facts(plan):
        result = web_search(fact)
        verified_facts.append({
            "claim": fact,
            "source": result.url,
            "verified": result.supports(fact)
        })
    
    # 基于验证过的事实生成最终回答
    return model.generate(
        f"Based on these verified facts: {verified_facts}\n"
        f"Answer: {question}"
    )

# 3. 结构化输出约束
response_schema = {
    "type": "object",
    "properties": {
        "answer": {"type": "string"},
        "confidence": {"type": "number", "minimum": 0, "maximum": 1},
        "sources": {"type": "array", "items": {"type": "string"}},
        "caveats": {"type": "string"}
    }
}
```

### 4.4 训练级缓解

| 方法 | 原理 | 效果 |
|------|------|------|
| **RLHF/DPO** | 让模型偏好真实回答 | 减少有害幻觉 |
| **Constitutional AI** | 原则约束 + 自我修正 | 减少不安全幻觉 |
| **知识蒸馏** | 从更准确的模型学习 | 提升小模型事实性 |
| **Factuality Fine-tuning** | 专门针对事实性做 SFT | 直接优化目标 |
| **"I don't know" 训练** | 训练模型说"不知道" | 减少强行编造 |

参见 [[RLHF 全链路]]、[[DPO-TRL实践]] 和 [[SFT 原理]]。

### 4.5 推理级缓解

```python
# 1. 降低温度
response = model.generate(temperature=0.1)  # 更确定性

# 2. 限制输出范围
system_prompt = "Only answer based on the provided documents. " \
                "If unsure, say 'I don't know'."

# 3. Chain-of-Verification (CoVe)
def chain_of_verification(question, initial_answer):
    """生成 → 计划验证 → 执行验证 → 修正"""
    # Step 1: 生成初始回答
    # Step 2: 从回答中提取需要验证的事实
    verification_questions = model.generate(
        f"List verification questions for: {initial_answer}"
    )
    # Step 3: 独立回答每个验证问题
    verified = []
    for vq in verification_questions:
        v_answer = model.generate(vq)  # 独立上下文
        verified.append((vq, v_answer))
    # Step 4: 基于验证结果修正
    final = model.generate(
        f"Original: {initial_answer}\n"
        f"Verification: {verified}\n"
        f"Revise the answer based on verification:"
    )
    return final
```

## 5. 企业级幻觉管理

### 评测框架

```python
class HallucinationEvaluator:
    def __init__(self, knowledge_base, nli_model):
        self.kb = knowledge_base
        self.nli = nli_model
    
    def evaluate(self, question, answer):
        """综合评估幻觉风险"""
        scores = {
            "factuality": self.check_facts(answer),
            "faithfulness": self.check_faithfulness(question, answer),
            "consistency": self.check_consistency(question, answer),
            "groundedness": self.check_grounding(answer)
        }
        
        risk_level = "LOW" if min(scores.values()) > 0.8 else \
                     "MEDIUM" if min(scores.values()) > 0.5 else "HIGH"
        
        return {"scores": scores, "risk": risk_level}
    
    def check_facts(self, answer):
        """原子事实验证"""
        facts = decompose_to_atomic_facts(answer)
        verified = sum(1 for f in facts if self.kb.verify(f))
        return verified / len(facts) if facts else 1.0
    
    def check_consistency(self, question, answer, n=3):
        """采样一致性"""
        samples = [generate(question) for _ in range(n)]
        consistency = compute_bertscore_pairwise(samples)
        return consistency
```

### 防线分层

```
Layer 1: Prompt 工程 — 明确指令、限定范围、要求引用
Layer 2: RAG — 检索外部知识作为事实基础
Layer 3: 后处理 — 事实核查、NLI 验证
Layer 4: 人类审核 — 高风险场景必须人工复核
Layer 5: 监控 — 线上持续监测幻觉率
```

## 6. 面试常见问题

1. **Q: 幻觉的根本原因是什么？**
   A: LLM 的训练目标是 next-token prediction（最大化似然），而非事实正确性。模型优化的是"统计上最可能的续写"，不是"真实正确的内容"。加上训练数据噪声和知识截止，幻觉是 LLM 的固有特性。

2. **Q: RAG 能完全消除幻觉吗？**
   A: 不能。RAG 有三个薄弱环节：(1) 检索质量差（召回不到相关文档）；(2) "Lost in the middle"（模型忽略中间的关键上下文）；(3) 生成时仍可能脱离检索内容。RAG 能显著降低但不能消除幻觉。

3. **Q: 怎么量化幻觉率？**
   A: 常用指标：FActScore（原子事实在知识库中可验证的比例）、NLI-based faithfulness（输出与来源的蕴含关系）、人工标注 hallucination rate。TruLens 提供 RAG 三角评估（Answer Relevance、Context Relevance、Groundedness）。

4. **Q: Self-Consistency 和 RAG 哪个更好？**
   A: 互补而非替代。Self-Consistency 不需要外部知识但成本 ×n；RAG 依赖检索质量但只需一次生成。最佳实践是 RAG + Self-Consistency + 后处理验证的组合。

5. **Q: 实践中怎么平衡幻觉率和用户体验？**
   A: 按场景分级：闲聊/创意 → 高容忍度（高 temperature）；事实查询 → RAG + 引用标注；高风险（医疗/法律）→ RAG + 后处理验证 + 人工审核 + 明确 disclaimer。避免过度保守（每个回答都说"我不确定"会损害可用性）。

## 相关笔记

- [[RAG 工程实践]] — 检索增强生成
- [[Embedding]] — 向量嵌入
- [[Embedding 选型]] — Embedding 模型选型
- [[RLHF 全链路]] — RLHF 训练
- [[SFT 原理]] — 有监督微调
- [[Prompt engineering 概述]] — Prompt 工程基础
- [[Transformer]] — Transformer 架构
