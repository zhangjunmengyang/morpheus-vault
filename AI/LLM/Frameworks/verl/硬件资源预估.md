---
brief: "verl 硬件资源预估——RL 训练 GPU 数量和显存需求的计算方法；模型大小/batch size/序列长度对显存的影响公式；PPO/GRPO 四模型 vs 两模型的资源对比，快速估算集群规模。"
title: "硬件资源预估"
type: concept
domain: ai/llm/frameworks/verl
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm/frameworks/verl
  - type/concept
---
# 硬件资源预估

> 参考：https://verl.readthedocs.io/en/latest/perf/device_tuning.html

## 核心问题

RL 训练比 SFT 吃资源得多 — 因为你需要同时跑 Actor、Critic、Reference Model、Reward Model 四个模型。资源没算对，要么 OOM 要么浪费。

## 显存计算公式

### 单模型显存估算

```
显存 ≈ 参数量 × bytes_per_param × multiplier

其中：
- bf16: bytes_per_param = 2
- multiplier 取决于训练阶段:
  - 推理: 1x (纯权重)
  - 训练: ~4x (权重 + 梯度 + optimizer states)
  - ZeRO-3: ~4x / N_gpus + overhead
```

### 典型场景估算

以 **Qwen2.5-7B** 做 GRPO 训练为例：

```python
# 参数量
params = 7e9

# Actor (训练态, bf16 + AdamW)
# 权重 2B + 梯度 2B + optimizer 8B (fp32 moment) = ~14B per param
actor_mem = params * 14 / 1e9  # ≈ 98 GB

# 用 ZeRO-3 分到 8 卡
actor_per_gpu = 98 / 8  # ≈ 12.3 GB

# Reference Model (推理态)
ref_mem = params * 2 / 1e9  # ≈ 14 GB
# 用 tensor parallel 分到 8 卡
ref_per_gpu = 14 / 8  # ≈ 1.75 GB

# Rollout 阶段还需要 KV cache
# KV cache ≈ 2 × n_layers × d_model × seq_len × batch × 2 bytes
kv_cache = 2 * 32 * 4096 * 2048 * 4 * 2 / 1e9  # ≈ 4.3 GB per GPU
```

**实际经验**：7B 模型做 GRPO，8×A100-80G 勉强够用，留给 KV cache 和 activation 的空间很紧。

## verl 的资源复用策略

verl 的核心设计：**同一组 GPU 在不同阶段扮演不同角色**。

```
Timeline:
├── Phase 1: Rollout (Actor 推理 + Ref 推理)
├── Phase 2: Reward 计算
├── Phase 3: Actor 训练 (梯度更新)
└── Phase 4: Critic 训练 (如果用 PPO)
```

由于各阶段是串行的，verl 会在阶段切换时释放前一阶段的显存（比如卸载 Ref Model），给下一阶段腾空间。这就是 HybridFlow 的精髓。

```python
# verl 配置中的资源映射
resource_pool_mapping = {
    "actor": "pool_0",       # 8 GPUs
    "rollout": "pool_0",     # 复用 actor 的 GPU
    "ref": "pool_0",         # 复用
    "critic": "pool_0",      # 复用
    "reward": "pool_0",      # 复用
}
```

**关键配置**：`offload` 参数决定模型在不活跃时是否卸载到 CPU。开启 offload 省显存但增加切换延迟。

## 不同规模的资源建议

| 模型规模 | 算法 | 最低配置 | 推荐配置 | 说明 |
|----------|------|----------|----------|------|
| 1.5B | GRPO | 2×A100-40G | 4×A100-40G | 小模型入门首选 |
| 7B | GRPO | 8×A100-80G | 8×A100-80G | 主流配置 |
| 7B | PPO | 16×A100-80G | 32×A100-80G | PPO 多个 Critic 很吃资源 |
| 14B | GRPO | 16×A100-80G | 32×A100-80G | 需要 ZeRO-3 + offload |
| 72B | GRPO | 64×A100-80G | 128×H100 | 生产级训练 |

## 调优检查清单

```bash
# 1. 检查显存使用峰值
nvidia-smi --query-gpu=memory.used --format=csv -l 1

# 2. 如果 OOM，按优先级尝试：
# a) 减小 rollout batch size
# b) 减小 max_prompt_length / max_response_length
# c) 开启 gradient_checkpointing
# d) 开启 offload
# e) 增加 ZeRO stage
# f) 加更多 GPU

# 3. 检查 GPU 利用率
watch -n 1 nvidia-smi
# 利用率持续低于 50%? 增大 batch size
```

## 常见陷阱

1. **KV cache 被低估**：长 sequence + 大 batch 时，KV cache 可以占 20-30GB。verl 默认会预分配
2. **GRPO 的 group_size 影响显存**：group_size=8 意味着每个 prompt 生成 8 个 response，显存随之翻倍
3. **Critic 训练意外 OOM**：PPO 的 Critic 更新需要存完整的 value head，容易被忽略
4. **多节点通信开销**：跨节点的 NCCL 通信比单节点慢 5-10x，尽量单节点放下

## 相关

- [[AI/LLM/Frameworks/verl/verl 概述|verl 概述]]
- [[AI/LLM/Frameworks/verl/HybridFlow|HybridFlow]]
- [[AI/LLM/Frameworks/verl/性能调优|性能调优]]
- [[AI/LLM/Frameworks/verl/配置文件|配置文件]]
- [[AI/LLM/Frameworks/verl/grafana 看板|grafana 看板]]
