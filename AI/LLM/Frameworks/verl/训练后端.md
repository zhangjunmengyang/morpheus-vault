---
title: "训练后端"
type: project
domain: ai/llm/frameworks/verl
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm/frameworks/verl
  - type/project
---
# 训练后端

> Megatron-LM 后端：https://verl.readthedocs.io/en/latest/workers/megatron_workers.html
> SGLang 后端：https://verl.readthedocs.io/en/latest/workers/sglang_worker.html

## 两个维度的后端

verl 在两个层面有"后端"选择：

1. **训练后端**：Actor/Critic 做梯度更新用什么框架 → FSDP / Megatron-LM
2. **推理后端**：Rollout 生成用什么引擎 → SGLang / vLLM

```
┌──────────────────────────────────────────┐
│                 verl                       │
│                                            │
│  Training Backend    Inference Backend     │
│  ┌───────────────┐  ┌──────────────────┐  │
│  │ FSDP (默认)    │  │ SGLang (推荐)    │  │
│  │ Megatron-LM   │  │ vLLM             │  │
│  └───────────────┘  └──────────────────┘  │
└──────────────────────────────────────────┘
```

## 训练后端对比

### FSDP (Fully Sharded Data Parallel)

PyTorch 原生的分布式训练方案，等同于 DeepSpeed ZeRO-3。

```python
# FSDP 配置
fsdp_config:
  sharding_strategy: "FULL_SHARD"  # ZeRO-3
  # SHARD_GRAD_OP = ZeRO-2
  # NO_SHARD = DDP
  
  cpu_offload: false
  mixed_precision: "bf16"
  
  # 自动 wrap 策略
  auto_wrap_policy: "transformer_layer"
  min_num_params: 1e6
```

**优点**：
- PyTorch 原生，不需要额外依赖
- 配置简单，开箱即用
- 支持 HuggingFace 模型无缝接入
- 社区维护好，bug 少

**缺点**：
- 不支持 Tensor Parallel（只有 Data Parallel）
- 超大模型（70B+）性能不如 Megatron
- 通信效率在大规模集群上有瓶颈

### Megatron-LM

NVIDIA 的分布式训练框架，支持 3D 并行（DP + TP + PP）。

```python
# Megatron 配置
megatron_config:
  tensor_parallel_size: 4     # TP: 模型张量切分到 4 张卡
  pipeline_parallel_size: 2   # PP: 模型层切分到 2 组
  # Data Parallel 自动计算: total_gpus / (TP * PP)
  
  sequence_parallel: true     # 序列并行，省激活显存
  use_flash_attn: true
```

**优点**：
- 3D 并行，大模型训练必备
- 通信效率高（TP 用 NVLink，PP 用 IB）
- 72B 以上模型几乎是唯一选择

**缺点**：
- 需要将 HuggingFace 模型转为 Megatron 格式
- 配置复杂，调试困难
- 模型兼容性有限（不是所有架构都支持）

### 选择建议

| 模型规模 | 推荐后端 | 理由 |
|----------|---------|------|
| ≤ 7B | FSDP | 简单够用 |
| 7B-14B | FSDP 或 Megatron | 看团队熟悉度 |
| 14B-72B | Megatron | 需要 TP |
| > 72B | Megatron | 必须 3D 并行 |

## 推理后端对比

### SGLang

```yaml
rollout:
  name: "sglang"
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.85
  
  # SGLang 特色
  enable_prefix_caching: true  # 多轮场景性能翻倍
  chunked_prefill: true
```

**核心优势**：
- **RadixAttention**：自动 prefix caching，多轮/共享前缀场景效果拔群
- **Chunked Prefill**：prefill 和 decode 可以 overlap
- 吞吐量通常比 vLLM 高 20-30%

### vLLM

```yaml
rollout:
  name: "vllm"
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.85
  
  # vLLM 参数
  enable_prefix_caching: false  # vLLM 也支持但默认关
  max_num_seqs: 256
```

**核心优势**：
- 生态成熟，模型支持最广
- PagedAttention 内存管理高效
- 文档和社区更完善

### 推理引擎选择

```python
# 性能测试经验 (Qwen2.5-7B, 8×A100):
#
# SGLang:
#   - 单轮 throughput: ~2500 tokens/s
#   - 多轮 throughput: ~3200 tokens/s (prefix cache 命中)
#   - 首 token 延迟: ~45ms
#
# vLLM:
#   - 单轮 throughput: ~2100 tokens/s
#   - 多轮 throughput: ~2300 tokens/s
#   - 首 token 延迟: ~55ms
#
# 结论: SGLang 在 verl 场景下更优
#       (GRPO group 内 prompt 相同，prefix cache 天然命中率高)
```

## Megatron 模型格式转换

用 Megatron 后端需要先做格式转换：

```bash
# HuggingFace → Megatron
python tools/convert_hf_to_megatron.py \
  --hf-model-path /path/to/qwen2.5-7b \
  --megatron-path /path/to/megatron_ckpt \
  --tp-size 4 \
  --pp-size 2

# 训练完后 Megatron → HuggingFace (用于推理部署)
python tools/convert_megatron_to_hf.py \
  --megatron-path /path/to/megatron_ckpt \
  --hf-model-path /path/to/output_hf \
  --tp-size 4 \
  --pp-size 2
```

## 混合使用

verl 的一个巧妙设计：训练和推理可以用不同后端。

```yaml
# 训练用 FSDP (简单)，推理用 SGLang (快)
actor_rollout_ref:
  actor:
    backend: "fsdp"
  rollout:
    name: "sglang"
```

这样既能享受 FSDP 的简单配置，又能用上 SGLang 的高性能推理。权重在阶段切换时自动同步。

## 相关

- [[AI/LLM/Frameworks/verl/verl 概述|verl 概述]]
- [[AI/LLM/Frameworks/verl/HybridFlow|HybridFlow]]
- [[AI/LLM/Frameworks/verl/性能调优|性能调优]]
- [[AI/LLM/Frameworks/verl/硬件资源预估|硬件资源预估]]
- [[AI/LLM/Frameworks/verl/verl 训练参数|verl 训练参数]]
- [[AI/LLM/Frameworks/verl/配置文件|配置文件]]
