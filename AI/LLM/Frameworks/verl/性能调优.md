---
title: "性能调优"
type: concept
domain: ai/llm/frameworks/verl
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm/frameworks/verl
  - type/concept
---
# 性能调优

> verl RL 训练的性能瓶颈分析和优化手段。RL 训练比 SFT 慢很多，优化空间也大。

## RL 训练的时间分布

一个典型的 PPO/GRPO step 的时间分布：

```
Rollout (生成)     40-60%  ← 最大瓶颈
Reward 计算        5-15%
Critic Forward     5-10%  (PPO only)
GAE 计算           < 1%
Actor Update       15-25%
Critic Update      5-10%  (PPO only)
数据传输/同步       5-10%
```

**Rollout 是绝对瓶颈**。Autoregressive 生成天然慢（memory-bound，每次只产一个 token）。

## Rollout 优化

### 1. 使用高效推理引擎

```yaml
# verl 支持 vLLM 和 SGLang 作为 rollout 引擎
# SGLang 通常更快（RadixAttention + continuous batching）
rollout:
  engine: sglang  # 推荐
  # engine: vllm
  
  # 关键参数
  tensor_parallel_size: 4
  max_num_seqs: 256      # 并发序列数
  max_model_len: 4096    # 最大序列长度
```

### 2. 调整生成参数

```yaml
# 减少 max_new_tokens 可以显著加速
# 但要确保模型能在限定长度内完成任务
generation:
  max_new_tokens: 1024   # 从 2048 降到 1024 → 速度翻倍
  temperature: 0.7
  top_p: 0.9
  
  # GRPO 的 group size 也影响速度
  # group_size=8 意味着每个 prompt 生成 8 条 response
  # 减少 group_size 可以加速但可能影响 advantage 估计质量
```

### 3. Rollout 与 Training 的 TP 分离

```yaml
# Rollout 和 Training 可以用不同的 TP size
# Rollout 用小 TP → 更多并发
# Training 用大 TP → 更大 batch size

# 例：8 张卡
# Rollout: TP=2, 4 组并行推理
# Training: TP=8, 1 组但更大的 per-GPU batch
```

## 显存优化

### Offload 策略

```yaml
# 训练阶段不需要 KV cache → offload
# Rollout 阶段不需要 optimizer states → offload

actor:
  offload_optimizer_states: true   # rollout 时 offload optimizer
  offload_param: false             # 参数不建议 offload（太慢）
```

### Gradient Checkpointing

```yaml
# 用时间换空间
actor:
  gradient_checkpointing: true
  # 显存节省约 30-50%
  # 训练速度下降约 20-30%
```

### Mixed Precision

```yaml
# BF16 训练是标配
training:
  bf16: true
  # FP16 对 RL 不太友好 — loss scale 容易出问题
  # FP32 太慢太费显存
```

## Batch Size 调优

```yaml
# RL 训练的 batch size 有多层含义：
# 1. rollout_batch_size: 一次生成多少条 response
# 2. train_micro_batch_size: 一次 forward/backward 处理多少条
# 3. train_batch_size: 一次参数更新用多少条（gradient accumulation）

# 经验值（7B 模型，8xA100-80GB）
rollout:
  batch_size: 256          # 尽可能大，受推理引擎限制
training:
  micro_batch_size: 8      # 受显存限制
  batch_size: 256          # = micro_batch_size × grad_accum_steps
  num_epochs: 1            # PPO 通常 1-4 epochs per batch
```

## 数据管道优化

```python
# 1. 数据预处理放在 CPU 上异步执行
# 2. Tokenization 提前做好，不要在训练循环中做
# 3. Padding 策略：pack sequences 减少浪费

# Pack sequences 示例
# 传统: [tokens] [pad] [pad] [pad]  ← 75% 浪费
#        [tokens] [tokens] [pad] [pad]
# Pack:  [tokens_1] [tokens_2] [tokens_3]  ← 紧密排列
#        [tokens_4] [tokens_5]
```

## Profiling 工具

```bash
# 1. verl 内置的 timing log
# 每个 step 会输出各阶段的耗时

# 2. NVIDIA Nsight Systems
nsys profile -o verl_trace python train.py
# 查看 GPU kernel 执行时间和空闲时间

# 3. PyTorch Profiler
# 在 verl 的训练循环中手动加
with torch.profiler.profile(
    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]
) as prof:
    trainer.step()
prof.export_chrome_trace("trace.json")
```

## 常见性能问题

| 现象 | 可能原因 | 解决方案 |
|------|---------|---------|
| GPU 利用率低 | Rollout 瓶颈 | 增大并发、用 SGLang |
| OOM | Batch size 太大 | 减小 micro_batch_size |
| 训练速度不稳定 | 生成长度波动 | 设置 max_new_tokens |
| 多节点慢 | 网络带宽 | 检查 IB/RoCE 配置 |
| Reward 计算慢 | RM 太大 | 用规则 reward 或小 RM |

## 相关

- [[AI/LLM/Frameworks/verl/HybridFlow|HybridFlow]] — 理解资源编排
- [[AI/LLM/Frameworks/verl/硬件资源预估|硬件资源预估]] — 资源规划
- [[AI/LLM/Frameworks/verl/verl 训练参数|verl 训练参数]] — 配置详解
- [[AI/LLM/Frameworks/verl/grafana 看板|grafana 看板]] — 监控训练过程
- [[AI/LLM/Infra/分布式训练|分布式训练]] — 并行策略基础
