---
title: "LLM 预训练与分布式训练 2026 技术全景"
brief: "LLM 预训练全链路深度笔记：从数据工程（清洗/去重/配比/课程学习）到目标函数（CLM/MLM/FIM/UL2），从分布式训练五大范式（DP/TP/PP/SP/EP）到 ZeRO/FSDP 显存优化，再到训练稳定性、MoE 训练、长上下文扩展和 SFT 工程实践；Chinchilla Scaling Law（arXiv:2203.15556）和 Megatron-LM（arXiv:1909.08053）是理解现代 LLM 训练的两大基石"
type: survey
domain: ai/llm/pretraining
created: "2026-02-20"
updated: "2026-02-22"
tags:
  - ai/llm
  - topic/pretraining
  - topic/distributed-training
  - topic/interview-prep
  - type/survey
status: complete
sources:
  - "GPT-3: Language Models are Few-Shot Learners arXiv:2005.14165 (Brown et al., 2020)"
  - "Chinchilla Scaling Laws: Training Compute-Optimal Large Language Models arXiv:2203.15556 (Hoffmann et al., 2022)"
  - "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism arXiv:1909.08053 (Shoeybi et al., 2019)"
  - "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models arXiv:1910.02054 (Rajbhandari et al., 2019)"
  - "FSDP: PyTorch Fully Sharded Data Parallel https://pytorch.org/docs/stable/fsdp.html"
  - "Switch Transformers arXiv:2101.03961 (Fedus et al., 2021)"
  - "RoFormer/RoPE arXiv:2104.09864 (Su et al., 2021)"
  - "YaRN arXiv:2309.00071 (Peng et al., 2023)"
  - "LIMA arXiv:2305.11206 (Zhou et al., 2023)"
related:
  - "[[AI/LLM/Pretraining/预训练原理|预训练原理]]"
  - "[[AI/LLM/Infra/模型并行策略|模型并行策略]]"
  - "[[AI/LLM/Pretraining/LLM-数据工程-2026-技术全景|数据工程 for LLM]]"
  - "[[AI/LLM/Infra/分布式训练|分布式训练]]"
  - "[[AI/LLM/SFT/EWC-LoRA-Continual-Learning-Low-Rank|EWC-LoRA]]"
  - "[[AI/LLM/Architecture/MoE 深度解析|MoE 深度解析]]"
  - "[[AI/LLM/Architecture/Transformer架构深度解析-2026技术全景|Transformer 架构全景]]"
  - "[[AI/LLM/SFT/SFT 原理|SFT 原理]]"
---

# LLM 预训练与分布式训练 2026 技术全景

> **Brief**：覆盖预训练全链路——从数据工程（清洗/去重/配比/课程学习）到分布式训练（DP/TP/PP/SP/EP + ZeRO/FSDP），从训练稳定性到 MoE 训练、长上下文扩展和 SFT 工程实践。每个主题包含技术原理、关键公式/伪代码、工程经验和面试考点。
>
> 核心来源：GPT-3 arXiv:2005.14165; Chinchilla arXiv:2203.15556; Megatron-LM arXiv:1909.08053; DeepSpeed ZeRO arXiv:1910.02054
>
> 最后更新：2026-02-22

---

## 目录

1. [预训练数据工程](#1-预训练数据工程)
2. [预训练目标函数演进](#2-预训练目标函数演进)
3. [分布式训练范式](#3-分布式训练范式)
4. [混合并行策略](#4-混合并行策略)
5. [训练稳定性](#5-训练稳定性)
6. [MoE 训练特殊挑战](#6-moe-训练特殊挑战)
7. [长上下文训练](#7-长上下文训练)
8. [SFT 工程实践](#8-sft-工程实践)
9. [2026 前沿趋势](#9-2026-前沿趋势)
10. [面试题与参考答案](#10-面试题与参考答案)

---

## 1. 预训练数据工程

### 1.1 数据清洗 (Data Cleaning)

#### 技术原理

预训练数据清洗是 LLM 质量的第一道关卡。原始网络数据（Common Crawl 等）中充斥着 HTML 残留、导航栏噪声、广告文本、乱码、色情/暴力内容、个人隐私信息等。清洗的核心目标是：**保留高质量自然语言文本，去除一切噪声**。

典型清洗 pipeline：

```
Raw HTML → 文本提取(trafilatura/resiliparse) → 语言检测(fastText lid) 
→ 规则过滤(长度/特殊字符/重复行) → 质量评分(perplexity/classifier) 
→ 安全过滤(toxicity/PII) → 最终语料
```

#### 关键技术细节

**1. 文本提取**
- `trafilatura`：基于启发式 + 机器学习的正文提取，对新闻/博客效果好
- `resiliparse`（Chatnoir）：更快、更轻量，适合大规模处理
- 关键指标：正文召回率 vs 噪声精度的 trade-off

**2. 语言检测**
- fastText lid.176.bin：176 种语言，单行推理 < 0.1ms
- 阈值设置：通常 confidence > 0.65 保留，太高会丢方言/混合语言文本
- 中文特殊处理：需要额外区分简体/繁体，以及中英混排文本

**3. 规则过滤（Heuristic Filters）**

```python
def heuristic_filter(doc: str) -> bool:
    """基于 C4/RefinedWeb/Dolma 的典型规则"""
    lines = doc.split('\n')
    words = doc.split()
    
    # 长度过滤
    if len(words) < 50 or len(words) > 100000:
        return False
    
    # 特殊字符比例
    alpha_ratio = sum(c.isalpha() for c in doc) / max(len(doc), 1)
    if alpha_ratio < 0.6:  # 中文需调整此阈值
        return False
    
    # 重复行比例（boilerplate 检测）
    unique_lines = set(lines)
    if len(unique_lines) / max(len(lines), 1) < 0.3:
        return False
    
    # 脏词密度
    dirty_word_count = count_dirty_words(doc)
    if dirty_word_count / len(words) > 0.01:
        return False
    
    # 句末标点比例（自然文本通常 > 0.1）
    end_punct_lines = sum(1 for l in lines if l.strip() and l.strip()[-1] in '.!?。！？')
    if end_punct_lines / max(len(lines), 1) < 0.05:
        return False
    
    return True
```

**4. PII 过滤**
- 正则匹配：邮箱、电话、身份证号、信用卡号
- NER 模型：人名、地址等更模糊的 PII
- 处理策略：替换为 `[EMAIL]`、`[PHONE]` 等 placeholder，而非直接删除（保持文本连贯性）

#### 工程经验

- **清洗顺序很重要**：先做廉价的规则过滤（速度快，能去掉 60-70% 的垃圾），再做昂贵的模型过滤
- **中文数据的特殊坑**：分词边界不明确，不能简单用空格 split；需要用 jieba 或字符级统计
- **不要过度清洗**：过于激进的过滤会引入偏见（如只留正式书面语，丢失口语/方言多样性）
- **清洗代码要版本管理**：数据质量的可复现性和模型代码一样重要

#### 面试考点

> Q: 为什么不能直接用 perplexity 做唯一的质量过滤？
> A: Perplexity 用参考模型计算，而参考模型本身有偏见——它倾向于认为"看过的"数据质量高。代码、表格、非英语文本的 perplexity 天然偏高，但不代表质量低。需要和规则过滤、分类器多路投票结合。

---

### 1.2 数据去重 (Deduplication)

#### 技术原理

去重是预训练数据工程中 ROI 最高的操作。研究表明，重复数据会导致：
- 模型过拟合到重复模式，降低泛化能力
- 训练效率下降（等效 token 减少）
- 特定文本被过度记忆（隐私泄露风险增加）

去重分为三个层次：

| 层次 | 方法 | 粒度 | 典型工具 |
|------|------|------|----------|
| 精确去重 | URL/hash 去重 | 文档级 | SHA-256 / MD5 |
| 近似去重 | MinHash + LSH | 文档级 | datasketch / text-dedup |
| 子串去重 | Suffix Array | 段落/句子级 | deduplicate-text-datasets |

#### MinHash LSH 去重详解

```python
# MinHash 的核心思想：
# 两个集合的 Jaccard 相似度 ≈ 它们 MinHash 签名的碰撞概率

def minhash_dedup(documents, num_perm=128, threshold=0.8):
    """
    MinHash + LSH 近似去重
    - num_perm: MinHash 排列数，越大越精确，计算越贵
    - threshold: Jaccard 相似度阈值
    """
    from datasketch import MinHash, MinHashLSH
    
    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)
    
    for doc_id, doc in enumerate(documents):
        # 构建 n-gram 集合（通常用 5-gram 或 13-gram）
        ngrams = set(get_ngrams(doc, n=5))
        
        mh = MinHash(num_perm=num_perm)
        for ngram in ngrams:
            mh.update(ngram.encode('utf-8'))
        
        # 查询是否有近似重复
        result = lsh.query(mh)
        if not result:
            lsh.insert(doc_id, mh)
        else:
            mark_as_duplicate(doc_id)
```

**LSH 参数选择**：
- `b` bands × `r` rows = `num_perm`
- 阈值近似为 `(1/b)^(1/r)`
- 例如 threshold=0.8, num_perm=128 → b=16, r=8

#### Suffix Array 子串去重

Lee et al. (2022) "Deduplicating Training Data Makes Language Models Better" 的方法：

1. 对整个语料建 Suffix Array
2. 找所有重复出现的 substring（长度 > 50 tokens）
3. 保留第一次出现，后续出现全部删除

这比文档级去重更彻底，能捕获：
- 跨文档的 copy-paste（如法律条款、新闻模板）
- 相同段落在不同文章中出现

代价是计算量大（O(n log n) 构建 + O(n) 扫描），但一次性投入值得。

#### 工程经验

- **去重比例**：Common Crawl 原始数据，MinHash 去重通常能去掉 30-50%，加上子串去重可达 60%+
- **先去重再清洗**：去重后数据量大幅减少，后续清洗成本降低
- **跨 snapshot 去重**：如果使用多个 Common Crawl snapshot，跨 snapshot 去重很重要
- **中文去重陷阱**：中文无空格分隔，n-gram 需要用字符级或分词后的 token 级
- **RefinedWeb 的经验**：他们发现 MinHash threshold=0.7 是比较好的平衡点（太高会遗漏近似重复，太低会误杀合理的相似文档）

#### 面试考点

> Q: MinHash 的时间复杂度是多少？为什么用 LSH 而不是直接两两比较？
> A: 直接两两比较是 O(n²)，n 为文档数。LSH 将签名哈希到桶中，期望时间复杂度降到近似 O(n)（每个文档只需和同桶文档比较）。MinHash 单个文档的签名计算是 O(|S| × num_perm)，其中 |S| 是 n-gram 集合大小。

---

### 1.3 质量过滤 (Quality Filtering)

#### 技术原理

质量过滤的核心问题是：**什么是"高质量"文本？** 不同的定义导致不同的过滤策略。

**方法 1: Perplexity-based Filtering（C4 风格）**

用一个在高质量语料（如 Wikipedia）上训练的 language model 计算文本 perplexity：

$$PPL(x) = \exp\left(-\frac{1}{N}\sum_{i=1}^{N}\log p(x_i|x_{<i})\right)$$

PPL 越低，文本越"像"高质量语料。但存在明显偏见（见上面的面试考点）。

**方法 2: Classifier-based Filtering（GPT-3/LLaMA 风格）**

训练一个二分类器，正例为 Wikipedia/书籍，负例为随机网络文本：

```python
# 典型实现：用 fastText 训练质量分类器
# 正例: Wikipedia paragraphs
# 负例: random Common Crawl samples

import fasttext

model = fasttext.train_supervised(
    input='quality_train.txt',  # __label__hq / __label__lq
    lr=0.1,
    epoch=5,
    wordNgrams=2,
    dim=256
)

def quality_score(text):
    labels, probs = model.predict(text.replace('\n', ' '))
    if labels[0] == '__label__hq':
        return probs[0]
    else:
        return 1 - probs[0]
```

**方法 3: LLM-as-Judge（2024-2026 趋势）**

用 GPT-4 / Claude 等强模型对小批量数据打分，训练蒸馏模型：

1. 用强模型对 10K-50K 样本打 1-5 质量分
2. 训练小模型（如 DeBERTa-large）做回归
3. 用小模型对全量数据打分

FineWeb-Edu 就是这种方法的成功案例——筛选教育相关的高质量内容。

#### 工程经验

- **多维度质量**：不存在单一的"质量"维度。信息密度、教育价值、写作质量、事实准确性都是不同的质量维度
- **阈值不是越高越好**：过高阈值会让数据同质化，损害模型的多样性和鲁棒性
- **领域特定过滤**：代码、数学、科学论文需要专门的过滤器（通用质量过滤器会误杀它们）
- **FineWeb 的教训**：他们发现去重 + 质量过滤的顺序对最终模型影响很大

---

### 1.4 数据配比 (Data Mixture)

#### 技术原理

预训练数据由多个领域/来源组成，配比直接影响模型能力分布。

```
典型配比（以 LLaMA 系列为参考）:
├── Web Text:       ~67%    (Common Crawl 过滤后)
├── Code:           ~8-15%  (GitHub/StackOverflow)  
├── Wikipedia:      ~4-5%   (多语言)
├── Books:          ~5-8%   (BookCorpus/PG/etc.)
├── Academic:       ~3-5%   (ArXiv/S2ORC)
├── Math:           ~2-4%   (GSM8K-style/MathPile)
├── Conversation:   ~2-3%   (Forum/Reddit)
└── Multilingual:   ~5-10%  (非英语网页)
```

#### 配比优化方法

**方法 1: DoReMi (Domain Reweighting with Miniature proxy Models)**

Google 2023 提出的自动配比方法：

1. 在小 proxy model 上训练多个单领域模型
2. 用 distributionally robust optimization (DRO) 找最优混合权重
3. 将权重应用到大模型训练

核心公式（Group DRO）：

$$\min_{\theta} \max_{\alpha \in \Delta_k} \sum_{i=1}^{k} \alpha_i \cdot L_i(\theta)$$

其中 $\alpha_i$ 是领域 $i$ 的权重，$L_i(\theta)$ 是领域 $i$ 的 loss，$\Delta_k$ 是 k-simplex。

**方法 2: Skill-based Mixture（2025+ 趋势）**

不按数据来源分，而是按"技能"分：

```python
skills = {
    'reasoning': {'weight': 0.25, 'sources': ['math', 'code', 'logic_puzzles']},
    'knowledge': {'weight': 0.30, 'sources': ['wiki', 'academic', 'encyclopedia']},
    'language':  {'weight': 0.20, 'sources': ['books', 'web_quality']},
    'coding':    {'weight': 0.15, 'sources': ['github', 'stackoverflow']},
    'multilingual': {'weight': 0.10, 'sources': ['cc_multilingual']},
}
```

**方法 3: 动态配比**

训练过程中根据各领域 loss 变化动态调整权重。如果某个领域 loss 下降缓慢，增加其比例。

#### 工程经验

- **代码比例是关键杠杆**：代码数据对推理能力有显著提升（代码的结构化和逻辑性有正向迁移）
- **重复采样高质量数据**：Wikipedia 等高质量数据可以重复 2-4 次（epochs），但过多重复会过拟合
- **数据比例 ≠ token 比例**：不同领域的平均文档长度差异很大，需要按 token 计算而非按文档数
- **Warmup 阶段的配比可以不同**：开始用更多高质量数据加速收敛，后期增加多样性

---

### 1.5 课程学习 (Curriculum Learning)

#### 技术原理

课程学习的核心假设：**模型应该先学简单的模式，再学复杂的模式**。这模拟了人类的学习过程。

在预训练中，课程学习主要体现在：

**1. 数据质量课程**

```
Phase 1 (0-20% tokens):  高质量数据为主 (wiki + books)
Phase 2 (20-60% tokens): 混合数据 (web + code + academic)
Phase 3 (60-90% tokens): 全量数据（加入更多多样性）
Phase 4 (90-100% tokens): 退火阶段 (annealing)，回到高质量数据
```

**2. 序列长度课程**

```python
def get_seq_length(step, total_steps):
    """渐进式增加序列长度"""
    if step < total_steps * 0.1:
        return 2048
    elif step < total_steps * 0.3:
        return 4096
    elif step < total_steps * 0.6:
        return 8192
    else:
        return max_seq_length  # 如 32768 或更长
```

这样做的好处：
- 短序列阶段 throughput 更高（batch size 可以更大）
- 模型先学局部模式，再学长程依赖
- 避免训练初期在长序列上浪费计算

**3. 难度课程**

按文本复杂度排序：简单文本（儿童读物/简单新闻）→ 中等（一般文章）→ 困难（学术论文/复杂代码）

**4. MiniCPM 的退火策略 (2024)**

MiniCPM 在训练末期进行 "annealing"：
- 最后 ~10% 的 steps 中，学习率从当前值 cosine decay 到 0
- 同时将数据配比调整为更高质量的子集
- 效果：在小模型上显著提升 benchmark 分数

#### 工程经验

- **课程学习的收益在大规模训练中更明显**：小规模实验可能看不到显著差异
- **退火阶段 (annealing) 是最实用的课程学习形式**：实现简单，效果确定
- **数据顺序的随机性很重要**：不能完全按课程排序，需要在每个阶段内保持随机性
- **记录每个阶段的数据配比**：方便后续分析和复现

#### 面试考点

> Q: 课程学习和数据配比的区别是什么？
> A: 数据配比是"全局静态"的——决定各来源的整体比例。课程学习是"时间动态"的——决定不同训练阶段使用什么数据。两者可以正交组合：在每个课程阶段内，仍然有一个配比策略。

---

## 2. 预训练目标函数演进

### 2.1 Causal Language Modeling (CLM)

#### 技术原理

CLM 是 GPT 系列的基础，也是当前主流预训练目标：

> 来源：GPT-3 arXiv:2005.14165 (Brown et al., 2020) 将 CLM 扩展到 175B 参数并验证了 few-shot prompting 范式

$$\mathcal{L}_{\text{CLM}} = -\sum_{t=1}^{T} \log p(x_t | x_1, \ldots, x_{t-1}; \theta)$$

每个 token 只能看到它左边的 context（因果注意力掩码）。

```python
# Causal attention mask
def causal_mask(seq_len):
    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)
    return mask.masked_fill(mask == 1, float('-inf'))

# CLM Loss
def clm_loss(logits, labels):
    """
    logits: [batch, seq_len, vocab_size]
    labels: [batch, seq_len]  # 即 input_ids shifted right
    """
    shift_logits = logits[..., :-1, :].contiguous()
    shift_labels = labels[..., 1:].contiguous()
    loss = F.cross_entropy(
        shift_logits.view(-1, vocab_size),
        shift_labels.view(-1),
        ignore_index=-100
    )
    return loss
```

**优点**：
- 实现简单，计算效率高（每个 token 都参与 loss 计算）
- 天然适合生成任务
- 可以无缝用于 few-shot prompting

**缺点**：
- 单向注意力，无法利用右侧 context
- 对理解类任务（如 NLU）不如双向模型

---

### 2.2 Masked Language Modeling (MLM)

#### 技术原理

BERT 的核心目标，随机遮蔽 15% 的 token 并预测：

$$\mathcal{L}_{\text{MLM}} = -\sum_{i \in \mathcal{M}} \log p(x_i | x_{\backslash\mathcal{M}}; \theta)$$

其中 $\mathcal{M}$ 是被遮蔽的 token 集合。

经典的 15% 遮蔽策略：
- 80% 替换为 `[MASK]`
- 10% 替换为随机 token
- 10% 保持不变

**为什么这样设计？** `[MASK]` 是训练时的特殊 token，推理时不存在。10% 保持不变和 10% 随机替换是为了缓解这种 train-test mismatch。

**局限**：
- 只有 15% 的 token 参与 loss，训练效率低
- 遮蔽 token 之间假设条件独立（实际不是）
- 不适合生成任务

---

### 2.3 Prefix Language Modeling (PrefixLM)

#### 技术原理

PrefixLM 是 CLM 和 MLM 的折中方案（T5、GLM 等使用）：

```
[prefix tokens: bidirectional attention] [target tokens: causal attention]
```

前缀部分使用双向注意力（可以看到前后 context），目标部分使用因果注意力（只能看到前缀 + 已生成的 token）。

```python
def prefix_lm_mask(prefix_len, total_len):
    """
    Prefix tokens: fully visible to each other
    Target tokens: causal (can see prefix + previous targets)
    """
    mask = torch.zeros(total_len, total_len)
    
    # Target tokens 之间是 causal
    for i in range(prefix_len, total_len):
        for j in range(i + 1, total_len):
            mask[i][j] = float('-inf')
    
    return mask
```

**优点**：对条件生成任务（如翻译、摘要）特别合适——encoding 阶段用双向注意力理解输入，decoding 阶段用因果注意力生成输出。

---

### 2.4 Fill-in-the-Middle (FIM)

#### 技术原理

FIM 是专门为代码补全设计的训练目标（OpenAI, 2022）。核心思想：不只是从左到右生成，还要能在给定上下文的中间位置插入内容。

```
原始序列: A B C D E F
FIM 变换: [prefix] A B [suffix] E F [middle] C D
```

```python
def fim_transform(tokens, fim_rate=0.5, mode='PSM'):
    """
    Fill-in-the-Middle 数据变换
    mode: 'PSM' (prefix-suffix-middle) 或 'SPM' (suffix-prefix-middle)
    """
    if random.random() > fim_rate:
        return tokens  # 50% 的样本不做 FIM，保持正常 CLM
    
    # 随机选择分割点
    split1 = random.randint(0, len(tokens))
    split2 = random.randint(split1, len(tokens))
    
    prefix = tokens[:split1]
    middle = tokens[split1:split2]
    suffix = tokens[split2:]
    
    if mode == 'PSM':
        return [FIM_PREFIX] + prefix + [FIM_SUFFIX] + suffix + [FIM_MIDDLE] + middle
    else:  # SPM
        return [FIM_SUFFIX] + suffix + [FIM_PREFIX] + prefix + [FIM_MIDDLE] + middle
```

**关键发现**：
- FIM 不会降低左到右生成的能力（"免费午餐"）
- 50% FIM rate 是较好的平衡点
- SPM 模式在某些场景下比 PSM 效果好（因为 suffix 作为"提示"放在最前面）

---

### 2.5 UL2 (Unifying Language Learning Paradigms)

#### 技术原理

Google 2023 提出的统一目标函数，结合了 CLM、MLM 和 PrefixLM 的优点：

```
UL2 定义了三种 "denoiser":
├── R-denoiser (Regular): 类似 BERT 的短跨度遮蔽（μ=3, r=0.15）
├── S-denoiser (Sequential): 类似 PrefixLM，遮蔽序列后半部分
└── X-denoiser (Extreme): 长跨度遮蔽（μ=32, r=0.5），需要更强的推理
```

每个样本随机选择一种 denoiser，并在输入前加入模式标记 `[R]`、`[S]`、`[X]`：

```python
def ul2_sample(tokens):
    mode = random.choice(['R', 'S', 'X'])
    
    if mode == 'R':
        # Regular: 短 span corruption (T5 style)
        corrupted, targets = span_corruption(tokens, mean_span=3, corrupt_rate=0.15)
    elif mode == 'S':
        # Sequential: prefix → generate suffix
        split = random.randint(len(tokens)//4, 3*len(tokens)//4)
        corrupted = tokens[:split]
        targets = tokens[split:]
    else:
        # Extreme: 长 span corruption
        corrupted, targets = span_corruption(tokens, mean_span=32, corrupt_rate=0.5)
    
    return [mode_token(mode)] + corrupted, targets
```

**UL2 的价值**：证明了一个模型可以同时学习多种语言建模目标，不同的 denoiser 互补而非冲突。

---

### 2.6 目标函数对比总结

| 目标 | 注意力 | 训练效率 | 生成能力 | 理解能力 | 代表模型 |
|------|--------|---------|---------|---------|---------|
| CLM | 因果（单向） | 高（100% tokens） | ★★★★★ | ★★★ | GPT, LLaMA |
| MLM | 双向 | 低（15% tokens） | ★★ | ★★★★★ | BERT |
| PrefixLM | 混合 | 中 | ★★★★ | ★★★★ | T5, GLM |
| FIM | 因果 + 重排 | 高 | ★★★★★ | ★★★ | Codex, StarCoder |
| UL2 | 混合 | 中 | ★★★★ | ★★★★★ | PaLM-2 |

#### 面试考点

> Q: 为什么 2024-2026 的主流大模型几乎都用 CLM 而不是 MLM？
> A: 三个原因：(1) CLM 的训练效率更高——每个 token 都参与 loss，而 MLM 只有 15%；(2) CLM 天然支持自回归生成，而 MLM 需要额外的 decoding 策略；(3) Scaling Laws 实验表明，在同等计算预算下，CLM 的 downstream 性能更好。GPT-3 证明了 CLM + few-shot prompting 可以替代 MLM + fine-tuning 的范式。

---

## 3. 分布式训练范式

### 3.1 数据并行 (Data Parallelism, DP)

#### 技术原理

最基础的分布式策略：每张卡持有完整的模型副本，数据按 batch 切分。

```
GPU 0: Model copy + Data batch 0 → Gradient 0 ─┐
GPU 1: Model copy + Data batch 1 → Gradient 1 ──┤── AllReduce → 平均梯度 → 更新
GPU 2: Model copy + Data batch 2 → Gradient 2 ──┤
GPU 3: Model copy + Data batch 3 → Gradient 3 ─┘
```

**AllReduce 实现**：Ring AllReduce 是最常用的算法：

$$\text{通信量} = 2 \times (N-1)/N \times M$$

其中 $N$ 是 GPU 数量，$M$ 是模型参数量（字节）。

```python
# PyTorch DDP 基本用法
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

model = MyModel().to(rank)
model = DDP(model, device_ids=[rank])

# 梯度同步在 backward() 中自动完成
loss = model(inputs).loss
loss.backward()  # 内部触发 AllReduce
optimizer.step()
```

**PyTorch DDP 的优化技巧**：
- **Gradient Bucketing**：将小梯度打包成大桶进行 AllReduce，提升通信效率
- **Computation-Communication Overlap**：反向传播时，已计算完的梯度立即开始 AllReduce，不必等全部计算完

#### 局限

- 每张卡必须放下完整模型 → 模型规模受单卡显存限制
- 7B 模型 FP16 需要 ~14GB 参数 + ~14GB 梯度 + ~56GB 优化器状态 = ~84GB（超过 A100 80GB）

---

### 3.2 张量并行 (Tensor Parallelism, TP)

#### 技术原理

Megatron-LM 提出的方法：将单个层内的矩阵运算切分到多张卡上。

> 来源：Megatron-LM arXiv:1909.08053 (Shoeybi et al., NVIDIA, 2019)

**MLP 的张量并行**：

$$Y = \text{GeLU}(XA) \cdot B$$

将 $A$ 按列切分，$B$ 按行切分：

```
GPU 0: Y_0 = GeLU(X @ A_0) @ B_0 ─┐
GPU 1: Y_1 = GeLU(X @ A_1) @ B_1 ──┤── AllReduce(sum) → Y
```

```python
# Megatron-style Column Parallel Linear
class ColumnParallelLinear(nn.Module):
    def __init__(self, in_features, out_features, tp_size):
        super().__init__()
        self.tp_size = tp_size
        # 每张卡只有 out_features/tp_size 列
        self.weight = nn.Parameter(
            torch.empty(out_features // tp_size, in_features)
        )
    
    def forward(self, x):
        # x: [batch, seq, hidden]
        # 本地矩阵乘法，无需通信
        output = F.linear(x, self.weight)  # [batch, seq, hidden/tp]
        return output  # 后续由 RowParallel 做 AllReduce

# Megatron-style Row Parallel Linear  
class RowParallelLinear(nn.Module):
    def __init__(self, in_features, out_features, tp_size):
        super().__init__()
        self.weight = nn.Parameter(
            torch.empty(out_features, in_features // tp_size)
        )
    
    def forward(self, x):
        output = F.linear(x, self.weight)
        # AllReduce 汇总所有 TP rank 的结果
        dist.all_reduce(output, op=dist.ReduceOp.SUM)
        return output
```

**Attention 的张量并行**：

Multi-Head Attention 天然适合 TP——将 attention heads 均匀分到各卡：

```
TP_size = 4, num_heads = 32
→ 每张卡负责 8 个 heads
→ Q, K, V 的投影矩阵按列切分
→ Output 投影矩阵按行切分
→ 每层 2 次 AllReduce（MLP 1 次 + Attention 1 次）
```

**通信量分析**：

每个 Transformer 层需要 2 次 forward AllReduce + 2 次 backward AllReduce：

$$\text{每层通信量} = 4 \times 2 \times b \times s \times h$$

其中 $b$ 是 batch size，$s$ 是序列长度，$h$ 是 hidden dimension。

#### 工程经验

- **TP 通信频繁但数据量小**：每层都需要通信，因此要求 GPU 间有高带宽互连（NVLink/NVSwitch）
- **TP 通常限制在单机内（8 卡）**：跨机的 IB 带宽不足以支撑 TP 的高频通信
- **TP degree 选择**：通常是 2, 4, 8（必须能整除 num_heads）
- **TP 与 Sequence Length 的 trade-off**：长序列下 TP 的通信量线性增长

#### 面试考点

> Q: 张量并行的通信发生在 forward 还是 backward？各几次？
> A: 标准 Megatron 实现中，forward 每层 2 次 AllReduce（MLP 一次 + Attention 一次），backward 同样 2 次。总计每层 4 次 AllReduce。如果用 sequence parallelism（下面会讲），forward 改为 AllGather + ReduceScatter 各一次。

---

### 3.3 流水线并行 (Pipeline Parallelism, PP)

#### 技术原理

将模型按层切分到不同设备上，形成流水线：

```
GPU 0: Layers 0-7    (Stage 0)
GPU 1: Layers 8-15   (Stage 1)
GPU 2: Layers 16-23  (Stage 2)
GPU 3: Layers 24-31  (Stage 3)
```

**朴素 PP 的问题**：巨大的 "pipeline bubble"（气泡）

```
GPU 0: [F0][  ][  ][  ][B0][  ][  ][  ]
GPU 1: [  ][F1][  ][  ][  ][B1][  ][  ]
GPU 2: [  ][  ][F2][  ][  ][  ][B2][  ]
GPU 3: [  ][  ][  ][F3][  ][  ][  ][B3]
                              ^ 大量空闲时间
```

**GPipe 方案**：将 micro-batch 拆分为 $m$ 个 micro-batch：

气泡比例 = $(p-1) / (m + p - 1)$

其中 $p$ 是 pipeline stages 数。增大 $m$ 可以减小气泡。

**1F1B (One Forward One Backward) Schedule**：

Megatron 和 DeepSpeed 采用的更高效调度：

```
Stage 0: F0 F1 F2 F3 B0 F4 B1 F5 B2 ... 
Stage 1:    F0 F1 F2    B0 F3 B1 F4 B2 ...
Stage 2:       F0 F1       B0 F2 B1 F3 B2 ...
Stage 3:          F0          B0 F1 B1 F2 B2 ...
```

1F1B 的优势：**显存更低**（不需要同时保存所有 micro-batch 的 activation）。

**Interleaved 1F1B（Virtual Pipeline Parallelism）**：

每张 GPU 分配多个不连续的层组（virtual stages），进一步减小气泡：

```
# 4 GPU, 32 layers, 2 virtual stages per GPU
GPU 0: Layers [0-3, 16-19]
GPU 1: Layers [4-7, 20-23]
GPU 2: Layers [8-11, 24-27]
GPU 3: Layers [12-15, 28-31]
```

气泡减半，但通信量增加（需要来回传输 activation）。

**Zero Bubble Pipeline Parallelism (2024)**：

Qi et al. 提出的近零气泡方案，通过将 backward 拆分为 B（计算输入梯度）和 W（计算权重梯度），实现更灵活的调度：

$$\text{Bubble ratio} \approx 0$$

#### 面试考点

> Q: 为什么流水线并行需要 micro-batch？不用 micro-batch 的效率有多低？
> A: 不用 micro-batch 时，每个 stage 在其他 stage 工作时完全空闲，利用率仅为 $1/p$（$p$ 个 stages 就只有 $1/p$）。比如 4 个 stage，利用率只有 25%。Micro-batch 使得 pipeline 可以重叠不同 micro-batch 的计算，将利用率提升到 $m/(m+p-1)$。

---

### 3.4 序列并行 (Sequence Parallelism, SP)

#### 技术原理

序列并行有两种不同含义，需要区分：

**1. Megatron-SP（配合 TP 使用）**

在 Tensor Parallel 的基础上，将 LayerNorm 和 Dropout 等非并行化操作按序列维度切分：

```
不使用 SP:
  [LayerNorm] 输入: [b, s, h] — 每张卡都有完整序列 — 冗余计算

使用 SP:
  [LayerNorm] 输入: [b, s/tp, h] — 每张卡只有 s/tp 的序列
  → AllGather → [b, s, h] → Attention (TP) → ReduceScatter → [b, s/tp, h]
```

Megatron-SP 的好处：
- 减少 activation 显存（每卡只存 1/TP 的 activation）
- 通信模式从 AllReduce 变为 AllGather + ReduceScatter（总通信量不变，但 ReduceScatter 可以和计算 overlap）

**2. Ring Attention / Context Parallelism**

将长序列按序列维度切分到多张卡，每张卡只计算部分 attention：

```python
# Ring Attention 的核心思想
# 将 KV 在 ring 上循环传递，每步计算部分 attention

def ring_attention(q_local, k_local, v_local, ring_group):
    """
    q_local: [b, s/n, h, d]  本地 query
    k_local, v_local: 初始时是本地的 KV
    """
    num_steps = get_world_size(ring_group)
    output = torch.zeros_like(q_local)
    max_score = torch.full(..., -inf)
    
    k_recv, v_recv = k_local, v_local
    
    for step in range(num_steps):
        # 异步发送当前 KV 到下一个 rank
        send_kv_async(k_recv, v_recv, next_rank)
        
        # 计算本地 Q 和当前 KV 的 attention
        attn_out, scores = attention(q_local, k_recv, v_recv)
        
        # Online softmax 累积
        output, max_score = online_softmax_update(
            output, max_score, attn_out, scores
        )
        
        # 接收上一个 rank 的 KV
        k_recv, v_recv = recv_kv(prev_rank)
    
    return output
```

这种方式可以训练超长序列（如 1M tokens），但通信量较大。

#### 工程经验

- **Megatron-SP 几乎是标配**：和 TP 一起使用时，activation 显存节省 TP_size 倍，几乎没有额外开销
- **Context Parallelism 适合超长序列**：当序列长度 > 128K 时，单卡放不下 attention 的 O(s²) 中间结果
- **Ring Attention 的带宽需求**：KV 在 ring 上传递，每步通信量 = 2 × b × s/n × h × d，n 个 step 总通信量 = O(b × s × h × d)

---

### 3.5 专家并行 (Expert Parallelism, EP)

#### 技术原理

专门用于 Mixture of Experts (MoE) 模型。每张 GPU 持有部分 experts，通过 All-to-All 通信将 token 路由到正确的 expert：

```
Input tokens: [t0, t1, t2, t3, t4, t5, t6, t7]
Router 决定:
  t0, t3, t5 → Expert 0 (GPU 0)
  t1, t4, t6 → Expert 1 (GPU 1)
  t2, t7     → Expert 2 (GPU 2)

Step 1: All-to-All dispatch (将 token 发送到对应 expert 的 GPU)
Step 2: 各 GPU 独立计算自己的 expert
Step 3: All-to-All combine (将结果发回原来的 GPU)
```

```python
def moe_forward(hidden_states, router, experts, ep_group):
    """
    hidden_states: [batch * seq, hidden]
    """
    # 路由决策
    router_logits = router(hidden_states)  # [batch*seq, num_experts]
    routing_weights, selected_experts = topk_gating(router_logits, k=2)
    
    # All-to-All dispatch: 按 expert 分组发送到对应 GPU
    dispatched = all_to_all_dispatch(hidden_states, selected_experts, ep_group)
    
    # 各 GPU 计算本地 expert
    expert_output = local_expert_forward(dispatched, experts)
    
    # All-to-All combine: 将结果发回
    combined = all_to_all_combine(expert_output, ep_group)
    
    # 加权聚合
    output = routing_weights * combined
    return output
```

**通信模式**：All-to-All 是一种"个性化"通信，不同于 AllReduce 的"集体"通信。在 GPU 间通信不均匀（受 routing 影响），对网络拓扑要求较高。

#### 面试考点

> Q: Expert Parallelism 和 Tensor Parallelism 的本质区别是什么？
> A: TP 是将每一层的计算均匀切分，每个 token 都需要所有 GPU 参与计算。EP 是将不同 expert 放在不同 GPU 上，每个 token 只路由到部分 GPU。TP 的通信是确定性的（AllReduce），EP 的通信是数据依赖的（All-to-All，取决于 routing 决策）。

---

## 4. 混合并行策略

### 4.1 3D 并行

#### 技术原理

将 DP、TP、PP 三种并行组合使用：

$$\text{Total GPUs} = DP \times TP \times PP$$

```
例如：64 GPU = DP8 × TP4 × PP2

Machine 0 (8 GPUs):
  [TP group: GPU 0-3] ← Pipeline Stage 0, DP rank 0
  [TP group: GPU 4-7] ← Pipeline Stage 1, DP rank 0

Machine 1 (8 GPUs):
  [TP group: GPU 0-3] ← Pipeline Stage 0, DP rank 1
  [TP group: GPU 4-7] ← Pipeline Stage 1, DP rank 1

... (共 8 台机器)
```

**设计原则**：

| 并行方式 | 通信频率 | 通信量 | 最佳放置 |
|---------|---------|--------|---------|
| TP | 极高（每层多次） | 中等 | 机内（NVLink） |
| PP | 中（每 micro-batch 一次） | 小（activation） | 机间 / 机内 |
| DP | 低（每 step 一次） | 大（全部梯度） | 机间（IB） |

**经验法则**：
- TP 放在单机内（TP ≤ 8）
- PP 可以跨机（通信量小）
- DP 跨机（通信量大但频率低，可以和计算 overlap）

### 4.2 ZeRO (Zero Redundancy Optimizer)

#### 技术原理

DeepSpeed 提出的 ZeRO 消除了数据并行中的显存冗余。

> 来源：ZeRO arXiv:1910.02054 (Rajbhandari et al., Microsoft, 2019)

回顾 DP 的显存占用：

```
模型参数 (Φ): 2Φ bytes (FP16)
梯度:          2Φ bytes (FP16)
优化器状态:     12Φ bytes (Adam FP32: params + momentum + variance)
─────────────────────────
总计:          16Φ bytes per GPU（每张卡都完整存储！）
```

**ZeRO 的三个阶段**：

| 阶段 | 切分内容 | 每卡显存 | 通信量 |
|------|---------|---------|--------|
| ZeRO-1 | 优化器状态 | 4Φ + 12Φ/N | = DP |
| ZeRO-2 | + 梯度 | 2Φ + 14Φ/N | = DP |
| ZeRO-3 | + 参数 | 16Φ/N | 1.5× DP |

其中 $N$ 是 DP degree。

```python
# ZeRO-3 的核心逻辑（简化版）
class ZeRO3Module:
    def forward(self, x):
        for layer in self.layers:
            # 收集完整参数（AllGather from all DP ranks）
            full_params = all_gather(layer.partitioned_params)
            
            # 前向计算
            x = layer(x, full_params)
            
            # 立即释放完整参数（只保留本 rank 的分片）
            del full_params
        return x
    
    def backward(self):
        for layer in reversed(self.layers):
            # 收集完整参数（AllGather）
            full_params = all_gather(layer.partitioned_params)
            
            # 反向计算得到梯度
            grad = compute_gradient(layer, full_params)
            
            # ReduceScatter：每个 rank 只保留自己分片的梯度
            partitioned_grad = reduce_scatter(grad)
            
            del full_params
```

**ZeRO-3 的通信量分析**：

- Forward: AllGather (参数) × L 次 = $2Φ \times (N-1)/N$
- Backward: AllGather (参数) × L 次 + ReduceScatter (梯度) × L 次 = $2Φ + 2Φ = 4Φ \times (N-1)/N$
- 总计 ≈ $6Φ$ vs DP 的 $4Φ$，增加 50%

### 4.3 FSDP (Fully Sharded Data Parallel)

#### 技术原理

PyTorch 原生的 ZeRO-3 实现。FSDP 和 DeepSpeed ZeRO-3 的核心思想相同，但实现细节有差异。

> 来源：PyTorch FSDP 官方文档 https://pytorch.org/docs/stable/fsdp.html

```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import ShardingStrategy

model = FSDP(
    model,
    sharding_strategy=ShardingStrategy.FULL_SHARD,  # ZeRO-3
    # SHARD_GRAD_OP = ZeRO-2
    # NO_SHARD = DDP
    mixed_precision=MixedPrecision(
        param_dtype=torch.bfloat16,
        reduce_dtype=torch.bfloat16,
        buffer_dtype=torch.bfloat16,
    ),
    auto_wrap_policy=size_based_auto_wrap_policy,
    use_orig_params=True,  # 兼容 torch.compile
)
```

### 4.4 FSDP vs DeepSpeed 对比

| 维度 | FSDP (PyTorch) | DeepSpeed ZeRO |
|------|---------------|----------------|
| **集成度** | PyTorch 原生 | 第三方库 |
| **ZeRO stages** | FULL_SHARD(3), SHARD_GRAD_OP(2) | Stage 1/2/3 |
| **Offload** | CPU offload 支持 | CPU + NVMe offload |
| **混合精度** | 原生 MixedPrecision | 自有 FP16/BF16 实现 |
| **torch.compile** | 支持（use_orig_params） | 兼容性较差 |
| **Pipeline Parallel** | 社区方案 | 原生支持 |
| **灵活性** | 中等 | 高（丰富的 config） |
| **调试友好** | 较好（PyTorch 原生） | 一般（封装较深） |
| **成熟度 (2026)** | 高（FSDP2 已稳定） | 高 |

#### 实际选型建议

```
模型 < 10B:
  → FSDP / DDP + gradient accumulation 即可
  
模型 10B - 70B:
  → FSDP (ZeRO-3) 或 DeepSpeed ZeRO-3
  → 如需 PP，倾向 DeepSpeed 或 Megatron-LM
  
模型 70B+:
  → Megatron-LM + 3D 并行（TP + PP + DP）
  → 或 DeepSpeed ZeRO-3 + PP
  
MoE 模型:
  → Megatron-LM + EP + TP + DP
  → DeepSpeed MoE
```

#### 工程经验

- **FSDP2（PyTorch 2.3+）**：重写了 FSDP 内部实现，更好支持 torch.compile、DTensor、混合 sharding 策略
- **Hybrid Sharding**：在机内用 FULL_SHARD，机间用 NO_SHARD（减少跨机通信）
- **Activation Checkpointing**：和 FSDP/ZeRO 配合使用，用计算换显存（通常 checkpoint 每 1-2 个 Transformer block）
- **CPU Offload 的坑**：理论上可以训练更大模型，但 PCIe 带宽是瓶颈，实际 throughput 会大幅下降

#### 面试考点

> Q: ZeRO-3 比 ZeRO-2 多了什么通信？为什么不总是用 ZeRO-3？
> A: ZeRO-3 额外需要在 forward 时 AllGather 参数（ZeRO-2 在 forward 时参数是完整的）。通信量增加约 50%。当显存充足时（如用 A100 80GB 训练 7B 模型），ZeRO-2 的 throughput 更高，因为省了 forward 的 AllGather。ZeRO-3 适合显存紧张的场景。

---

## 5. 训练稳定性

### 5.1 Loss Spike 处理

#### 现象与原因

Loss spike 是大模型训练中最令人头疼的问题之一。表现为 loss 突然飙升然后缓慢恢复（或不恢复）。

**常见原因**：

1. **数据问题**（最常见）
   - 脏数据 batch（HTML 残留、乱码、极端长度）
   - 特定 domain 的数据突然切换
   - 数据 loader bug 导致重复或跳过

2. **数值不稳定**
   - FP16 溢出（gradient 超过 65504）
   - 某些 attention head 的 softmax 数值爆炸
   - LayerNorm 的方差接近 0

3. **学习率过高**
   - 当前 step 的学习率对模型状态来说太大
   - Warmup 结束后的突然变化

4. **硬件问题**
   - GPU 故障导致 NaN
   - 网络丢包导致梯度同步不一致

#### 应对策略

```python
class LossSpikeHandler:
    def __init__(self, window_size=100, spike_threshold=2.0):
        self.loss_history = deque(maxlen=window_size)
        self.spike_threshold = spike_threshold
    
    def check_and_handle(self, current_loss, step):
        if len(self.loss_history) < 10:
            self.loss_history.append(current_loss)
            return 'continue'
        
        mean_loss = np.mean(self.loss_history)
        std_loss = np.std(self.loss_history)
        
        # 检测 spike
        if current_loss > mean_loss + self.spike_threshold * std_loss:
            logger.warning(f"Loss spike detected at step {step}: "
                          f"{current_loss:.4f} vs mean {mean_loss:.4f}")
            
            # 策略 1: 跳过这个 batch 的更新
            return 'skip_update'
            
            # 策略 2: 回滚到最近的 checkpoint
            # return 'rollback'
            
            # 策略 3: 降低学习率
            # return 'reduce_lr'
        
        self.loss_history.append(current_loss)
        return 'continue'
```

**工业级做法（参考 PaLM、LLaMA 训练）**：

1. **频繁保存 checkpoint**（每 100-500 steps）
2. **监控 gradient norm**：如果 grad norm 突然增大 10x+，预示 spike
3. **Spike 发生后**：
   - 回滚到 spike 前 100-200 steps 的 checkpoint
   - 跳过导致 spike 的数据（记录 data index）
   - 继续训练
4. **PaLM 的经验**：他们遇到了约 20 次 loss spike，每次都通过回滚 + 跳过数据解决

---

### 5.2 梯度裁剪 (Gradient Clipping)

#### 技术原理

防止单个 batch 的异常大梯度破坏模型：

$$\hat{g} = \begin{cases} g & \text{if } \|g\| \leq c \\ c \cdot \frac{g}{\|g\|} & \text{if } \|g\| > c \end{cases}$$

```python
# PyTorch 实现
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 在分布式训练中，需要注意 grad norm 的计算范围
# 正确做法：所有 rank 的 grad 一起计算 norm（FSDP/DDP 自动处理）
# 错误做法：每个 rank 独立 clip（会导致不同 rank 的梯度尺度不一致）
```

**典型参数设置**：
- `max_norm = 1.0`：最常见的选择
- 对于不稳定的训练，可以尝试 `max_norm = 0.5` 或 `0.3`
- 监控 clip 频率：如果 > 50% 的 step 都在 clip，说明 LR 可能过高

---

### 5.3 学习率调度 (Learning Rate Schedule)

#### 主流调度策略

**1. Warmup + Cosine Decay（最常用）**

$$\text{lr}(t) = \begin{cases} \text{lr}_{\max} \cdot \frac{t}{T_{\text{warmup}}} & t \leq T_{\text{warmup}} \\ \text{lr}_{\min} + \frac{\text{lr}_{\max} - \text{lr}_{\min}}{2} \left(1 + \cos\left(\frac{\pi (t - T_{\text{warmup}})}{T_{\text{total}} - T_{\text{warmup}}}\right)\right) & t > T_{\text{warmup}} \end{cases}$$

```python
def cosine_schedule(step, warmup_steps, total_steps, max_lr, min_lr):
    if step < warmup_steps:
        return max_lr * step / warmup_steps
    
    progress = (step - warmup_steps) / (total_steps - warmup_steps)
    return min_lr + 0.5 * (max_lr - min_lr) * (1 + math.cos(math.pi * progress))
```

**2. WSD (Warmup-Stable-Decay)**

MiniCPM 等使用的三阶段策略：

```
|--warmup--|------stable------|--decay--|
    lr ↗        lr = const        lr ↘
```

好处：stable 阶段可以灵活决定何时开始 decay（根据 loss 曲线判断）。

**3. Inverse Square Root**

$$\text{lr}(t) = \text{lr}_{\max} \cdot \min\left(\frac{1}{\sqrt{t}}, \frac{t}{T_{\text{warmup}}^{3/2}}\right)$$

T5 等 encoder-decoder 模型常用。

**典型超参数**（以 7B 模型为例）：

| 参数 | 典型值 |
|------|--------|
| max_lr | 3e-4 |
| min_lr | 3e-5 (max_lr 的 10%) |
| warmup_steps | 2000 |
| weight_decay | 0.1 |
| adam_beta1 | 0.9 |
| adam_beta2 | 0.95 |
| adam_epsilon | 1e-8 |

**模型越大，学习率越小**：

| 模型规模 | 建议 max_lr |
|---------|------------|
| 1B | 5e-4 |
| 7B | 3e-4 |
| 13B | 2e-4 |
| 70B | 1.5e-4 |
| 175B+ | 6e-5 ~ 1e-4 |

---

### 5.4 BF16 vs FP16

#### 技术原理

```
FP32: 1 sign + 8 exponent + 23 mantissa  → 范围大、精度高
FP16: 1 sign + 5 exponent + 10 mantissa  → 范围小、精度中
BF16: 1 sign + 8 exponent + 7 mantissa   → 范围大、精度低

         范围          精度(有效位)   特殊值
FP32:  ±3.4×10³⁸     ~7 digits      NaN, Inf
FP16:  ±65504         ~3.3 digits    NaN, Inf, easy overflow!
BF16:  ±3.4×10³⁸     ~2.4 digits    NaN, Inf
```

**FP16 的主要问题**：

1. **Overflow**：gradient > 65504 就变成 inf → NaN 传播
2. **Underflow**：gradient < 6×10⁻⁸ 就变成 0 → 梯度消失
3. **需要 Loss Scaling**：人为放大 loss 来避免 underflow

```python
# FP16 训练需要 loss scaling
scaler = torch.cuda.amp.GradScaler()

with torch.cuda.amp.autocast(dtype=torch.float16):
    loss = model(inputs)

scaler.scale(loss).backward()  # 放大 loss
scaler.unscale_(optimizer)     # 还原梯度
torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
scaler.step(optimizer)
scaler.update()  # 动态调整 scale factor
```

**BF16 的优势**：

1. **范围和 FP32 一样大**：不需要 loss scaling
2. **训练更稳定**：没有 overflow 风险
3. **实现更简单**：不需要 GradScaler

**BF16 的代价**：
- 精度低于 FP16（7 bit mantissa vs 10 bit）
- 某些精度敏感的操作（如 softmax 中的大数减法）可能受影响
- 需要 Ampere+ GPU（A100/H100/A800/H800）

#### 实际选择

```
2024-2026 的共识：
├── 训练：BF16 为主（稳定性 >> 精度损失）
├── 推理：FP16 或量化（INT8/FP8）
├── 关键操作用 FP32：loss 计算、softmax、LayerNorm 的归约
└── H100 + FP8：最新的 Transformer Engine 支持 FP8 训练
```

#### 面试考点

> Q: 为什么 BF16 精度更低但训练效果不比 FP16 差？
> A: 深度学习训练对数值范围比精度更敏感。梯度和 activation 的动态范围很大（可能跨越多个数量级），FP16 的窄范围容易 overflow/underflow，而 BF16 的范围和 FP32 一样，避免了这些问题。精度的微小损失被 SGD 的随机性"吸收"了，不影响收敛。

---

## 6. MoE 训练特殊挑战

### 6.1 MoE 基础架构

#### 技术原理

Mixture of Experts 用稀疏激活替代 dense 计算，在增加参数量的同时保持计算量不变：

```
Standard FFN:
  output = W2 · activation(W1 · x)
  FLOPs: 2 × hidden × ffn_hidden × 2 = 8h²（假设 ffn=4h）

MoE FFN (Top-2 of N experts):
  gate_scores = softmax(W_gate · x)     # [N]
  top2_experts = argmax(gate_scores, k=2)
  output = Σ gate_scores[i] × Expert_i(x)  for i in top2
  FLOPs: 2 × (8h²) / N × 2 ≈ 原来的 2/N
```

**DeepSeek-V3 的 MoE 设计（2024-2025 标杆）**：

```
- 256 routing experts + 1 shared expert
- Top-8 routing（每个 token 激活 8/256 个 expert）
- Fine-grained expert segmentation（将大 expert 拆成小 expert）
- Auxiliary-loss-free load balancing
- Multi-Token Prediction (MTP) 辅助目标
```

---

### 6.2 负载均衡 (Load Balancing)

#### 问题描述

如果 routing 不均匀，某些 expert 处理大量 token（热点），其他 expert 空闲：
- **计算浪费**：总 latency 取决于最慢的 expert
- **Expert collapse**：得不到训练的 expert 越来越差，被分配更少 token → 恶性循环

#### 解决方案

**方法 1: Auxiliary Loss（Switch Transformer 风格）**

$$\mathcal{L}_{\text{aux}} = \alpha \cdot N \sum_{i=1}^{N} f_i \cdot P_i$$

其中：
- $f_i = \frac{\text{tokens routed to expert } i}{\text{total tokens}}$（实际频率）
- $P_i = \frac{1}{T}\sum_{t=1}^{T} p_i(x_t)$（平均 routing 概率）
- $\alpha$ 是平衡系数（通常 0.01）

这个 loss 鼓励 $f_i$ 和 $P_i$ 都趋向均匀分布 $1/N$。

```python
def load_balance_loss(router_probs, expert_indices, num_experts, alpha=0.01):
    """
    router_probs: [batch*seq, num_experts] - softmax 后的概率
    expert_indices: [batch*seq, top_k] - 选中的 expert id
    """
    # 计算每个 expert 的实际频率
    one_hot = F.one_hot(expert_indices, num_experts).float()  # [..., top_k, E]
    f = one_hot.sum(dim=[0,1]) / one_hot.sum()  # [E] 实际频率
    
    # 计算每个 expert 的平均概率
    P = router_probs.mean(dim=0)  # [E] 平均概率
    
    loss = alpha * num_experts * (f * P).sum()
    return loss
```

**方法 2: Expert Choice Routing（反向路由）**

不是让 token 选 expert，而是让 expert 选 token：

```python
def expert_choice_routing(hidden_states, router, num_experts, capacity):
    """
    每个 expert 选择 top-capacity 个 token
    保证完美负载均衡
    """
    scores = router(hidden_states)  # [num_tokens, num_experts]
    scores = scores.T  # [num_experts, num_tokens]
    
    # 每个 expert 选 top-k tokens
    topk_values, topk_indices = torch.topk(scores, k=capacity, dim=1)
    
    return topk_indices, F.softmax(topk_values, dim=1)
```

优点：完美均衡。缺点：某些 token 可能不被任何 expert 选中（丢弃）或被多个选中。

**方法 3: DeepSeek-V3 的 Auxiliary-Loss-Free 方法**

DeepSeek-V3 引入了一种不需要 auxiliary loss 的负载均衡方法：

```python
def auxiliary_loss_free_balancing(router_logits, bias, num_experts):
    """
    在 router 的 logits 上加一个可学习的 bias
    bias 不参与 softmax 概率计算（不影响 routing weight）
    但参与 argmax 选择（影响 routing decision）
    """
    # Routing 概率用原始 logits
    routing_weights = F.softmax(router_logits, dim=-1)
    
    # Expert 选择用 logits + bias
    selection_logits = router_logits + bias  # bias 是可调节的
    selected_experts = torch.topk(selection_logits, k=top_k, dim=-1)
    
    # 动态调整 bias：过载的 expert 降低 bias，空闲的 expert 提高 bias
    update_bias_based_on_load(bias, selected_experts)
    
    return routing_weights, selected_experts
```

核心思想：解耦 "routing weight"（用于加权求和）和 "routing decision"（用于选择 expert），通过 bias 控制决策而不干扰权重。

---

### 6.3 Expert Collapse

#### 问题描述

部分 expert 在训练中逐渐"死掉"：
- 权重趋向相似（loss of diversity）
- 某些 expert 几乎不被选择
- 退化为 dense model（只有几个 expert 工作）

#### 应对策略

1. **Expert dropout**：训练时随机关闭部分 expert，迫使所有 expert 都被训练
2. **Expert 重初始化**：监控 expert 使用频率，长期不活跃的 expert 用活跃 expert + noise 重初始化
3. **Z-loss**：惩罚 router logits 的数值过大（防止 router 过于"自信"）

$$\mathcal{L}_z = \frac{1}{T}\sum_{t=1}^{T} \left(\log \sum_{i=1}^{N} \exp(z_i^{(t)})\right)^2$$

4. **Shared expert**：始终激活一个共享 expert（DeepSeek 系列），保证即使 routing expert 不均也有基线输出

---

### 6.4 Routing 策略

| 策略 | 优点 | 缺点 | 代表 |
|------|------|------|------|
| Top-1 | 计算最少 | 表达力弱，不稳定 | Switch Transformer |
| Top-2 | 平衡性好 | 主流选择 | ST-MoE, Mixtral |
| Top-K (K>2) | 表达力强 | 计算多 | DeepSeek-V3 (Top-8) |
| Expert Choice | 完美均衡 | Token 丢弃 | - |
| Hash Routing | 无需学习 | 语义无关 | - |

#### 面试考点

> Q: MoE 模型的 "总参数量" 和 "激活参数量" 有什么区别？为什么这个区别很重要？
> A: 总参数量包括所有 expert 的参数（如 Mixtral 8x7B 总参数 ~47B），激活参数量是每个 token 实际使用的参数（~13B）。区别很重要因为：(1) 推理时的 FLOPs 取决于激活参数；(2) 显存取决于总参数（所有 expert 都要加载）；(3) Scaling Laws 应该用激活参数而非总参数来分析。这意味着 MoE 模型"用显存换计算"。

---

## 7. 长上下文训练

### 7.1 RoPE (Rotary Position Embedding) 基础

#### 技术原理

RoPE 通过旋转矩阵编码位置信息：

$$f(x_m, m) = x_m e^{im\theta}$$

其中 $m$ 是位置，$\theta_j = 10000^{-2j/d}$ 是频率。

展开到实数域：

$$\begin{pmatrix} \cos m\theta_j & -\sin m\theta_j \\ \sin m\theta_j & \cos m\theta_j \end{pmatrix} \begin{pmatrix} x_m^{(2j)} \\ x_m^{(2j+1)} \end{pmatrix}$$

**RoPE 的关键性质**：
- **相对位置编码**：$\langle f(q_m, m), f(k_n, n) \rangle$ 只依赖 $m-n$
- **衰减性**：长距离的 attention score 自然衰减
- **无需可学习参数**：完全由数学函数定义

#### 外推问题

模型在 L=4096 上预训练后，直接处理 L=8192 的序列会崩溃。原因是：
- 位置 $m > L$ 的旋转角度 $m\theta$ 超出了训练时见过的范围
- 高频分量（小 j、大 $\theta_j$）的旋转角度超出 $2\pi$ 的整数倍
- Attention pattern 变得混乱

---

### 7.2 位置编码外推方法

#### 方法 1: Position Interpolation (PI)

Meta 提出的最简单方法——缩放位置 index：

$$m' = m \cdot \frac{L}{L'} \quad \text{(将 } [0, L'] \text{ 压缩到 } [0, L] \text{)}$$

```python
def position_interpolation(position_ids, original_max_len, target_max_len):
    scale = original_max_len / target_max_len
    return position_ids * scale
```

**问题**：所有频率维度统一缩放。低频分量（编码长程关系）被过度压缩，高频分量（编码局部关系）被不必要地缩放。

#### 方法 2: NTK-Aware Interpolation

关键洞察：不同频率维度应该有不同的缩放策略。

**数学形式**：修改 base frequency：

$$\theta_j' = \left(\text{base} \cdot \alpha^{d/(d-2)}\right)^{-2j/d}$$

其中 $\alpha = L'/L$ 是扩展比例。

```python
def ntk_aware_rope(dim, max_position, original_max_len, base=10000):
    alpha = max_position / original_max_len
    base_new = base * alpha ** (dim / (dim - 2))
    
    inv_freq = 1.0 / (base_new ** (torch.arange(0, dim, 2).float() / dim))
    return inv_freq
```

**直觉理解**：
- 高频维度（j 小）：几乎不缩放（局部关系不需要改变）
- 低频维度（j 大）：大幅缩放（长程关系需要适应新的序列长度）
- 这是一种 "spectral interpolation"

**NTK-Aware 的重要发现**：可以不需要任何微调就能扩展上下文！（虽然效果不如微调后的方法）

#### 方法 3: NTK-by-parts

进一步精细化——将频率维度分为三段：

```python
def ntk_by_parts(dim, max_position, original_max_len, base=10000, 
                  beta=1, gamma=4):
    """
    beta, gamma: 控制哪些频率维度被插值
    - 波长 < beta*L 的高频维度：不缩放
    - 波长 > gamma*L 的低频维度：完全插值（PI）
    - 中间的维度：线性过渡
    """
    alpha = max_position / original_max_len
    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))
    
    wavelengths = 2 * math.pi / inv_freq
    
    # 计算每个维度的缩放因子
    scales = torch.ones_like(inv_freq)
    for i, wl in enumerate(wavelengths):
        if wl / original_max_len < beta:
            scales[i] = 1.0  # 高频：不缩放
        elif wl / original_max_len > gamma:
            scales[i] = 1.0 / alpha  # 低频：完全插值
        else:
            # 中间：平滑过渡
            ratio = (wl / original_max_len - beta) / (gamma - beta)
            scales[i] = (1.0 - ratio) + ratio / alpha
    
    return inv_freq * scales
```

#### 方法 4: YaRN (Yet another RoPE extensioN)

YaRN 综合了 NTK-by-parts + attention temperature scaling：

$$\text{attention}(Q, K) = \frac{QK^T}{\sqrt{d} \cdot t}$$

其中 $t = 0.1 \ln(\alpha) + 1$ 是 temperature 因子。

```python
class YaRNScaledRotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position, original_max_len, base=10000,
                 beta=1, gamma=32, attn_factor=0.1):
        super().__init__()
        self.alpha = max_position / original_max_len
        
        # NTK-by-parts 频率缩放
        self.inv_freq = compute_ntk_by_parts(dim, max_position, 
                                              original_max_len, base, beta, gamma)
        
        # Attention temperature
        self.mscale = attn_factor * math.log(self.alpha) + 1.0
    
    def forward(self, q, k, position_ids):
        # 应用旋转
        cos, sin = self._compute_cos_sin(position_ids)
        q_rot = apply_rotary(q, cos, sin)
        k_rot = apply_rotary(k, cos, sin)
        
        # 温度缩放（在 attention score 计算时）
        return q_rot, k_rot, self.mscale
```

**YaRN 的优势**：
- 只需要原始训练数据的 ~0.1% 进行微调
- 训练 steps 只需 400-600（非常少）
- 可以将 4K 模型扩展到 64K-128K

---

### 7.3 渐进式上下文扩展

#### 实践方案

分阶段扩展上下文长度，每个阶段微调少量 steps：

```
Phase 1: 4K → 16K    (1000 steps, lr=2e-5)
Phase 2: 16K → 64K   (1000 steps, lr=1e-5)
Phase 3: 64K → 256K  (500 steps, lr=5e-6)
```

**关键技巧**：
- **ABF (Adjusted Base Frequency)**：LLaMA 3 使用的方法，直接修改 RoPE base 为 500,000（原始为 10,000）
- **渐进式增加 base**：4K(base=10K) → 16K(base=50K) → 64K(base=500K)
- **数据准备**：每个阶段需要对应长度的真实长文本（不能只靠拼接短文本）

#### 2025-2026 的趋势

- **原生长上下文预训练**：越来越多模型在预训练阶段就用大 base frequency（如 500K-1M），直接支持长上下文
- **Ring Attention + 渐进扩展**：先用 8K 训练主体，然后用 Ring Attention 扩展到 1M
- **长上下文 ≠ 长上下文利用**：扩展到 1M 上下文窗口 ≠ 模型能有效利用 1M 内的信息（"Lost in the Middle" 问题仍然存在）

#### 面试考点

> Q: 为什么 RoPE 不能直接外推到更长的序列？NTK-Aware 和 YaRN 的核心区别是什么？
> A: RoPE 的旋转角度 $m\theta$ 在超出训练范围时，模型从未学过如何处理这些角度值，导致 attention 计算错误。NTK-Aware 通过修改 base frequency 做一种"全局"的频率缩放，核心是用一个参数 $\alpha$ 调整整个频谱。YaRN 更精细：(1) 使用 NTK-by-parts 对不同频率分段处理；(2) 引入 attention temperature 缩放来补偿分布偏移。YaRN 需要的微调数据和 steps 更少，效果更好。

---

## 8. SFT 工程实践

### 8.1 数据质量 > 数量

#### 核心发现

**LIMA 论文的核心结论**："Less Is More for Alignment"

- 仅 1000 条高质量 SFT 数据，就能让 LLaMA-65B 达到接近 GPT-4 的对话能力
- 数据质量的几个维度：
  - **多样性**：覆盖不同任务类型和领域
  - **回复质量**：详细、有结构、有帮助
  - **格式一致性**：统一的对话风格和格式

**实践经验**：

```
数据量 vs 质量的 ROI 曲线:

质量分布     数据量    效果
├── 低质量     100K     差（模型学到噪声模式）
├── 中等质量   10K      中
├── 高质量     1K-5K    好
└── 高质量     5K-50K   最佳（超过这个量提升边际递减）
```

#### 数据构造方法

**方法 1: 人工标注**（最贵但最好）

```
成本估算:
- 单条高质量标注: ¥50-200（含审核）
- 5000 条数据集: ¥25万-100万
- 建议: 20% 预算用于标注，80% 用于审核和迭代
```

**方法 2: 强模型蒸馏**

```python
# 用 GPT-4/Claude 生成 SFT 数据
def generate_sft_data(seed_prompts, strong_model):
    dataset = []
    for prompt in seed_prompts:
        # 多次采样，选最好的
        responses = [strong_model.generate(prompt, temperature=0.7) 
                     for _ in range(3)]
        
        # 质量评估（可以用另一个模型或人工）
        best_response = quality_ranker(prompt, responses)
        
        dataset.append({
            'instruction': prompt,
            'response': best_response
        })
    return dataset
```

**方法 3: Self-Instruct / Evol-Instruct**

让模型自己生成指令-回复对，然后过滤：

```
Evol-Instruct 的进化策略:
1. 增加约束 (Add Constraints)
2. 加深推理 (Deepening)  
3. 具体化 (Concretizing)
4. 增加推理步骤 (Increase Reasoning Steps)
5. 扩展话题 (Breadth)

示例:
原始: "写一首诗"
进化: "用五言绝句的格式，以'春雨'为题，写一首表达游子思乡之情的诗，
       要求每句至少包含一个典故"
```

---

### 8.2 对话格式

#### ChatML 格式（最通用）

```
<|im_start|>system
你是一个有帮助的 AI 助手。<|im_end|>
<|im_start|>user
请解释什么是 Transformer。<|im_end|>
<|im_start|>assistant
Transformer 是一种基于自注意力机制的神经网络架构...<|im_end|>
```

#### LLaMA 格式

```
<s>[INST] <<SYS>>
你是一个有帮助的 AI 助手。
<</SYS>>

请解释什么是 Transformer。 [/INST] Transformer 是... </s>
```

#### Loss Masking 策略

**核心原则：只对 assistant 的回复计算 loss，不对 system/user 计算 loss。**

```python
def create_labels(input_ids, tokenizer):
    """
    只对 assistant 回复部分计算 loss
    """
    labels = input_ids.clone()
    
    # 找到所有 assistant 回复的 span
    assistant_spans = find_assistant_spans(input_ids, tokenizer)
    
    # 非 assistant 部分设为 -100（ignore_index）
    mask = torch.ones_like(labels) * -100
    for start, end in assistant_spans:
        mask[start:end] = labels[start:end]
    
    return mask
```

**为什么要做 Loss Masking？**
- 如果对 user 输入也计算 loss，模型会学习"复读"用户的话
- System prompt 是固定模板，不需要学习
- 只学习"如何回复"，不学习"如何提问"

---

### 8.3 多轮训练技巧

#### 技术要点

**1. 多轮对话的样本构造**

```python
def construct_multiturn_sample(conversation):
    """
    conversation: [
        {"role": "user", "content": "你好"},
        {"role": "assistant", "content": "你好！有什么可以帮你的？"},
        {"role": "user", "content": "介绍一下北京"},
        {"role": "assistant", "content": "北京是中国的首都..."},
    ]
    """
    input_ids = []
    labels = []
    
    for turn in conversation:
        turn_ids = tokenizer.encode(format_turn(turn))
        input_ids.extend(turn_ids)
        
        if turn["role"] == "assistant":
            labels.extend(turn_ids)  # 计算 loss
        else:
            labels.extend([-100] * len(turn_ids))  # 不计算 loss
    
    return input_ids, labels
```

**2. 多轮 vs 单轮的比例**

```
推荐配比:
├── 单轮 QA: 30-40%
├── 2-3 轮对话: 30-40%
├── 4-8 轮复杂对话: 15-20%
└── 超长对话 (8+ 轮): 5-10%
```

**3. 避免多轮退化**

问题：模型可能在多轮对话中逐渐退化（后面的回复质量下降）。

解决方案：
- 使用"渐进式难度"的多轮数据——后面的轮次更有挑战性
- 在 loss 中对后续轮次给更高权重
- 加入"拒绝无关问题"的训练数据

**4. Packing vs Padding**

```python
# Padding: 每个样本独立，短样本补 PAD token
# 问题: 浪费计算（PAD token 不参与 loss 但占用显存和计算）

# Packing: 多个样本拼接到一个序列中
# [Sample1][Sample2][Sample3]
# 需要特殊的 attention mask 防止跨样本注意力

def create_packing_attention_mask(sample_boundaries, seq_len):
    """
    sample_boundaries: [(0, 200), (200, 500), (500, 1024)]
    每个样本只能 attend 到自己的 tokens
    """
    mask = torch.zeros(seq_len, seq_len)
    for start, end in sample_boundaries:
        mask[start:end, start:end] = 1
    return mask
```

Packing 可以提升训练效率 1.5-3x（取决于原始数据的长度分布）。

#### 工程经验

- **SFT 的学习率要比预训练低 10-100 倍**：通常 1e-5 到 5e-5
- **Epochs 通常 2-5 个**：过多 epochs 会过拟合（SFT 数据量小）
- **Evaluation 很重要**：不能只看 loss，要用 MT-Bench / AlpacaEval 等评估实际效果
- **数据去污染**：确保 SFT 数据不包含 benchmark 的测试集
- **NEFTune**：在 embedding 上加噪声（uniform noise），简单但有效的正则化

```python
# NEFTune: 在 forward 时给 embedding 加噪声
def neftune_forward(model, input_ids, noise_alpha=5):
    embeddings = model.embed_tokens(input_ids)
    dims = torch.tensor(embeddings.size(1) * embeddings.size(2))
    mag_norm = noise_alpha / torch.sqrt(dims)
    noise = torch.zeros_like(embeddings).uniform_(-mag_norm, mag_norm)
    embeddings = embeddings + noise
    return model.forward_from_embeddings(embeddings)
```

#### 面试考点

> Q: SFT 阶段为什么容易过拟合？如何缓解？
> A: SFT 数据量通常只有几千到几万条（vs 预训练的万亿 token），模型参数远大于数据量，容易过拟合。缓解方法：(1) 控制 epochs (2-5)；(2) 低学习率 (1e-5 ~ 5e-5)；(3) 数据增强/多样性；(4) NEFTune 正则化；(5) LoRA 等 PEFT 方法减少可训练参数；(6) 早停 (early stopping) 基于 evaluation metric。

---

## 9. 2026 前沿趋势

### 9.1 Scaling Laws 新发现

#### Chinchilla 的修正与超越

**原始 Chinchilla Law (2022)**：

> 来源：Hoffmann et al. arXiv:2203.15556 "Training Compute-Optimal Large Language Models"

$$L(N, D) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta}$$

最优配比：$N \propto C^{0.5}$, $D \propto C^{0.5}$（模型大小和数据量等比例增长）。

**2024-2026 的修正**：

1. **过度训练是值得的**（LLaMA 的教训）
   - Chinchilla 假设训练一次后就不再推理——但实际上推理成本远大于训练成本
   - 对于部署场景，训练更小但数据量更大的模型（过度训练）是更优策略
   - LLaMA-7B 用了 1T tokens（Chinchilla-optimal 约 150B），但推理效率大幅提升

2. **数据质量维度**
   - Scaling Laws 假设数据质量恒定，但实际上数据质量可以被优化
   - FineWeb 等高质量数据集的出现表明：同等计算量下，高质量数据的"等效 token 数"更高
   - 有学者提出将数据质量作为 Scaling Law 的第四个维度

3. **重复数据的 Scaling Behavior**
   - Muennighoff et al. (2023) "Scaling Data-Constrained Language Models" 研究了数据不足时的策略
   - 结论：当数据不够时，重复数据（多 epochs）的效果在 4 epochs 内接近线性，之后迅速衰减
   - 合成数据可以部分替代真实数据（但有天花板）

$$L(N, D, R) = E + \frac{A}{N^\alpha} + \frac{B}{D^\beta} \cdot g(R)$$

其中 $R$ 是重复次数，$g(R)$ 是重复惩罚函数。

4. **Inference Scaling Laws（2025-2026 热点）**

   除了训练阶段的 Scaling Law，推理阶段的计算也遵循 Scaling Law：
   
   - **Chain-of-Thought Scaling**：增加推理时的 token 数（"thinking" tokens）可以提升性能
   - **Best-of-N Scaling**：生成 N 个回答，选最好的——N 越大效果越好，遵循对数关系
   - **OpenAI o1/o3 的路线**：用推理计算换更好的答案

### 9.2 小模型高效训练趋势

#### 2025-2026 的共识转变

从 "bigger is better" 到 "smaller but smarter"：

**1. 蒸馏训练 (Distillation)**

```python
def distillation_loss(student_logits, teacher_logits, labels, 
                       temperature=2.0, alpha=0.5):
    """
    student_logits: 学生模型输出
    teacher_logits: 教师模型输出
    labels: 真实标签
    temperature: 蒸馏温度（越高，软标签越平滑）
    alpha: KL loss vs CE loss 的权重
    """
    # 软标签 KL 散度
    soft_student = F.log_softmax(student_logits / temperature, dim=-1)
    soft_teacher = F.softmax(teacher_logits / temperature, dim=-1)
    kl_loss = F.kl_div(soft_student, soft_teacher, reduction='batchnorm') * (temperature ** 2)
    
    # 硬标签交叉熵
    ce_loss = F.cross_entropy(student_logits, labels)
    
    return alpha * kl_loss + (1 - alpha) * ce_loss
```

**2. 数据配方 > 模型大小**

Phi 系列（Microsoft）证明了 "textbook-quality data" 可以让小模型达到大模型的效果：
- Phi-1 (1.3B)：代码能力接近 GPT-3.5
- Phi-2 (2.7B)：常识推理接近 LLaMA-70B（在部分 benchmark 上）
- 关键：教科书级别的合成数据 + 精心设计的数据配比

**3. Pruning + Distillation**

先训练大模型，然后通过结构化剪枝 + 蒸馏得到小模型：

```
Train 70B → Prune to 13B → Distill with 70B as teacher → 13B (better than direct 13B)
```

LLM-Pruner、ShortGPT、SliceGPT 等工具使这个流程日益成熟。

**4. MoE 的小型化**

- DeepSeek-V3 的 Fine-Grained Expert 策略
- 每个 expert 更小，数量更多，路由更精确
- "小 expert、多路由" 比 "大 expert、少路由" 更高效

### 9.3 训练基础设施演进

#### 硬件趋势

```
GPU 演进:
├── A100 (2020): 80GB HBM2e, 312 TFLOPS (FP16)
├── H100 (2022): 80GB HBM3, 989 TFLOPS (FP16), FP8 support
├── H200 (2024): 141GB HBM3e, 989 TFLOPS (FP16)
├── B100/B200 (2025): 192GB HBM3e, ~2000 TFLOPS
└── GB300 (2026): NVLink72, 大幅提升集群互联带宽

互联演进:
├── NVLink 3 (A100): 600 GB/s
├── NVLink 4 (H100): 900 GB/s
├── NVSwitch (DGX): 全互联，任意 GPU 间等速
└── NVLink 5 (Blackwell): 1800 GB/s, 支持 72-GPU NVLink domain
```

#### 软件栈

```
2026 主流训练框架:
├── Megatron-LM: 大规模预训练的事实标准
├── DeepSpeed: ZeRO + MoE + 推理优化
├── PyTorch FSDP2: 原生分布式，和 torch.compile 深度集成
├── JAX/XLA: Google 内部主力，TPU 最佳搭档
├── NeMo: NVIDIA 的端到端训练平台
└── ColossalAI: 国产分布式训练框架
```

### 9.4 2026 年值得关注的方向

1. **FP8/FP4 训练**：H100 原生支持 FP8，Blackwell 进一步支持 FP4，可以将 throughput 翻倍
2. **Multi-Token Prediction (MTP)**：DeepSeek-V3 证明了同时预测多个 token 的有效性
3. **Test-Time Compute Scaling**：o1/o3 路线的持续深化
4. **合成数据的 Scaling Law**：合成数据能走多远？何时到天花板？
5. **模型合并 (Model Merging)**：DARE、TIES 等方法让不同能力的模型可以直接合并
6. **持续预训练 (Continual Pre-training)**：如何在不遗忘的情况下更新知识
7. **多模态统一训练**：图像、音频、视频、文本的统一预训练目标
8. **Agent 训练范式**：为 tool-use 和 multi-step reasoning 专门设计的训练策略

---

## 10. 面试题与参考答案

### 题目 1: 数据去重的必要性

**Q: 为什么预训练数据需要去重？不去重会有什么后果？MinHash 的核心原理是什么？**

**A:**
不去重的后果：(1) 模型过拟合到重复内容，降低泛化能力；(2) 训练效率下降——重复数据不提供新信息但消耗计算；(3) 隐私风险增加——重复出现的文本更容易被模型"记忆"并在推理时泄露。

MinHash 原理：将文档表示为 n-gram 集合，通过 $k$ 个随机哈希函数取每个集合的最小值构成签名。两个集合的 MinHash 碰撞概率等于它们的 Jaccard 相似度 $J(A,B) = |A \cap B| / |A \cup B|$。配合 LSH（Locality-Sensitive Hashing）将签名分桶，实现近似 O(n) 的近似重复检测。

---

### 题目 2: CLM vs MLM

**Q: 对比 CLM 和 MLM 的优缺点。为什么当前主流 LLM 都用 CLM？**

**A:**
CLM (单向自回归)：每个 token 只看左侧上下文。优点——训练效率高（所有 token 参与 loss）、天然支持生成。缺点——无法利用右侧信息。

MLM (双向遮蔽)：随机遮蔽 15% token 进行预测。优点——双向上下文理解能力强。缺点——只有 15% token 参与 loss（训练效率低 ~7x）、不适合生成任务、存在 pretrain-finetune mismatch（[MASK] token）。

主流选 CLM 的原因：(1) GPT-3 证明 CLM + few-shot prompting 可以 match MLM + fine-tuning；(2) CLM 的训练 token 效率更高；(3) 自回归生成是 LLM 最重要的应用形式；(4) Scaling Laws 在 CLM 上研究更充分。

---

### 题目 3: 张量并行通信分析

**Q: 在 Megatron-LM 的张量并行中，一个 Transformer 层的 forward pass 需要几次通信？分别发生在哪里？**

**A:**
2 次 AllReduce（或等价的 AllGather + ReduceScatter）。

(1) Self-Attention 模块：Q、K、V 的投影按 head 维度切分（Column Parallel），Output 投影按输入维度切分（Row Parallel）。Row Parallel 的输出需要 AllReduce 将各 rank 的部分和相加。→ 1 次 AllReduce。

(2) MLP 模块：第一个线性层 Column Parallel（W1 按列切分），第二个线性层 Row Parallel（W2 按行切分），同样需要 AllReduce。→ 1 次 AllReduce。

如果启用 Sequence Parallelism (Megatron-SP)，AllReduce 被拆成 AllGather + ReduceScatter，总通信量不变但可以更好地 overlap。

---

### 题目 4: ZeRO 三阶段

**Q: 解释 ZeRO-1/2/3 分别切分什么、节省多少显存、增加多少通信？**

**A:**
假设模型参数 Φ，N 个 GPU，Adam 优化器。单卡完整占用：2Φ(参数) + 2Φ(梯度) + 12Φ(优化器状态) = 16Φ。

- **ZeRO-1**：切分优化器状态。每卡：2Φ + 2Φ + 12Φ/N。通信量 = DDP 相同（1 次 AllReduce gradient）。
- **ZeRO-2**：切分优化器状态 + 梯度。每卡：2Φ + (2+12)Φ/N。通信量 = DDP 相同（将 AllReduce 拆为 ReduceScatter，每 rank 只保留自己分片的梯度）。
- **ZeRO-3**：切分一切。每卡：16Φ/N。通信量 ≈ 1.5× DDP（forward 多一次 AllGather 参数）。

---

### 题目 5: 流水线并行气泡

**Q: 流水线并行的气泡比例公式是什么？如何减小气泡？**

**A:**
GPipe 气泡比例 = $(p-1)/(m+p-1)$，其中 $p$ 是 pipeline stages，$m$ 是 micro-batches。

减小气泡的方法：
1. 增大 $m$（micro-batch 数量），但增加显存（GPipe 需保存所有 activation）
2. 使用 1F1B schedule（交错 forward 和 backward），减少峰值显存
3. 使用 Interleaved 1F1B（virtual pipeline parallelism），每 GPU 多个 stage
4. Zero Bubble Pipeline（Qi et al., 2024），将 backward 拆分为 B 和 W，接近零气泡

---

### 题目 6: BF16 vs FP16

**Q: 为什么 BF16 比 FP16 更适合大模型训练？FP16 训练需要额外做什么？**

**A:**
BF16 有 8 位指数（和 FP32 相同），范围达 ±3.4×10³⁸；FP16 只有 5 位指数，范围仅 ±65504。

FP16 的问题：(1) gradient 容易 overflow（> 65504 → inf）；(2) 小梯度 underflow（< 6×10⁻⁸ → 0）。因此 FP16 训练必须使用 Loss Scaling：放大 loss → 放大 gradient → 避免 underflow → 更新前除回 scale。还需要动态调整 scale factor（GradScaler）。

BF16 不需要 loss scaling，训练更稳定。虽然精度（7 bit mantissa）低于 FP16（10 bit），但深度学习对范围比精度更敏感——SGD 的随机性能容忍小的精度误差，但不能容忍 NaN/overflow。

---

### 题目 7: MoE 负载均衡

**Q: MoE 模型为什么需要负载均衡？Switch Transformer 的 auxiliary loss 是如何工作的？**

**A:**
不均衡的后果：(1) 热点 expert 成为计算瓶颈（总 latency 取决于最慢的 expert）；(2) 冷门 expert 得不到训练→退化→被分配更少 token→恶性循环（expert collapse）；(3) GPU 利用率不均。

Switch Transformer 的 auxiliary loss：$\mathcal{L}_{aux} = \alpha \cdot N \sum_i f_i \cdot P_i$，其中 $f_i$ 是 expert $i$ 的实际 token 频率，$P_i$ 是 expert $i$ 的平均路由概率。当某个 expert 的 $f_i$ 和 $P_i$ 都高时，loss 就大，迫使 router 分散流量。$\alpha$ 通常取 0.01，太大会干扰主 loss。

DeepSeek-V3 提出了 auxiliary-loss-free 方法：在 routing logits 上加可调 bias，bias 影响 expert 选择但不影响 routing weight 的 softmax 概率，避免了 auxiliary loss 对训练信号的干扰。

---

### 题目 8: RoPE 外推

**Q: 模型在 4K 上下文训练后，为什么不能直接处理 8K 的序列？Position Interpolation 和 NTK-Aware 的区别？**

**A:**
RoPE 通过 $m\theta_j$ 旋转编码位置，$\theta_j = 10000^{-2j/d}$。当 $m > L_{train}$，旋转角度超出训练分布，模型没见过这些角度值，attention 计算出错。

Position Interpolation (PI)：将位置 $m$ 线性缩放到 $[0, L_{train}]$ 范围内，$m' = m \cdot L_{train}/L_{new}$。问题——所有频率维度统一缩放，高频维度（编码局部关系）被不必要地压缩，损害局部性能。

NTK-Aware：修改 base frequency 为 $base' = base \cdot \alpha^{d/(d-2)}$，使得高频维度几乎不变（保持局部编码），低频维度大幅缩放（适应长距离）。这是一种谱域的非均匀插值，比 PI 的均匀插值更合理。

---

### 题目 9: SFT Loss Masking

**Q: SFT 训练时为什么要对 user 部分做 loss masking？如果不做会怎样？**

**A:**
SFT 的目标是让模型学习"如何回答"，而非"如何提问"。如果对 user 部分也计算 loss：(1) 模型会学习复读用户的输入模式；(2) 系统提示的固定模板会被过度拟合；(3) 对话中 user 和 assistant 的分布不同，混合计算 loss 会造成梯度信号混乱；(4) 实际推理时模型不需要生成 user 的话。

不做 loss masking 的后果：模型可能在回复中混入用户语气、重复问题，或在对话边界处产生混乱。

---

### 题目 10: Scaling Law 应用

**Q: Chinchilla Scaling Law 说什么？实际训练中为什么经常"违反" Chinchilla？**

**A:**
Chinchilla Law：给定计算预算 $C$，最优模型大小 $N$ 和数据量 $D$ 应等比例增长。具体地：$N \propto C^{0.5}$, $D \propto C^{0.5}$。例如 70B 模型的 Chinchilla-optimal 数据量约 1.4T tokens。

"违反"的原因——Chinchilla 只优化训练成本，没考虑推理成本。实际部署时，推理成本远大于训练成本（模型要服务数百万用户）。更小的模型 + 更多数据（过度训练）= 训练贵一点，但推理便宜很多。

例如 LLaMA-7B 用了 ~1T tokens（是 Chinchilla-optimal 的 ~10 倍），Meta 多花训练计算，换来更小的推理模型。这实际上是一种"inference-aware scaling law"——优化的是 total cost = training cost + inference cost × expected queries。

---

### 题目 11: 混合并行设计

**Q: 给你 128 个 H100 GPU（16 台 8-GPU 机器），要训练一个 70B dense 模型，你怎么设计并行策略？**

**A:**
推荐方案：TP=8, PP=2, DP=8（8×2×8=128）

设计思路：
- **TP=8**：放在机内（NVLink 900GB/s 高带宽），70B 模型的 hidden_dim=8192，8 路 TP 每个 head 分配 = 8192/8/128 = 合理
- **PP=2**：跨 2 台机器（IB 通信量小——只传 activation），80 层 / 2 = 每 stage 40 层
- **DP=8**：剩余 128/(8×2) = 8 路数据并行，梯度同步用 AllReduce 或 ZeRO-2
- 如果显存不够（70B × 16bytes ≈ 1.12TB），在 DP 维度用 ZeRO-2（切分梯度+优化器状态）
- Activation Checkpointing: 每 2 层 checkpoint 一次
- Micro-batch 数量: 16-32（减少 PP 气泡，目标气泡率 < 5%）

---

### 题目 12: 数据工程实践

**Q: 你负责构建一个中文大模型的预训练数据 pipeline，请描述完整流程和关键决策。**

**A:**
完整流程：

1. **数据收集**：Common Crawl 中文子集 + 百科 + 书籍 + 论坛 + 代码 + 学术论文
2. **HTML 到文本**：trafilatura 提取正文，保留段落结构
3. **语言检测**：fastText lid 过滤非中文内容（阈值 0.65+）
4. **去重（最先做）**：
   - URL 去重
   - MinHash LSH 近似去重（字符 5-gram，threshold=0.7）
   - 可选：suffix array 子串去重
5. **规则过滤**：
   - 长度过滤（< 100 字或 > 50 万字删除）
   - 特殊字符比例过滤
   - 重复行/段比例过滤
   - 脏词过滤
6. **质量过滤**：
   - fastText 质量分类器（正例：百科+教科书，负例：低质量网页）
   - 取 top 60-70% 的数据
7. **PII 过滤**：正则 + NER 检测手机号/身份证号/邮箱
8. **安全过滤**：toxicity classifier 去除有害内容
9. **数据配比**：
   - Web 60% + 百科 8% + 书籍 8% + 代码 12% + 学术 5% + 其他 7%
10. **Tokenization**：BPE（兼顾中英，词表大小 64K-128K）
11. **打包**：按 max_seq_len 打包成二进制格式，建索引

关键决策：(1) 去重在清洗之前（减少后续计算量）；(2) 代码比例稍高（提升推理能力）；(3) 多阶段质量过滤（先廉价规则，后昂贵模型）。

---

### 题目 13: 训练不稳定排查

**Q: 训练 13B 模型到 50% 时出现 loss spike 且不恢复，你的排查步骤是什么？**

**A:**
排查步骤（从简单到复杂）：

1. **检查 gradient norm 日志**：spike 前是否有 grad norm 异常增大？如果是，可能是数据问题或 LR 过高
2. **检查数据**：回溯 spike 时的 data batch index，检查是否有异常数据（极端长度、乱码、重复数据突然大量出现）
3. **检查硬件**：GPU 错误日志、NCCL 超时、节点掉线
4. **检查是否有 NaN/Inf**：在 loss、activation、gradient 中检查
5. **对比 learning rate schedule**：spike 是否发生在 LR 变化点附近

恢复策略：
1. **回滚 checkpoint**：回退到 spike 前最近的 checkpoint
2. **降低 LR**：恢复时将 learning rate 降低 50%，逐步回升
3. **跳过问题数据**：如果定位到特定数据 batch 导致，跳过该段数据
4. **加强 gradient clipping**：从 1.0 降低到 0.5 或更低
5. **启用 loss spike detection**：自动检测 loss 异常时保存状态并回滚

预防措施：
- 训练前做数据质量审计，抽样检查
- 设置 gradient clipping = 1.0（默认）
- 使用 BF16 而非 FP16（BF16 动态范围更大，不易溢出）
- 定期保存 checkpoint（每 500-1000 steps）
- 监控 gradient norm、loss、activation 统计量

---

---

## 🔧 落地应用

### 预训练：什么时候自己训，什么时候用开源？

| 场景 | 建议 | 理由 |
|------|------|------|
| 通用助手 | 用开源 base model + SFT | 训练成本太高，LLaMA/Qwen 等已足够好 |
| 垂直领域（医疗/法律/金融） | Continual Pretrain + SFT | 领域知识需要在预训练阶段注入 |
| 超大规模/国家级模型 | 从零预训练 | 数据安全、架构自主可控 |
| 多模态 | 用文本 base model + 多模态续训 | LLaVA/Qwen-VL 的成功路线 |

### 分布式训练：选型决策树

```
模型 < 10B → FSDP / DDP + gradient accumulation
模型 10B-70B → FSDP (ZeRO-3) 或 DeepSpeed ZeRO-3
模型 70B+ → Megatron-LM 3D 并行 (TP+PP+DP)
MoE 模型 → Megatron-LM + EP + TP + DP

单机多卡 → TP 放机内（NVLink）
多机 → PP 跨机（通信量小），DP 跨机（频率低可 overlap）
```

> 来源：Megatron-LM arXiv:1909.08053 定义了 TP 的通信模式；ZeRO arXiv:1910.02054 定义了三阶段显存切分

### 工程实践 Checklist

- [ ] 数据：去重（MinHash threshold=0.7）→ 质量过滤 → 配比设计
- [ ] 训练：BF16 + gradient clipping=1.0 + cosine LR schedule
- [ ] 监控：loss 曲线 + gradient norm + activation 统计量
- [ ] 稳定性：每 500 steps 存 checkpoint，设置 loss spike 回滚机制
- [ ] 评估：不只看 loss，要用下游 benchmark 评估

---

## 💡 启发与思考

### So What？

这篇笔记串联了预训练的"全链路"——从最初的数据清洗到最终的 SFT 微调。几个关键认知：

1. **数据工程是被低估的杠杆**：Chinchilla（arXiv:2203.15556）告诉我们数据和模型要等比例扩展，但 FineWeb/Phi 系列证明高质量数据的"等效 token 数"远高于低质量数据。在算力有限时，投资数据质量的 ROI 最高。

2. **分布式训练是"通信 vs 计算"的博弈**：TP 通信频繁但数据量小（放机内 NVLink），PP 通信量小但有 bubble（跨机），DP 数据量大但频率低（可 overlap）。理解这个 trade-off 就理解了 3D 并行的设计哲学。

3. **Scaling Law 不是铁律，而是指导原则**：Chinchilla 的"等比例扩展"在考虑推理成本后就不再最优——LLaMA 用 10 倍过度训练换来更小的推理模型。Scaling Law 的适用范围随着新发现不断修正。

### 局限与未解问题

- **数据质量没有统一度量**：什么是"高质量"取决于下游任务，目前没有 universal quality metric
- **Scaling Law 对 MoE 不完全适用**：MoE 的"激活参数"vs"总参数"使 Scaling Law 分析更复杂
- **训练稳定性仍然靠经验**：loss spike 的根因诊断缺乏系统方法论，Google 训练 PaLM 遇到 20 次 spike 都靠手动回滚
- **长上下文利用率低**：扩展到 1M 上下文 ≠ 模型能有效利用 1M 内的信息（"Lost in the Middle" 问题未解决）

### 脑暴拓展

- **数据工程的自动化**：能否用 LLM 自动优化数据 pipeline？用小 proxy model 做 DoReMi 式配比优化已在 Google 验证有效
- **训练-推理一体化 Scaling Law**：将 Test-Time Compute 纳入 Scaling Law 考量，统一优化训练预算和推理预算的分配
- **MoE + 长上下文的协同**：不同 expert 处理不同位置范围的 token？让"长程专家"和"局部专家"分工协作？

> 🔗 See also:
> - [[AI/LLM/Architecture/MoE 深度解析|MoE 深度解析]] — MoE 架构的详细剖析
> - [[AI/LLM/Architecture/Transformer架构深度解析-2026技术全景|Transformer 架构全景]] — 注意力机制和位置编码
> - [[AI/LLM/SFT/SFT 原理|SFT 原理]] — 预训练之后的第一步对齐
> - [[AI/LLM/Inference/Test-Time-Compute|Test-Time Compute]] — 推理时扩展，训练 Scaling Law 的正交补充
> - [[AI/LLM/Infra/DeepSpeed|DeepSpeed]] — ZeRO 优化器的工程实现

---

## 📚 推荐阅读

### 原始论文
- [Language Models are Few-Shot Learners (GPT-3)](https://arxiv.org/abs/2005.14165) — 证明 CLM + few-shot prompting 范式的可行性，预训练规模化的里程碑 ⭐⭐⭐⭐⭐
- [Training Compute-Optimal Large Language Models (Chinchilla)](https://arxiv.org/abs/2203.15556) — Scaling Law 最重要的修正，数据和模型要等比例扩展 ⭐⭐⭐⭐⭐
- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) — 张量并行的奠基论文，大规模训练必读 ⭐⭐⭐⭐⭐
- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054) — 三阶段显存优化，理解 FSDP/DeepSpeed 的理论基础 ⭐⭐⭐⭐⭐
- [Switch Transformers](https://arxiv.org/abs/2101.03961) — MoE 在 Transformer 中的工程化里程碑 ⭐⭐⭐⭐
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) — RoPE 原始论文，理解位置编码外推的起点 ⭐⭐⭐⭐
- [YaRN: Efficient Context Window Extension of Large Language Models](https://arxiv.org/abs/2309.00071) — 长上下文扩展的最佳实践 ⭐⭐⭐⭐

### 深度解读
- [The FineWeb Dataset Blog (HuggingFace)](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1) — 预训练数据工程的最佳实战案例，覆盖清洗/去重/质量过滤全流程 ⭐⭐⭐⭐⭐
- [LLaMA 2 Technical Report](https://arxiv.org/abs/2307.09288) — Meta 的预训练+对齐全流程工程经验 ⭐⭐⭐⭐⭐
- [Scaling Data-Constrained Language Models (Muennighoff et al.)](https://arxiv.org/abs/2305.16264) — 数据不够时怎么办？重复数据和合成数据的 Scaling 行为 ⭐⭐⭐⭐

### 实践资源
- [Megatron-LM GitHub](https://github.com/NVIDIA/Megatron-LM) — NVIDIA 官方大模型训练框架 ⭐⭐⭐⭐⭐
- [DeepSpeed](https://www.deepspeed.ai/) — 微软分布式训练框架，ZeRO + MoE + 推理优化 ⭐⭐⭐⭐⭐
- [PyTorch FSDP Tutorial](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html) — PyTorch 原生分布式训练教程 ⭐⭐⭐⭐
- [NanoGPT (Karpathy)](https://github.com/karpathy/nanoGPT) — 最小但完整的 GPT 预训练实现，学习预训练的最佳入门 ⭐⭐⭐⭐⭐

---

*本文完成于 2026-02-20，2026-02-22 补充出处/推荐阅读/落地应用/启发思考。覆盖 LLM 预训练数据工程、分布式训练范式、训练稳定性、MoE 训练、长上下文训练、SFT 工程实践等核心方向。共 13 道面试题。*