---
title: "贝尔曼方程"
brief: "贝尔曼方程（Bellman Equation，1957）——强化学习的数学基石，描述当前状态价值与后继状态的递归关系；包含状态价值函数 V(s) 和动作价值函数 Q(s,a) 两种形式，Bellman Optimality Equation 是求解最优策略的核心。"
type: concept
domain: ai/llm/rl/fundamentals
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm/rl/fundamentals
  - type/concept
---
# 贝尔曼方程

贝尔曼方程（Bellman Equation）是强化学习的数学基石，由 Richard Bellman 于 1957 年提出。它描述了**当前状态的价值与后继状态的价值之间的递归关系**，是几乎所有 RL 算法的理论出发点。

## 基础定义

### 马尔可夫决策过程（MDP）

RL 问题通常建模为 MDP $(S, A, P, R, \gamma)$：
- $S$：状态空间
- $A$：动作空间
- $P(s'|s, a)$：状态转移概率
- $R(s, a)$：奖励函数
- $\gamma \in [0, 1)$：折扣因子

### 累积回报

$$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \cdots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

折扣因子 $\gamma$ 控制对未来奖励的重视程度。$\gamma$ 越接近 1，越关注长期回报。

## 四种价值函数

### 状态价值函数 $V^\pi(s)$

在状态 $s$ 下遵循策略 $\pi$ 的期望累积回报：

$$V^\pi(s) = \mathbb{E}_\pi[G_t | S_t = s] = \mathbb{E}_\pi\left[\sum_{k=0}^{\infty} \gamma^k R_{t+k+1} \bigg| S_t = s\right]$$

### 动作价值函数 $Q^\pi(s, a)$

在状态 $s$ 下执行动作 $a$，然后遵循策略 $\pi$ 的期望累积回报：

$$Q^\pi(s, a) = \mathbb{E}_\pi[G_t | S_t = s, A_t = a]$$

两者的关系：$V^\pi(s) = \sum_a \pi(a|s) Q^\pi(s, a)$

## 贝尔曼期望方程

对于策略 $\pi$，价值函数满足递归关系：

$$V^\pi(s) = \sum_a \pi(a|s) \left[R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^\pi(s')\right]$$

$$Q^\pi(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) \sum_{a'} \pi(a'|s') Q^\pi(s', a')$$

直觉：**当前状态的价值 = 即时奖励 + 折扣后的下一状态期望价值**。

## 贝尔曼最优方程

定义最优价值函数 $V^*(s) = \max_\pi V^\pi(s)$：

$$V^*(s) = \max_a \left[R(s, a) + \gamma \sum_{s'} P(s'|s, a) V^*(s')\right]$$

$$Q^*(s, a) = R(s, a) + \gamma \sum_{s'} P(s'|s, a) \max_{a'} Q^*(s', a')$$

最优策略就是在每个状态选择使 $Q^*$ 最大的动作：$\pi^*(s) = \arg\max_a Q^*(s, a)$。

## 求解方法

### 动态规划（已知环境模型）

- **策略迭代**：交替进行策略评估（解贝尔曼期望方程）和策略改进（贪心策略）
- **价值迭代**：直接迭代贝尔曼最优方程

$$V_{k+1}(s) = \max_a \left[R(s, a) + \gamma \sum_{s'} P(s'|s, a) V_k(s')\right]$$

### 时序差分（TD Learning）

不需要环境模型，从采样中学习：

$$V(S_t) \leftarrow V(S_t) + \alpha \left[R_{t+1} + \gamma V(S_{t+1}) - V(S_t)\right]$$

其中 $\delta_t = R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$ 是 **TD 误差**。TD 误差在 PPO 中的 GAE（Generalized Advantage Estimation）中起核心作用。

### Q-Learning

$$Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \left[R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(S_t, A_t)\right]$$

Q-Learning 是 off-policy 的——更新使用 $\max_{a'}$ 而不是实际采取的动作。这是 DQN 的基础。

## 在 LLM 对齐中的应用

虽然 LLM 的 RLHF 不直接使用贝尔曼方程的 tabular 形式，但核心概念完全一致：

| RL 概念 | LLM 对齐中的对应 |
|---------|-----------------|
| 状态 $s$ | 当前 prompt + 已生成的 token 序列 |
| 动作 $a$ | 下一个 token |
| 策略 $\pi$ | LLM 的输出分布 |
| 奖励 $R$ | Reward Model 的评分 |
| 价值函数 $V$ | PPO 中的 Critic 网络 |

**PPO 的 Advantage 估计** 直接基于贝尔曼方程：

$$A_t = \sum_{l=0}^{T-t} (\gamma \lambda)^l \delta_{t+l}, \quad \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$

**GRPO 的简化**：去掉 Critic 网络，用 group reward 的均值和方差来估计 advantage，避免了训练价值网络的复杂性。

## 关键洞察

1. **贝尔曼方程的递归结构**启发了所有基于 bootstrapping 的 RL 方法
2. **折扣因子 $\gamma$ 的选择**在 LLM 中比传统 RL 更微妙——token 级奖励 vs 序列级奖励
3. **从 PPO 到 GRPO 的演进**，本质上是在简化贝尔曼方程中 value function 的估计方式

## 相关

- [[AI/LLM/RL/Fundamentals/On-Policy vs Off-Policy|On-Policy vs Off-Policy]]
- [[AI/LLM/RL/Fundamentals/为什么 PPO 优于 PG|为什么 PPO 优于 PG]]
- [[AI/Foundations/Math/概率与分布|概率与分布]]
- [[AI/Foundations/Math/信息论|信息论]]
