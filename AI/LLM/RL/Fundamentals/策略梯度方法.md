---
title: "策略梯度方法"
brief: "策略梯度方法（Policy Gradient）——直接对策略参数化并用梯度上升最大化期望回报，核心公式 nablaJ(theta)=E[nabla log pi(a|s)·Q(s,a)]；是 PPO/GRPO 等现代 LLM alignment 算法的数学基础。"
type: concept
domain: ai/llm/rl/fundamentals
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm/rl/fundamentals
  - type/concept
---
# 策略梯度方法

# 一、策略参数化

基于策略梯度的方法首先需要将策略参数化，即直接将策略  参数化为 ，其中  是策略的参数，表示在状态  下选择动作  的概率，并且处处可微。简而言之，**参数化策略是一个处处可微的概率分布**。

然后，目标函数就可表示为  ，即是一个关于参数  的函数。为了最大化目标函数  ，可以**使用梯度上升法**，即通过计算目标函数关于参数  的梯度  来更新参数  ，如式（1）所示：

通常为了方便，会将梯度上升法转化为梯度下降法，即**通过最小化目标函数的负值**来更新参数  ，如式（2）所示：

也就是说，只要能定义出目标函数  并求出其梯度  ，就能利用梯度下降法来更新参数  ，从而使得策略  逐步逼近最优策略  。

怎么定义关于策略的目标函数  呢？

可以围绕最大化长期回报这一核心思想来展开。具体地，可以从两个角度来定义：

- 一是基于 **轨迹概率密度 **的方式
- 二是基于 **平稳分布或状态访问分布 **的方式，也叫做占用测度  推导，下面将分别介绍这两种推导方式。
## 1.1 基于轨迹推导

### 期望

回顾 MDP，aagent 观察当前状态 s 执行动作 a 获得奖励 r，不断重复形成轨迹。

每次有限步数的交互称为**回合** 

**回合最大步数**记为  或者 

把状态、动作、奖励的序列称为**轨迹** ，如 

将概率相乘，完整**轨迹概率密度计算公式，也记为 **

 

即初始状态概率 * 每一步的状态下采取动作的概率 * 转移到下个状态的概率，轨迹概率是关于 的函数，最终目标函数表示为概率密度与回报乘积的积分，实际含义是 **从 ****采样的轨迹回报期望**

### 求导

为了最大化目标函数，需要梯度上升 ，为了实际使用方便，取负转换为梯度下降 

推导：

因为概率密度函数是非常复杂的多项乘积，不好求导，所以先取 log 再求导

代入得：

因为 唯一的 项只有 ，因此 可以替换为 ，代入得：

其中回报可以有多种选择，最基础的形式是对所有步数奖励求和

也可以代入得：

## 1.2 基于占用测度推导

### 期望

回顾状态价值相关部分，设环境初始状态为  ，那么目标函数  可以表示为初始状态分布  与对应状态价值  的乘积在所有初始状态上的积分。

其中  是初始状态分布，是状态价值。状态价值指从状态  开始，智能体在策略  指导下所能获得的未来（折扣）回报的期望。

根据[贝尔曼方程](https://xcndat88znbu.feishu.cn/wiki/MPYmwYUvriWusskyprWcvRvFnfd)可知，状态价值  还可以通过动作价值函数表示

这样一来，目标函数  就可以写成关于策略  或者策略参数  的函数。

乍看初始状态分布  似乎与策略参数  无关，因此在计算梯度  时可以将其视为常数项直接提到积分号外面。

然而，**实际上初始状态分布 **** 会影响智能体后续的状态访问分布**（  ），进而影响目标函数  的值。

这里强依赖于 [马尔科夫](https://xcndat88znbu.feishu.cn/wiki/MHmCw7fFmibvWUkgUsbcG1qlnth)中关于平稳分布的概念。

**虽然初始状态分布 **** 会影响状态访问分布，但经过多次迭代后，状态访问分布会逐渐趋于平稳分布 ** 。因此，可以将目标函数  中的初始状态分布  替换为平稳分布  ，即表示为平稳分布与对应状态价值的乘积和。

期望形式：

同样地，只要能求出目标函数的梯度，就能利用梯度上升法更新参数  ，从而使得策略  逐步逼近最优，即使得目标函数的值最大化。

### 求导

## 1.3 推导等价性

# 二、通用表达式及建模

# 三、REINFORCE 算法

## 相关

- [[AI/LLM/RL/PPO/PPO 原理|PPO 原理]]
- [[AI/LLM/RL/GRPO/GRPO 深度理解|GRPO 深度理解]]
- [[AI/Foundations/Math/连续优化|连续优化]]
