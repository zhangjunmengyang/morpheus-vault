---
title: "1. RL & LLMs"
type: tutorial
domain: ai/llm/rl/fundamentals
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm/rl/fundamentals
  - type/tutorial
---
# 1. RL & LLMs

## What is Reinforcement Learning (RL)?

强化学习是通过反复试验的过程进行的：

Step

Process

Description

1. Observation 1. 观察

The agent observes the environment
代理观察环境

The agent takes in information about its current state and surroundings
代理接收其当前状态和周围环境的信息

2. Action 2. 动作

The agent takes an action based on its current policy
代理根据当前策略采取行动

Using its learned strategy (policy), the agent decides what to do next
使用其学习到的策略（策略），智能体决定接下来该做什么

3. Feedback 反馈

The environment gives the agent a reward
环境会给代理一个奖励

The agent receives feedback on how good or bad its action was
代理会收到对其行为好坏的反馈

4. Learning 学习 -

The agent updates its policy based on the reward
代理根据奖励更新其策略

The agent adjusts its strategy - reinforcing actions that led to high rewards and avoiding those that led low rewards
代理调整其策略——强化导致高奖励的行为，避免导致低奖励的行为

5. Iteration 第 5 次迭代

Repeat the process 重复此过程

This cycle continues, allowing the agent to continuously improve its decision-making
这个循环会持续进行，使智能体能够不断改进其决策能力

那么，为什么强化学习对大型语言模型如此重要呢？

训练非常优秀的LLMs是很有挑战性的。我们可以用互联网上的大量文本对其进行训练，使它们在句子中预测下一个单词的能力变得非常出色。这就是它们如何学习生成流畅且语法正确的文本的原因，正如我们在第二章中所学的那样。

然而，仅仅流利是不够的。我们希望我们的LLMs不仅能很好地串联词语，还希望他们能够做到：

- **Helpful**: Provide useful and relevant information.
有益的：提供有用且相关的信息。
- **Harmless**: Avoid generating toxic, biased, or harmful content.
无害：避免生成有毒、有偏见或有害的内容。
- **Aligned with Human Preferences**: Respond in ways that humans find natural, helpful, and engaging.
与人类偏好一致：以人类觉得自然、有帮助且引人入胜的方式回应。
预训练 LLM 方法，这些方法大多依赖于从文本数据中预测下一个单词，有时在这些方面表现不佳。

SFT在生成结构化输出方面非常出色，但在生成有益、无害且对齐的响应方面可能效果较差。我们在第 11 章探讨监督训练。

微调后的模型可能会生成流畅且结构化的文本，但这些文本仍然可能是事实错误的、带有偏见的，或者根本不能以有益的方式回答用户的问题。

进入强化学习！RL 为我们提供了一种方法，可以对这些预训练的LLMs进行微调，以更好地实现这些期望的品质。

## Reinforcement Learning from Human Feedback (RLHF)

一种非常流行的使语言模型与人类价值观对齐的技术是人类反馈的强化学习（RLHF）。在 RLHF 中，我们使用人类反馈作为 RL 中的“奖励”信号的代理。这是如何运作的：

1. Get Human Preferences: We might ask humans to compare different responses generated by the LLM for the same input prompt and tell us which response they prefer. For example, we might show a human two different answers to the question “What is the capital of France?” and ask them “Which answer is better?“.
获取人类偏好：我们可能会请人类比较同一个输入提示下由LLM生成的不同回答，并告诉我们他们更喜欢哪个回答。例如，我们可能会向人类展示两个不同的关于“法国的首都是什么？”问题的答案，并询问他们“哪个答案更好？”。
1. Train a Reward Model: We use this human preference data to train a separate model called a reward model. This reward model learns to predict what kind of responses humans will prefer. It learns to score responses based on helpfulness, harmlessness, and alignment with human preferences.
训练奖励模型：我们使用这些人类偏好数据来训练一个称为奖励模型的单独模型。这个奖励模型学会了预测人类会偏好哪种回答。它学会了根据有用性、无害性和与人类偏好的一致性来评分回答。
1. Fine-tune the LLM with RL: Now we use the reward model as the environment for our LLM agent. The LLM generates responses (actions), and the reward model scores these responses (provides rewards). In essence, we’re training the LLM to produce text that our reward model (which learned from human preferences) thinks is good.
Fine-tune the LLM 通过 RL：现在我们使用奖励模型作为 LLM 代理的环境。LLM 生成响应（动作），奖励模型对这些响应进行评分（提供奖励）。本质上，我们是在训练 LLM 生成奖励模型（根据人类偏好学习）认为好的文本。
![image](assets/W9jxdE1S2oC4eZxZM1lcWhIjnXb.png)

从总体上看，让我们来看看在LLMs中使用 RL 的好处：

Benefit

Description

Improved Control  提高控制能力

RL allows us to have more control over the kind of text LLMs generate. We can guide them to produce text that is more aligned with specific goals, like being helpful, creative, or concise.
RL 使我们能够对生成文本的类型有更多控制。我们可以引导它们生成更符合特定目标的文本，比如更有帮助性、更具创意性或更简洁。

Enhanced Alignment with Human Values
更好的与人类价值观对齐

RLHF, in particular, helps us align LLMs with complex and often subjective human preferences. It’s hard to write down rules for “what makes a good answer,” but humans can easily judge and compare responses. RLHF lets the model learn from these human judgments.
特别是 RLHF 帮助我们使模型与复杂的、往往具有主观性的人类偏好对齐。很难用规则来定义“什么是好的答案”，但人类很容易判断和比较不同的回答。RLHF 让模型从这些人类的判断中学习。

Mitigating Undesirable Behaviors
缓解不良行为

RL can be used to reduce negative behaviors in LLMs, such as generating toxic language, spreading misinformation, or exhibiting biases. By designing rewards that penalize these behaviors, we can nudge the model to avoid them.
强化学习可以用于减少LLMs中的负面行为，例如生成有毒语言、传播虚假信息或表现出偏见。通过设计惩罚这些行为的奖励，可以使模型避免这些行为。

强化学习从人类反馈中被用于训练当今许多最受欢迎的LLMs，例如 OpenAI 的 GPT-4、Google 的 Gemini 和 DeepSeek 的 R1。强化学习从人类反馈中存在多种技术，复杂性和精巧程度各不相同。在本章中，我们将重点介绍组相对策略优化（GRPO），这是一种已被证明对训练LLMs非常有效的强化学习从人类反馈技术，这些模型是有用的、无害的，并且与人类偏好保持一致。

## Why should we care about GRPO (Group Relative Policy Optimization)?

更多的 trainer 参考：https://huggingface.co/docs/trl/main/en/ppo_trainer

有许多 RLHF 的技术，但本课程专注于 GRPO，因为它在语言模型的强化学习方面代表了重要进步。

让我们简要考虑一下其他两种流行的 RLHF 技术：

- Proximal Policy Optimization (PPO)
代理策略优化（PPO）
- Direct Preference Optimization (DPO)
直接偏好优化（DPO）
代理策略优化（PPO）是最早的一种高效的强化学习人类反馈技术之一。它使用策略梯度方法根据来自单独的奖励模型的奖励来更新策略。

直接偏好优化（DPO）后来被开发出来，作为一种更简单的技术，它不需要单独的奖励模型，而是直接使用偏好数据。本质上，将问题重新定义为选择响应和拒绝响应之间的分类任务。

与 DPO 和 PPO 不同，GRPO 将相似的样本分组在一起并作为一组进行比较。基于组的方法提供了与其他方法相比更稳定的梯度和更好的收敛特性。

GRPO 不使用像 DPO 那样的偏好数据，而是使用模型或函数提供的奖励信号来比较相似样本组。

GRPO 在获取奖励信号方面具有灵活性 - 它可以使用奖励模型（就像 PPO 做的那样），但并不严格需要一个。这是因为 GRPO 可以从任何可以评估响应质量的函数或模型中获取奖励信号。

例如，我们可以使用长度函数来奖励较短的响应，使用数学求解器来验证解的正确性，或者使用事实正确性函数来奖励更符合事实的响应。这种灵活性使 GRPO 在不同的对齐任务中特别具有通用性。

---

## See Also

- [[AI/LLM/RL/Fundamentals/RL 概览|RL 概览]] — 同方向伴侣笔记，概念互补
- [[AI/LLM/RL/GRPO/GRPO 深度理解|GRPO 深度理解]] — RL for LLMs 的主流算法
- [[AI/LLM/RL/_MOC|RL MOC]] — LLM 强化学习全图谱
- [[AI/Foundations/ML-Basics/机器学习|机器学习]] — RL 的 ML 基础
