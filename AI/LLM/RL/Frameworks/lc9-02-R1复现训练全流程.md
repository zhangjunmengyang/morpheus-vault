# R1 复现训练全流程

> 基于 MA-RLHF 仓库 `r1/` 目录整理，使用 HuggingFace TRL + DeepSpeed 复现 R1-like 训练流水线。
> 来源：`sft_test.py`, `sft.py`(ma-rlhf), `grpo.py`(0字节占位), `config.py`, `dpo.py`, `README.md`, `deepspeed_zero1.yaml`
> **注意**：`r1/grpo.py`, `r1/sft.py`, `r1/config.py` 为 0 字节占位文件（待实现），实际代码参考 `r1/sft_test.py` 和 `ma-rlhf/sft.py`，verl 目录下的 GRPO 实现见姊妹笔记。

---

## 1. R1 复现的完整流程

### 1.1 训练流水线

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│  Stage 0        │    │  Stage 1         │    │  Stage 2        │
│  (可选) CPT     │───▶│  SFT Cold Start  │───▶│  GRPO RLVR      │
│  领域适配       │    │  格式对齐        │    │  推理能力涌现   │
└─────────────────┘    └──────────────────┘    └─────────────────┘
        │                      │                       │
   Base Model            SFT Model              RL Model
   + 领域知识          + 对话格式              + 推理链 (CoT)
                       + 基础指令跟随          + 自我验证
```

### 1.2 各阶段目标

| 阶段 | 目标 | Loss 类型 | 数据 |
|------|------|-----------|------|
| CPT | 注入领域知识 | CLM（全序列） | 领域文本 |
| SFT Cold Start | 学会对话格式 + 基础推理 | Masked CLM（只算 response） | instruction-response 对 |
| GRPO RLVR | 涌现深度推理能力 | RL Policy Gradient + KL | 数学题 + 规则奖励 |

### 1.3 仓库支持的完整能力

根据 README.md：
1. Dense/MoE Base Model + HuggingFace Transformers/PEFT/TRL
2. DeepSpeed Multi-GPU ZeRO-1/2/3 训练
3. DPO 训练 (TRL)
4. **GRPO RLVR 训练 (verl)**
5. Agentic-RL Case

---

## 2. SFT Cold Start 阶段

### 2.1 代码注解（`sft_test.py` — 简化版 SFT）

```python
# === sft_test.py 完整注解 ===
# 硬件：3090×2，17.42 分钟训练 1 epoch alpaca 数据集
# 启动：deepspeed sft_test.py 或 accelerate launch --num_processes=2 sft_test.py

from trl import SFTTrainer, SFTConfig
from peft import LoraConfig

# 系统提示词——定义模型角色
DEFINIED_SYSTEM_PROMPT = '你是小冬瓜智能体,请安全详细回答用户 USER 的问题'

def create_model(name):
    """QLoRA 量化加载模型"""
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,               # 4-bit 量化
        bnb_4bit_quant_type="nf4",       # NormalFloat4 量化类型
        bnb_4bit_compute_dtype=torch.bfloat16  # 计算用 bf16
    )
    model = AutoModelForCausalLM.from_pretrained(
        name,
        quantization_config=bnb_config,   # QLoRA 量化配置
        device_map='auto',
        trust_remote_code=True,
        dtype=torch.bfloat16,
        use_cache=False,                  # 训练时必须关闭 KV cache
    )
    return model, tokenizer

def get_lora_config():
    """LoRA 配置——注意 target_modules 包含 lm_head"""
    peft_config = LoraConfig(
        r=64,                             # LoRA rank
        lora_alpha=8,                     # 缩放系数（alpha/r = 0.125）
        bias="none",                      # 不训练 bias
        task_type="CAUSAL_LM",
        target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'lm_head'],
        # ↑ 对 lm_head 也加 LoRA，这在学习新 token 时很重要
    )
    return peft_config

def get_dataset_alpaca():
    """将 Alpaca 数据转为 TRL 的 messages 格式"""
    dataset = load_dataset('tatsu-lab/alpaca')
    def map_cat_inst_input(example):
        # TRL SFT 要求 messages 格式（Chat Template 自动处理）
        example['messages'] = [
            {'role': 'system', 'content': DEFINIED_SYSTEM_PROMPT},
            {'role': 'user', 'content': example['instruction'] + example['input']},
            {'role': 'assistant', 'content': example['output']},
        ]
        return example
    return dataset.map(map_cat_inst_input, ...)

# TRL SFTConfig 训练配置
config = SFTConfig(
    output_dir=output_name,
    per_device_train_batch_size=16,
    gradient_accumulation_steps=4,       # 有效 batch = 16 × 4 × 2GPU = 128
    max_length=512,
    num_train_epochs=1,
    bf16=True,
    deepspeed='./ds.json',               # DeepSpeed ZeRO-1
)
```

### 2.2 数据格式：Chat Template

TRL 的 SFT 使用 **messages 格式**，由 tokenizer 的 `apply_chat_template` 自动转换：

```python
# 原始 messages 格式
messages = [
    {"role": "system", "content": "你是小冬瓜智能体..."},
    {"role": "user", "content": "简述强化学习PPO算法"},
    {"role": "assistant", "content": "PPO 是一种..."}
]

# Qwen3 Chat Template 转换结果（示意）
# <|im_start|>system\n你是小冬瓜智能体...<|im_end|>
# <|im_start|>user\n简述强化学习PPO算法<|im_end|>
# <|im_start|>assistant\nPPO 是一种...<|im_end|>
```

### 2.3 Loss Mask 机制

SFT 的关键：**只对 response 部分计算 loss**，prompt 部分做 mask。

`ma-rlhf/sft.py` 使用 TRL 的 `DataCollatorForCompletionOnlyLM`：

```python
# 定义 response 开始标记
response_template = "###Answer:"
response_template_id = tokenizer.encode(response_template, add_special_tokens=False)[1:]

# Collator 会将 response_template 之前的所有 token 的 label 设为 -100
collator = DataCollatorForCompletionOnlyLM(response_template_id, tokenizer=tokenizer)
# -100 是 CrossEntropyLoss 的 ignore_index，不参与梯度计算
```

### 2.4 LoRA 配置详解

```python
# 标准 SFT LoRA
LoraConfig(
    r=64,               # rank=64，较大的 rank 拟合能力更强
    lora_alpha=8,        # 实际缩放 = alpha/r = 0.125
    bias="none",         # 不训练 bias，节省参数
    task_type="CAUSAL_LM",
    # target_modules 默认只有 q_proj, k_proj
    # 扩展到 v_proj, o_proj, lm_head 可以提升效果
)
```

| LoRA 参数 | 推荐值 | 说明 |
|-----------|--------|------|
| `r` | 32~64 | 小模型用 64，大模型可用 32 |
| `lora_alpha` | 8~16 | alpha/r 控制 LoRA 更新的幅度 |
| `target_modules` | 至少 q/k/v_proj | 加 lm_head 对新 token 学习很关键 |
| `lora_dropout` | 0~0.05 | 小数据集可加 dropout 防过拟合 |

---

## 3. GRPO RLVR 阶段

### 3.1 整体架构

R1 的 GRPO 训练在 verl 框架下完成（详见姊妹笔记 `verl-GRPO-实战指南.md`），核心流程：

```
For each training step:
  1. 从数据集采样 batch_size 个 prompt
  2. 对每个 prompt，用 sglang 采样 n=8 个 response
  3. 用规则 reward 给每个 response 打分（0 或 1）
  4. 组内归一化得到 advantage：A_i = (r_i - mean) / std
  5. 用 Policy Gradient + KL loss 更新 Actor
  6. 重复直到收敛
```

### 3.2 Reward Function 设计

R1 场景使用 **RLVR**（RL with Verifiable Reward）：

```python
# 规则奖励的核心：答案是否正确
# GSM8K 格式
def check_gsm8k(response, ground_truth):
    # 从 response 中提取 "####" 后的数字
    match = re.search(r"#### (\-?[\d\.,]+)", response)
    if match:
        answer = match.group(1).replace(",", "")
        return float(answer == ground_truth)  # 正确=1, 错误=0
    return 0.0

# MATH 格式（LaTeX \boxed{}）
def check_math(response, ground_truth):
    match = re.search(r'\\boxed{((?:[^{}]|\{[^{}]*\})*)}', response)
    if match:
        return float(match.group(1) == ground_truth)
    return 0.0
```

### 3.3 关键超参

| 参数 | 值 | 说明 |
|------|-----|------|
| `algorithm.adv_estimator` | `grpo` | GRPO 算法 |
| `rollout.n` | 8 | 每 prompt 采样 8 个 |
| `actor.kl_loss_coef` | 0.001 | KL 约束系数（小值允许更多探索） |
| `actor.kl_loss_type` | `low_var_kl` | 低方差 KL 估计 |
| `actor.optim.lr` | 1e-6 | RL 阶段用极小学习率 |
| `algorithm.use_kl_in_reward` | False | KL 不加在 reward 中，而在 loss 中 |

---

## 4. DeepSpeed ZeRO 配置

### 4.1 `deepspeed_zero1.yaml`（SFT 阶段用）

```yaml
compute_environment: LOCAL_MACHINE
debug: false
deepspeed_config:
  deepspeed_multinode_launcher: standard
  gradient_accumulation_steps: 1      # 梯度累积（通常在 Trainer 中设置）
  zero3_init_flag: false              # 不使用 ZeRO-3 初始化
  zero_stage: 1                       # ZeRO Stage 1：只分片 Optimizer States
distributed_type: DEEPSPEED
downcast_bf16: 'no'
machine_rank: 0
main_training_function: main
mixed_precision: 'bf16'               # BF16 混合精度
num_machines: 1
num_processes: 8                      # 8 GPU
rdzv_backend: static
same_network: true
use_cpu: false
```

### 4.2 `ds.json`（简化版 DeepSpeed 配置）

```json
{
  "train_micro_batch_size_per_gpu": "auto",  // 自动匹配 Trainer 设置
  "zero_optimization": {
    "stage": 1                               // ZeRO-1: 分片 optimizer states
  }
}
```

### 4.3 ZeRO Stage 选择指南

| Stage | 分片内容 | 显存节省 | 通信开销 | 适用场景 |
|-------|----------|----------|----------|----------|
| ZeRO-1 | Optimizer States | ~4x | 最低 | **SFT + LoRA（推荐）** |
| ZeRO-2 | + Gradients | ~8x | 中等 | 全参微调 |
| ZeRO-3 | + Parameters | ~N×（N=GPU数） | 最高 | 超大模型推理+训练 |

**R1 场景建议**：SFT 阶段用 ZeRO-1 + LoRA（轻量），GRPO 阶段由 verl 的 Megatron 处理。

---

## 5. 支持的模型

### 5.1 已验证模型

| 模型 | 类型 | 参数量 | SFT 支持 | GRPO 支持 |
|------|------|--------|----------|-----------|
| Qwen3-0.6B | Dense | 0.6B | ✅ | ✅ (3090×2) |
| Qwen3-8B | Dense | 8B | ✅ | ✅ (4090×4) |
| Llama-3-8B | Dense | 8B | ✅ | ✅ |
| Llama-3-70B | Dense | 70B | ✅ (ZeRO-3) | - |

### 5.2 Dense vs MoE

仓库声明支持 MoE（如 DeepSeek-V3/R1 架构），但需注意：
- MoE 模型的 TP 切分更复杂（expert parallel）
- verl 的 Megatron 后端支持 `expert_model_parallel_size`
- LoRA 适配 MoE 时需指定 `target_modules` 覆盖 expert 层

---

## 6. 推理能力涌现的机制

### 6.1 为什么 RLVR 能让模型"学会思考"？

**SFT 的局限**：
- SFT 通过模仿学习，模型只能复制训练数据中的推理模式
- 如果训练数据中没有 CoT（链式思考），模型不会自发产生 CoT
- SFT 优化的是"对每个 token 的预测"，不关心最终答案是否正确

**RLVR 的突破**：
- RL 只看**最终答案是否正确**（outcome-based reward）
- 模型自由探索：可以尝试任何推理路径，只要最终答案对就得正奖励
- **关键涌现**：模型发现"写出中间步骤 → 更容易得到正确答案 → 更高 reward"
- 这就是为什么 R1-Zero 能在没有 CoT 数据的情况下**自发学会 Chain-of-Thought**

### 6.2 涌现的具体表现

1. **思考链变长**：随着训练，模型输出中自发出现 `<think>...</think>` 标签
2. **自我验证**：模型学会"让我检查一下"的模式
3. **回溯修正**：模型学会"等等，这一步有问题"然后重新计算
4. **格式自组织**：模型自发使用 `####` 或 `\boxed{}` 标记最终答案（因为规则奖励需要）

### 6.3 SFT Cold Start 的作用

虽然 R1-Zero 证明可以直接从 Base Model 做 RL，但 SFT Cold Start 能：
1. **对齐格式**：让模型学会 Chat 格式，避免 RL 阶段浪费探索预算在格式学习上
2. **稳定训练**：Base Model 的输出分布太散，RL 容易 reward hacking
3. **加速收敛**：SFT 后的模型已有基本推理能力，RL 只需"强化"而非"从零学习"

---

## 7. 面试考点

### Q1: SFT Cold Start 的必要性？直接从 Base Model 做 RL 行不行？

**参考答案**：

可以但不推荐。DeepSeek-R1-Zero 证明了直接从 Base Model 做 RL 也能涌现推理能力，但有以下问题：
1. **可读性差**：R1-Zero 的输出混合多种语言、格式混乱
2. **训练不稳**：Base Model 的初始策略质量低，RL 探索效率差
3. **收敛慢**：需要更多训练步数

SFT Cold Start 的价值：
- 用少量高质量 CoT 数据（如 1K~10K 条）做 SFT
- 让模型学会基本格式和初步推理
- RL 阶段在此基础上"涌现"更深层的推理能力
- 实际工程中几乎都用 SFT → RL 两阶段方案

### Q2: GRPO 比 PPO 在 R1 场景的优势？

**参考答案**：

| 维度 | PPO | GRPO |
|------|-----|------|
| Critic | 需要训练 Value Network（参数量≈Actor） | **无需 Critic** |
| 显存 | Critic 占大量显存 | 节省 ~50% 显存 |
| Baseline | GAE 通过 Critic 估计 | 同一 prompt 多次采样的均值 |
| 实现复杂度 | Critic 的训练稳定性是难点 | 只需保证采样数量足够 |
| 适用条件 | 通用（连续/离散 reward） | **二元/稀疏 reward 效果最好** |

R1 场景用 GRPO 的原因：
1. 数学推理的 reward 是二元的（对/错），GRPO 的组内归一化天然适配
2. 省去 Critic 后显存可以给更大的 batch 或更多采样（n=8）
3. 不用担心 Critic 拟合质量——数学 reward 无噪声

### Q3: SFT 中 Loss Mask 为什么重要？不做 mask 会怎样？

**参考答案**：

Loss Mask 指在 SFT 时只对 response 部分计算 loss，prompt 部分设为 `-100`（ignore）。

**为什么需要**：
- 如果不做 mask，模型会把梯度分配到"记住 prompt"上
- 这导致模型过拟合到特定 prompt 模式，泛化能力下降
- 极端情况下模型会"复读"prompt 而不是生成 response

**不做 mask 的后果**：
- 模型倾向于输出训练数据中常见的 prompt 片段
- 在新 prompt 上表现差
- 训练 loss 虚低（prompt 是确定性文本，很容易"预测"）

**TRL 的实现**：`DataCollatorForCompletionOnlyLM` 通过 `response_template` 定位 response 起始位置，将之前所有 token 的 label 设为 -100。

### Q4: DeepSpeed ZeRO-1/2/3 分别在什么场景下使用？

**参考答案**：

**ZeRO-1**（分片 Optimizer States）：
- 显存节省最小但通信开销最低
- 适用于 **LoRA + 中小模型**（如 7B + LoRA）
- 本项目 SFT 阶段使用

**ZeRO-2**（+ 分片 Gradients）：
- 适用于 **全参微调 7B~13B**
- 或不用 LoRA 的 SFT

**ZeRO-3**（+ 分片 Parameters）：
- 适用于 **70B+ 超大模型**
- 代价是每次 forward/backward 都需要 all-gather 参数
- 可配合 CPU offload 使用更大模型，但速度慢

**选择原则**：
- 能用 ZeRO-1 就不用 2，能用 2 就不用 3
- LoRA 训练通常 ZeRO-1 足够（可训练参数少）
- 全参数微调根据模型大小选择

---

## 附录

### A. 推理验证代码（`generate.py`）

```python
# 用训练好的 SFT 模型做推理
model = AutoModelForCausalLM.from_pretrained(
    './output/qwen3_sft/checkpoint-407',
    dtype=torch.bfloat16,
    device_map='cuda:0'
)
messages = [
    {'role': 'system', 'content': '你是小冬瓜智能体...'},
    {'role': 'user', 'content': '简述强化学习PPO算法'},
]
# apply_chat_template 自动使用模型的 Chat 格式
prompt = tokenizer.apply_chat_template([messages], tokenize=False, add_generation_prompt=True)
output = model.generate(inputs['input_ids'], max_new_tokens=512, do_sample=True, temperature=1.0)
# skip_special_tokens=False 用于调试——看模型是否正确使用特殊 token
print(tokenizer.decode(output[0], skip_special_tokens=False))
```

### B. 文件路径速查

| 文件 | 状态 | 作用 |
|------|------|------|
| `r1/sft_test.py` | ✅ 完整 | 简化版 SFT（QLoRA + Alpaca） |
| `r1/sft.py` | ⬜ 空文件 | SFT 占位（待实现） |
| `r1/grpo.py` | ⬜ 空文件 | GRPO 占位（实际用 verl） |
| `r1/config.py` | ⬜ 空文件 | 配置占位 |
| `r1/dpo.py` | ⬜ 空文件 | DPO 占位 |
| `r1/deepspeed_zero1.yaml` | ✅ 完整 | Accelerate + DeepSpeed ZeRO-1 配置 |
| `r1/ds.json` | ✅ 完整 | 简化版 DeepSpeed 配置 |
| `r1/generate.py` | ✅ 完整 | SFT 模型推理验证 |
| `r1/benchmark.py` | ⬜ 空文件 | 评测占位 |
| `ma-rlhf/sft.py` | ✅ 完整 | 完整版 SFT（含 Loss Mask） |
| `ma-rlhf/ppo.py` | ✅ 完整 | TRL PPO 训练 |
