---
brief: "GRPO æ”¹è¿›å…¨æ™¯ï¼šä¸ƒç»´æ¡†æ¶ï¼ˆä¼˜åŠ¿ä¼°è®¡/å¥–åŠ±å¡‘å½¢/KLçº¦æŸ/é•¿åº¦è§„èŒƒ/ç†µæ§åˆ¶/å¤šç›®æ ‡/Diversityï¼‰ç³»ç»Ÿæ¢³ç† 2026 å¹´ GRPO è¡ç”Ÿç®—æ³•ï¼›DAPO/VAPO/Dr. GRPO/ProGRPO/MASPO/SAPO/GSPO ç­‰ 15+ å˜ç§çš„æ ¸å¿ƒæ”¹è¿›æ–¹å‘å¯¹æ¯”åˆ†æã€‚å…³é”®è¾¹ç•Œï¼šGRPO åœ¨ multi-turn åœºæ™¯æ— æ”¶æ•›ä¿è¯ï¼ˆSeeUPO ä¸å¯èƒ½å®šç†ï¼‰ï¼Œå•è½®æ¨ç†ä»æ˜¯é¦–é€‰ã€‚"
title: "GRPO æ”¹è¿›å…¨æ™¯åˆ†æï¼š2026 å¹´ä¸ƒç»´æ¡†æ¶"
type: synthesis
domain: ai/llm/rl
tags:
  - ai/llm/rl
  - grpo
  - survey
  - interview-prep
  - type/synthesis
date: 2026-02-20
updated: 2026-02-24
sources:
  - "GRPO/DeepSeekMath: arXiv:2402.03300"
  - "DAPO: arXiv:2503.14476 (ByteDance/æ¸…åï¼ŒNeurIPS 2025)"
  - "MASPO: arXiv:2602.17550 (Meituan+Fudanç­‰)"
  - "SAPO: arXiv:2511.20347 (Qwenå›¢é˜Ÿï¼ŒQwen3-VLç”Ÿäº§)"
  - "GSPO: arXiv:2507.18071 (Qwen3å›¢é˜Ÿ)"
  - "STAPO: arXiv:2602.15620 (æ¸…å+æ»´æ»´)"
  - "DEEP-GRPO: arXiv:2602.14169 (ICMLæŠ•ç¨¿)"
  - "Goldilocks RL: arXiv:2602.14868 (Apple+EPFL)"
  - "Jet-RL: arXiv:2601.14243 (MIT HAN Lab)"
  - "SeeUPO: arXiv:2602.06554 (Tongyi Lab) â€” multi-turnæ”¶æ•›è¾¹ç•Œå®šç†"
  - "IntroLLM: arXiv:2602.13035 (Diversityç»´åº¦ï¼Œhierarchicalæ¸©åº¦policy)"
  - "ProGRPO: arXiv:2602.05281"
  - "RePO: arXiv:2602.10819"
---

# GRPO æ”¹è¿›å…¨æ™¯åˆ†æï¼š2026 å¹´å…­ç»´æ¡†æ¶

**ç±»å‹**: ç»¼åˆåˆ†æ / é¢è¯•çº§å…ƒåˆ†æ  
**è¦†ç›–æ—¶é—´**: 2025-10 ~ 2026-02  
**å†™ä½œæ—¥æœŸ**: 2026-02-20  
**çŠ¶æ€**: v1ï¼ˆè¦†ç›– 6 ç¯‡æ ¸å¿ƒè®ºæ–‡ï¼ŒæŒç»­æ›´æ–°ï¼‰

---

## ä¸ºä»€ä¹ˆå†™è¿™ç¯‡

GRPOï¼ˆGroup Relative Policy Optimizationï¼‰åœ¨ DeepSeek-R1 ä¹‹åæˆä¸º RLVR çš„æ ‡å‡†ç®—æ³•ã€‚åœ¨ 2025 ç§‹å­£åˆ° 2026 å¹´åˆï¼Œå‡ºç°äº†å¤§é‡é’ˆå¯¹ GRPO å„ç§ç¼ºé™·çš„æ”¹è¿›å·¥ä½œã€‚

è¿™äº›å·¥ä½œæ•£è½åœ¨ä¸åŒè®ºæ–‡ä¸­ï¼Œæ¯ç¯‡éƒ½è¯´è‡ªå·±è§£å†³äº†"GRPO çš„é—®é¢˜"ã€‚ä½†é—®é¢˜æ˜¯â€”â€”**GRPO æœ‰å¾ˆå¤šä¸åŒå±‚æ¬¡çš„é—®é¢˜ï¼Œè¿™äº›è®ºæ–‡å®é™…ä¸Šæ˜¯åœ¨ä¿®ä¸åŒçš„æ¼æ´**ã€‚

è¿™ç¯‡ç¬”è®°çš„ç›®æ ‡ï¼šæŠŠæ‰€æœ‰æ”¹è¿›å·¥ä½œ**å®šä½åˆ°æ­£ç¡®çš„å±‚æ¬¡**ï¼Œå»ºç«‹ä¸€ä¸ªç»Ÿä¸€çš„åˆ†ææ¡†æ¶ã€‚

---

## GRPO çš„æ ¸å¿ƒå…¬å¼ï¼ˆåŸºå‡†ï¼‰

å¯¹äº query qï¼Œé‡‡æ · G ä¸ª responseï¼š

```
J(Î¸) = E[1/G Î£áµ¢ 1/|oáµ¢| Î£â‚œ min(ráµ¢,â‚œ Â· Aáµ¢,â‚œ, clip(ráµ¢,â‚œ, 1-Îµ, 1+Îµ) Â· Aáµ¢,â‚œ)]
```

å…¶ä¸­ï¼š
- ráµ¢,â‚œ = Ï€_Î¸(oáµ¢,â‚œ|q) / Ï€_Î¸_old(oáµ¢,â‚œ|q)ï¼ˆé‡è¦æ€§æ¯”ç‡ï¼‰
- Aáµ¢ = (ráµ¢ - mean(r)) / std(r)ï¼ˆç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼‰
- clip é˜²æ­¢è¿‡å¤§çš„ç­–ç•¥æ›´æ–°

**GRPO vs PPO çš„ç®€åŒ–**ï¼šç”¨ç»„å†…å‡å€¼æ›¿ä»£ critic value model â†’ å»æ‰ criticï¼Œä½†åŒæ—¶å¤±å»äº† token çº§åˆ«çš„ credit assignmentã€‚

---

## ä¸ƒç»´æ”¹è¿›æ¡†æ¶ï¼ˆv2ï¼šæ–°å¢ Diversity ç»´åº¦ï¼‰

> **v2 æ›´æ–°ï¼ˆ2026-02-21ï¼‰**ï¼šè¡¥å……ç»´åº¦ä¸ƒ"Diversity/Entropy"ï¼Œæ”¶å½• ProGRPO å’Œ RePOã€‚

### ç»´åº¦ä¸ƒï¼šDiversity å±‚ â€” å¦‚ä½•ä¿ç•™å¤šæ¡æ­£ç¡®è·¯å¾„ï¼Ÿ

**é—®é¢˜**ï¼šGRPO advantage åªçœ‹ rewardï¼Œä¸çœ‹è·¯å¾„çš„ç”Ÿæˆæ¦‚ç‡ã€‚é«˜æ¦‚ç‡çš„ã€Œä¸»æµè§£æ³•ã€æ¯æ¬¡è¢«é‡‡æ ·éƒ½å¾—æ­£ advantageï¼Œä½é¢‘ä½†åŒæ ·æ­£ç¡®çš„è·¯å¾„æ¦‚ç‡è¶Šæ¥è¶Šä½ã€‚è®­ç»ƒå pass@1 å°šå¯ï¼Œpass@k æš´è·Œâ€”â€”å¤šæ ·æ€§æ­»äº†ã€‚

**ProGRPO**ï¼ˆarXiv 2602.05281ï¼ŒPengyi Li et al.ï¼‰
å‘ç°ï¼šentropy collapse çš„æ ¹å› åœ¨ advantage æœ¬èº«ï¼Œè€Œéå¤–éƒ¨ç†µæ­£åˆ™åŒ–ä¸è¶³
è§£æ³•ï¼šARMï¼ˆAdvantage Re-weighting Mechanismï¼‰
- `c_Î¸(q)`ï¼šprompt ç½®ä¿¡åº¦ï¼ˆæ¨¡å‹å¯¹è¯¥é—®é¢˜çš„ç†Ÿæ‚‰ç¨‹åº¦ï¼‰
- `c_Î¸(o|q)`ï¼šanswer ç½®ä¿¡åº¦ï¼ˆå¯¹è¯¥æ¡è·¯å¾„çš„ç”Ÿæˆè‡ªä¿¡åº¦ï¼‰
- `Ãƒ_i = A_i + Î±(c_Î¸(q) - c_Î¸(o|q))`
- é«˜ç½®ä¿¡è·¯å¾„ï¼ˆdominant solutionï¼‰â†’ advantage æ‰“æŠ˜æ‰£ï¼›ä½é¢‘æ­£ç¡®è·¯å¾„ â†’ advantage åŠ æƒ
- åªå¯¹ä½æ¦‚ç‡ tokenï¼ˆçº¦ 20%ï¼‰åšé•¿åº¦å½’ä¸€åŒ–ï¼Œé¿å… trivial token ç¨€é‡Šä¿¡å·
æ•ˆæœï¼šPass@1 +5.7%ï¼ŒPass@32 +13.9%ï¼ˆQwen2.5-7Bï¼‰ï¼ŒCodeForces rating +180

ä¸ entropy regularization çš„åŒºåˆ«ï¼šentropy bonus æ˜¯å¤–éƒ¨å¼ºåˆ¶å¤šæ ·æ€§ï¼ŒARM æ˜¯å†…éƒ¨é‡å¡‘ advantage ç»“æ„ï¼Œæ›´ principledï¼Œä¸ç ´åæ•´ä½“ objectiveã€‚

**RePO**ï¼ˆarXiv 2602.10819ï¼ŒLinxuan Xia et al.ï¼‰
è§†è§’ï¼šä» off-policy çŸ¥è¯†åˆ©ç”¨è§’åº¦è§£ hard sample é‡‡ä¸åˆ°çš„ diversity é—®é¢˜
å‘ç°ï¼šLUFFYï¼ˆoff-policy RLï¼‰å¤±è´¥çš„æ ¹å› æ˜¯è¯è¡¨ä¸ä¸€è‡´å¯¼è‡´ importance ratio å¤±æ§
è§£æ³•ï¼šRephrasing Policy Optimization
1. è®©æ¨¡å‹è¯»æ‡‚ off-policy ä¸“å®¶è§£æ³•ï¼Œç„¶åç”¨è‡ªå·±çš„è¯é‡å†™ â†’ on-policy å…¼å®¹è½¨è¿¹
2. åªåœ¨ group å¤±è´¥ç‡ â‰¥ Ï æ—¶æ‰æ³¨å…¥é‡å†™è½¨è¿¹ï¼ˆæ›¿æ¢æœ€å·® rolloutï¼‰
3. å¯¹æ­£å¸¸é—®é¢˜ä¿æŒçº¯ on-policyï¼Œä¸æ±¡æŸ“åˆ†å¸ƒ

RePO å’Œ ProGRPO è§£å†³äº† diversity çš„ä¸¤ä¸ªä¸åŒæ¥æºï¼š
- ProGRPO â†’ å·²é‡‡åˆ°çš„æ­£ç¡®è·¯å¾„é‡Œï¼Œæ‰¶æŒä½é¢‘è·¯å¾„
- RePO â†’ é‡‡ä¸åˆ°çš„æ­£ç¡®è·¯å¾„ï¼Œé€šè¿‡çŸ¥è¯†å†…åŒ–å¼•å…¥

---

### ç»´åº¦ä¸€ï¼šToken å±‚ â€” å“ªäº› token ä¸è¯¥å­¦ï¼Ÿ

**é—®é¢˜**ï¼šåºåˆ—çº§å¥–åŠ±å‡åŒ€åˆ†é…ç»™æ‰€æœ‰ tokenï¼Œä½†æŸäº› token æ˜¯"è™šå‡ä¿¡å·æº"â€”â€”å®ƒä»¬çš„æ¢¯åº¦ç ´åè®­ç»ƒç¨³å®šæ€§ã€‚

**STAPO**ï¼ˆarXiv 2602.15620ï¼ŒTsinghua + DiDiï¼‰  
å‘ç°ï¼š0.01% çš„ token æ»¡è¶³ä¸‰ä¸ªæ¡ä»¶ï¼ˆä½æ¦‚ç‡ + ä½ç†µ + æ­£ä¼˜åŠ¿ï¼‰â†’ è¿™ç±» token å¯¹è®­ç»ƒè´¡çŒ®æç«¯æ¢¯åº¦ â†’ å¼•å‘ entropy collapse  
è§£æ³•ï¼šS2T maskï¼ˆSpurious-to-Truncateï¼‰â€”â€”æŠŠè¿™ç±» token çš„ clip æˆªæ–­ä» 1+Îµ æå‰åˆ° 1  
æ•ˆæœï¼š+7.13% vs GRPO on MATH benchmarksï¼›entropy ç¨³å®š

**MASPO**ï¼ˆarXiv 2602.17550 âœ…ï¼ŒMSRAï¼ŒXiaoliang Fu/Xunliang Caiï¼‰  
å‘ç°ï¼šæ­£/è´Ÿæ ·æœ¬çš„æ¦‚ç‡è´¨é‡åˆ†å¸ƒä¸å¹³è¡¡ï¼ˆæ­£æ ·æœ¬ token æ¦‚ç‡ > 0.5 å å¤šæ•°ï¼Œè´Ÿæ ·æœ¬ç›¸åï¼‰â†’ å›ºå®š clip Îµ å¯¹ä¸¤ç±»æ ·æœ¬æ•ˆæœä¸åŒ  
è§£æ³•ï¼šSoft Adaptive Trust Regionâ€”â€”æ ¹æ®æ¯ä¸ª token çš„æ¦‚ç‡è´¨é‡åŠ¨æ€è°ƒæ•´ clip èŒƒå›´  
æ•ˆæœï¼šæ¯”å›ºå®š clip GRPO æå‡ ~5%

**å…±åŒæ´å¯Ÿ**ï¼štoken çº§åˆ«çš„æ¢¯åº¦åˆ†ææ¯”åºåˆ—çº§åˆ«çš„å¥–åŠ±åˆ†ææ›´é‡è¦ã€‚GRPO æŠŠæ‰€æœ‰ token ä¸€è§†åŒä»ï¼Œæ˜¯æ ¹æœ¬ç¼ºé™·ã€‚

---

### ç»´åº¦äºŒï¼šExploration å±‚ â€” æ¢ç´¢ä»ä½•å¤„æ¥ï¼Ÿ

**é—®é¢˜**ï¼šGRPO çš„ root sampling åœ¨ç”Ÿæˆå¼€å§‹æ—¶å›ºå®šäº†èµ·ç‚¹ï¼Œpolicy åªåœ¨é«˜æ¦‚ç‡åŒºåŸŸæ¢ç´¢ â†’ åœ¨å›°éš¾é—®é¢˜ä¸Š diversity ä¸¥é‡ä¸è¶³ã€‚

**DEEP-GRPO**ï¼ˆarXiv 2602.14169ï¼ŒICML æŠ•ç¨¿ï¼‰  
å‘ç°ï¼šN=8â†’64 ä¸ªé‡‡æ ·å‡ ä¹ä¸æå‡æ€§èƒ½ï¼ˆroot é”å®šäº†æ¢ç´¢åŒºåŸŸï¼‰  
è§£æ³•ï¼šPivot-driven Resampling + Logistic Regression Recoverability ä¼°è®¡  
- æ‰¾åˆ°æ¨ç†é“¾ä¸­çš„å…³é”®å†³ç­–ç‚¹ï¼ˆpivotï¼‰
- åœ¨ pivot å¤„é‡æ–°åˆ†æ”¯ï¼Œå¼ºåˆ¶æ¢ç´¢ pivot ä¹‹åçš„åˆ†å‰è·¯å¾„
- ç”¨ Q(t) âˆ P(success|s_{<t}) Ã— (t/T)^Î³ é€‰æ‹©æœ€æœ‰ä»·å€¼çš„ pivot
æ•ˆæœï¼šavg 54.0% vs 51.4% (Dr.GRPO)

**QeRL / Jet-RL**ï¼ˆarXiv 2510.11696 / 2601.14243ï¼ŒSong Han labï¼‰  
å‘ç°çš„å‰¯ä½œç”¨ï¼šé‡åŒ–å™ªå£° â†’ policy entropy å¢åŠ  â†’ æ›´å¥½æ¢ç´¢  
è¿™ä¸æ˜¯ç›®çš„è®¾è®¡ï¼Œä½† QeRL æŠŠå®ƒç³»ç»ŸåŒ–ä¸º AQNï¼ˆAdaptive Quantization Noiseï¼‰  
æ•ˆæœï¼šreward åœ¨ 200 æ­¥å†…å¿«é€Ÿä¸Šå‡ï¼ˆvs vanilla LoRA éœ€ 500+ æ­¥ï¼‰

**IntroLLM**ï¼ˆarXiv 2602.13035ï¼ŒICML æŠ•ç¨¿ï¼Œ2026-02-13ï¼‰  
å‘ç°ï¼šæ¸©åº¦æ˜¯ RLVR æœ€è¢«å¿½è§†çš„æ§åˆ¶å˜é‡â€”â€”å›ºå®šæ¸©åº¦æ— æ³•é€‚åº” token ä½ç½®ã€prompt éš¾åº¦ã€è®­ç»ƒé˜¶æ®µçš„å˜åŒ–  
è§£æ³•ï¼šHierarchical RL â€” ä» LLM **å†…éƒ¨éšçŠ¶æ€** hâ‚œ å­¦ä¹  temperature policy Ï€Ï•(Ï„â‚œ|hâ‚œ)  
- è½»é‡ MLP headï¼ˆd/2 bottleneckï¼‰ä»æœ€åä¸€å±‚ decoder åˆ†æ”¯
- æ··åˆç¦»æ•£-è¿ç»­åŠ¨ä½œï¼šBernoulli gateï¼ˆæ˜¯å¦æ›´æ–°ï¼‰+ Beta åˆ†å¸ƒï¼ˆè¿ç»­å€¼é‡‡æ ·ï¼‰
- GRPO coordinate ascent è”åˆä¼˜åŒ– token policy Î¸ å’Œ temperature policy Ï•
æ•ˆæœï¼šä¸€è‡´ä¼˜äºå›ºå®šæ¸©åº¦å’Œå¯å‘å¼è‡ªé€‚åº”ï¼›é«˜æ¸©è‡ªç„¶åˆ†é…åˆ°æ¨ç†è½¬æŠ˜ç‚¹ï¼Œä½æ¸©åˆ°æ•°å€¼è®¡ç®—/ç­”æ¡ˆåˆæˆ  
å…³é”®æ´å¯Ÿï¼šè¿™æ˜¯ä¸‰çº§æ¢ç´¢æ§åˆ¶ç²¾åº¦çš„æœ€é«˜çº§ â€” trajectory-levelï¼ˆç²—ï¼‰â†’ token-levelï¼ˆä¸­ï¼‰â†’ **hidden-state-conditioned token-levelï¼ˆæœ€ç»†ï¼‰**

**å…±åŒæ´å¯Ÿ**ï¼šGRPO çš„æ¢ç´¢ä¾èµ–æ¸©åº¦å’Œ group sizeï¼Œè¿™æ˜¯éå¸¸ç²—ç³™çš„æœºåˆ¶ã€‚Pivot çº§åˆ«çš„ targeted explorationï¼ˆDEEP-GRPOï¼‰ã€parameter-level entropy injectionï¼ˆQeRLï¼‰ã€internal-state-conditioned temperatureï¼ˆIntroLLMï¼‰æ˜¯ä¸‰ç§ä¸åŒç²¾åº¦çš„è·¯å¾„ï¼Œå¯ä»¥ç»„åˆã€‚

---

### ç»´åº¦ä¸‰ï¼šSample å±‚ â€” ç”¨ä»€ä¹ˆæ ·çš„é¢˜è®­ï¼Ÿ

**é—®é¢˜**ï¼šGRPO å¯¹è®­ç»ƒæ ·æœ¬ä¸€è§†åŒä»ï¼Œä½†ä¸åŒéš¾åº¦çš„é¢˜å¯¹å­¦ä¹ è´¡çŒ®ä¸åŒï¼šå¤ªç®€å•ï¼ˆå…¨å¯¹ï¼‰= é›¶æ¢¯åº¦ä¿¡å·ï¼›å¤ªéš¾ï¼ˆå…¨é”™ï¼‰= ä¹Ÿæ˜¯é›¶æ¢¯åº¦ã€‚æœ€æœ‰ä»·å€¼çš„æ ·æœ¬åœ¨"èƒ½åŠ›è¾¹ç•Œ"é™„è¿‘ã€‚

**Goldilocks RL**ï¼ˆarXiv 2602.14868ï¼ŒApple + EPFLï¼‰  
æ ¸å¿ƒæ•°å­¦ï¼š`||âˆ‡L_PG|| = âˆš(p_q(1-p_q))`  
å½“ p_q â‰ˆ 0.5 æ—¶æ¢¯åº¦æœ€å¤§ï¼Œå³"åˆšå¥½èƒ½åšå¯¹ä¸€åŠ"çš„é¢˜æ¢¯åº¦æœ€ä¸°å¯Œ  
è§£æ³•ï¼šç”¨ Teacher LMï¼ˆå°è€Œå¿«ï¼‰é¢„æµ‹æ¯é“é¢˜çš„ utilityï¼Œåªé€‰æ¢¯åº¦ä¸°å¯Œçš„é¢˜è®­ç»ƒ  
Teacher LM åˆ¤æ–­ï¼šæå‰é‡‡æ ·å°‘é‡ responsesï¼Œä¼°ç®—è¯¥é¢˜å¯¹å½“å‰æ¨¡å‹çš„ pass rate  
æ•ˆæœï¼šåœ¨"Edge of Competence"æ•°æ®ä¸Šè®­ç»ƒæ¯”å…¨é‡ random æ ·æœ¬é«˜ ~15% æ•ˆç‡

**éšå«çš„ curriculum**ï¼šè¿™ä¸åªæ˜¯éš¾åº¦ç­›é€‰ï¼Œæ˜¯ adaptive curriculum learning çš„éšå¼å®ç°â€”â€”éšç€æ¨¡å‹èƒ½åŠ›æå‡ï¼Œé€‚åˆå®ƒçš„é¢˜é›†ä¹Ÿåœ¨åŠ¨æ€å˜åŒ–ã€‚

**PACED-RL**ï¼ˆarXiv 2602.12642ï¼ŒICML æŠ•ç¨¿ï¼ŒDohyung Kim ç­‰ï¼‰â€” GFlowNet æ¡†æ¶ä¸‹çš„ Sample é€‰æ‹©  
æ ¸å¿ƒå‘ç°ï¼šGFlowNet è®­ç»ƒæ—¶å¿…é¡»å­¦çš„å¯å­¦ä¹ é…åˆ†å‡½æ•° Z_Ï†(x) å®é™…ä¸Šç¼–ç äº†åœ¨çº¿å‡†ç¡®ç‡ï¼š  
`p_old(x) = Î²Â·log Z*(x) - Î²Â·D_KL(Ï€_old || Ï€_Î¸)` â†’ KL é¡¹å¯è¿‘ä¼¼å¿½ç•¥ï¼ˆå®éªŒæœ€å¤§å€¼ < 4Ã—10â»Â³ï¼‰  
`p_old(x) â‰ˆ Î²Â·log Z_Ï†(x)`  
è¿™æ„å‘³ç€ä¸­é—´éš¾åº¦é¢˜çš„ç­›é€‰å¯ä»¥**é›¶é¢å¤–å¼€é”€**åœ°åšï¼ˆä»£ä»·å·²æ‘Šå…¥ GFlowNet è®­ç»ƒï¼‰ã€‚  
ç»„ä»¶ 1ï¼šç”¨ Z_Ï† ä¼°è®¡æ¯é¢˜å‡†ç¡®ç‡ï¼Œä¼˜å…ˆé€‰ accuracyâ‰ˆ0.5 çš„é¢˜ï¼ˆä¸ Goldilocks æ®Šé€”åŒå½’ï¼ï¼‰  
ç»„ä»¶ 2ï¼šaccuracy estimation error ä¼˜å…ˆçš„ replayï¼ˆåˆ©ç”¨ GFlowNet off-policy å®¹å¿åº¦ï¼‰  
æ•ˆæœï¼šAIME pass@1 vs GRPO +29.1%ï¼Œvs FlowRL +40.0%ï¼›pass@kï¼ˆå¤šæ ·æ€§ï¼‰åŒæ ·æå‡

**Sample ç»´åº¦ä¸¤æ¡è·¯**ï¼š
```
Goldilocksï¼ˆGRPO æ¡†æ¶ï¼‰ï¼šTeacher LM â†’ utility ä¼°è®¡ â†’ ä¸­é—´éš¾åº¦  
PACED-RLï¼ˆGFlowNet æ¡†æ¶ï¼‰ï¼šZ_Ï† â†’ åœ¨çº¿å‡†ç¡®ç‡ä¼°è®¡ â†’ ä¸­é—´éš¾åº¦  
```
ä¸¤è€…ç”¨ä¸åŒæ•°å­¦å·¥å…·å‘ç°äº†**åŒä¸€ä¸ª empirical è§„å¾‹**ï¼ˆä¸­é—´éš¾åº¦æœ€æœ‰æ•ˆï¼‰ï¼Œç›¸äº’å°è¯ã€‚

**å…±åŒæ´å¯Ÿ**ï¼šæ•°æ®é€‰æ‹©æ¯”ç®—æ³•æ”¹è¿›çš„å¤©èŠ±æ¿æ›´ä½ä½†æ›´ç¨³å®šã€‚å¯¹ production ç³»ç»Ÿæ¥è¯´ï¼ŒGoldilocks/PACED-RL ç±»ä¼¼çš„ sample filter å¯èƒ½æ¯”å¤æ‚çš„ç®—æ³•æ”¹è¿›æ›´å®ç”¨ã€‚GFlowNet è·¯çº¿é¢å¤–ä¿æŒäº†è¾“å‡ºå¤šæ ·æ€§â€”â€”è¿™å¯¹ test-time scalingï¼ˆpass@kï¼‰æœ‰ç›´æ¥ä»·å€¼ã€‚

---

### ç»´åº¦å››ï¼šTrust Region å±‚ â€” clip è¯¥æ€ä¹ˆè®¾ï¼Ÿ

**é—®é¢˜**ï¼šå›ºå®šçš„ Îµï¼ˆclip èŒƒå›´ï¼‰æ˜¯ GRPO æœ€å¤§çš„è¶…å‚æ•°ä¹‹ä¸€ï¼Œä½†å¯¹ä¸åŒ token åº”è¯¥ä¸ä¸€æ ·ï¼š

- é«˜æ¦‚ç‡ tokenï¼ˆæ¨¡å‹å·²ç»å¾ˆç¡®å®šï¼‰ï¼šå° Îµï¼Œä¸éœ€è¦å¤§å¹…æ›´æ–°
- ä½æ¦‚ç‡ tokenï¼ˆæ¨¡å‹ä¸ç¡®å®šï¼‰ï¼šå¤§ Îµï¼Œå…è®¸æ›´å¤§å¹…åº¦çš„ç­–ç•¥æ›´æ–°
- è´Ÿæ ·æœ¬ token å’Œæ­£æ ·æœ¬ tokenï¼šæ¦‚ç‡è´¨é‡åˆ†å¸ƒä¸åŒï¼Œclip æ•ˆæœä¸åŒ

**MASPO** å·²åœ¨ç»´åº¦ä¸€ä¸­è®¨è®ºï¼Œä½† Trust Region è§†è§’æ›´æ¸…æ™°ï¼š  
Soft Adaptive Trust Region çš„æœ¬è´¨æ˜¯**æŠŠ clip ä»å…¨å±€è¶…å‚æ•°å˜æˆ token çº§åˆ«çš„åŠ¨æ€å‡½æ•°**ã€‚

**SAPO**ï¼ˆarXiv 2511.20347ï¼Œ2025-11-25ï¼ŒQwen å›¢é˜Ÿ Chang Gao ç­‰ï¼‰  
æ ¸å¿ƒæ´å¯Ÿï¼šhard clip çš„æ¢¯åº¦æˆªæ–­æ˜¯éè¿ç»­çš„ï¼Œå¯¼è‡´ clip å†…å¤–æ¢¯åº¦æ–­å´–ã€‚  
è§£æ³•ï¼šsigmoid è½¯é—¨æ§â€”â€”æ¢¯åº¦æƒé‡ = **sechÂ²(Ï„/2 Â· (râˆ’1))**ï¼Œåœ¨ r=1 æ—¶æ»¡æƒé‡ï¼Œéšåå·®å¹³æ»‘æŒ‡æ•°è¡°å‡ã€‚  
ä¸å¯¹ç§°æ¸©åº¦ï¼šÏ„_neg > Ï„_posï¼Œå› ä¸ºè´Ÿ advantage æ¢¯åº¦å½±å“ |V| ä¸ª unsampled tokenï¼Œæ›´ä¸ç¨³å®šã€‚  
ç†è®ºåˆ†æï¼šåœ¨ (A1) å°æ­¥é•¿ + (A2) ä½åºåˆ—å†…æ–¹å·® æ¡ä»¶ä¸‹ï¼ŒSAPO é€€åŒ–ä¸º GSPO çš„è¿ç»­ç‰ˆæœ¬ï¼ˆsechÂ² åºåˆ—é—¨æ§ï¼‰ã€‚  
ç”Ÿäº§éªŒè¯ï¼šQwen3-VL å…¨ç³»åˆ—ç”¨ SAPO è®­ç»ƒã€‚  
ä¸ GSPO å…³ç³»ï¼šåŒä¸€ Qwen å›¢é˜Ÿï¼ŒGSPO(2025-07) â†’ SAPO(2025-11)ï¼Œæ˜¯å¯¹ GSPO ç¡¬è£å‰ªçš„ç›´æ¥æ”¹è¿›ã€‚  
**å¼±ç‚¹**ï¼šåœ¨é«˜ stalenessï¼ˆN=64ï¼‰ä¸‹å´©æºƒï¼ˆ18.4%ï¼‰ï¼ŒVESPO åŒæ¡ä»¶ 58.5%â€”â€”token-level è½¯åŒ–ä»ç¼ºä¹åºåˆ—çº§ IS æ–¹å·®ç†è®ºã€‚

**ä¸ DAPO/VAPO çš„å…³ç³»**ï¼š
- DAPOï¼ˆByteDanceï¼‰ï¼šæé«˜ clip ä¸Šç•Œé˜² entropy collapse + token-level loss
- VAPOï¼ˆBytedanceï¼‰ï¼šVariance-Aware ä¼˜åŠ¿ä¼°è®¡ï¼Œå¯¹é«˜æ–¹å·® token ä¿å®ˆæ›´æ–°
- MASPOï¼šProbability-Mass Aware trust region
- **SAPO**ï¼šsechÂ² è½¯è¡°å‡ï¼Œè¿ç»­ä¿¡ä»»åŸŸ

å››è€…éƒ½åœ¨è§£å†³å›ºå®š Îµ çš„é—®é¢˜ï¼Œä½†åˆ‡å…¥è§’åº¦ä¸åŒï¼ˆhyper / variance / mass / **softness**ï¼‰ã€‚

---

### ç»´åº¦äº”ï¼šOff-Policy å±‚ â€” è®­ç»ƒ-æ¨ç†ç²¾åº¦ä¸€è‡´æ€§

**é—®é¢˜**ï¼šRL è®­ç»ƒè¦æ±‚ rollout å’Œ logit evaluation ä½¿ç”¨**ç›¸åŒ**çš„ç­–ç•¥ã€‚ä½†å½“ rollout ç”¨ä½ç²¾åº¦ï¼ˆFP8ï¼‰è€Œ evaluation ç”¨é«˜ç²¾åº¦ï¼ˆBF16ï¼‰æ—¶ï¼Œä¸¤è€…å®é™…ä¸Šæ˜¯ä¸åŒçš„ç­–ç•¥ â†’ å¼•å…¥ off-policy åå·®ã€‚

Off-policy æœ‰ä¸‰ä¸ªå®è·µæ¥æºï¼ˆVESPO æ˜ç¡®åˆ—ä¸¾ï¼‰ï¼š
1. mini-batch åˆ†å‰²ï¼ˆå¤§ rollout â†’ å¤šä¸ª mini-batch é¡ºåºæ›´æ–°ï¼Œåé¢çš„ batch ä½¿ç”¨è¿‡æœŸå‚æ•°ï¼‰
2. å¼‚æ­¥è®­ç»ƒï¼ˆrollout å’Œ training å¹¶è¡Œï¼Œrollout æ°¸è¿œè½åï¼‰
3. train-inference mismatchï¼ˆFSDP/Megatron vs vLLM/SGLang å®ç°ä¸åŒï¼ŒMoE è·¯ç”±å·®å¼‚æ”¾å¤§ï¼‰

**Jet-RL**ï¼ˆarXiv 2601.14243ï¼ŒSong Han labï¼‰ï¼š**æ¶ˆé™¤** off-policy æ¥æº  
ç»Ÿä¸€ rollout å’Œ training forward pass åˆ° FP8ï¼Œæ¶ˆé™¤ç²¾åº¦å·®  
æ ¸å¿ƒï¼šG_infer âŠ† G_train_fwd â†’ rollout æ˜¯è®­ç»ƒå‰å‘å›¾å­é›†ï¼Œä¸å¼•å…¥åå·®  
æ•ˆæœï¼šrollout +33%ï¼Œtraining +41%ï¼ŒE2E +16%ï¼›ç²¾åº¦æŸå¤± <1%

**Stable Asynchrony / VCPO**ï¼ˆSong Han labï¼Œ2/19 æäº¤ï¼ŒID æœªç¡®è®¤ï¼‰ï¼š**é€‚åº”** off-policy  
ESS-based Learning Rate Scalingï¼šæ ¹æ®æœ‰æ•ˆæ ·æœ¬é‡åŠ¨æ€è°ƒæ•´ LR  
Closed-form minimum-variance baseline è¿›ä¸€æ­¥ç¨³å®šè®­ç»ƒ  

**VESPO**ï¼ˆarXiv 2602.10693ï¼Œ2/11ï¼‰ï¼š**çº æ­£** off-policyï¼ˆç®—æ³•å±‚é¢ï¼‰  
æ ¸å¿ƒæ´å¯Ÿï¼šæ‰€æœ‰ importance weight reshapingï¼ˆGRPO clipã€GSPO é•¿åº¦å½’ä¸€åŒ–ï¼‰éƒ½éšå¼å®šä¹‰äº†ä¸€ä¸ª proposal distribution Q  
å˜åˆ†æ¨å¯¼ï¼šmin KL(Qâ€–Î¼) + KL(Qâ€–Ï€) s.t. E_Q[W] â‰¤ C â†’ é—­åˆå½¢å¼ kernelï¼š  
```
Ï•(W) = W^Î± Â· exp(-Î»W)
```
ç»“æœï¼šN=64 staleness ä¸‹ avg=58.5%ï¼ˆGRPO 44.7%ï¼ŒSAPO 18.4% collapseï¼‰  
å…¨å¼‚æ­¥è®­ç»ƒä¸‹å”¯ä¸€ç¨³å®šçš„æ–¹æ³•  

**å››ç§æ–¹æ³•çš„ staleness å¯¹æ¯”ï¼ˆVESPO è®ºæ–‡æ•°æ®ï¼‰**ï¼š

| æ–¹æ³• | N=16 staleness | N=64 staleness |
|------|--------------|--------------|
| GRPO | ~57% | ~44.7% |
| SAPO | ~52% | **~18.4%ï¼ˆå´©æºƒï¼‰** |
| VCPO (est.) | ç¨³å®š | ç¨³å®š |
| VESPO | ~58% | **~58.5%ï¼ˆç¨³å®šï¼‰** |

**OAPL**ï¼ˆarXiv 2602.19362ï¼Œ2/25ï¼ŒCornell+Databricks+Harvardï¼‰ï¼š**æ”¾å¼ƒ ISï¼Œä»ç†è®ºå‡ºå‘**
æ ¸å¿ƒæ´å¯Ÿï¼šä¸å…¶ç”¨ IS æŠŠ off-policy æ•°æ®ä¼ªè£…æˆ on-policyï¼Œä¸å¦‚ç›´æ¥ä» KL-regularized RL closed-form è§£æ¨å¯¼ä¸€ä¸ªåŸç”Ÿ off-policy ç®—æ³•ã€‚
æ¨å¯¼ï¼š$\max_\pi E[r] - \beta \text{KL}(\pi \| \pi_{vllm})$ â†’ closed-form $\pi^* \propto \pi_{vllm} \cdot e^{r/\beta}$ â†’ æœ€ä¼˜åŒ–ç›®æ ‡ç­‰ä»· squared regression loss
æ•ˆæœï¼šå…è®¸ 400 æ­¥ policy lagï¼›AIME25/HMMT25/BRUMO25 è¶…è¶Š GRPO+ISï¼›LiveCodeBench v5 ç”¨ 1/3 ç”Ÿæˆé‡åŒ¹é… DeepCoder
å‚è§ï¼š[[AI/LLM/RL/Other-Algorithms/OAPL-Off-Policy-RL-LLM-Reasoning|OAPL]] â˜…â˜…â˜…â˜…â˜…

**å››ç§è·¯å¾„æ­£äº¤ï¼Œå¯ç»„åˆ**ï¼š
- Jet-RLï¼ˆç³»ç»Ÿå±‚ï¼šæ¶ˆé™¤æ¥æºï¼‰
- VCPOï¼ˆä¼˜åŒ–å±‚ï¼šåŠ¨æ€é€‚åº” LRï¼‰
- VESPOï¼ˆç®—æ³•å±‚ï¼šè½¯çº æ­£ ISï¼‰
- OAPLï¼ˆç®—æ³•å±‚ï¼šæ”¾å¼ƒ ISï¼Œclosed-form squared lossï¼‰
- SAPOï¼ˆToken-level è½¯è¡°å‡ï¼Œé€‚åˆ on-policy/è¿‘ on-policy åœºæ™¯ï¼‰

**å…±åŒæ´å¯Ÿ**ï¼šoff-policy æ˜¯ RL å®ç°ä¸­æœ€éšè”½çš„ bugã€‚å¾ˆå¤šå·¥ç¨‹å›¢é˜Ÿä¸çŸ¥é“ä»–ä»¬çš„"on-policy"ç³»ç»Ÿå®é™…ä¸Šå·²ç»æ‚„æ‚„å˜æˆäº† off-policyã€‚é‡åŒ–ã€å¼‚æ­¥ã€é‡ç”¨ rolloutã€mini-batch åˆ†å‰²éƒ½æ˜¯æ¥æºã€‚GSPO çš„é•¿åº¦å½’ä¸€åŒ–å®é™…ä¸Šå¼•å…¥äº†é•¿åº¦åå·®ï¼ˆæ›´é•¿çš„åºåˆ—æ›´éš¾è¢« clip â†’ æ­£åé¦ˆ â†’ collapseï¼‰ã€‚

---

### ç»´åº¦å…­ï¼šSystem å±‚ â€” é‡åŒ–ä¸æ•ˆç‡

**é—®é¢˜**ï¼šå¤§æ¨¡å‹ RL è®­ç»ƒçš„è®¡ç®—ç“¶é¢ˆä¸»è¦åœ¨ rollout é˜¶æ®µï¼ˆå  60-80% æ—¶é—´ï¼‰ã€‚å¦‚ä½•åœ¨ä¿æŒ on-policy å‡è®¾çš„å‰æä¸‹åŠ é€Ÿï¼Ÿ

**QeRL**ï¼ˆarXiv 2510.11696ï¼ŒICLR 2026ï¼ŒSong Han lab å‰ä½œï¼‰  
è·¯å¾„ï¼šNVFP4 æƒé‡é‡åŒ– + LoRA å‚æ•°é«˜æ•ˆå¾®è°ƒ + AQN æ¢ç´¢å¢ç›Š  
æ„å¤–å‘ç°ï¼šé‡åŒ–å™ªå£°åœ¨ RL ä¸­æ˜¯æœ‰ç›Šçš„ï¼ˆå¢åŠ  entropy â†’ ä¿ƒè¿›æ¢ç´¢ï¼‰  
æ•ˆæœï¼š1.5Ã— rollout åŠ é€Ÿï¼Œå• H100 å¯è®­ 32Bï¼›æ€§èƒ½è¶…è¶Š BF16 LoRA  
é™åˆ¶ï¼šéœ€è¦ Hopper/Blackwell GPUï¼ˆH100+ï¼‰ï¼›LoRA é™åˆ¶äº†è¡¨è¾¾èƒ½åŠ›

**Jet-RL**ï¼ˆarXiv 2601.14243ï¼ŒICLR 2026ï¼ŒSong Han labï¼‰  
è·¯å¾„ï¼šç»Ÿä¸€ FP8 flowï¼ˆrollout å’Œ training åŒç²¾åº¦ï¼‰  
ä¸åŒäº QeRLï¼šå…¨å‚æ•°è€Œé LoRAï¼›å…³æ³¨ on-policy ä¿è¯è€Œéé‡åŒ–æ¢ç´¢å¢ç›Š  
æ•ˆæœï¼šE2E +16%ï¼›ç²¾åº¦æŸå¤± <1%ï¼›å…¼å®¹ VeRL/SLIME/OpenRLHF

**äº’è¡¥å…³ç³»**ï¼šQeRL è§£å†³"å•å¡èµ„æºçº¦æŸ"ï¼ŒJet-RL è§£å†³"ç²¾åº¦ä¸€è‡´æ€§"ã€‚æœªæ¥çš„å·¥ä½œå¯èƒ½æ˜¯ FP4 + on-policy ç»Ÿä¸€ï¼ˆQeRL æ€è·¯ + Jet-RL åŸåˆ™ï¼‰ã€‚

---

## ä¸ƒç»´æ€»ç»“è¡¨ï¼ˆv2ï¼‰

| ç»´åº¦ | æ ¸å¿ƒé—®é¢˜ | ä»£è¡¨è®ºæ–‡ | è§£æ³•æœ¬è´¨ | å…³é”®æ•°å­— |
|------|---------|---------|---------|---------|
| **Diversity** | å¤šæ¡æ­£ç¡®è·¯å¾„è¢«å‹æ­» | ProGRPO / RePO | æ¦‚ç‡ç½®ä¿¡åº¦é‡è°ƒ advantage / off-policy çŸ¥è¯†å†…åŒ– | Pass@32 +13.9% / hard sample åˆ©ç”¨ç‡â†‘ |
| **Token** | å“ªäº› token æœ‰æ¯’ | STAPO / MASPO | åŸºäº token å±æ€§çš„æ¢¯åº¦ mask / adaptive clip | +7.13% / +5% |
| **Exploration** | æ¢ç´¢åŒºåŸŸå¦‚ä½•æ‰©å¤§ | DEEP-GRPO / QeRL | Pivot åˆ†æ”¯é‡‡æ · / é‡åŒ–å™ªå£°å¢ entropy | avg +2.6% / reward 2.5Ã— æ›´å¿« |
| **Sample** | å“ªé“é¢˜æ¢¯åº¦æœ€ä¸°å¯Œ | Goldilocks | Teacher LM é¢„æµ‹ utilityï¼Œé€‰ edge-of-competence | ~15% æ•°æ®æ•ˆç‡æå‡ |
| **Trust Region** | clip è¯¥æ€ä¹ˆé€‚åº” | MASPO / DAPO / VAPO / **SAPO** | Probability-mass adaptive / variance-aware / **sechÂ²è½¯è¡°å‡** | +5% |
| **Off-Policy** | ç²¾åº¦/å¼‚æ­¥å¼•å…¥åå·® | Jet-RL / VCPO | ç»Ÿä¸€ flow / ESS-based LR scaling | E2E +16% / ç¨³å®šæ€§æ˜¾è‘—æå‡ |
| **System** | è®¡ç®—æ•ˆç‡ | QeRL / Jet-RL | FP4+LoRA / ç»Ÿä¸€ FP8 | å• H100 è®­ 32B / +16% E2E |

---

## æ·±å±‚ç»Ÿä¸€è§†è§’ï¼šä¸ºä»€ä¹ˆæ‰€æœ‰é—®é¢˜éƒ½æŒ‡å‘åŒä¸€æ ¹å› ï¼Ÿ

è¡¨é¢ä¸Šï¼Œå…­ä¸ªç»´åº¦æ˜¯ç‹¬ç«‹çš„ã€‚ä½†å®ƒä»¬éƒ½æœ‰ä¸€ä¸ªå…±åŒçš„æ ¹å› ï¼š

> **GRPO ç”¨åºåˆ—çº§å¥–åŠ±è®­ç»ƒ token çº§å†³ç­–ï¼Œå‡è®¾æ‰€æœ‰ token å‡åŒ€å¯äº¤æ¢ï¼ˆi.i.d.ï¼‰ï¼Œä½†å®é™…ä¸Šå®ƒä»¬é«˜åº¦å¼‚æ„ã€‚**

- Token å±‚é—®é¢˜ï¼šä¸åŒ token çš„æ¢¯åº¦å¼‚æ„ï¼Œä¸è¯¥å‡åŒ€å¯¹å¾…
- Exploration å±‚é—®é¢˜ï¼šä¸åŒä½ç½®çš„ token å¯¹äºæ¢ç´¢ä»·å€¼å¼‚æ„ï¼ˆpivot vs æ™®é€š tokenï¼‰
- Sample å±‚é—®é¢˜ï¼šä¸åŒéš¾åº¦çš„é¢˜å¯¹æ¨¡å‹çš„æ¢¯åº¦è´¡çŒ®å¼‚æ„
- Trust Region é—®é¢˜ï¼šä¸åŒæ¦‚ç‡åŒºé—´çš„ token éœ€è¦å¼‚æ„çš„ clip èŒƒå›´
- Off-Policy é—®é¢˜ï¼šrollout å’Œ evaluation çš„ token åˆ†å¸ƒæœ¬åº”åŒè´¨ï¼Œä½†è¢«å„ç§å› ç´ ç ´å
- System å±‚é—®é¢˜ï¼šrollout é˜¶æ®µçš„ token ç”Ÿæˆå’Œ training é˜¶æ®µçš„æ¢¯åº¦è®¡ç®—æœ‰å¼‚æ„çš„èµ„æºéœ€æ±‚

**çœŸæ­£çš„è§£æ³•**ï¼štoken çº§åˆ«çš„å¯†é›†å¥–åŠ±ï¼ˆdense rewardï¼‰ï¼Œè®©æ¯ä¸ª token æœ‰è‡ªå·±çš„ credit assignmentã€‚

ä½†è¿™éœ€è¦ä¸€ä¸ªå¯é çš„ token-level value modelâ€”â€”è¿™æ°å¥½æ˜¯ GRPO ä¸ºäº†å»æ‰ critic è€Œé¿å¼€çš„ä¸œè¥¿ã€‚  
**æ‚–è®º**ï¼šGRPO è¶Šæ”¹è¶Šå¤æ‚ï¼Œæœ€ç»ˆå¯èƒ½ç»•å› PPO+critic çš„è·¯ã€‚

---

## é¢è¯• FAQ

**Q: GRPO å’Œ PPO çš„æ ¸å¿ƒåŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ**  
A: GRPO ç”¨ group å†…å‡å€¼æ›¿ä»£ critic çš„ value estimationï¼Œçœæ‰äº† critic modelï¼Œä½†ä»£ä»·æ˜¯æ— æ³•åš token çº§åˆ«çš„ credit assignmentã€‚æ‰€æœ‰åŒç»„çš„ token å…±äº«åŒä¸€ä¸ª advantageã€‚

**Q: ä¸ºä»€ä¹ˆ GRPO ä¼šå¯¼è‡´ entropy collapseï¼Ÿ**  
A: å› ä¸ºæŸäº› tokenï¼ˆSTAPO ç§°ä¹‹ä¸º spurious tokensï¼‰çš„æ¢¯åº¦æå¤§â€”â€”ä½æ¦‚ç‡ã€ä½ç†µã€æ­£ä¼˜åŠ¿ä¸‰è€…å åŠ ï¼Œè¿™ç±» token çš„æ¢¯åº¦åœ¨ clip ä¹‹å‰å·²ç»å¾ˆå¤§ï¼Œè®­ç»ƒä¼šè¿‡åº¦ä¼˜åŒ–è¿™ç±» tokenï¼Œå¯¼è‡´å…¶ä»– token çš„æ¦‚ç‡è¢«å‹ç¼©ï¼Œentropy ä¸‹é™ã€‚

**Q: è¿™äº›æ”¹è¿›é‡Œï¼Œä»€ä¹ˆæœ€é€‚åˆ production ç³»ç»Ÿï¼Ÿ**  
A: ä¼˜å…ˆçº§ï¼š  
1. **Goldilocksï¼ˆæ ·æœ¬ç­›é€‰ï¼‰**ï¼šç‹¬ç«‹äºç®—æ³•ï¼Œä½æˆæœ¬é«˜æ”¶ç›Š
2. **STAPOï¼ˆS2T maskï¼‰**ï¼šå•è¡Œä»£ç ä¿®æ”¹ï¼Œç¨³å®šæ€§æå‡æ˜¾è‘—
3. **Jet-RLï¼ˆç»Ÿä¸€ç²¾åº¦ï¼‰**ï¼šå¦‚æœç”¨ FP8ï¼Œå¿…é¡»ç»Ÿä¸€ï¼Œå¦åˆ™æœ‰éšæ€§ off-policy
4. **QeRLï¼ˆé‡åŒ–åŠ é€Ÿï¼‰**ï¼šèµ„æºå—é™æ—¶é¦–é€‰
5. **DEEP-GRPOï¼ˆPivot resamplingï¼‰**ï¼šå›°éš¾é—®é¢˜ä¸Šæœ‰æ˜¾è‘—æ”¶ç›Šï¼Œä½†å®ç°å¤æ‚

**Q: é‡åŒ–åœ¨ SFT å’Œ RL ä¸­æ•ˆæœä¸ºä»€ä¹ˆç›¸åï¼Ÿ**  
A: SFT ç›®æ ‡æ˜¯åˆ†å¸ƒåŒ¹é…ï¼ˆæœ€å°åŒ– KLï¼‰ï¼Œé‡åŒ–å™ªå£°åç¦»ç›®æ ‡ = æœ‰å®³ã€‚RL ç›®æ ‡æ˜¯ reward æœ€å¤§åŒ–ï¼Œé‡åŒ–å™ªå£°å¢åŠ  policy entropy = ä¿ƒè¿›æ¢ç´¢ = æœ‰ç›Šã€‚é—®é¢˜ç»“æ„ä¸åŒï¼Œå™ªå£°çš„æ•ˆæœå°±ä¸åŒã€‚

**Q: GRPO çš„ off-policy é—®é¢˜æœ‰å¤šä¸¥é‡ï¼Ÿ**  
A: æ¯”å¤§å¤šæ•°äººæ„è¯†åˆ°çš„ä¸¥é‡ã€‚ä»»ä½• rollout å’Œ evaluation ç²¾åº¦ä¸ä¸€è‡´çš„ç³»ç»Ÿï¼ˆFP8 rollout + BF16 evalï¼‰éƒ½æœ‰ off-policy åå·®ã€‚Jet-RL å®éªŒæ˜¾ç¤ºï¼Œnaive FP8 rollout çš„ç²¾åº¦æŸå¤± >5%ï¼Œç»Ÿä¸€ FP8 flow åé™åˆ° <1%ã€‚åœ¨å¼‚æ­¥è®­ç»ƒä¸­æ›´ä¸¥é‡ï¼ˆVCPO åŠ¨æœºï¼‰ã€‚

---

## ç»´åº¦å…«ï¼ˆè¡¥å……ï¼‰ï¼šDifficulty Debiasing â€” std å½’ä¸€åŒ–çš„éšæ‚£

> **2026-02-25 è¡¥å……**ï¼šNoRD (CVPR 2026) åœ¨è‡ªåŠ¨é©¾é©¶ VLA é¢†åŸŸé¦–æ¬¡å®è¯äº† difficulty bias å¯¹å¼± SFT ç­–ç•¥çš„ç³»ç»Ÿæ€§ç ´åï¼Œå¹¶éªŒè¯äº† Dr. GRPO çš„æœ‰æ•ˆæ€§ã€‚

**é—®é¢˜**ï¼šGRPO çš„ advantage è®¡ç®—ä¸­ `std(r)` å½’ä¸€åŒ–å¼•å…¥äº†éšæ€§çš„éš¾åº¦åå·®ï¼š
- é«˜æ–¹å·® groupï¼ˆä¸­ç­‰éš¾åº¦æ ·æœ¬ï¼‰â†’ std å¤§ â†’ advantage è¢«å‹ç¼© â†’ **æœ‰æ•ˆæ¢¯åº¦å‡ ä¹ä¸ºé›¶**
- ä½æ–¹å·® groupï¼ˆæç®€å•æˆ–æéš¾æ ·æœ¬ï¼‰â†’ std å° â†’ advantage è¢«æ”¾å¤§ â†’ **æ¢¯åº¦é›†ä¸­åœ¨æ— ä»·å€¼çš„æ ·æœ¬ä¸Š**

åæœï¼šGRPO å®é™…åªä»"ç®€å•æ ·æœ¬"ï¼ˆå…¨å¯¹ or å…¨é”™ï¼‰ä¸­å­¦ä¹ ï¼Œå¿½è§†äº†èƒ½åŠ›è¾¹ç•Œå¤„æœ€æœ‰ä»·å€¼çš„"ä¸­ç­‰éš¾åº¦æ ·æœ¬"ã€‚

**Dr. GRPO**ï¼ˆLiu et al., 2029ï¼ŒåŸå‘è¡¨äº LLM æ•°å­¦æ¨ç†ï¼‰  
è§£æ³•æå…¶ç®€å•ï¼š**å»æ‰ std å½’ä¸€åŒ–**ã€‚  
`A_i = r(o_i|x) - mean_j(r(o_j|x))` ï¼ˆå°±è¿™ä¸€è¡Œæ”¹åŠ¨ï¼‰  
åŠ ä¸Š DAPO é£æ ¼éå¯¹ç§° clipping + æ—  KL æ­£åˆ™ï¼Œä¿æŒè®­ç»ƒç¨³å®šã€‚

**NoRD (2602.21172, CVPR 2026) â€” è·¨åŸŸå®è¯éªŒè¯**  
- ä»»åŠ¡ï¼šè‡ªåŠ¨é©¾é©¶ VLAï¼ˆQwen-2.5VL-3Bï¼ŒWaymo/NAVSIM benchmarkï¼‰  
- å‘ç°ï¼šå¼± SFT ç­–ç•¥ï¼ˆ80k æ ·æœ¬ï¼Œæ— æ¨ç†æ ‡æ³¨ï¼‰åœ¨ PDM score [0.2, 0.65] èŒƒå›´å†…äº§ç”Ÿé«˜ intra-group variance çš„ majority æ ·æœ¬  
- GRPOï¼š+0.67% PDMï¼ˆæ— æ•ˆï¼‰ï¼›Dr. GRPOï¼š**+11.68% PDM**  
- ç»“è®ºï¼šdifficulty bias **ä¸åªæ˜¯ LLM æ¨ç†é—®é¢˜**ï¼Œè€Œæ˜¯ä»»ä½• reward åˆ†å¸ƒæåŒ– + å¼± SFT ç­–ç•¥ç»„åˆçš„æ™®éç°è±¡

**é€‚ç”¨åˆ¤æ–­**ï¼šå½“å‡ºç°ä»¥ä¸‹æƒ…å†µæ—¶ï¼ŒDr. GRPO > GRPOï¼š
1. SFT ç­–ç•¥ç›¸å¯¹å¼±ï¼ˆæ•°æ®å°‘ã€æ— æ¨ç†æ ‡æ³¨ã€cold startï¼‰
2. Reward åˆ†å¸ƒæåŒ–ï¼ˆbimodalï¼šç®€å•å…¨å¯¹ + å›°éš¾å…¨é”™ï¼‰
3. ä¸­ç­‰éš¾åº¦æ ·æœ¬å å¤šæ•°ï¼ˆmajority è½åœ¨é«˜æ–¹å·®åŒºé—´ï¼‰

**ä¸ Goldilocks/Sample å±‚çš„å…³ç³»**ï¼š  
Goldilocks = ç­›é€‰æ‰ std=0 çš„æç«¯æ ·æœ¬ï¼ˆæ•°æ®å±‚ï¼‰  
Dr. GRPO = å¯¹ std å¤§çš„æ ·æœ¬ä¸æƒ©ç½šï¼ˆç®—æ³•å±‚ï¼‰  
ä¸¤è€…äº’è¡¥ï¼šä¸€ä¸ªå‰ç½®è¿‡æ»¤ï¼Œä¸€ä¸ªåç½®ä¸æ­§è§†ã€‚

---

## å¼€æ”¾é—®é¢˜

1. **Token çº§åˆ«å¯†é›†å¥–åŠ±**ï¼šèƒ½å¦è®¾è®¡è½»é‡çº§çš„ per-token reward modelï¼Œè€Œä¸éœ€è¦å®Œæ•´çš„ criticï¼Ÿ
2. **Exploration ç»„åˆæ•ˆåº”**ï¼šDEEP-GRPOï¼ˆpivot resamplingï¼‰+ QeRLï¼ˆentropy injectionï¼‰èƒ½å¦å åŠ è€Œä¸ç›¸äº’å¹²æ‰°ï¼Ÿ
3. **Goldilocks çš„ online ç‰ˆæœ¬**ï¼šTeacher LM é¢„æµ‹ utility æ˜¯ offline çš„ï¼Œèƒ½å¦åšåˆ° online adaptive curriculumï¼Ÿ
4. **ç³»ç»Ÿå±‚å’Œç®—æ³•å±‚çš„ co-design**ï¼šJet-RL å’Œ VCPO è§£å†³äº†ç³»ç»Ÿå±‚ off-policyï¼Œä½†æ˜¯å¦æœ‰ç®—æ³•å±‚èƒ½å¤Ÿå®¹å¿ä¸€å®šç¨‹åº¦çš„ off-policyï¼Ÿï¼ˆoff-policy RL ç®—æ³•ï¼Œå¦‚ V-trace/IMPALAï¼‰
5. **è¾¹ç•Œæ‰©å±•ï¼šnon-verifiable ä»»åŠ¡**ï¼šRLRRï¼ˆarXiv:2602.16802ï¼ŒICLR 2026ï¼‰ç”¨ reference-guided judge ä¸ºå¯¹é½ä»»åŠ¡é€ äº†è½¯ verifierï¼ŒæŠŠ RLVR çš„èƒ½åŠ›è¾¹ç•Œå‘ non-verifiable åŸŸæ¨è¿›ã€‚ä¸ƒç»´æ¡†æ¶ç›®å‰å‡è®¾ verifiable reward å­˜åœ¨â€”â€”non-verifiable åœºæ™¯ä¸‹ï¼Œè½¯ verifier çš„è¯¯å·®ç‡ï¼ˆ~21%ï¼‰å¦‚ä½•å½±å“è¿™ä¸ƒä¸ªç»´åº¦çš„æ”¹è¿›æ•ˆæœï¼ŸSee: [[AI/LLM/RL/Theory/RLRR-Reference-Guided-Alignment-Non-Verifiable|RLRR]]

---

## å¼•ç”¨è®ºæ–‡

- ProGRPO: arXiv 2602.05281 (Pengyi Li et al.) â€” Diversity ç»´åº¦ï¼ŒARM æ¦‚ç‡ç½®ä¿¡åº¦é‡åŠ æƒ
- RePO: arXiv 2602.10819 (Linxuan Xia et al.) â€” Diversity ç»´åº¦ï¼Œoff-policy çŸ¥è¯†å†…åŒ–
- STAPO: arXiv 2602.15620 (Tsinghua + DiDi)
- MASPO: arXiv 2602.17xxx (MSRA, Xiaoliang Fu/Xunliang Cai)
- DEEP-GRPO: arXiv 2602.14169 (ICML submission)
- Goldilocks RL: arXiv 2602.14868 (Apple + EPFL)
- Jet-RL: arXiv 2601.14243 (Song Han lab, NVIDIA/MIT)
- QeRL: arXiv 2510.11696 (Song Han lab, NVIDIA/MIT, ICLR 2026)
- Stable Asynchrony / VCPO: ~arXiv 2602.1xxxx (Song Han lab, 2/19 æäº¤ï¼ŒID å¾…ç¡®è®¤)
- VESPO: arXiv 2602.10693 (å˜åˆ† IS reshapingï¼Œoff-policy ç†è®ºæœ€ä¸¥æ ¼)
- SAPO: arXiv 2511.20347 (Qwen å›¢é˜Ÿï¼ŒsechÂ² è½¯é—¨æ§ï¼ŒQwen3-VL ç”Ÿäº§ä½¿ç”¨)
- GSPO: arXiv 2507.18071 (Qwen å›¢é˜Ÿï¼Œsequence-level IS ratioï¼ŒSAPO å‰é©±)
- AT-RL: arXiv 2602.11455 (å¤šæ¨¡æ€è§†è§‰é”šç‚¹ credit assignment)
- Dr. GRPO: (Liu et al., 2029ï¼ŒåŸæ–‡è§ GRPO/Dr-GRPO-Unbiased-Optimization.md) â€” difficulty bias å» std å½’ä¸€åŒ–
- NoRD: arXiv 2602.21172 (Applied Intuition + UC Berkeley, CVPR 2026) â€” è‡ªåŠ¨é©¾é©¶ VLAï¼Œé¦–æ¬¡åœ¨é LLM æ¨ç†é¢†åŸŸéªŒè¯ Dr. GRPOï¼Œå¼± SFT + Dr. GRPO æ— æ¨ç†æ•°æ®è¾¾åˆ° SOTA

---

## see-also

- [[AI/LLM/RL/Other-Algorithms/STAPO-Spurious-Token-Aware-Policy-Optimization|STAPO]] â€” Token çº§åˆ«ç»´åº¦
- [[AI/LLM/RL/Other-Algorithms/MASPO-Mass-Adaptive-Soft-Policy-Optimization|MASPO]] â€” å¤šç»´ GRPO æ”¹è¿›
- [[AI/LLM/RL/Other-Algorithms/DEEP-GRPO-Deep-Dense-Exploration-Pivot-Resampling|DEEP-GRPO]] â€” æ¢ç´¢ç»´åº¦
- [[AI/LLM/RL/Other-Algorithms/Goldilocks-RL-Task-Difficulty-Curriculum|Goldilocks RL]] â€” æ ·æœ¬ç»´åº¦
- [[AI/LLM/RL/Frameworks/Jet-RL-FP8-On-Policy-RL-Training|Jet-RL]] â€” ç³»ç»Ÿç²¾åº¦ç»´åº¦
- [[AI/LLM/RL/Frameworks/QeRL-Quantization-Enhanced-RL|QeRL]] â€” é‡åŒ–æ¢ç´¢ç»´åº¦
- [[AI/LLM/RL/Other-Algorithms/Stable-Asynchrony-VCPO-Off-Policy-RL|VCPO]] â€” ç³»ç»Ÿå¼‚æ­¥ off-policy ç»´åº¦
- [[AI/LLM/RL/Other-Algorithms/VESPO-Variational-Sequence-Policy-Optimization|VESPO]] â€” å˜åˆ† off-policy ä¿®æ­£ï¼Œç†è®ºæœ€ä¸¥æ ¼
- [[AI/LLM/RL/Other-Algorithms/SAPO-Soft-Adaptive-Policy-Optimization|SAPO]] â€” sechÂ² è½¯é—¨æ§ï¼ŒQwen3-VL ç”Ÿäº§
- [[AI/LLM/RL/Other-Algorithms/GSPO-Group-Sequence-Policy-Optimization|GSPO]] â€” åºåˆ—çº§ IS ratioï¼ˆSAPO å‰é©±ï¼‰
- [[AI/LLM/RL/GRPO/Dr-GRPO-Unbiased-Optimization|Dr. GRPO]] â€” å» std å½’ä¸€åŒ–ï¼Œdifficulty debiasing
- [[AI/LLM/RL/Other-Algorithms/NoRD-Dr-GRPO-Reasoning-Free-VLA-Autonomous-Driving|NoRD]] â€” è‡ªåŠ¨é©¾é©¶ VLAï¼ŒDr. GRPO è·¨åŸŸå®è¯ (CVPR 2026)
- [[AI/LLM/RL/Other-Algorithms/AT-RL-Anchor-Token-Reinforcement-Learning-Multimodal|AT-RL]] â€” å¤šæ¨¡æ€ç»´åº¦ credit assignment
- [[AI/LLM/RL/Theory/RL-Training-Stability-2026-Unified-Analysis|RL è®­ç»ƒç¨³å®šæ€§ 2026 ç»Ÿä¸€åˆ†æ]] â€” ä¸æœ¬æ–‡äº’è¡¥ï¼Œèšç„¦ç¨³å®šæ€§è€Œéåˆ†ç±»æ¡†æ¶
- [[AI/LLM/RL/Other-Algorithms/OAPL-Off-Policy-RL-LLM-Reasoning|OAPL]] â€” ç›®æ ‡å‡½æ•°èŒƒå¼è½¬ç§»ï¼šKL-regularized closed-form â†’ squared regressionï¼Œæ”¾å¼ƒ IS
- [[AI/LLM/RL/Other-Algorithms/LAD-Learning-Advantage-Distribution|LAD]] â€” ç›®æ ‡å‡½æ•°èŒƒå¼è½¬ç§»ï¼šadvantage è¯±å¯¼åˆ†å¸ƒåŒ¹é…ï¼ˆf-divergenceï¼‰ï¼Œè‡ªç„¶ä¿ç•™å¤šæ¨¡å¼è½¨è¿¹ï¼›ä¸ OAPL æ­£äº¤å¯ç»„åˆ
- [[AI/Agent/Agentic-RL/HiPER-Hierarchical-Plan-Execute-RL-Credit-Assignment|HiPERï¼ˆICML 2026ï¼‰]] â€” GRPO çš„ Agent æ‰©å±•æ–¹å‘ï¼šStarPO â†’ multi-turn trajectory-level GRPO â†’ HiPER çš„ segment-level HAEï¼ˆä¸‰æ­¥æ¼”è¿›ï¼‰
- [[AI/Agent/Agentic-RL/RAGEN-StarPO-Multi-Turn-RL-Self-Evolution|RAGEN & StarPO]] â€” GRPO åœ¨ multi-turn agent åœºæ™¯çš„ç¨³å®šæ€§æŒ‘æˆ˜ï¼ˆEcho Trapï¼‰ï¼ŒåŠ StarPO æ¡†æ¶
- [[AI/LLM/RL/GRPO/ProGRPO-Probabilistic-Advantage-Reweighting|ProGRPO]] â€” Diversity ç»´åº¦ï¼šARM æ¦‚ç‡ä¿¡å·é‡è°ƒ advantage
- [[AI/LLM/RL/Other-Algorithms/RePO-Rephrasing-Policy-Optimization|RePO]] â€” Diversity ç»´åº¦ï¼šoff-policy çŸ¥è¯†å†…åŒ–åˆ° on-policy å…¼å®¹è½¨è¿¹
- [[AI/Agent/Agentic-RL/SeeUPO-Sequence-Level-Agentic-RL-Convergence-Guarantees|SeeUPOï¼ˆarXiv:2602.06554ï¼‰]] âš ï¸ â€” **GRPO çš„ç†è®ºè¾¹ç•Œ**ï¼šä¸å¯èƒ½å®šç†è¯æ˜ GRAE+PPUï¼ˆGRPOä¸»ä½“ï¼‰åœ¨ multi-turn contextual bandit ä¸­æ— æ”¶æ•›ä¿è¯ï¼›å•è½®æ¨ç† GRPO ä»æœ‰æ•ˆï¼Œå¤šè½® Agent è®­ç»ƒéœ€æ¢ SeeUPO é€†åºæ›´æ–°

---

## ğŸ”§ è½åœ°åº”ç”¨

- **å•è½®æ¨ç† RLVRï¼ˆæ•°å­¦/ä»£ç ï¼‰**ï¼šGRPO æ˜¯æ ‡å‡†èµ·ç‚¹ â†’ é‡åˆ° entropy collapse åŠ  DAPO çš„ clip-higher + entropy bonus â†’ trust region è¿‡ç¡¬åŠ  MASPO/SAPO è½¯åŒ– â†’ æ¢ç´¢æ­»åŠ  DEEP-GRPO pivot resampling â†’ è¯¾ç¨‹å­¦ä¹ åŠ  Goldilocks RL é€‰ pâ‰ˆ0.5 é¢˜
- **å¤§è§„æ¨¡å¼‚æ­¥è®­ç»ƒï¼ˆ70B+ï¼‰**ï¼šgeneration/training è§£è€¦åç”¨ VCPO æ–¹å·®æ§åˆ¶ + Jet-RL FP8 ç²¾åº¦ä¸€è‡´æ€§ï¼Œé˜² off-policy drift
- **å¤šè½® Agent è®­ç»ƒ**ï¼šâš ï¸ ä¸è¦ç”¨ GRPOï¼variance normalization ç ´å multi-turn æ”¶æ•›ï¼ˆSeeUPO å®šç†ï¼‰â†’ åˆ‡æ¢ SeeUPO é€†åºæ›´æ–°
- **é€‰å“ªä¸ªæ”¹è¿›**ï¼šåªæœ‰ä¸€ä¸ªé—®é¢˜ç”¨ DAPOï¼ˆç”Ÿäº§éªŒè¯æœ€å……åˆ†ï¼‰ï¼›æƒ³ç»Ÿä¸€ä¿®ä¸‰ä¸ªé—®é¢˜ç”¨ MASPOï¼›ç”Ÿäº§ç¯å¢ƒç”¨ Qwen å›¢é˜Ÿ SAPO/GSPOï¼ˆQwen3-VL åœ¨è·‘ï¼‰
- **é¢è¯•è¯æœ¯**ï¼šä¸ƒç»´æ¡†æ¶æ˜¯å›ç­”"GRPO æœ‰ä»€ä¹ˆé—®é¢˜/æ€ä¹ˆæ”¹è¿›"çš„æ ‡å‡†ç»“æ„ï¼Œå…ˆåˆ†ç»´åº¦å†è¯´å…·ä½“è®ºæ–‡ï¼Œæ¯”ç›´æ¥èƒŒè®ºæ–‡åå­—æ¸…æ™°10å€

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

- **"æ”¹è¿› GRPO"çš„è®ºæ–‡å·²ç»å¤šåˆ°éœ€è¦å…ƒåˆ†æ**ï¼šæ¯ç¯‡éƒ½å£°ç§°è§£å†³äº†"GRPO çš„é—®é¢˜"ï¼Œä½†å®é™…ä¸Šä¿®çš„æ˜¯ä¸åŒå±‚æ¬¡çš„ç¼ºé™·â€”â€”ä¸ƒç»´æ¡†æ¶çš„ä»·å€¼å°±åœ¨äºæ­¤ï¼Œè®©ä½ ä¸€çœ¼çœ‹æ¸…æ¯ç¯‡è®ºæ–‡çš„çœŸæ­£è´¡çŒ®åœ¨å“ªä¸ªç»´åº¦
- **SeeUPO å®šç†æ”¹å˜äº†è®¨è®ºæ¡†æ¶**ï¼šä¹‹å‰è¯´ GRPO åœ¨ multi-turn ä¸ç¨³å®šæ˜¯ç»éªŒè§‚å¯Ÿï¼Œç°åœ¨æœ‰äº†æ•°å­¦è¯æ˜ã€‚è¿™æ„å‘³ç€ä¸ƒç»´æ¡†æ¶çš„æ‰€æœ‰æ”¹è¿›éƒ½æ˜¯é’ˆå¯¹**å•è½® GRPO**çš„â€”â€”multi-turn åœºæ™¯éœ€è¦å®Œå…¨ä¸åŒçš„ç®—æ³•æ—
- **Qwen å›¢é˜Ÿç”Ÿäº§éªŒè¯çš„ç®—æ³•æ˜¯æœ€å€¼å¾—ç›¸ä¿¡çš„**ï¼šSAPOï¼ˆsechÂ² è½¯é—¨æ§ï¼ŒQwen3-VLï¼‰/ GSPOï¼ˆåºåˆ—çº§ ISï¼ŒQwen3ï¼‰éƒ½æ˜¯è¢«å¤§è§„æ¨¡ç”Ÿäº§è®­ç»ƒéªŒè¯è¿‡çš„â€”â€”å­¦æœ¯è®ºæ–‡å¥½ä½†æ²¡æœ‰å·¥ä¸šçº§è§„æ¨¡éªŒè¯çš„ç®—æ³•ï¼Œåœ¨å®é™…éƒ¨ç½²æ—¶è¦è°¨æ…
- **Diversity ç»´åº¦æ˜¯æœ€è¢«ä½ä¼°çš„**ï¼šå¤§å®¶éƒ½åœ¨æ”¹ trust region / KL çº¦æŸï¼Œä½† rollout å¤šæ ·æ€§å´©å¡Œï¼ˆwithin-state æ¢ç´¢æ­»äº¡ï¼‰æ‰æ˜¯ hard exploration çš„æ ¹å› ã€‚VAM/QeRL/IntroLLM è¿™æ¡çº¿åœ¨ 2026 ä¸‹åŠå¹´å¯èƒ½ä¼šçˆ†å‘

## ğŸ“š æ¨èé˜…è¯»

1. **DAPO**ï¼ˆarXiv:2503.14476ï¼‰â€” GRPO æœ€é‡è¦çš„å·¥ä¸šçº§æ”¹è¿›ï¼Œå››é¡¹æ”¹è¿›éƒ½æœ‰å……åˆ†çš„ ablationï¼ŒNeurIPS 2025
2. **MASPO**ï¼ˆarXiv:2602.17550ï¼‰â€” ä¸‰ç»´ç»Ÿä¸€æ”¹è¿›æ¡†æ¶ï¼Œæ˜¯ç†è§£"trust region æ”¹è¿›å…¨å®¶æ¡¶"çš„å¥½æ•™æ
3. **SAPO**ï¼ˆarXiv:2511.20347ï¼‰â€” Qwen å›¢é˜Ÿï¼ŒsechÂ² è½¯é—¨æ§ï¼Œç”Ÿäº§éªŒè¯ï¼›ä¸ GSPO ä¸€èµ·è¯»ç†è§£ Qwen RL æ ˆ
4. **SeeUPO**ï¼ˆarXiv:2602.06554ï¼‰â€” å¿…è¯»ï¼šç†è§£ GRPO çš„ç†è®ºè¾¹ç•Œï¼ˆmulti-turn æ— æ”¶æ•›ä¿è¯ï¼‰ï¼Œæ­£ç¡®å®šä½æœ¬å…¨æ™¯çš„é€‚ç”¨èŒƒå›´
