---
title: "å¼ºåŒ–å­¦ä¹ ä¸RLHFåº”ç”¨-2026å…¨æ™¯"
brief: "ä»ç»å…¸ RLï¼ˆMDP/DQN/PPO/SACï¼‰åˆ° LLM å¯¹é½ï¼ˆRLHF/DPO/GRPOï¼‰å†åˆ°æ¨ç†æ—¶æœç´¢ï¼ˆo1/R1ï¼‰çš„å®Œæ•´æŠ€æœ¯æ ˆç»¼è¿°ã€‚æ ¸å¿ƒæ´å¯Ÿï¼š2025-2026 RL ä¸å†åªæ˜¯å¯¹é½å·¥å…·ï¼Œè€Œæ˜¯æ¶Œç°æ¨ç†èƒ½åŠ›çš„å…³é”®èŒƒå¼ï¼ˆtest-time compute scalingï¼‰ã€‚é¢è¯•æ·±åº¦å‚è€ƒçº§ç¬”è®°ã€‚"
date: 2026-02-21
updated: 2026-02-22
tags:
  - ai/llm/rl
  - ai/llm/alignment
  - type/survey
  - interview/hot
domain: ai/llm/rl
status: complete
rating: â˜…â˜…â˜…â˜…â˜…
sources:
  - "Schulman et al. Proximal Policy Optimization Algorithms. arXiv:1707.06347"
  - "Ouyang et al. Training language models to follow instructions with human feedback (InstructGPT). arXiv:2203.02155"
  - "Bai et al. Constitutional AI: Harmlessness from AI Feedback. arXiv:2212.08073"
  - "Shao et al. DeepSeekMath: Pushing the Limits of Mathematical Reasoning (GRPO). arXiv:2402.03300"
  - "DeepSeek-AI. DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via RL. arXiv:2501.12948"
  - "Rafailov et al. Direct Preference Optimization. arXiv:2305.18290"
  - "Haarnoja et al. Soft Actor-Critic. arXiv:1801.01290"
related:
  - "[[AI/LLM/RL/RLHF-DPO-2026-æŠ€æœ¯å…¨æ™¯|RLHF DPO æŠ€æœ¯å…¨æ™¯]]"
  - "[[AI/LLM/RL/Fundamentals/å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸç†|RL æ•°å­¦åŸç†]]"
  - "[[AI/LLM/RL/Theory/GRPO-Improvement-Panorama-2026|GRPO æ”¹è¿›å…¨æ™¯]]"
  - "[[AI/LLM/Architecture/DeepSeek-R1|DeepSeek-R1]]"
  - "[[Career/AIé¢è¯•é€ŸæŸ¥æ‰‹å†Œ|AI é¢è¯•é€ŸæŸ¥æ‰‹å†Œ]]"
archived_by: librarian
archived_date: 2026-02-21
---

# å¼ºåŒ–å­¦ä¹ ä¸ RLHF åº”ç”¨â€”â€”2026 æŠ€æœ¯å…¨æ™¯

> é¢è¯•æ·±åº¦å‚è€ƒã€‚è¦†ç›–ä»ç»å…¸ RL åˆ° LLM å¯¹é½çš„å®Œæ•´æŠ€æœ¯æ ˆã€‚

---

## ä¸€ã€RL åŸºç¡€ï¼šä» MDP åˆ°æ ¸å¿ƒäºŒåˆ†æ³•

### 1.1 é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPï¼‰

RL çš„æ•°å­¦åŸºç¡€æ˜¯ MDP äº”å…ƒç»„ $(S, A, P, R, \gamma)$ï¼š

- **$S$**ï¼šçŠ¶æ€ç©ºé—´
- **$A$**ï¼šåŠ¨ä½œç©ºé—´
- **$P(s'|s,a)$**ï¼šçŠ¶æ€è½¬ç§»æ¦‚ç‡
- **$R(s,a,s')$**ï¼šå³æ—¶å¥–åŠ±
- **$\gamma \in [0,1)$**ï¼šæŠ˜æ‰£å› å­ï¼Œæƒè¡¡çŸ­æœŸä¸é•¿æœŸæ”¶ç›Š

**Markov æ€§**ï¼š$P(s_{t+1}|s_t,a_t) = P(s_{t+1}|s_0,...,s_t,a_0,...,a_t)$ï¼Œæœªæ¥åªä¾èµ–å½“å‰çŠ¶æ€ã€‚

### 1.2 Bellman æ–¹ç¨‹

**çŠ¶æ€ä»·å€¼å‡½æ•°**ï¼š$V^\pi(s) = \mathbb{E}_\pi[\sum_{t=0}^{\infty}\gamma^t R_t | s_0=s]$

**åŠ¨ä½œä»·å€¼å‡½æ•°**ï¼š$Q^\pi(s,a) = \mathbb{E}_\pi[\sum_{t=0}^{\infty}\gamma^t R_t | s_0=s, a_0=a]$

**Bellman æœŸæœ›æ–¹ç¨‹**ï¼š
$$V^\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^\pi(s')]$$

**Bellman æœ€ä¼˜æ–¹ç¨‹**ï¼š
$$V^*(s) = \max_a \sum_{s'} P(s'|s,a)[R(s,a,s') + \gamma V^*(s')]$$

### 1.3 æ ¸å¿ƒäºŒåˆ†æ³•

| ç»´åº¦ | ç±»å‹ A | ç±»å‹ B | æœ¬è´¨åŒºåˆ« |
|------|--------|--------|----------|
| **æ–¹æ³•è®º** | Value-basedï¼ˆDQNï¼‰ | Policy-basedï¼ˆPGï¼‰ | å­¦ Q å€¼ vs ç›´æ¥å­¦ç­–ç•¥ |
| **æ•°æ®ä½¿ç”¨** | Off-policyï¼ˆDQN/SACï¼‰ | On-policyï¼ˆPPO/A2Cï¼‰ | èƒ½å¦å¤ç”¨æ—§æ•°æ® |
| **æ¢ç´¢ç­–ç•¥** | Explorationï¼ˆÎµ-greedyï¼‰ | Exploitationï¼ˆgreedyï¼‰ | è¯•æ–° vs ç”¨å·²çŸ¥æœ€ä¼˜ |

**Policy-based ä¼˜åŠ¿**ï¼šå¯å¤„ç†è¿ç»­åŠ¨ä½œç©ºé—´ã€éšæœºç­–ç•¥ã€æ›´å¥½æ”¶æ•›æ€§  
**Value-based ä¼˜åŠ¿**ï¼šæ ·æœ¬æ•ˆç‡é«˜ï¼ˆoff-policy å¯å¤ç”¨æ•°æ®ï¼‰ã€æ›´ç¨³å®š

**Exploration vs Exploitation** å¹³è¡¡æ–¹æ³•ï¼š
- Îµ-greedyï¼šä»¥ Îµ æ¦‚ç‡éšæœºæ¢ç´¢
- UCBï¼ˆUpper Confidence Boundï¼‰ï¼šè€ƒè™‘ä¸ç¡®å®šæ€§çš„ä¹è§‚æ¢ç´¢
- Boltzmann/Softmaxï¼šæŒ‰ Q å€¼æ¦‚ç‡é€‰æ‹©
- Intrinsic Motivationï¼šå¥½å¥‡å¿ƒé©±åŠ¨ï¼ˆICM/RNDï¼‰

### é¢è¯•é«˜é¢‘é—®é¢˜

**Q: ä¸ºä»€ä¹ˆéœ€è¦æŠ˜æ‰£å› å­ Î³ï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ æ•°å­¦ä¸Šä¿è¯æ— é™åºåˆ—å’Œæ”¶æ•›ï¼›â‘¡å»ºæ¨¡æ—¶é—´åå¥½ï¼ˆè¿‘æœŸå¥–åŠ±æ›´ç¡®å®šï¼‰ï¼›â‘¢Î³â†’0 çŸ­è§†ï¼ŒÎ³â†’1 è¿œè§†ï¼›â‘£Î³=1 åœ¨ episodic ä»»åŠ¡ä¸­å¯ç”¨ä½† continuing ä»»åŠ¡ä¼šå‘æ•£ã€‚

**Q: On-policy å’Œ Off-policy çš„æœ¬è´¨åŒºåˆ«ï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šOn-policyï¼ˆå¦‚ SARSA/PPOï¼‰ç”¨å½“å‰ç­–ç•¥é‡‡æ ·çš„æ•°æ®æ›´æ–°å½“å‰ç­–ç•¥ï¼Œæ•°æ®ä¸å¯å¤ç”¨ï¼›Off-policyï¼ˆå¦‚ Q-Learning/SACï¼‰å­¦ä¹ çš„ç­–ç•¥ä¸æ•°æ®é‡‡é›†ç­–ç•¥ä¸åŒï¼Œå¯ç”¨ Replay Buffer æé«˜æ ·æœ¬æ•ˆç‡ã€‚å…³é”®å…¬å¼å·®å¼‚åœ¨äº importance sampling ratioã€‚

---

## äºŒã€ç»å…¸ç®—æ³•å…¨æ™¯

### 2.1 Q-Learning â†’ DQN

**Q-Learning**ï¼ˆTabularï¼‰ï¼š
$$Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$$

**DQN** çš„ä¸¤ä¸ªå…³é”®åˆ›æ–°ï¼š
1. **Experience Replay**ï¼šå­˜å‚¨ $(s,a,r,s')$ åˆ° buffer éšæœºé‡‡æ ·ï¼Œæ‰“ç ´æ•°æ®ç›¸å…³æ€§
2. **Target Network**ï¼šç”¨å»¶è¿Ÿæ›´æ–°çš„ $\hat{Q}$ è®¡ç®— TD targetï¼Œå‡å°‘è®­ç»ƒéœ‡è¡
   - $y = r + \gamma \max_{a'} \hat{Q}(s',a'; \theta^-)$ï¼Œ$\theta^-$ æ¯ C æ­¥åŒæ­¥

**DQN æ”¹è¿›æ—**ï¼š
- **Double DQN**ï¼šè§£è€¦åŠ¨ä½œé€‰æ‹©å’Œè¯„ä¼°ï¼Œè§£å†³ Q å€¼è¿‡ä¼°è®¡
- **Dueling DQN**ï¼šåˆ†ç¦» V(s) å’Œ A(s,a)ï¼ŒåŠ é€Ÿå­¦ä¹ 
- **Prioritized Replay**ï¼šæŒ‰ TD-error ä¼˜å…ˆé‡‡æ ·é‡è¦ç»éªŒ
- **Rainbow**ï¼šé›†æˆ 6 é¡¹æ”¹è¿›çš„ SOTA

### 2.2 Policy Gradient ç³»åˆ—

**REINFORCE**ï¼ˆvanilla PGï¼‰ï¼š
$$\nabla_\theta J(\theta) = \mathbb{E}_\pi[\nabla_\theta \log\pi_\theta(a|s) \cdot G_t]$$

æ ¸å¿ƒæ€æƒ³ï¼šå¢å¤§é«˜å›æŠ¥è½¨è¿¹åŠ¨ä½œçš„æ¦‚ç‡ï¼Œé™ä½ä½å›æŠ¥çš„ã€‚

**é—®é¢˜**ï¼šæ–¹å·®æå¤§ï¼ˆ$G_t$ åŒ…å«æ•´æ¡è½¨è¿¹çš„éšæœºæ€§ï¼‰ã€‚

**è§£å†³**ï¼šå¼•å…¥ baseline å‡æ–¹å·®ï¼š
$$\nabla_\theta J(\theta) = \mathbb{E}_\pi[\nabla_\theta \log\pi_\theta(a|s) \cdot (G_t - b(s))]$$

### 2.3 Actor-Critic æ¶æ„

**A2C**ï¼ˆAdvantage Actor-Criticï¼‰ï¼š
- Actorï¼šç­–ç•¥ç½‘ç»œ $\pi_\theta$
- Criticï¼šä»·å€¼ç½‘ç»œ $V_\phi$
- Advantageï¼š$A(s,a) = Q(s,a) - V(s) \approx r + \gamma V(s') - V(s)$ï¼ˆTD è¯¯å·®ï¼‰

**A3C**ï¼šå¤šçº¿ç¨‹å¹¶è¡Œ A2Cï¼Œåˆ©ç”¨å¼‚æ­¥æ¢¯åº¦æ›´æ–°ã€‚2026 å·²è¢« PPO å–ä»£ã€‚

### 2.4 PPOï¼ˆåç»­ä¸“é¢˜è¯¦è§£ï¼‰

### 2.5 SACï¼ˆSoft Actor-Criticï¼‰

**æ ¸å¿ƒæ€æƒ³**ï¼šæœ€å¤§åŒ–å¥–åŠ± + ç­–ç•¥ç†µï¼ˆmaximum entropy frameworkï¼‰
$$J(\pi) = \sum_t \mathbb{E}[r(s_t,a_t) + \alpha \mathcal{H}(\pi(\cdot|s_t))]$$

- Off-policy + è‡ªåŠ¨æ¸©åº¦è°ƒèŠ‚ $\alpha$
- å¤©ç„¶é¼“åŠ±æ¢ç´¢ï¼Œå¯¹è¶…å‚ä¸æ•æ„Ÿ
- è¿ç»­æ§åˆ¶ä»»åŠ¡ï¼ˆæœºå™¨äººï¼‰çš„é»˜è®¤é€‰æ‹©

### 2.6 TD3ï¼ˆTwin Delayed DDPGï¼‰

ä¸‰ä¸ªå…³é”®æŠ€å·§ï¼š
1. **Twin Critics**ï¼šå–ä¸¤ä¸ª Q ç½‘ç»œæœ€å°å€¼ï¼Œç¼“è§£è¿‡ä¼°è®¡
2. **Delayed Policy Updates**ï¼šCritic æ›´æ–° d æ¬¡åæ‰æ›´æ–° Actor
3. **Target Policy Smoothing**ï¼šç»™ target action åŠ å™ªå£°åšæ­£åˆ™åŒ–

### é¢è¯•é«˜é¢‘é—®é¢˜

**Q: DQN ä¸ºä»€ä¹ˆéœ€è¦ Experience Replay å’Œ Target Networkï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ Replay æ‰“ç ´æ—¶é—´ç›¸å…³æ€§ï¼Œä½¿ mini-batch è¿‘ä¼¼ i.i.d.ï¼Œæ»¡è¶³ SGD å‡è®¾ï¼›â‘¡Replay æé«˜æ•°æ®åˆ©ç”¨ç‡ï¼ˆoff-policyï¼‰ï¼›â‘¢Target Network ç¨³å®š TD targetï¼ˆbootstrap ç›®æ ‡ä¸åŠ¨ï¼‰ï¼Œé¿å…"ç”¨è‡ªå·±æ›´æ–°è‡ªå·±"çš„æ­£åé¦ˆéœ‡è¡ã€‚äºŒè€…ç¼ºä¸€è®­ç»ƒéƒ½ä¼šå´©æºƒã€‚

**Q: SAC vs PPO æ€ä¹ˆé€‰ï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šè¿ç»­æ§åˆ¶ã€æœºå™¨äººã€éœ€è¦é«˜æ ·æœ¬æ•ˆç‡ â†’ SACï¼ˆoff-policyï¼‰ï¼›ç¦»æ•£åŠ¨ä½œã€LLM å¯¹é½ã€è¿½æ±‚è®­ç»ƒç¨³å®šæ€§ â†’ PPOï¼ˆon-policyï¼‰ã€‚SAC åœ¨åŒæ ·çš„ç¯å¢ƒäº¤äº’æ­¥æ•°ä¸‹é€šå¸¸æ€§èƒ½æ›´é«˜ï¼Œä½† PPO å®ç°ç®€å•ã€è¶…å‚é²æ£’ã€‚

---

## ä¸‰ã€PPO æ·±åº¦è§£æ

### 3.1 ä» TRPO åˆ° PPO

**TRPO**ï¼ˆTrust Region Policy Optimizationï¼‰ï¼š
$$\max_\theta \mathbb{E}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{old}}(a|s)} A^{\pi_{old}}(s,a)\right] \quad \text{s.t.} \quad \mathbb{E}[D_{KL}(\pi_{\theta_{old}} \| \pi_\theta)] \leq \delta$$

TRPO ç”¨ KL çº¦æŸä¿è¯å•è°ƒæ”¹è¿›ï¼Œä½†éœ€è¦äºŒé˜¶ä¼˜åŒ–ï¼ˆFisher ä¿¡æ¯çŸ©é˜µ + å…±è½­æ¢¯åº¦ï¼‰ï¼Œå·¥ç¨‹å¤æ‚ã€‚

**PPO çš„æ ¸å¿ƒæ´å¯Ÿ**ï¼šç”¨ clipping è¿‘ä¼¼ trust regionï¼Œä¸€é˜¶ä¼˜åŒ–å³å¯ã€‚

> æ¥æºï¼šSchulman et al., "Proximal Policy Optimization Algorithms", arXiv:1707.06347, Sec. 3

### 3.2 Clipped Objective æ•°å­¦æ¨å¯¼

å®šä¹‰ importance sampling ratioï¼š$r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$

**PPO-Clip ç›®æ ‡**ï¼š
$$L^{CLIP}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta) A_t, \; \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t\right)\right]$$

**ç›´è§‰**ï¼š
- å½“ $A_t > 0$ï¼ˆå¥½åŠ¨ä½œï¼‰ï¼š$r_t$ å¢å¤§ä½†è¢« clip åˆ° $1+\epsilon$ï¼Œé˜²æ­¢è¿‡åº¦æ›´æ–°
- å½“ $A_t < 0$ï¼ˆå·®åŠ¨ä½œï¼‰ï¼š$r_t$ å‡å°ä½†è¢« clip åˆ° $1-\epsilon$ï¼ŒåŒæ ·é™åˆ¶å¹…åº¦
- $\epsilon$ é€šå¸¸å– 0.1-0.2ï¼Œå®šä¹‰äº† trust region çš„å®½åº¦

**å®Œæ•´ PPO æŸå¤±**ï¼š
$$L = L^{CLIP} - c_1 L^{VF} + c_2 S[\pi_\theta]$$

å…¶ä¸­ $L^{VF}$ æ˜¯ Value å‡½æ•°çš„ MSE lossï¼Œ$S$ æ˜¯ç†µ bonusï¼ˆé¼“åŠ±æ¢ç´¢ï¼‰ã€‚

### 3.3 GAEï¼ˆGeneralized Advantage Estimationï¼‰

å¤šæ­¥ TD çš„åŠ æƒå¹³å‡ï¼Œå¹³è¡¡åå·®å’Œæ–¹å·®ï¼š

$$\hat{A}_t^{GAE(\gamma,\lambda)} = \sum_{l=0}^{\infty}(\gamma\lambda)^l \delta_{t+l}$$

å…¶ä¸­ $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ æ˜¯ TD è¯¯å·®ã€‚

- $\lambda=0$ï¼š$\hat{A}_t = \delta_t$ï¼Œçº¯ 1-step TDï¼Œä½æ–¹å·®é«˜åå·®
- $\lambda=1$ï¼š$\hat{A}_t = \sum \gamma^l \delta_{t+l} = G_t - V(s_t)$ï¼Œè’™ç‰¹å¡æ´›ï¼Œé«˜æ–¹å·®ä½åå·®
- **$\lambda=0.95$ æ˜¯é»˜è®¤å€¼**ï¼Œåœ¨ LLM RLHF ä¸­ä¹Ÿæ²¿ç”¨

### 3.4 KL Penalty vs Clipping

| æ–¹é¢ | KL Penalty | Clipping |
|------|-----------|----------|
| æœºåˆ¶ | åœ¨ loss ä¸­åŠ  $\beta \cdot D_{KL}$ é¡¹ | è£å‰ª ratio åˆ° $[1-\epsilon, 1+\epsilon]$ |
| è¶…å‚ | $\beta$ éœ€è‡ªé€‚åº”è°ƒæ•´ï¼ˆå¤æ‚ï¼‰ | $\epsilon$ å›ºå®šï¼ˆç®€å•ï¼‰ |
| çµæ´»æ€§ | ç†è®ºæ›´ä¼˜é›… | å·¥ç¨‹æ›´å¥½ç”¨ |
| å®è·µ | InstructGPT ç”¨ KL penalty | ç»å¤§å¤šæ•° PPO å®ç°ç”¨ clipping |

OpenAI çš„ InstructGPT/ChatGPT å®é™…ä½¿ç”¨ **KL penalty + clipping æ··åˆ**æ–¹æ¡ˆã€‚

### 3.5 PPO ä¸ºä»€ä¹ˆæˆä¸º LLM å¯¹é½çš„é»˜è®¤é€‰æ‹©

1. **ç¨³å®šæ€§**ï¼šClipping å¤©ç„¶é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œä¸ä¼šä¸€æ­¥è·‘å
2. **é€šç”¨æ€§**ï¼šç¦»æ•£ï¼ˆtoken ç”Ÿæˆï¼‰å’Œè¿ç»­ç©ºé—´éƒ½é€‚ç”¨
3. **å¯æ‰©å±•**ï¼šä¸€é˜¶ä¼˜åŒ–ï¼Œå¯åœ¨å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿä¸Šé«˜æ•ˆè¿è¡Œ
4. **KL çº¦æŸè‡ªç„¶å¥‘åˆ**ï¼šRLHF éœ€è¦æ–°ç­–ç•¥ä¸åç¦» SFT å¤ªè¿œï¼ŒPPO çš„ trust region æ°å¥½æ»¡è¶³
5. **å·¥ç¨‹æˆç†Ÿåº¦**ï¼šOpenAI éªŒè¯è¿‡ï¼ŒTRL/DeepSpeed-Chat/OpenRLHF éƒ½æœ‰æˆç†Ÿå®ç°

### é¢è¯•é«˜é¢‘é—®é¢˜

**Q: PPO çš„ clip æ˜¯æ€ä¹ˆèµ·ä½œç”¨çš„ï¼Ÿèƒ½æ¨å¯¼ä¸€ä¸‹å—ï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šç”»ä¸¤ç§æƒ…å†µçš„å›¾â€”â€”$A>0$ æ—¶ clip é™åˆ¶ $r_t$ çš„ä¸Šç•Œä¸º $1+\epsilon$ï¼Œé˜²æ­¢å¯¹å¥½åŠ¨ä½œè¿‡åº¦å¼ºåŒ–ï¼›$A<0$ æ—¶ clip é™åˆ¶ $r_t$ çš„ä¸‹ç•Œä¸º $1-\epsilon$ï¼Œé˜²æ­¢å¯¹å·®åŠ¨ä½œè¿‡åº¦æƒ©ç½šã€‚å– min ä¿è¯ clip æ˜¯ä¿å®ˆçš„ï¼ˆpessimistic boundï¼‰ã€‚æ•ˆæœç­‰ä»·äº TRPO çš„ trust regionï¼Œä½†åªéœ€ä¸€é˜¶æ¢¯åº¦ã€‚

**Q: GAE çš„ Î» å¦‚ä½•å½±å“è®­ç»ƒï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼š$\lambda$ æ§åˆ¶åå·®-æ–¹å·® tradeoffã€‚$\lambda$â†’0 æ–¹å·®ä½ä½† critic è¯¯å·®ä¼šä¼ æ’­ï¼ˆé«˜åå·®ï¼‰ï¼›$\lambda$â†’1 åå·®ä½ä½†è½¨è¿¹å™ªå£°å¤§ï¼ˆé«˜æ–¹å·®ï¼‰ã€‚$\lambda=0.95$ æ˜¯é»˜è®¤æœ€ä¼˜ã€‚å½“ critic å‡†ç¡®æ—¶å¯é€‚å½“é™ä½ $\lambda$ï¼›å½“ reward ç¨€ç–æ—¶éœ€æé«˜ $\lambda$ã€‚

---

## å››ã€RLHF å…¨é“¾è·¯

### 4.1 ä¸‰é˜¶æ®µæµç¨‹

> æ¥æºï¼šOuyang et al., "Training language models to follow instructions with human feedback" (InstructGPT), arXiv:2203.02155, Fig. 2

```mermaid
flowchart LR
    A["é¢„è®­ç»ƒæ¨¡å‹"] -->|"Stage 1: SFT"| B["SFT æ¨¡å‹"]
    B -->|"äººç±»åå¥½å¯¹æ¯”æ•°æ®"| C["Stage 2: RM è®­ç»ƒ"]
    C --> D["Reward Model"]
    B -->|"Stage 3: PPO å¯¹é½"| E["PPO ä¼˜åŒ–"]
    D -->|"RM æ‰“åˆ†æŒ‡å¯¼"| E
    E --> F["å¯¹é½åæ¨¡å‹"]
```

### 4.2 Stage 1: SFT

- åœ¨é«˜è´¨é‡æŒ‡ä»¤-å›å¤æ•°æ®ä¸Šå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹
- æ•°æ®é‡ï¼šå‡ åƒåˆ°å‡ ä¸‡æ¡ï¼ˆè´¨é‡è¿œé‡äºæ•°é‡ï¼‰
- **å‘ç‚¹**ï¼šchat template ä¸ä¸€è‡´ã€loss maskingï¼ˆåªå¯¹ assistant å›å¤ç®— lossï¼‰ã€è¿‡æ‹Ÿåˆï¼ˆ2-3 epochsï¼‰

### 4.3 Stage 2: Reward Model è®­ç»ƒ

**Bradley-Terry æ¨¡å‹**ï¼ˆInstructGPT è®ºæ–‡çš„ RM è®­ç»ƒæ ¸å¿ƒï¼‰ï¼š

> æ¥æºï¼šOuyang et al., "Training language models to follow instructions with human feedback", arXiv:2203.02155, Sec. 3.2

$$P(y_w \succ y_l | x) = \sigma(r_\theta(x, y_w) - r_\theta(x, y_l))$$

å…¶ä¸­ $y_w$ æ˜¯äººç±»åå¥½çš„å›ç­”ï¼Œ$y_l$ æ˜¯ä¸åå¥½çš„ã€‚

**RM Loss**ï¼š
$$\mathcal{L}_{RM} = -\mathbb{E}_{(x,y_w,y_l)}\left[\log\sigma(r_\theta(x,y_w) - r_\theta(x,y_l))\right]$$

**RM æ¶æ„**ï¼šé€šå¸¸åŸºäº SFT æ¨¡å‹ï¼Œå»æ‰ LM headï¼ŒåŠ ä¸€ä¸ª scalar head è¾“å‡ºå¥–åŠ±å€¼ã€‚

**å…³é”®æŒ‘æˆ˜**ï¼š
- æ ‡æ³¨ä¸€è‡´æ€§ä»… 65-75%ï¼ˆäººç±»æœ¬èº«å°±ä¸ä¸€è‡´ï¼‰
- **Length bias**ï¼šRM å€¾å‘ç»™é•¿å›ç­”é«˜åˆ†
- **Position bias**ï¼šæ ‡æ³¨è€…å€¾å‘é€‰ç¬¬ä¸€ä¸ªé€‰é¡¹
- **Verbosity bias**ï¼šè¯¦ç»†ä½†ç©ºæ´çš„å›ç­”å¾—é«˜åˆ†

### 4.4 Stage 3: PPO å¯¹é½

**RLHF ä¼˜åŒ–ç›®æ ‡**ï¼š
$$\max_{\pi_\theta} \mathbb{E}_{x\sim D, y\sim\pi_\theta(\cdot|x)}\left[r_\phi(x,y)\right] - \beta \cdot D_{KL}(\pi_\theta \| \pi_{ref})$$

**å››ä¸ªæ¨¡å‹åŒæ—¶å­˜åœ¨**ï¼š
1. **Actor**ï¼ˆ$\pi_\theta$ï¼‰ï¼šæ­£åœ¨ä¼˜åŒ–çš„ç­–ç•¥
2. **Critic**ï¼ˆ$V_\psi$ï¼‰ï¼šä¼°è®¡çŠ¶æ€ä»·å€¼
3. **Reward Model**ï¼ˆ$r_\phi$ï¼‰ï¼šå†»ç»“ï¼Œç»™ç”Ÿæˆæ‰“åˆ†
4. **Reference Model**ï¼ˆ$\pi_{ref}$ï¼‰ï¼šå†»ç»“çš„ SFT æ¨¡å‹ï¼Œç”¨äºè®¡ç®— KL penalty

**KL æ•£åº¦çº¦æŸçš„ä½œç”¨**ï¼š
- é˜²æ­¢ç­–ç•¥åç¦» SFT å¤ªè¿œï¼ˆä¿æŒè¯­è¨€è´¨é‡ï¼‰
- æŠµå¾¡ reward hackingï¼ˆé™åˆ¶æ¨¡å‹"ä½œå¼Šç©ºé—´"ï¼‰
- $\beta$ æ§åˆ¶ä¿å®ˆç¨‹åº¦ï¼š$\beta$ å¤§ â†’ è´´è¿‘ SFTï¼Œ$\beta$ å° â†’ æ›´æ¿€è¿›ä¼˜åŒ– reward

### 4.5 Reward Hacking

**å¸¸è§è¡¨ç°**ï¼š
- **Length exploitation**ï¼šç”Ÿæˆè¿‡é•¿ä½†ä½è´¨é‡å›ç­”éª—å–é«˜ RM åˆ†
- **Sycophancy**ï¼šè¿åˆç”¨æˆ·åå¥½è€Œéç»™å‡ºæ­£ç¡®ç­”æ¡ˆ
- **Formatting tricks**ï¼šç”¨ markdown/emoji ç­‰è·å–é«˜åˆ†
- **Distribution shift**ï¼šç­–ç•¥åç§»å RM æ³›åŒ–èƒ½åŠ›ä¸‹é™

**æ£€æµ‹ä¿¡å·**ï¼šRM score â†‘ ä½† human eval â†“ï¼ˆGoodhart's Lawï¼‰

**é˜²å¾¡æ‰‹æ®µ**ï¼š
- KL penaltyï¼ˆæœ€åŸºæœ¬ï¼‰
- RM ensembleï¼ˆå¤šæ¨¡å‹æŠ•ç¥¨å‡å°‘ä¸ªä½“åè§ï¼‰
- Length normalizationï¼ˆæŒ‰é•¿åº¦å½’ä¸€åŒ– rewardï¼‰
- è¿­ä»£ retrain RMï¼ˆç”¨æ–°ç­–ç•¥æ•°æ®æ›´æ–° RMï¼‰
- **Verifiable rewards**ï¼ˆä»£ç /æ•°å­¦æœ‰ ground truthï¼‰â€”â€”æ ¹æœ¬æ€§è§£å†³æ–¹æ¡ˆ

### é¢è¯•é«˜é¢‘é—®é¢˜

**Q: RLHF ä¸ºä»€ä¹ˆéœ€è¦ KL çº¦æŸï¼Ÿå»æ‰ä¼šæ€æ ·ï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ æ—  KL çº¦æŸ â†’ ç­–ç•¥è¿‡åº¦ä¼˜åŒ– RM â†’ reward hackingï¼ˆGoodhart's Lawï¼‰ï¼›â‘¡æ¨¡å‹å¯èƒ½é€€åŒ–ä¸ºä¸é€šé¡ºä½†é«˜ RM åˆ†çš„è¾“å‡ºï¼ˆè¯­è¨€è´¨é‡å´©æºƒï¼‰ï¼›â‘¢KL æä¾›æ­£åˆ™åŒ–æ•ˆæœï¼Œç­‰ä»·äºåœ¨ SFT é™„è¿‘åšçº¦æŸä¼˜åŒ–ï¼›â‘£$\beta$ æ˜¯å…³é”®è¶…å‚ï¼Œè¿‡å¤§æ¬ ä¼˜åŒ–è¿‡å°è¿‡æ‹Ÿåˆã€‚

**Q: RLHF çš„ PPO å’Œæ¸¸æˆ RL çš„ PPO æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ ç¯å¢ƒä¸åŒâ€”â€”LLM çš„"ç¯å¢ƒ"æ˜¯ RM è€ŒéçœŸå®ç¯å¢ƒï¼ŒRM å¯èƒ½æœ‰åï¼›â‘¡åŠ¨ä½œç©ºé—´å·¨å¤§â€”â€”è¯è¡¨ 32K-150K çš„ç¦»æ•£ç©ºé—´ï¼›â‘¢Reward ç¨€ç–â€”â€”æ•´ä¸ªå›å¤åªæœ‰ä¸€ä¸ª rewardï¼ˆsequence levelï¼‰ï¼Œä¸æ˜¯æ¯æ­¥éƒ½æœ‰ï¼›â‘£å¤šäº† KL çº¦æŸé¡¹ï¼›â‘¤è®¡ç®—è§„æ¨¡å®Œå…¨ä¸åŒâ€”â€”4 ä¸ªå¤§æ¨¡å‹å¹¶è¡Œã€‚

---

## äº”ã€RLHF æ›¿ä»£æ–¹æ¡ˆ

### 5.1 DPOï¼ˆDirect Preference Optimizationï¼‰

**æ ¸å¿ƒæ¨å¯¼**ï¼ˆRafailov et al., arXiv:2305.18290, Sec. 4ï¼‰ï¼šä» RLHF çš„ KL-constrained ä¼˜åŒ–å‡ºå‘ï¼Œæœ€ä¼˜ç­–ç•¥æœ‰è§£æè§£ï¼š
$$\pi^*(y|x) = \frac{1}{Z(x)} \pi_{ref}(y|x) \exp\left(\frac{1}{\beta} r(x,y)\right)$$

åè§£ rewardï¼š
$$r(x,y) = \beta \log\frac{\pi^*(y|x)}{\pi_{ref}(y|x)} + \beta \log Z(x)$$

ä»£å…¥ Bradley-Terry æ¨¡å‹ï¼Œ**$Z(x)$ åœ¨åšå·®æ—¶æ¶ˆæ‰**ï¼š

$$\mathcal{L}_{DPO} = -\mathbb{E}\left[\log\sigma\left(\beta\log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta\log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)}\right)\right]$$

**ä¼˜åŠ¿**ï¼š
- 2 æ¨¡å‹ï¼ˆpolicy + referenceï¼‰ï¼Œæ— éœ€ RM å’Œ Critic
- è®­ç»ƒç¨³å®šï¼Œç±» supervised learning
- å·¥ç¨‹ç®€å•ï¼ŒTRL å‡ è¡Œä»£ç æå®š

**åŠ£åŠ¿**ï¼š
- **Offline**ï¼šæ•°æ®æ¥è‡ªæ—§ç­–ç•¥ï¼Œæ— åœ¨çº¿æ¢ç´¢
- **Mode collapse**ï¼šæ²¡æœ‰ç†µ bonusï¼Œå®¹æ˜“åç¼©åˆ°å•ä¸€å›ç­”æ¨¡å¼
- **å¯¹å™ªå£°æ•æ„Ÿ**ï¼šåå¥½æ ‡ç­¾é”™è¯¯ç›´æ¥å½±å“æ¢¯åº¦æ–¹å‘

### 5.2 ORPOï¼ˆOdds Ratio Preference Optimizationï¼‰

- ä¸éœ€è¦ reference modelï¼Œå°†åå¥½ä¼˜åŒ–å’Œ SFT åˆå¹¶ä¸ºä¸€æ­¥
- ç”¨ odds ratio æ›¿ä»£ log probability ratio
- æ›´ç®€æ´ä½†æ€§èƒ½å¤©èŠ±æ¿ç•¥ä½äº DPO

### 5.3 KTOï¼ˆKahneman-Tversky Optimizationï¼‰

- **åªéœ€ binary signal**ï¼šæ¯æ¡æ•°æ®æ ‡æ³¨"å¥½"æˆ–"å"ï¼Œä¸éœ€è¦ pairwise æ¯”è¾ƒ
- åŸºäº Kahneman-Tversky **å‰æ™¯ç†è®º**ï¼Œloss aversion
- æ ‡æ³¨æˆæœ¬æ›´ä½ï¼Œé€‚åˆé«˜é£é™©åœºæ™¯ï¼ˆæ³•å¾‹/åŒ»ç–—ï¼‰
- å®éªŒè¡¨æ˜æ€§èƒ½ä¸ DPO æŒå¹³ç”šè‡³ç•¥ä¼˜

### 5.4 GRPOï¼ˆåç»­ä¸“é¢˜è¯¦è§£ï¼‰

### 5.5 RLAIFï¼ˆConstitutional AIï¼‰

> æ¥æºï¼šBai et al., "Constitutional AI: Harmlessness from AI Feedback", arXiv:2212.08073

- ç”¨ AI ä»£æ›¿äººç±»ç”Ÿæˆåå¥½æ•°æ®
- æµç¨‹ï¼šâ‘ LLM ç”Ÿæˆå›å¤ â†’ â‘¡AI æ ¹æ®"å®ªæ³•åŸåˆ™"è¯„åˆ¤ â†’ â‘¢ç”¨ AI åå¥½è®­ç»ƒ RM â†’ â‘£PPO
- Anthropic çš„æ ¸å¿ƒæ–¹æ³•è®ºï¼Œscalable ä½†å¯èƒ½æ”¾å¤§ AI åè§

### 5.6 Online vs Offline Preference Learning

| ç»´åº¦ | Onlineï¼ˆPPO/GRPOï¼‰ | Offlineï¼ˆDPO/KTOï¼‰ |
|------|-------|---------|
| æ•°æ®æ¥æº | å½“å‰ç­–ç•¥å®æ—¶ç”Ÿæˆ | é¢„å…ˆæ”¶é›†çš„å›ºå®šæ•°æ®é›† |
| Distribution matching | âœ… on-policy | âŒ off-policy |
| æ¢ç´¢èƒ½åŠ› | âœ… æŒç»­æ¢ç´¢æ–°å›ç­” | âŒ å—é™äºå·²æœ‰æ•°æ® |
| å·¥ç¨‹å¤æ‚åº¦ | é«˜ï¼ˆéœ€åœ¨çº¿ç”Ÿæˆ+è¯„åˆ†ï¼‰ | ä½ï¼ˆçº¯ supervisedï¼‰ |
| æ€§èƒ½å¤©èŠ±æ¿ | æ›´é«˜ | è¾ƒä½ |

**2026 å…±è¯†**ï¼šOnline > Offlineï¼Œä½† Offline çš„ç®€å•æ€§ä½¿å…¶åœ¨å®è·µä¸­å¹¿æ³›ä½¿ç”¨ã€‚æœ€ä¼˜æ–¹æ¡ˆæ˜¯ **Iterative DPO / Online DPO**ï¼ˆæ¯è½®ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆæ–°æ•°æ®ï¼‰ã€‚

### é¢è¯•é«˜é¢‘é—®é¢˜

**Q: æ¨å¯¼ DPO çš„ loss functionã€‚**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ ä» RLHF çš„ KL-constrained RL ç›®æ ‡å‡ºå‘ï¼›â‘¡å†™å‡ºæœ€ä¼˜ç­–ç•¥çš„é—­å¼è§£ï¼ˆGibbs distributionï¼‰ï¼›â‘¢åè§£ reward è¡¨è¾¾å¼ $r = \beta \log(\pi/\pi_{ref}) + \beta \log Z$ï¼›â‘£ä»£å…¥ Bradley-Terry æ¨¡å‹ $P(y_w \succ y_l) = \sigma(r_w - r_l)$ï¼›â‘¤$\log Z(x)$ åœ¨ $r_w - r_l$ åšå·®æ—¶æ¶ˆæ‰ï¼›â‘¥å¾—åˆ°åªä¾èµ–ç­–ç•¥æ¦‚ç‡çš„ lossã€‚å…³é”®æ´å¯Ÿï¼šDPO å°† RL é—®é¢˜è½¬åŒ–ä¸º classification é—®é¢˜ã€‚

**Q: DPO å’Œ RLHF å“ªä¸ªå¥½ï¼Ÿä»€ä¹ˆæ—¶å€™é€‰å“ªä¸ªï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ DPO ç®€å•ç¨³å®šä½† offline æ— æ¢ç´¢ï¼Œå¯èƒ½ mode collapseï¼›â‘¡RLHF æ€§èƒ½å¤©èŠ±æ¿æ›´é«˜ä½†å·¥ç¨‹å¤æ‚ï¼›â‘¢æœ‰ verifiable rewardï¼ˆä»£ç /æ•°å­¦ï¼‰â†’ GRPO/RLVRï¼›â‘£æ•°æ®å……è¶³ä¸”è¿½æ±‚ç®€å• â†’ DPOï¼›â‘¤è¿½æ±‚æè‡´æ€§èƒ½ â†’ Online RLï¼›â‘¥2026 è¶‹åŠ¿ï¼šIterative DPO æŠ˜ä¸­æ–¹æ¡ˆã€‚

---

## å…­ã€GRPO ä¸“é¢˜ï¼šDeepSeek çš„åˆ›æ–°

### 6.1 GRPO æ ¸å¿ƒæ€æƒ³

**Group Relative Policy Optimization**ï¼ˆDeepSeek-Math â†’ R1ï¼‰ï¼š

> æ¥æºï¼šShao et al., "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models", arXiv:2402.03300, Sec. 3.2

å¯¹æ¯ä¸ª prompt $x$ï¼Œç”¨å½“å‰ç­–ç•¥ç”Ÿæˆ $G$ ä¸ªå€™é€‰å›ç­” $\{y_1, ..., y_G\}$ï¼Œå„è‡ªè·å¾— reward $\{r_1, ..., r_G\}$ã€‚

**ç»„å†…æ ‡å‡†åŒ– advantage**ï¼š
$$\hat{A}_i = \frac{r_i - \mu}{\sigma}, \quad \mu = \frac{1}{G}\sum_j r_j, \quad \sigma = \text{std}(\{r_j\})$$

**GRPO ç›®æ ‡**ï¼š
$$\mathcal{L}_{GRPO} = \mathbb{E}\left[\frac{1}{G}\sum_{i=1}^{G} \min\left(r_t(\theta)\hat{A}_i, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_i\right) - \beta D_{KL}(\pi_\theta \| \pi_{ref})\right]$$

### 6.2 vs PPO çš„å…³é”®åŒºåˆ«

| ç»´åº¦ | PPO | GRPO |
|------|-----|------|
| Critic | éœ€è¦ Value Network $V_\psi$ | **ä¸éœ€è¦ Critic** |
| Advantage | GAEï¼ˆéœ€ $V(s)$ï¼‰ | ç»„å†…ç›¸å¯¹æ ‡å‡†åŒ– |
| æ˜¾å­˜ | 4 æ¨¡å‹ | 3 æ¨¡å‹ï¼ˆçœ ~25%ï¼‰ |
| é€‚ç”¨ reward | è¿ç»­å¯†é›† | ç‰¹åˆ«é€‚åˆ **ç¨€ç–/äºŒå€¼ reward** |

### 6.3 Rule-based Reward è®¾è®¡

DeepSeek-R1 çš„ reward ä¸ç”¨ RMï¼Œè€Œæ˜¯ç”¨**è§„åˆ™éªŒè¯**ï¼š

- **æ•°å­¦**ï¼šç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆç²¾ç¡®åŒ¹é…ï¼ˆæˆ–æ•°å€¼è¿‘ä¼¼ï¼‰
- **ä»£ç **ï¼šæµ‹è¯•ç”¨ä¾‹é€šè¿‡ç‡
- **æ ¼å¼**ï¼šæ˜¯å¦åŒ…å« `<think>...</think>` æ¨ç†è¿‡ç¨‹
- **é•¿åº¦**ï¼šæƒ©ç½šè¿‡çŸ­æˆ–è¿‡é•¿å›ç­”

$$r(x,y) = \alpha \cdot r_{accuracy} + \beta \cdot r_{format} + \gamma \cdot r_{length}$$

### 6.4 DeepSeek-R1 çš„ RL è®­ç»ƒæµç¨‹

1. **Cold Start**ï¼šå°‘é‡ long-CoT æ•°æ® SFTï¼Œè®©æ¨¡å‹å­¦ä¼šè¾“å‡ºæ¨ç†æ ¼å¼
2. **RL Stage 1**ï¼šGRPO + rule-based rewardï¼Œä» base model æ¶Œç°æ¨ç†èƒ½åŠ›
3. **Rejection Sampling**ï¼šç”¨ RL æ¨¡å‹ç”Ÿæˆé«˜è´¨é‡æ¨ç†è½¨è¿¹
4. **SFT Stage 2**ï¼šåœ¨æ··åˆæ•°æ®ä¸Šå†åš SFTï¼ˆæ¨ç† + éæ¨ç†ä»»åŠ¡ï¼‰
5. **RL Stage 2**ï¼šæœ€ç»ˆ GRPO å¯¹é½ï¼ŒåŒæ—¶ä¼˜åŒ–æ¨ç†å‡†ç¡®æ€§å’Œé€šç”¨èƒ½åŠ›

**å…³é”®å‘ç°**ï¼š
- **æ¶Œç°ç°è±¡**ï¼šæ¨¡å‹è‡ªå‘å­¦ä¼š CoT æ¨ç†ã€self-reflectionã€"aha moment"
- RL **ä¸æ˜¯æ•™æ¨ç†**ï¼Œè€Œæ˜¯**æ¿€å‘é¢„è®­ç»ƒä¸­å·²æœ‰çš„ latent capability**
- è’¸é¦å¯ä»¥å°†æ¨ç†èƒ½åŠ›è½¬ç§»åˆ°å°æ¨¡å‹ï¼ˆR1-distill-Qwen-32B è¶…è¶Š GPT-4oï¼‰

### 6.5 DAPO å¯¹ GRPO çš„æ”¹è¿›

**Decoupled Alignment from Preference Optimization**ï¼ˆByteDanceï¼‰ï¼š
1. **Decoupled Clipping**ï¼šæ­£å‘å¥–åŠ±ç”¨æ›´å¤§çš„ clip èŒƒå›´ $(1-\epsilon_1, 1+\epsilon_2)$ï¼Œ$\epsilon_2 > \epsilon_1$
2. **Dynamic Sampling**ï¼šè¿‡æ»¤å…¨å¯¹/å…¨é”™çš„ prompt groupï¼ˆæ— ä¿¡æ¯é‡ï¼‰
3. **Token-Level Loss**ï¼šæŒ‰ token å½’ä¸€åŒ– lossï¼Œé˜²æ­¢é•¿ response ä¸»å¯¼æ¢¯åº¦
4. **Overlong Reward Shaping**ï¼šè¶…é•¿å›ç­” soft penalty è€Œéç¡¬æˆªæ–­

### é¢è¯•é«˜é¢‘é—®é¢˜

**Q: GRPO ä¸ºä»€ä¹ˆä¸éœ€è¦ Criticï¼Ÿè¿™æ ·åšæœ‰ä»€ä¹ˆé—®é¢˜ï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ GRPO ç”¨åŒç»„å›ç­”çš„ç›¸å¯¹ reward åš baselineï¼Œä¸éœ€è¦å­¦ä¹ ä¸€ä¸ª Value Networkï¼›â‘¡æœ¬è´¨ä¸Šæ˜¯ç»„å†… REINFORCE + variance reduction via normalizationï¼›â‘¢ä¼˜åŠ¿ï¼šçœæ˜¾å­˜ã€å®ç°ç®€å•ã€å¯¹ç¨€ç– reward å‹å¥½ï¼›â‘£é—®é¢˜ï¼šéœ€è¦å¤šæ¬¡é‡‡æ ·ï¼ˆG=8-64ï¼‰ï¼Œå¢åŠ ç”Ÿæˆæˆæœ¬ï¼›â‘¤ç»„å†…æ–¹å·®è¿‡å°æ—¶ advantage ä¼°è®¡ä¸ç¨³å®šï¼›â‘¥åªé€‚åˆ per-episode rewardï¼Œä¸é€‚åˆ dense rewardã€‚

**Q: DeepSeek-R1 çš„ "aha moment" æ˜¯æ€ä¹ˆå›äº‹ï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ åœ¨çº¯ RL è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹åœ¨æŸä¸ªé˜¶æ®µçªç„¶å¼€å§‹è¾“å‡ºåŒ…å«è‡ªæˆ‘çº æ­£ã€åæ€æ€§è¯­è¨€çš„æ¨ç†é“¾ï¼›â‘¡è¿™ä¸æ˜¯è®­ç»ƒæ•°æ®æ•™çš„ï¼Œè€Œæ˜¯ RL æ¢ç´¢è¿‡ç¨‹ä¸­æ¶Œç°çš„è¡Œä¸ºï¼›â‘¢è¯´æ˜å¤§æ¨¡å‹é¢„è®­ç»ƒæ—¶å·²ç»è·å¾—äº†æ¨ç†çš„ latent capabilityï¼ŒRL åªæ˜¯æ¿€æ´»äº†å®ƒï¼›â‘£ç±»ä¼¼ scaling law ä¸­çš„æ¶Œç°èƒ½åŠ›ï¼Œä½†è¿™æ¬¡æ˜¯é€šè¿‡ RL è§¦å‘è€Œéæ¨¡å‹è§„æ¨¡ã€‚

---

## ä¸ƒã€å¤š Agent RLï¼ˆMARLï¼‰

### 7.1 MARL åŸºç¡€

**æ ¸å¿ƒæŒ‘æˆ˜**ï¼šæ¯ä¸ª agent çš„"ç¯å¢ƒ"åŒ…å«å…¶ä»– agent çš„è¡Œä¸º â†’ **éå¹³ç¨³æ€§**

**èŒƒå¼åˆ†ç±»**ï¼š
- **å…¨åˆä½œ**ï¼ˆCooperativeï¼‰ï¼šå…±äº«å¥–åŠ±ï¼Œå›¢é˜Ÿæœ€ä¼˜ï¼ˆå¦‚ StarCraft å¾®æ“ï¼‰
- **å…¨ç«äº‰**ï¼ˆCompetitiveï¼‰ï¼šé›¶å’Œåšå¼ˆï¼ˆå¦‚å›´æ£‹ã€æ‰‘å…‹ï¼‰
- **æ··åˆ**ï¼ˆMixedï¼‰ï¼šåˆä½œ+ç«äº‰ï¼ˆå¦‚å¤šäººæ¸¸æˆã€è‡ªåŠ¨é©¾é©¶ï¼‰

**å­¦ä¹ æ¡†æ¶**ï¼š
- **CTDE**ï¼ˆCentralized Training Decentralized Executionï¼‰ï¼šè®­ç»ƒæ—¶å…±äº«ä¿¡æ¯ï¼Œæ‰§è¡Œæ—¶ç‹¬ç«‹å†³ç­–
- **Independent Learners**ï¼šæ¯ä¸ª agent ç‹¬ç«‹ç”¨å• agent RLï¼Œç®€å•ä½†å¿½ç•¥åè°ƒ
- **Communication Learning**ï¼šagent é—´å­¦ä¹ é€šä¿¡åè®®

### 7.2 ç»å…¸ç®—æ³•

- **QMIX**ï¼šå°†å…¨å±€ Q åˆ†è§£ä¸ºå„ agent çš„å±€éƒ¨ Q çš„å•è°ƒæ··åˆï¼Œä¿è¯åˆ†å¸ƒå¼æ‰§è¡Œ
- **CommNet**ï¼šå­¦ä¹  agent é—´è¿ç»­é€šä¿¡å‘é‡
- **MAPPO**ï¼šPPO ç›´æ¥åº”ç”¨åˆ° MARLï¼Œcentralized critic + decentralized actorsï¼Œç®€å•é«˜æ•ˆ
- **AlphaStar**ï¼šSC2 çš„ league trainingï¼Œå¤š agent è‡ªå¯¹å¼ˆç”Ÿæ€

### 7.3 LLM Agent ä¸­çš„ MARL åº”ç”¨

**2026 å‰æ²¿**ï¼š
- **Multi-Agent Debate**ï¼šå¤šä¸ª LLM agent è¾©è®ºæ±‚è§£ï¼Œæé«˜æ¨ç†å‡†ç¡®æ€§
- **Agent åˆ†å·¥**ï¼šPlanner Agent + Coder Agent + Reviewer Agent åä½œ
- **Agent åšå¼ˆ**ï¼šçº¢é˜Ÿ vs è“é˜Ÿï¼ˆæ”»é˜²å®‰å…¨æµ‹è¯•ï¼‰
- **Negotiation**ï¼šLLM agent å­¦ä¹ è°ˆåˆ¤ç­–ç•¥ï¼ˆå•†ä¸š/å¤–äº¤åœºæ™¯ï¼‰

**æŒ‘æˆ˜**ï¼š
- Agent é—´è‡ªç„¶è¯­è¨€é€šä¿¡æ˜¯ lossy channelï¼ˆä¿¡æ¯ä¸¢å¤±+è¯¯è§£ï¼‰
- ä¿¡ç”¨åˆ†é…éš¾ï¼ˆCredit Assignmentï¼‰ï¼šå›¢é˜ŸæˆåŠŸå½’å› äºå“ªä¸ª agentï¼Ÿ
- æ‰©å±•æ€§ï¼šagent æ•°é‡å¢å¤š â†’ çŠ¶æ€ç©ºé—´æŒ‡æ•°çˆ†ç‚¸

### é¢è¯•é«˜é¢‘é—®é¢˜

**Q: ä¸ºä»€ä¹ˆ MARL æ¯”å• agent RL éš¾å¾—å¤šï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ éå¹³ç¨³æ€§â€”â€”æ¯ä¸ª agent çš„"ç¯å¢ƒ"åœ¨åŠ¨ï¼ˆå…¶ä»– agent åœ¨å­¦ä¹ ï¼‰ï¼›â‘¡è”åˆåŠ¨ä½œç©ºé—´æŒ‡æ•°å¢é•¿ $|A|^N$ï¼›â‘¢ä¿¡ç”¨åˆ†é…é—®é¢˜â€”â€”å›¢é˜Ÿå¥–åŠ±å¦‚ä½•æ‹†åˆ†ç»™ä¸ªä½“ï¼›â‘£é€šä¿¡å¼€é”€â€”â€”agent é—´éœ€è¦åè°ƒä½†é€šä¿¡æœ‰é™ï¼›â‘¤ç†è®ºä¿è¯å¼±â€”â€”ä¸å†æœ‰ Bellman æœ€ä¼˜æ€§è¿™æ ·çš„ç®€æ´ä¿è¯ã€‚

---

## å…«ã€RL for LLMï¼šæ¨ç†æ—¶è®¡ç®—ä¸æœç´¢

### 8.1 Test-time Compute Scaling

**æ ¸å¿ƒæ€æƒ³**ï¼ˆOpenAI o1/o3, DeepSeek R1ï¼‰ï¼šåœ¨æ¨ç†é˜¶æ®µèŠ±æ›´å¤šè®¡ç®—æ¢æ›´é«˜å‡†ç¡®ç‡ã€‚

**ä¸¤ç§è·¯çº¿**ï¼š
- **Internal CoT**ï¼šæ¨¡å‹è‡ªèº«ç”Ÿæˆé•¿æ¨ç†é“¾ï¼ˆo1/R1 è·¯çº¿ï¼‰ï¼Œé€šè¿‡ RL è®­ç»ƒè·å¾—
- **External Search**ï¼šç”¨ MCTS/beam search åœ¨è§£ç©ºé—´æœç´¢ï¼Œéœ€è¦ reward model æŒ‡å¯¼

**Scaling Law**ï¼šæ¨ç† FLOPs âˆ å‡†ç¡®ç‡æå‡ï¼Œä½†è¾¹é™…é€’å‡ã€‚åœ¨éš¾é¢˜ä¸ŠæŠ•å…¥æ›´å¤šæ¨ç†è®¡ç®—æ¯”å¢å¤§æ¨¡å‹æ›´é«˜æ•ˆã€‚

### 8.2 Process Reward Model vs Outcome Reward Model

| ç»´åº¦ | ORM | PRM |
|------|-----|-----|
| ç²’åº¦ | æ•´ä¸ªå›å¤ä¸€ä¸ªåˆ†æ•° | æ¯ä¸ªæ¨ç†æ­¥éª¤ä¸€ä¸ªåˆ†æ•° |
| æ ‡æ³¨ | ç®€å•ï¼ˆå¥½/åï¼‰ | æ˜‚è´µï¼ˆæ¯æ­¥æ ‡æ³¨æ­£ç¡®æ€§ï¼‰ |
| Signal | ç¨€ç– | å¯†é›† |
| æœç´¢é…åˆ | beam search / rejection sampling | tree search / step-level search |
| é€‚ç”¨åœºæ™¯ | é€šç”¨ | æ•°å­¦/ä»£ç æ¨ç† |

**PRM + Tree Search** åœ¨æ•°å­¦æ¨ç†ä¸Šæ˜¾è‘—ä¼˜äº ORMï¼ˆMath-Shepherd, OpenAI PRM800Kï¼‰ã€‚

### 8.3 MCTS + LLM

å°† LLM çš„æ¨ç†è¿‡ç¨‹å»ºæ¨¡ä¸ºæœç´¢æ ‘ï¼š
- **çŠ¶æ€**ï¼šå½“å‰æ¨ç†æ­¥éª¤åºåˆ—
- **åŠ¨ä½œ**ï¼šç”Ÿæˆä¸‹ä¸€ä¸ªæ¨ç†æ­¥éª¤
- **Reward**ï¼šPRM è¯„åˆ†æˆ–æœ€ç»ˆç­”æ¡ˆæ­£ç¡®æ€§

**æµç¨‹**ï¼šSelection â†’ Expansionï¼ˆLLM ç”Ÿæˆå€™é€‰æ­¥éª¤ï¼‰â†’ Simulation â†’ Backpropagation

**ä»£è¡¨å·¥ä½œ**ï¼šAlphaGeometryï¼ˆæ•°å­¦å¥¥èµ›ï¼‰ã€AlphaCode/AlphaCode2ï¼ˆä»£ç ç«èµ›ï¼‰

### 8.4 Self-Play

- **å¯¹æŠ—æ€§ Self-Play**ï¼šæ¨¡å‹ä¸è‡ªå·±çš„å†å²ç‰ˆæœ¬åšå¼ˆï¼ˆAlphaGo/AlphaZeroï¼‰
- **åˆä½œæ€§ Self-Play**ï¼šæ¨¡å‹è‡ªæˆ‘è¾©è®ºæé«˜æ¨ç†ï¼ˆLLM debateï¼‰
- **è¿­ä»£ Self-Play**ï¼šç”Ÿæˆæ•°æ® â†’ ç­›é€‰ â†’ è®­ç»ƒ â†’ ç”Ÿæˆæ›´å¥½æ•°æ® â†’ ...
- **Self-Rewarding LLM**ï¼šæ¨¡å‹åŒæ—¶ä½œä¸º generator å’Œ judgeï¼Œè¿­ä»£è‡ªæˆ‘æ”¹è¿›

### é¢è¯•é«˜é¢‘é—®é¢˜

**Q: Process RM vs Outcome RM æ€ä¹ˆé€‰ï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ æ•°å­¦/ä»£ç ç­‰æœ‰æ˜ç¡®ä¸­é—´æ­¥éª¤åˆ¤æ–­çš„ä»»åŠ¡ â†’ PRM æ˜¾è‘—æ›´ä¼˜ï¼›â‘¡é€šç”¨å¯¹è¯/åˆ›æ„å†™ä½œ â†’ ORM è¶³å¤Ÿï¼ˆä¸­é—´æ­¥éª¤éš¾å®šä¹‰ï¼‰ï¼›â‘¢PRM çš„æ ‡æ³¨æˆæœ¬é«˜ 10-100Ã—ï¼Œå¯ç”¨è‡ªåŠ¨æ ‡æ³¨ï¼ˆMath-Shepherd: ç”¨è’™ç‰¹å¡æ´›ä¼°è®¡æ¯æ­¥æ­£ç¡®ç‡ï¼‰é™ä½æˆæœ¬ï¼›â‘£PRM çš„ä»·å€¼åœ¨äº dense signal å¯æŒ‡å¯¼ tree searchã€‚

**Q: Test-time compute scaling çš„æé™åœ¨å“ªï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ å¯¹å¯éªŒè¯é—®é¢˜ï¼ˆæ•°å­¦/ä»£ç ï¼‰æ•ˆæœæ˜¾è‘—ï¼Œå¯¹å¼€æ”¾å¼é—®é¢˜æå‡æœ‰é™ï¼›â‘¡è¾¹é™…æ”¶ç›Šé€’å‡â€”â€”ç®€å•é—®é¢˜å¾ˆå¿«é¥±å’Œï¼Œæéš¾é—®é¢˜å¯èƒ½æ— è§£ï¼›â‘¢æˆæœ¬ï¼šæ¯æ¬¡æ¨ç†æ¶ˆè€—å¤§é‡ç®—åŠ›ï¼Œå»¶è¿Ÿé«˜ï¼ˆo1 å¯èƒ½æ€è€ƒ 10+ ç§’ï¼‰ï¼›â‘£ä¸æ¨¡å‹èƒ½åŠ›çš„å…³ç³»ï¼šæ¨¡å‹å¤ªå¼±æ—¶å†å¤šæ¨ç†è®¡ç®—ä¹Ÿæ— æµäºäº‹ï¼ˆéœ€è¦åŸºç¡€èƒ½åŠ›æ‰“åº•ï¼‰ã€‚

---

## ä¹ã€RL åœ¨æ¨èç³»ç»Ÿä¸­çš„åº”ç”¨

### 9.1 Bandit ç®—æ³•

æ¨èç³»ç»Ÿçš„æœ€åŸºæœ¬ RL å½¢å¼â€”â€”å•æ­¥å†³ç­–çš„ exploration-exploitation é—®é¢˜ã€‚

**UCBï¼ˆUpper Confidence Boundï¼‰**ï¼š
$$a^* = \arg\max_a \left[\hat{\mu}_a + c\sqrt{\frac{\ln t}{N_a}}\right]$$

é€‰æ‹©"ä¹è§‚ä¼°è®¡"æœ€é«˜çš„è‡‚ï¼Œè‡ªç„¶å¹³è¡¡æ¢ç´¢ï¼ˆä¸ç¡®å®šæ€§é«˜ â†’ æ¢ç´¢ï¼‰å’Œåˆ©ç”¨ï¼ˆæœŸæœ›é«˜ â†’ åˆ©ç”¨ï¼‰ã€‚

**Thompson Sampling**ï¼š
- ä¸ºæ¯ä¸ªè‡‚ç»´æŠ¤ reward çš„åéªŒåˆ†å¸ƒï¼ˆå¦‚ Beta åˆ†å¸ƒï¼‰
- æ¯æ¬¡ä»åéªŒé‡‡æ ·ï¼Œé€‰æ‹©é‡‡æ ·å€¼æœ€é«˜çš„è‡‚
- è´å¶æ–¯æ–¹æ³•ï¼Œç†è®º regret æœ€ä¼˜
- å®è·µä¸­æ¯” UCB æ›´å¥½ï¼Œç‰¹åˆ«æ˜¯ delayed feedback åœºæ™¯

### 9.2 Contextual Bandits

åŠ å…¥ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆç”¨æˆ·ç‰¹å¾ã€item ç‰¹å¾ï¼‰ï¼š
$$a^* = \arg\max_a f(x, a; \theta)$$

- **LinUCB**ï¼šçº¿æ€§æ¨¡å‹ + UCB ç½®ä¿¡åŒºé—´
- **Neural Contextual Bandits**ï¼šç”¨ç¥ç»ç½‘ç»œå»ºæ¨¡ $f$
- **åº”ç”¨**ï¼šæ–°é—»æ¨èï¼ˆæ¯æ¡æ–°é—»æ˜¯ä¸€ä¸ªè‡‚ï¼‰ã€å¹¿å‘ŠæŠ•æ”¾ï¼ˆæ¯ä¸ªå¹¿å‘Šæ˜¯ä¸€ä¸ªè‡‚ï¼‰

### 9.3 Full RL in RecSys

**Slate Optimization**ï¼šä¸€æ¬¡æ¨èå¤šä¸ª item çš„åˆ—è¡¨ï¼ˆslateï¼‰ï¼Œè€ƒè™‘ item é—´äº¤äº’

- **SlateQ**ï¼šå°† slate å»ºæ¨¡ä¸º combinatorial actionï¼ŒQ å€¼åˆ†è§£
- **ç”¨æˆ·æ¨¡å‹**ï¼šå°†ç”¨æˆ·çš„æµè§ˆ/ç‚¹å‡»/è´­ä¹°å»ºæ¨¡ä¸º MDP
- **é•¿æœŸä»·å€¼ä¼˜åŒ–**ï¼šä¸åªä¼˜åŒ–å³æ—¶ç‚¹å‡»ï¼Œè€ƒè™‘ç”¨æˆ·ç•™å­˜/ç”Ÿå‘½å‘¨æœŸä»·å€¼
- **æŒ‘æˆ˜**ï¼šçŠ¶æ€ç©ºé—´å·¨å¤§ã€reward å»¶è¿Ÿï¼ˆè´­ä¹°å¯èƒ½åœ¨å‡ å¤©åï¼‰ã€A/B test çš„ exploration cost

### é¢è¯•é«˜é¢‘é—®é¢˜

**Q: Thompson Sampling vs UCB çš„ä¼˜åŠ£ï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ Thompson Sampling æ˜¯è´å¶æ–¯æ–¹æ³•ï¼ˆç»´æŠ¤åéªŒåˆ†å¸ƒï¼‰ï¼ŒUCB æ˜¯é¢‘ç‡æ–¹æ³•ï¼ˆç»´æŠ¤ç½®ä¿¡åŒºé—´ï¼‰ï¼›â‘¡TS åœ¨å»¶è¿Ÿåé¦ˆã€æ‰¹é‡å†³ç­–åœºæ™¯æ›´å¥½ï¼ˆåéªŒè‡ªç„¶å¤„ç†ä¸ç¡®å®šæ€§ï¼‰ï¼›â‘¢UCB ç†è®ºåˆ†ææ›´æ¸…æ™°ï¼ˆå¯æ¨å¯¼ regret boundï¼‰ï¼›â‘£å®è·µä¸­ TS é€šå¸¸ç•¥ä¼˜ä¸”æ›´å®¹æ˜“å®ç°ï¼ˆä¸éœ€è¦è°ƒ c å‚æ•°ï¼‰ï¼›â‘¤TS å¯è‡ªç„¶æ‰©å±•åˆ°éå¹³ç¨³ç¯å¢ƒï¼ˆç”¨æ»‘åŠ¨çª—å£æ›´æ–°åéªŒï¼‰ã€‚

---

## åã€RL åœ¨æœºå™¨äººä¸è‡ªåŠ¨é©¾é©¶ä¸­çš„åº”ç”¨

### 10.1 Sim-to-Real Transfer

**æ ¸å¿ƒå›°éš¾**ï¼šæ¨¡æ‹Ÿå™¨ä¸çœŸå®ä¸–ç•Œçš„å·®è·ï¼ˆreality gapï¼‰

**Domain Randomization**ï¼šè®­ç»ƒæ—¶éšæœºåŒ–æ¨¡æ‹Ÿå™¨å‚æ•°ï¼ˆæ‘©æ“¦ã€å…‰ç…§ã€è´¨é‡...ï¼‰ï¼Œè®©ç­–ç•¥å¯¹ç¯å¢ƒå˜åŒ–é²æ£’

**System Identification**ï¼šç²¾ç¡®å»ºæ¨¡çœŸå®ç‰©ç†å‚æ•°

**Progressive Transfer**ï¼šå…ˆåœ¨ç®€å•ä»¿çœŸè®­ç»ƒï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦æ¥è¿‘çœŸå®

### 10.2 Curriculum Learning

- ä»ç®€å•ä»»åŠ¡å¼€å§‹ï¼Œé€æ­¥å¢åŠ éš¾åº¦
- **Automatic Curriculum**ï¼šæ ¹æ® agent çš„å­¦ä¹ è¿›åº¦è‡ªåŠ¨è°ƒèŠ‚éš¾åº¦
- **Self-Paced Learning**ï¼šagent è‡ªå·±é€‰æ‹©é€‚å½“éš¾åº¦çš„ä»»åŠ¡
- **åº”ç”¨**ï¼šçµå·§æ‰‹æ“ä½œï¼ˆOpenAI Rubik's Cubeï¼‰ã€è¶³å¼æœºå™¨äººè¡Œèµ°

### 10.3 Safety Constraintsï¼ˆConstrained MDPï¼‰

$$\max_\pi J(\pi) \quad \text{s.t.} \quad C_i(\pi) \leq d_i, \quad i=1,...,k$$

- **Lagrangian Method**ï¼šå°†çº¦æŸè½¬ä¸ºç½šé¡¹ï¼Œç”¨å¯¹å¶ä¼˜åŒ–
- **CPO**ï¼ˆConstrained Policy Optimizationï¼‰ï¼štrust region å†…æ»¡è¶³çº¦æŸçš„æ›´æ–°
- **Shield/Safety Layer**ï¼šåœ¨ action è¾“å‡ºååŠ å®‰å…¨æ£€æŸ¥å±‚ï¼Œç¡¬çº¦æŸ
- **åº”ç”¨**ï¼šè‡ªåŠ¨é©¾é©¶ï¼ˆä¸å¯ç¢°æ’/è¿è§„ï¼‰ã€å·¥ä¸šæœºå™¨äººï¼ˆåŠ›çŸ©é™åˆ¶ï¼‰

### 10.4 2026 å‰æ²¿

- **Foundation Model for Robotics**ï¼šVLAï¼ˆVision-Language-Actionï¼‰æ¨¡å‹ï¼ˆRT-2/Octo/Ï€0ï¼‰
- **World Models**ï¼šå­¦ä¹ ç¯å¢ƒåŠ¨åŠ›å­¦æ¨¡å‹ â†’ åœ¨æ¨¡å‹ä¸­åš planningï¼ˆå‡å°‘çœŸå®äº¤äº’ï¼‰
- **Humanoid Control**ï¼šTesla Optimus / Figure ç”¨ RL è®­ç»ƒäººå½¢æœºå™¨äºº

### é¢è¯•é«˜é¢‘é—®é¢˜

**Q: Sim-to-Real çš„æ ¸å¿ƒæŒ‘æˆ˜å’Œè§£å†³æ–¹æ¡ˆï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ Reality gapâ€”â€”æ¨¡æ‹Ÿå™¨æ— æ³•å®Œç¾è¿˜åŸçœŸå®ç‰©ç†ï¼ˆæ‘©æ“¦ã€æ¥è§¦ã€å»¶è¿Ÿï¼‰ï¼›â‘¡Domain Randomizationï¼šéšæœºåŒ–å‚æ•°è®©ç­–ç•¥é²æ£’ï¼Œç®€å•æœ‰æ•ˆä½†å¯èƒ½è¿‡äºä¿å®ˆï¼›â‘¢System Identificationï¼šç²¾ç¡®å»ºæ¨¡ä½†æˆæœ¬é«˜ä¸”ä»æœ‰è¯¯å·®ï¼›â‘£Sim+Real æ··åˆè®­ç»ƒï¼šå…ˆåœ¨ä»¿çœŸé¢„è®­ç»ƒå†åœ¨çœŸå®åœºæ™¯ fine-tuneï¼›â‘¤World Modelï¼šå­¦ä¹ çœŸå®ç¯å¢ƒåŠ¨åŠ›å­¦ï¼Œå‡å°‘ä»¿çœŸä¾èµ–ã€‚

---

## åä¸€ã€RL å·¥ç¨‹å®è·µ

### 11.1 è¶…å‚è°ƒä¼˜æŒ‡å—

| è¶…å‚ | æ¨èå€¼ | æ³¨æ„äº‹é¡¹ |
|------|--------|----------|
| Learning Rate | 1e-5 ~ 5e-5ï¼ˆLLMï¼‰ | PPO å¯¹ LR æ•æ„Ÿï¼Œè¿‡å¤§ç­–ç•¥å´©æºƒ |
| Batch Size | 128-512ï¼ˆpromptsï¼‰ | è¶Šå¤§è¶Šç¨³å®šï¼Œä½†å†…å­˜é™åˆ¶ |
| Clip Range Îµ | 0.1-0.2 | å¤ªå°æ›´æ–°æ…¢ï¼Œå¤ªå¤§ç­–ç•¥ä¸ç¨³å®š |
| GAE Î» | 0.95 | å¯†é›† reward å¯é™åˆ° 0.9 |
| KL Penalty Î² | 0.01-0.1 | ç›‘æ§ KL åŠ¨æ€è°ƒæ•´ |
| Epochs per batch | 1-4 | è¶…è¿‡ 4 æ¬¡å®¹æ˜“è¿‡æ‹Ÿåˆ |
| Mini-batch size | 64-256 | å¤ªå°æ–¹å·®å¤§ï¼Œå¤ªå¤§å¤±å»éšæœºæ€§ |
| Generation Group G | 8-64ï¼ˆGRPOï¼‰ | è¶Šå¤§ advantage ä¼°è®¡è¶Šå‡†ä½†æˆæœ¬è¶Šé«˜ |

### 11.2 è®­ç»ƒä¸ç¨³å®šé—®é¢˜è¯Šæ–­

**ç—‡çŠ¶ â†’ åŸå›  â†’ è§£æ³•**ï¼š

1. **Reward éœ‡è¡ä¸æ”¶æ•›**
   - åŸå› ï¼šLR è¿‡å¤§ / clip range è¿‡å¤§
   - è§£æ³•ï¼šé™ LRã€å‡å° Îµ

2. **KL æŒç»­å¢å¤§**
   - åŸå› ï¼šç­–ç•¥åç¦» SFT å¤ªè¿œ
   - è§£æ³•ï¼šå¢å¤§ Î² / é‡æ–°åˆå§‹åŒ– reference model

3. **Entropy å¿«é€Ÿä¸‹é™**
   - åŸå› ï¼šç­–ç•¥è¿‡æ—©åç¼©ï¼ˆmode collapseï¼‰
   - è§£æ³•ï¼šå¢å¤§ entropy bonus / é™ä½ Î² / å¢åŠ æ•°æ®å¤šæ ·æ€§

4. **Reward hackingï¼ˆRM score â†‘, human eval â†“ï¼‰**
   - åŸå› ï¼šRM æœ‰å / ç­–ç•¥æ‰¾åˆ°æ¼æ´
   - è§£æ³•ï¼šRM ensemble / length normalization / retrain RM

5. **Loss spike**
   - åŸå› ï¼šæ•°æ®å¼‚å¸¸ / æ•°å€¼æº¢å‡º
   - è§£æ³•ï¼šgradient clipping + BF16 + å›æ»š checkpoint

### 11.3 Reward Shaping

**åŸåˆ™**ï¼šå¡‘é€  reward ä¸æ”¹å˜æœ€ä¼˜ç­–ç•¥ï¼ˆpotential-based reward shapingï¼‰

$$r'(s,a,s') = r(s,a,s') + \gamma\Phi(s') - \Phi(s)$$

å…¶ä¸­ $\Phi$ æ˜¯ä»»æ„åŠ¿å‡½æ•°ï¼Œä¿è¯æœ€ä¼˜ç­–ç•¥ä¸å˜ã€‚

**å®è·µæŠ€å·§**ï¼š
- é¿å… reward å¤ªç¨€ç–ï¼ˆä¸­é—´ reward å¼•å¯¼å­¦ä¹ æ–¹å‘ï¼‰
- é¿å… reward å¤ªå¯†é›†ï¼ˆå®¹æ˜“è¢« hackï¼‰
- Multi-objective rewardï¼šåŠ æƒç»„åˆï¼Œæ³¨æ„å½’ä¸€åŒ–
- ä¸Šçº¿å‰ç”¨ human eval éªŒè¯ reward ä¸çœŸå®è´¨é‡çš„ç›¸å…³æ€§

### 11.4 ç¯å¢ƒè®¾è®¡

- **Action Space**ï¼štoken-level vs sentence-levelï¼Œç²’åº¦å½±å“è®­ç»ƒæ•ˆç‡
- **Observation**ï¼šprompt + å·²ç”Ÿæˆ tokens + optional context
- **Episode å®šä¹‰**ï¼šä¸€æ¬¡å®Œæ•´ç”Ÿæˆ = ä¸€ä¸ª episode
- **Parallelism**ï¼švLLM/SGLang åšå¤§è§„æ¨¡å¹¶è¡Œ generation æ˜¯è®­ç»ƒç“¶é¢ˆçš„å…³é”®ä¼˜åŒ–

### é¢è¯•é«˜é¢‘é—®é¢˜

**Q: RLHF è®­ç»ƒä¸ç¨³å®šæ€ä¹ˆæ’æŸ¥ï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šçœ‹å››ä¸ªå…³é”®æŒ‡æ ‡ï¼šâ‘ Reward è¶‹åŠ¿ï¼ˆåº”å¹³ç¨³ä¸Šå‡ï¼‰ï¼›â‘¡KL æ•£åº¦ï¼ˆåº”åœ¨ target èŒƒå›´å†…ï¼‰ï¼›â‘¢Policy entropyï¼ˆä¸åº”è¿‡å¿«ä¸‹é™ï¼‰ï¼›â‘£Generation qualityï¼ˆå®šæœŸäººå·¥ spot-checkï¼‰ã€‚æ ¹æ®å¼‚å¸¸æŒ‡æ ‡å®šä½åŸå› ï¼šreward éœ‡è¡â†’é™ LRï¼›KL çˆ†ç‚¸â†’å¢ Î²ï¼›entropy åç¼©â†’åŠ  entropy bonusã€‚æ°¸è¿œç”¨ wandb è®°å½•æ‰€æœ‰è®­ç»ƒæŒ‡æ ‡ã€‚

---

## åäºŒã€2026 å‰æ²¿è¶‹åŠ¿

### 12.1 Reasoning via RLï¼ˆo1/R1 è·¯çº¿ï¼‰

**èŒƒå¼è½¬æ¢**ï¼šä» "scaling pretraining data/compute" åˆ° "scaling test-time compute"

- **OpenAI o1/o3**ï¼šé€šè¿‡ RL è®­ç»ƒæ¨¡å‹ç”Ÿæˆå†…éƒ¨æ¨ç†é“¾ï¼ˆhidden CoTï¼‰ï¼Œåœ¨æ•°å­¦/ç¼–ç¨‹ä¸Šå¤§å¹…è¶…è¶Šä¼ ç»Ÿæ¨¡å‹
- **DeepSeek R1**ï¼šå¼€æºå®ç°ï¼Œè¯æ˜çº¯ RLï¼ˆGRPO + rule-based rewardï¼‰å¯æ¶Œç°æ¨ç†
- **å…³é”®æ´å¯Ÿ**ï¼šRL ä¸æ˜¯æ•™æ¨¡å‹æ–°çŸ¥è¯†ï¼Œæ˜¯æ¿€æ´»å·²æœ‰çš„æ¨ç†èƒ½åŠ›
- **è’¸é¦**ï¼šå¤§æ¨ç†æ¨¡å‹ â†’ å°æ¨¡å‹ï¼ˆR1-distillï¼‰ï¼Œä¿ç•™å¤§éƒ¨åˆ†æ¨ç†èƒ½åŠ›

### 12.2 RL-based Code Generation

- **AlphaCode2**ï¼šMCTS + å¤§é‡é‡‡æ · + èšç±» + é‡æ’åº
- **SWE-agent**ï¼šRL è®­ç»ƒ agent ä¿®å¤çœŸå® GitHub issue
- **Reward è®¾è®¡**ï¼šæµ‹è¯•ç”¨ä¾‹é€šè¿‡ç‡ = å®Œç¾çš„ verifiable reward
- **è¶‹åŠ¿**ï¼šä»£ç æ˜¯ RL æœ€ä½³åº”ç”¨åœºæ™¯â€”â€”reward æ˜ç¡®ã€å¯è‡ªåŠ¨éªŒè¯ã€å¯å¤§è§„æ¨¡é‡‡æ ·

### 12.3 Agent RLï¼ˆAgentic RL loopsï¼‰

- **Agent è®­ç»ƒ**ï¼šç”¨ RL è®­ç»ƒ LLM agent å­¦ä¼šä½¿ç”¨å·¥å…·ã€å¤šæ­¥æ¨ç†ã€é”™è¯¯æ¢å¤
- **ç¯å¢ƒ**ï¼šWebArena / OSWorld / SWE-bench æä¾› agent è®­ç»ƒç¯å¢ƒ
- **Reward**ï¼šä»»åŠ¡å®Œæˆç‡ + æ­¥æ•°æ•ˆç‡ + å®‰å…¨çº¦æŸ
- **ä¸ prompt engineering å¯¹æ¯”**ï¼šRL è®­ç»ƒçš„ agent æ¯” prompt-based agent æ›´é²æ£’

### 12.4 Offline RL å¤å…´

**é—®é¢˜**ï¼šåœ¨çº¿ RL éœ€è¦å¤§é‡ç¯å¢ƒäº¤äº’ï¼Œæˆæœ¬é«˜ä¸”é£é™©å¤§ã€‚

**Offline RL**ï¼šä»å›ºå®šæ•°æ®é›†å­¦ä¹ ç­–ç•¥ï¼Œä¸åšé¢å¤–äº¤äº’ã€‚

- **CQL**ï¼ˆConservative Q-Learningï¼‰ï¼šæƒ©ç½š OOD action çš„ Q å€¼
- **IQL**ï¼ˆImplicit Q-Learningï¼‰ï¼šé¿å…æŸ¥è¯¢æœªè§è¿‡çš„åŠ¨ä½œ
- **Decision Transformer**ï¼šå°† RL å»ºæ¨¡ä¸º sequence modeling é—®é¢˜

**LLM åœºæ™¯åº”ç”¨**ï¼š
- ç”¨ç°æœ‰å¯¹è¯æ—¥å¿—åš offline RLï¼ˆä¸éœ€è¦åœ¨çº¿ generationï¼‰
- DPO æœ¬è´¨ä¸Šæ˜¯ä¸€ç§ offline RL
- Offline-to-Onlineï¼šå…ˆ offline é¢„è®­ç»ƒç­–ç•¥ï¼Œå†å°‘é‡ online fine-tune

### 12.5 Multi-Reward Optimization

2026 æ¨¡å‹éœ€è¦åŒæ—¶ä¼˜åŒ–å¤šä¸ªç»´åº¦ï¼šhelpfulnessã€harmlessnessã€honestyã€reasoning accuracyã€code quality...

- **å¤šå¥–åŠ±èåˆ**ï¼šåŠ æƒæ±‚å’Œï¼ˆç®€å•ä½†éœ€è¦è°ƒæƒé‡ï¼‰
- **Constrained Optimization**ï¼šSafe RLHFï¼ˆhelpful ä¸ºç›®æ ‡ï¼Œharmless ä¸ºçº¦æŸï¼‰
- **Pareto Optimization**ï¼šæœç´¢å¸•ç´¯æ‰˜æœ€ä¼˜å‰æ²¿
- **Conditional Training**ï¼šæ ¹æ® system prompt åŠ¨æ€è°ƒæ•´ reward æƒé‡

### é¢è¯•é«˜é¢‘é—®é¢˜

**Q: ä¸ºä»€ä¹ˆè¯´ 2025-2026 æ˜¯ RL for LLM çš„åˆ†æ°´å²­ï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ o1/R1 è¯æ˜ RL å¯ä»¥æ¶Œç°æ¨ç†èƒ½åŠ›ï¼Œä¸åªæ˜¯å¯¹é½å·¥å…·ï¼›â‘¡ä» scaling pretraining â†’ scaling test-time compute çš„èŒƒå¼è½¬ç§»ï¼›â‘¢GRPO ç­‰æ–¹æ³•å¤§å¹…é™ä½ RL è®­ç»ƒæˆæœ¬ï¼ˆæ— éœ€ RM/Criticï¼‰ï¼›â‘£ä»£ç /æ•°å­¦ç­‰é¢†åŸŸæœ‰å¤©ç„¶ verifiable rewardï¼ŒRL æ•ˆæœè¿œè¶… SFTï¼›â‘¤Agent RL å¼€å§‹è½åœ°â€”â€”ç”¨ RL è®­ç»ƒèƒ½ä½¿ç”¨å·¥å…·çš„ agentï¼›â‘¥å¼€æºç¤¾åŒºè·Ÿè¿›è¿…é€Ÿï¼ˆTRL/OpenRLHFï¼‰ï¼Œä¸å†æ˜¯ OpenAI ä¸“æœ‰èƒ½åŠ›ã€‚

**Q: æœªæ¥ 3 å¹´ RL åœ¨ AI ä¸­æœ€å¤§çš„åº”ç”¨æ–¹å‘æ˜¯ä»€ä¹ˆï¼Ÿ**
> **ç­”é¢˜è¦ç‚¹**ï¼šâ‘ Reasoning via RL ä¼šæŒç»­å‘å±•â€”â€”æ¨ç†èƒ½åŠ›æˆä¸ºæ¨¡å‹æ ¸å¿ƒç«äº‰åŠ›ï¼›â‘¡Code Agent RLâ€”â€”è‡ªä¸»ç¼–ç¨‹æ˜¯æœ€å…·å•†ä¸šä»·å€¼çš„åº”ç”¨ï¼›â‘¢Multi-Agent RLâ€”â€”å¤æ‚ä»»åŠ¡éœ€è¦ agent åä½œï¼›â‘£Robotics RLâ€”â€”VLA æ¨¡å‹è®© RL åœ¨å…·èº«æ™ºèƒ½ä¸­è½åœ°ï¼›â‘¤Offline RL + äººç±»åé¦ˆæ•°æ®â€”â€”é™ä½ RL é—¨æ§›è®©æ›´å¤šå›¢é˜Ÿèƒ½åšå¯¹é½ã€‚

---

## é™„å½•ï¼šæ ¸å¿ƒå…¬å¼é€ŸæŸ¥

| å…¬å¼ | åç§° | åœºæ™¯ |
|------|------|------|
| $Q(s,a) \leftarrow Q + \alpha[r + \gamma\max Q' - Q]$ | Q-Learning | è¡¨æ ¼å‹ RL |
| $\nabla J = \mathbb{E}[\nabla\log\pi \cdot A]$ | Policy Gradient | ç­–ç•¥ä¼˜åŒ– |
| $L = \mathbb{E}[\min(rA, \text{clip}(r)A)]$ | PPO-Clip | LLM å¯¹é½ |
| $\hat{A}^{GAE} = \sum(\gamma\lambda)^l\delta_{t+l}$ | GAE | ä¼˜åŠ¿ä¼°è®¡ |
| $\hat{A}_i = (r_i-\mu)/\sigma$ | GRPO | ç»„å†…æ ‡å‡†åŒ– |
| $L = -\log\sigma(\beta\log\frac{\pi_w}{\pi_{ref,w}} - \beta\log\frac{\pi_l}{\pi_{ref,l}})$ | DPO | ç¦»çº¿åå¥½ |
| $P(w\succ l) = \sigma(r_w - r_l)$ | Bradley-Terry | RM è®­ç»ƒ |
| $J = \mathbb{E}[r] - \beta D_{KL}(\pi\|\pi_{ref})$ | RLHF Objective | KL çº¦æŸä¼˜åŒ– |

---

## å‚è€ƒè®ºæ–‡

1. **PPO**: Schulman et al., "Proximal Policy Optimization Algorithms" (2017)
2. **InstructGPT/RLHF**: Ouyang et al., "Training language models to follow instructions with human feedback" (2022)
3. **DPO**: Rafailov et al., "Direct Preference Optimization" (2023)
4. **GRPO/DeepSeek-R1**: DeepSeek-AI, "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning" (2025)
5. **KTO**: Ethayarajh et al., "KTO: Model Alignment as Prospect Theoretic Optimization" (2024)
6. **DAPO**: Yu et al., "DAPO: An Open-Source LLM Reinforcement Learning System" (2025)
7. **PRM**: Lightman et al., "Let's Verify Step by Step" (2023)
8. **Self-Play**: Silver et al., "Mastering the game of Go without human knowledge" (2017)
9. **SAC**: Haarnoja et al., "Soft Actor-Critic" (2018)
10. **Constrained MDP**: Achiam et al., "Constrained Policy Optimization" (2017)

---

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **LLM å¯¹é½å¾®è°ƒ**ï¼šä½¿ç”¨ TRL åº“çš„ PPOTrainer / GRPOTrainerï¼Œå¯¹ SFT æ¨¡å‹è¿›è¡Œ RLHF/GRPO å¯¹é½ã€‚GRPO å› æ— éœ€ Critic æ›´é€‚åˆæ˜¾å­˜å—é™åœºæ™¯
- **ä»£ç /æ•°å­¦è‡ªåŠ¨è¯„æµ‹**ï¼šrule-based reward + GRPO æ˜¯å½“å‰æ€§ä»·æ¯”æœ€é«˜çš„æ–¹æ¡ˆâ€”â€”æµ‹è¯•ç”¨ä¾‹é€šè¿‡ç‡ä½œä¸º verifiable rewardï¼Œæ— éœ€äººå·¥æ ‡æ³¨
- **æ¨èç³»ç»Ÿ A/B æµ‹è¯•**ï¼šThompson Sampling åšåœ¨çº¿æ¢ç´¢ï¼Œé€‚ç”¨äºæ–°é—»/å¹¿å‘Šæ¨èçš„å†·å¯åŠ¨å’ŒæŒç»­ä¼˜åŒ–

### å·¥ç¨‹å®ç°è¦ç‚¹
- PPO è¶…å‚ LR=1e-5~5e-5ã€Îµ=0.1~0.2ã€Î²=0.01~0.1ï¼Œç›‘æ§ KL æ•£åº¦å’Œ entropy é˜²å´©æºƒ
- GRPO çš„ Group Size G=8~64ï¼Œè¶Šå¤§ advantage ä¼°è®¡è¶Šå‡†ä½†ç”Ÿæˆæˆæœ¬è¶Šé«˜
- è®­ç»ƒä¸ç¨³å®šé¦–æŸ¥å››æŒ‡æ ‡ï¼šReward è¶‹åŠ¿ã€KL æ•£åº¦ã€Policy entropyã€Generation quality

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: RLHF çš„ PPO å’Œæ¸¸æˆ RL çš„ PPO æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
  A: â‘ ç¯å¢ƒæ˜¯ RM éçœŸå®ç¯å¢ƒ â‘¡è¯è¡¨ 32K-150K ç¦»æ•£ç©ºé—´ â‘¢Reward ç¨€ç–ï¼ˆsequence-levelï¼‰â‘£å¤š KL çº¦æŸé¡¹ â‘¤4 å¤§æ¨¡å‹å¹¶è¡Œ
- Q: æ¨å¯¼ DPO çš„ loss function
  A: ä» KL-constrained RL â†’ æœ€ä¼˜ç­–ç•¥é—­å¼è§£ â†’ åè§£ reward â†’ ä»£å…¥ BT æ¨¡å‹ â†’ log Z æ¶ˆæ‰

---

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- **RL ä¸å†åªæ˜¯å¯¹é½å·¥å…·**ï¼šo1/R1 è¯æ˜ RL å¯ä»¥æ¶Œç°æ¨ç†èƒ½åŠ›ã€‚è¿™æ„å‘³ç€æœªæ¥ Agent çš„æ ¸å¿ƒç«äº‰åŠ›ä¸ä»…æ˜¯ prompt engineeringï¼Œè€Œæ˜¯ RL è®­ç»ƒå‡ºçš„è§„åˆ’å’Œæ¨ç†èƒ½åŠ›
- **Verifiable Reward æ˜¯é‡‘çŸ¿**ï¼šä»£ç /æ•°å­¦æœ‰å¤©ç„¶ ground truthï¼Œè¿™æ¡è·¯çº¿çš„ ROI è¿œé«˜äºä¾èµ–äººç±»æ ‡æ³¨çš„ RLHF

### æœªè§£é—®é¢˜ä¸å±€é™
- Reward Hacking ä»æ— æ ¹æœ¬æ€§è§£å†³â€”â€”Goodhart's Law åœ¨ RL å¯¹é½ä¸­æ˜¯ç³»ç»Ÿæ€§é£é™©
- GRPO çš„ç»„å†…æ ‡å‡†åŒ–åœ¨ç»„å†…æ–¹å·®æå°æ—¶ä¸ç¨³å®šï¼ŒDAPO çš„ Dynamic Sampling åªæ˜¯ç¼“è§£è€Œéæ ¹æ²»
- Test-time Compute Scaling å¯¹å¼€æ”¾å¼é—®é¢˜ï¼ˆåˆ›æ„å†™ä½œã€ä¸»è§‚åˆ¤æ–­ï¼‰æ•ˆæœæœ‰é™

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- å¦‚æœæŠŠ [[AI/LLM/RL/Theory/GRPO-Improvement-Panorama-2026|GRPO]] å’Œ [[AI/Agent/AI-Agent-2026-æŠ€æœ¯å…¨æ™¯|Agent ç³»ç»Ÿ]] ç»“åˆï¼Œå¯èƒ½å®ç° Agent è‡ªä¸»å­¦ä¹ ä½¿ç”¨æ–°å·¥å…·çš„é—­ç¯â€”â€”è¿™æ˜¯ Agentic RL çš„æ ¸å¿ƒæ–¹å‘
- 6 ä¸ªæœˆåé¢„æµ‹ï¼šGRPO å˜ä½“ + Verifiable Reward ä¼šæˆä¸ºä¸­å°å›¢é˜Ÿåš RL çš„é»˜è®¤é€‰æ‹©ï¼Œå–ä»£ PPO

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) â€” PPO åŸè®ºæ–‡ï¼ŒClipping æ€æƒ³çš„æºå¤´ï¼Œå¿…è¯»
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) â€” InstructGPT/RLHF ä¸‰é˜¶æ®µæµç¨‹çš„å¥ åŸºè®ºæ–‡
- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073) â€” RLAIF æ–¹æ³•è®ºï¼Œç”¨ AI ä»£æ›¿äººç±»æ ‡æ³¨åå¥½
- [DeepSeekMath (GRPO)](https://arxiv.org/abs/2402.03300) â€” GRPO ç®—æ³•åŸè®ºæ–‡ï¼Œæ—  Critic çš„ RL
- [DeepSeek-R1](https://arxiv.org/abs/2501.12948) â€” çº¯ RL æ¶Œç°æ¨ç†èƒ½åŠ›çš„é‡Œç¨‹ç¢‘

### æ·±åº¦è§£è¯»
- [HuggingFace RLHF Blog](https://huggingface.co/blog/rlhf) â€” RLHF å·¥ç¨‹å®è·µæœ€ä½³å…¥é—¨ â­â­â­â­
- [Lilian Weng: "Reward Hacking in RLHF"](https://lilianweng.github.io/) â€” Reward Hacking çš„ç³»ç»Ÿæ€§åˆ†æ â­â­â­â­â­

### å®è·µèµ„æº
- [TRL (Transformer Reinforcement Learning)](https://github.com/huggingface/trl) â€” PPO/DPO/GRPO Trainer çš„å®˜æ–¹å®ç°
- [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) â€” åˆ†å¸ƒå¼ RLHF è®­ç»ƒæ¡†æ¶ï¼Œæ”¯æŒ Ray + vLLM

---

## See Also

> ğŸ”— See also: [[AI/LLM/RL/ç›®å½•|RL MOC]] â€” å¼ºåŒ–å­¦ä¹ çŸ¥è¯†åŸŸå…¨ç´¢å¼•
> ğŸ”— See also: [[AI/LLM/RL/RLHF-DPO-2026-æŠ€æœ¯å…¨æ™¯|RLHF DPO 2026 æŠ€æœ¯å…¨æ™¯]] â€” äº’è¡¥ï¼šæœ¬æ–‡ä¾§é‡ç»å…¸ RL + é¢è¯•æ¡†æ¶ï¼Œè¯¥æ–‡ä¾§é‡å¯¹é½æŠ€æœ¯ RLHF/DPO/å®ªæ³• AI
> ğŸ”— See also: [[AI/LLM/RL/Fundamentals/å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸç†|å¼ºåŒ–å­¦ä¹ æ•°å­¦åŸç†]] â€” æœ¬æ–‡å…¬å¼æ¨å¯¼çš„åŸºç¡€å±‚ï¼ˆBellman/GAE è¯¦ç»†æ¨å¯¼ï¼‰
> ğŸ”— See also: [[AI/LLM/RL/Theory/GRPO-Improvement-Panorama-2026|GRPO æ”¹è¿›å…¨æ™¯]] â€” æœ¬æ–‡ GRPO ç« èŠ‚çš„ä¸ƒç»´æ·±åº¦æ‰©å±•
> ğŸ”— See also: [[Career/AIé¢è¯•é€ŸæŸ¥æ‰‹å†Œ|AI é¢è¯•é€ŸæŸ¥æ‰‹å†Œ]] â€” åŒç³»åˆ—é€ŸæŸ¥ç‰ˆï¼ˆ30 åˆ†é’Ÿç‰ˆ vs æœ¬æ–‡æ·±åº¦ç‰ˆï¼‰
