---
title: "å¼ºåŒ–å­¦ä¹  for LLM"
type: moc
domain: ai/llm/rl
tags:
  - ai/llm/rl
  - type/reference
---

# ğŸ¯ å¼ºåŒ–å­¦ä¹  for LLM

> LLM Post-Training çš„æ ¸å¿ƒæ–¹å‘ â€” ä» RLHF åˆ° GRPO å†åˆ° Agentic RL

## åŸºç¡€ç†è®º (Fundamentals)
- [[AI/LLM/RL/Fundamentals/é©¬å°”ç§‘å¤«|é©¬å°”ç§‘å¤«]] â€” MDP åŸºç¡€
- [[AI/LLM/RL/Fundamentals/è´å°”æ›¼æ–¹ç¨‹|è´å°”æ›¼æ–¹ç¨‹]] â€” ä»·å€¼å‡½æ•°
- [[AI/LLM/RL/Fundamentals/ç­–ç•¥æ¢¯åº¦æ–¹æ³•|ç­–ç•¥æ¢¯åº¦æ–¹æ³•]] â€” PG æ—ç®—æ³•åŸºç¡€
- [[AI/LLM/RL/Fundamentals/On-Policy vs Off-Policy|On-Policy vs Off-Policy]]
- [[AI/LLM/RL/Fundamentals/KLæ•£åº¦|KLæ•£åº¦]] â€” æ­£åˆ™åŒ–æ ¸å¿ƒæ¦‚å¿µ
- [[AI/LLM/RL/Fundamentals/MCTS|MCTS]] â€” è’™ç‰¹å¡æ´›æ ‘æœç´¢
- [[AI/LLM/RL/Fundamentals/ä¸ºä»€ä¹ˆ PPO ä¼˜äº PG|ä¸ºä»€ä¹ˆ PPO ä¼˜äº PG]]
- [[AI/LLM/RL/Fundamentals/PPL è®¡ç®— äº¤å‰ç†µæŸå¤±ä¸ ignore_index|PPL è®¡ç®—]]
- [[AI/LLM/RL/Fundamentals/RL æ¦‚è§ˆ|RL æ¦‚è§ˆ]]
- [[AI/LLM/RL/Fundamentals/RL & LLMs å…¥é—¨|RL & LLMs å…¥é—¨]] â€” HF Course
- [[AI/LLM/RL/Fundamentals/HF Deep RL Course|HF Deep RL Course]]
- [[AI/LLM/RL/Fundamentals/å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸç†|å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸç†]]
- [[AI/LLM/RL/Theory/RLVR-Edge-of-Competence|RLVR at the Edge of Competence]] â€” èƒ½åŠ›è¾¹ç•Œä¸Šçš„ RLVRï¼Œç ”ç©¶è®­ç»ƒä¿¡å·æœ‰æ•ˆåŒºé—´

## æ ¸å¿ƒç®—æ³•

### PPO
- [[AI/LLM/RL/PPO/PPO åŸç†|PPO åŸç†]] â€” æœ€ç»å…¸çš„ RLHF ç®—æ³•
- [[AI/LLM/RL/PPO/PPO-TRLå®è·µ|PPO-TRLå®è·µ]]
- [[AI/LLM/RL/PPO/PPO-verlå®è·µ|PPO-verlå®è·µ]]

### GRPO â­ï¼ˆé‡ç‚¹æ–¹å‘ï¼‰
- [[AI/LLM/RL/GRPO/GRPO æ·±åº¦ç†è§£|GRPO æ·±åº¦ç†è§£]] â€” æ ¸å¿ƒåŸç†
- [[AI/LLM/RL/GRPO/DeepSeek R1 å­¦ä¹ ç¬”è®°|DeepSeek R1 å­¦ä¹ ç¬”è®°]]
- [[AI/LLM/RL/GRPO/DeepSeek-Math|DeepSeek-Math]] â€” æ•°å­¦æ¨ç†è®ºæ–‡
- [[AI/LLM/RL/GRPO/Blockwise-Advantage-Estimation|Blockwise Advantage Estimation]] â€” GRPO credit assignment æ”¹è¿›
- [[AI/LLM/RL/GRPO/TRL ä¸­å®ç° GRPO|TRL ä¸­å®ç° GRPO]]
- [[AI/LLM/RL/GRPO/GRPO-TRLå®è·µ|GRPO-TRLå®è·µ]]
- [[AI/LLM/RL/GRPO/GRPO-verlå®è·µ|GRPO-verlå®è·µ]]
- [[AI/LLM/RL/GRPO/GRPO-Unslothå®è·µ|GRPO-Unslothå®è·µ]]
- [[AI/LLM/RL/GRPO/GRPO-demo|GRPO-demo]]
- [[AI/LLM/RL/GRPO/OpenR1|OpenR1]]
- [[AI/RL/iGRPO|iGRPO]] â€” è¿­ä»£å¼è‡ªåé¦ˆ GRPO (arXiv:2602.09000)

### DPO
- [[AI/LLM/RL/DPO/DPO-TRLå®è·µ|DPO-TRLå®è·µ]]
- [[AI/LLM/RL/DPO/DPO-Unslothå®è·µ|DPO-Unslothå®è·µ]]

### DAPO
- [[AI/LLM/RL/DAPO/DAPO-verlå®è·µ|DAPO-verlå®è·µ]]

### KTO
- [[AI/LLM/RL/KTO/KTO-TRLå®è·µ|KTO-TRLå®è·µ]]

### RLOO
- [[AI/LLM/RL/RLOO/RLOO-TRLå®è·µ|RLOO-TRLå®è·µ]]

### å…¶ä»–ç®—æ³• (Other-Algorithms)
- [[AI/LLM/RL/Other-Algorithms/DCPO è®ºæ–‡|DCPO]] â€” Dynamic Clipping
- [[AI/LLM/RL/Other-Algorithms/Beyond Correctness è®ºæ–‡|Beyond Correctness]] â€” Process + Outcome Rewards
- [[AI/LLM/RL/Other-Algorithms/GPG-verlå®è·µ|GPG]]
- [[AI/LLM/RL/Other-Algorithms/OPO-verlå®è·µ|OPO]]
- [[AI/LLM/RL/Other-Algorithms/SPIN-verlå®è·µ|SPIN]]
- [[AI/LLM/RL/Other-Algorithms/SPPO-verlå®è·µ|SPPO]]
- [[AI/LLM/RL/Other-Algorithms/CollabLLM-verlå®è·µ|CollabLLM]]
- [[AI/LLM/RL/Other-Algorithms/OpenRS-Pairwise-Adaptive-Rubric|OpenRS]] â€” Pairwise Adaptive Rubricï¼Œnon-verifiable reward å¯¹é½ï¼Œè§£å†³ reward hackingï¼ˆarXiv:2602.14069ï¼‰
- [[AI/LLM/RL/Other-Algorithms/GSPO-Unslothå®è·µ|GSPO]]
- [[AI/LLM/RL/Other-Algorithms/MEL-Meta-Experience-Learning|MEL]] â€” Meta-Experience Learning
- [[AI/LLM/RL/Other-Algorithms/CM2 â€” Checklist Rewardså¤šè½®Tool Use RL|CM2]] â€” Checklist Rewards å¤šè½® Tool Use RL
- [[AI/LLM/RL/Other-Algorithms/SkillRL â€” é€’å½’æŠ€èƒ½å¢å¼ºçš„Agentæ¼”åŒ–|SkillRL]] â€” é€’å½’æŠ€èƒ½å¢å¼º Agent æ¼”åŒ–
- [[AI/LLM/RL/Other-Algorithms/RLTF-RL-from-Text-Feedback|RLTF]] â€” RL from Text Feedbackï¼Œæ–‡æœ¬åé¦ˆå¥–åŠ±è®¾è®¡ï¼ˆarXiv:2602.02482ï¼‰
- [[AI/LLM/RL/Other-Algorithms/HiPER-Hierarchical-RL-Credit-Assignment|HiPER]] â€” åˆ†å±‚ RL + æ˜¾å¼ Credit Assignmentï¼Œå¤šæ­¥ Agent é•¿ horizonï¼ˆarXiv:2602.16165ï¼‰â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/LACONIC-Length-Constrained-RL|LACONIC]] â€” Primal-Dual RL æ§åˆ¶ CoT è¾“å‡ºé•¿åº¦ï¼Œæ¨ç†æ•ˆç‡ï¼ˆarXiv:2602.14468ï¼‰â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/E-SPL-Evolutionary-System-Prompt-Learning|E-SPL]] â€” RL æƒé‡æ›´æ–°ï¼ˆç¨‹åºæ€§çŸ¥è¯†ï¼‰+ è¿›åŒ–ç®—æ³• system prompt ä¼˜åŒ–ï¼ˆå£°æ˜æ€§çŸ¥è¯†ï¼‰è”åˆè®­ç»ƒï¼›AIME25 56.3â†’60.6%ï¼ˆarXiv:2602.14697ï¼‰â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/GEPA-Reflective-Prompt-Evolution|GEPA]] â­ â€” çº¯ prompt è¿›åŒ–è¶…è¶Š GRPOï¼ˆ5/6ä»»åŠ¡ï¼‰ï¼Œrollout å‡å°‘ 35xï¼›E-SPL=GEPA+RLï¼›ICLR 2026 Oralï¼ŒUCB+Stanford+MITï¼ˆarXiv:2507.19457ï¼‰â˜…â˜…â˜…â˜…â˜…

## ç»¼è¿°ä¸æ·±åº¦ç¬”è®°
- [[AI/LLM/RL/RLHF å…¨é“¾è·¯|RLHF å…¨é“¾è·¯]] â€” å®Œæ•´ RLHF ä¸‰é˜¶æ®µ
- [[AI/LLM/RL/RLHF-DPO-2026-æŠ€æœ¯å…¨æ™¯|RLHF/DPO 2026 æŠ€æœ¯å…¨æ™¯]] â€” é¢è¯•æ­¦å™¨ç‰ˆï¼Œ1147è¡Œï¼ŒRLHFâ†’RLAIFâ†’DPO å…¨é“¾è·¯ï¼ˆ2026-02-20ï¼‰
- [[AI/LLM/RL/å¯¹é½æŠ€æœ¯ç»¼è¿°|å¯¹é½æŠ€æœ¯ç»¼è¿°]] â€” RLHF â†’ DPO â†’ ORPO â†’ KTO â†’ SteerLM â†’ Constitutional AI
- [[AI/LLM/RL/RARL-Reward-Modeling-Survey|RARL Reward Modeling Survey]] â€” RL reasoning alignment ç»¼è¿°

## ç›¸å…³ MOC
- â†‘ ä¸Šçº§ï¼š[[AI/LLM/_MOC]]
- â†’ äº¤å‰ï¼š[[AI/Agent/_MOC]]ï¼ˆAgentic RLï¼‰
