---
title: "å¼ºåŒ–å­¦ä¹  for LLM"
type: moc
domain: ai/llm/rl
tags:
  - ai/llm/rl
  - type/reference
---

# ğŸ¯ å¼ºåŒ–å­¦ä¹  for LLM

> LLM Post-Training çš„æ ¸å¿ƒæ–¹å‘ â€” ä» RLHF åˆ° GRPO å†åˆ° Agentic RL

## åŸºç¡€ç†è®º (Fundamentals)
- [[AI/LLM/RL/Fundamentals/é©¬å°”ç§‘å¤«|é©¬å°”ç§‘å¤«]] â€” MDP åŸºç¡€
- [[AI/LLM/RL/Fundamentals/è´å°”æ›¼æ–¹ç¨‹|è´å°”æ›¼æ–¹ç¨‹]] â€” ä»·å€¼å‡½æ•°
- [[AI/LLM/RL/Fundamentals/ç­–ç•¥æ¢¯åº¦æ–¹æ³•|ç­–ç•¥æ¢¯åº¦æ–¹æ³•]] â€” PG æ—ç®—æ³•åŸºç¡€
- [[AI/LLM/RL/Fundamentals/On-Policy vs Off-Policy|On-Policy vs Off-Policy]]
- [[AI/LLM/RL/Fundamentals/KLæ•£åº¦|KLæ•£åº¦]] â€” æ­£åˆ™åŒ–æ ¸å¿ƒæ¦‚å¿µ
- [[AI/LLM/RL/Fundamentals/MCTS|MCTS]] â€” è’™ç‰¹å¡æ´›æ ‘æœç´¢
- [[AI/LLM/RL/Fundamentals/ä¸ºä»€ä¹ˆ PPO ä¼˜äº PG|ä¸ºä»€ä¹ˆ PPO ä¼˜äº PG]]
- [[AI/LLM/RL/Fundamentals/PPL è®¡ç®— äº¤å‰ç†µæŸå¤±ä¸ ignore_index|PPL è®¡ç®—]]
- [[AI/LLM/RL/Fundamentals/RL æ¦‚è§ˆ|RL æ¦‚è§ˆ]]
- [[AI/LLM/RL/Fundamentals/RL & LLMs å…¥é—¨|RL & LLMs å…¥é—¨]] â€” HF Course
- [[AI/LLM/RL/Fundamentals/HF Deep RL Course|HF Deep RL Course]]
- [[AI/LLM/RL/Fundamentals/å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸç†|å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸç†]]
- [[AI/LLM/RL/Theory/RLVR-Edge-of-Competence|RLVR at the Edge of Competence]] â€” èƒ½åŠ›è¾¹ç•Œä¸Šçš„ RLVRï¼Œç ”ç©¶è®­ç»ƒä¿¡å·æœ‰æ•ˆåŒºé—´

## æ ¸å¿ƒç®—æ³•

### PPO
- [[AI/LLM/RL/PPO/PPO åŸç†|PPO åŸç†]] â€” æœ€ç»å…¸çš„ RLHF ç®—æ³•
- [[AI/LLM/RL/PPO/PPO-TRLå®è·µ|PPO-TRLå®è·µ]]
- [[AI/LLM/RL/PPO/PPO-verlå®è·µ|PPO-verlå®è·µ]]

### GRPO â­ï¼ˆé‡ç‚¹æ–¹å‘ï¼‰
- [[AI/LLM/RL/GRPO/GRPO æ·±åº¦ç†è§£|GRPO æ·±åº¦ç†è§£]] â€” æ ¸å¿ƒåŸç†
- [[AI/LLM/RL/GRPO/DeepSeek R1 å­¦ä¹ ç¬”è®°|DeepSeek R1 å­¦ä¹ ç¬”è®°]]
- [[AI/LLM/RL/GRPO/DeepSeek-Math|DeepSeek-Math]] â€” æ•°å­¦æ¨ç†è®ºæ–‡
- [[AI/LLM/RL/GRPO/Blockwise-Advantage-Estimation|Blockwise Advantage Estimation]] â€” GRPO credit assignment æ”¹è¿›
- [[AI/LLM/RL/GRPO/TRL ä¸­å®ç° GRPO|TRL ä¸­å®ç° GRPO]]
- [[AI/LLM/RL/GRPO/GRPO-TRLå®è·µ|GRPO-TRLå®è·µ]]
- [[AI/LLM/RL/GRPO/GRPO-verlå®è·µ|GRPO-verlå®è·µ]]
- [[AI/LLM/RL/GRPO/GRPO-Unslothå®è·µ|GRPO-Unslothå®è·µ]]
- [[AI/LLM/RL/GRPO/GRPO-demo|GRPO-demo]]
- [[AI/LLM/RL/GRPO/OpenR1|OpenR1]]
- [[AI/RL/iGRPO|iGRPO]] â€” è¿­ä»£å¼è‡ªåé¦ˆ GRPO (arXiv:2602.09000)
- [[AI/LLM/RL/GRPO/ProGRPO-Probabilistic-Advantage-Reweighting|ProGRPO]] â€” Probabilistic Group Relative POï¼šåœ¨ advantage å±‚å¼•å…¥æ¦‚ç‡ç½®ä¿¡åº¦é‡åŠ æƒï¼Œ"æ‰¶å¼±æŠ‘å¼º"å¯¹æŠ— entropy collapseï¼›Pass@32 æ¯” GRPO +13.9%ï¼›â˜…â˜…â˜…â˜…ï¼ˆarXiv:2602.05281ï¼‰

### DPO
- [[AI/LLM/RL/DPO/DPO-TRLå®è·µ|DPO-TRLå®è·µ]]
- [[AI/LLM/RL/DPO/DPO-Unslothå®è·µ|DPO-Unslothå®è·µ]]

### DAPO
- [[AI/LLM/RL/DAPO/DAPO-verlå®è·µ|DAPO-verlå®è·µ]]

### KTO
- [[AI/LLM/RL/KTO/KTO-TRLå®è·µ|KTO-TRLå®è·µ]]

### RLOO
- [[AI/LLM/RL/RLOO/RLOO-TRLå®è·µ|RLOO-TRLå®è·µ]]

### å…¶ä»–ç®—æ³• (Other-Algorithms)
- [[AI/LLM/RL/Other-Algorithms/DCPO è®ºæ–‡|DCPO]] â€” Dynamic Clipping
- [[AI/LLM/RL/Other-Algorithms/Beyond Correctness è®ºæ–‡|Beyond Correctness]] â€” Process + Outcome Rewards
- [[AI/LLM/RL/Other-Algorithms/GPG-verlå®è·µ|GPG]]
- [[AI/LLM/RL/Other-Algorithms/OPO-verlå®è·µ|OPO]]
- [[AI/LLM/RL/Other-Algorithms/SPIN-verlå®è·µ|SPIN]]
- [[AI/LLM/RL/Other-Algorithms/SPPO-verlå®è·µ|SPPO]]
- [[AI/LLM/RL/Other-Algorithms/CollabLLM-verlå®è·µ|CollabLLM]]
- [[AI/LLM/RL/Other-Algorithms/OpenRS-Pairwise-Adaptive-Rubric|OpenRS]] â€” Pairwise Adaptive Rubricï¼Œnon-verifiable reward å¯¹é½ï¼Œè§£å†³ reward hackingï¼ˆarXiv:2602.14069ï¼‰
- [[AI/LLM/RL/Other-Algorithms/GSPO-Unslothå®è·µ|GSPOï¼ˆUnslothå®è·µç‰ˆï¼‰]]
- [[AI/LLM/RL/Other-Algorithms/GSPO-Group-Sequence-Policy-Optimization|GSPOï¼ˆQwen3å›¢é˜Ÿæ­£å¼ç‰ˆï¼‰]] â€” Alibaba/Qwen å›¢é˜Ÿï¼šåºåˆ—çº§ IS æ›¿ä»£ token çº§ ISï¼Œä»æ•°å­¦ä¸Šæ¶ˆè§£åºåˆ—å¥–åŠ±ä¸ token æ›´æ–°çš„å¯¹é½é”™è¯¯ï¼›Qwen3 RL post-training å®é™…åœ¨ç”¨ï¼›â˜…â˜…â˜…â˜…ï¼ˆarXiv:2507.18071ï¼‰
- [[AI/LLM/RL/Other-Algorithms/MEL-Meta-Experience-Learning|MEL]] â€” Meta-Experience Learning
- [[AI/LLM/RL/Other-Algorithms/CM2 â€” Checklist Rewardså¤šè½®Tool Use RL|CM2]] â€” Checklist Rewards å¤šè½® Tool Use RL
- [[AI/LLM/RL/Other-Algorithms/SkillRL â€” é€’å½’æŠ€èƒ½å¢å¼ºçš„Agentæ¼”åŒ–|SkillRL]] â€” é€’å½’æŠ€èƒ½å¢å¼º Agent æ¼”åŒ–
- [[AI/LLM/RL/Other-Algorithms/RLTF-RL-from-Text-Feedback|RLTF]] â€” RL from Text Feedbackï¼Œæ–‡æœ¬åé¦ˆå¥–åŠ±è®¾è®¡ï¼ˆarXiv:2602.02482ï¼‰
- [[AI/LLM/RL/Other-Algorithms/HiPER-Hierarchical-RL-Credit-Assignment|HiPER]] â€” åˆ†å±‚ RL + æ˜¾å¼ Credit Assignmentï¼Œå¤šæ­¥ Agent é•¿ horizonï¼ˆarXiv:2602.16165ï¼‰â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/LACONIC-Length-Constrained-RL|LACONIC]] â€” Primal-Dual RL æ§åˆ¶ CoT è¾“å‡ºé•¿åº¦ï¼Œæ¨ç†æ•ˆç‡ï¼ˆarXiv:2602.14468ï¼‰â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/E-SPL-Evolutionary-System-Prompt-Learning|E-SPL]] â€” RL æƒé‡æ›´æ–°ï¼ˆç¨‹åºæ€§çŸ¥è¯†ï¼‰+ è¿›åŒ–ç®—æ³• system prompt ä¼˜åŒ–ï¼ˆå£°æ˜æ€§çŸ¥è¯†ï¼‰è”åˆè®­ç»ƒï¼›AIME25 56.3â†’60.6%ï¼ˆarXiv:2602.14697ï¼‰â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/GEPA-Reflective-Prompt-Evolution|GEPA]] â­ â€” çº¯ prompt è¿›åŒ–è¶…è¶Š GRPOï¼ˆ5/6ä»»åŠ¡ï¼‰ï¼Œrollout å‡å°‘ 35xï¼›E-SPL=GEPA+RLï¼›ICLR 2026 Oralï¼ŒUCB+Stanford+MITï¼ˆarXiv:2507.19457ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/Goldilocks-RL-Task-Difficulty-Curriculum|Goldilocks RL]] â€” Teacher æ¨¡å‹åœ¨çº¿é¢„æµ‹é¢˜ç›®éš¾åº¦ï¼Œé€‰ pâ‰ˆ0.5 çš„æ ·æœ¬è®­ç»ƒï¼Œé€ƒç¦» sparse reward ä½æ•ˆé™·é˜±ï¼›Apple+EPFLï¼ˆarXiv:2602.14868ï¼‰â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/PACED-RL-Partition-Function-Difficulty-Scheduler|PACED-RL]] â­ â€” GFlowNet é…åˆ†å‡½æ•° Z_Ï† åŒç”¨ï¼šæ—¢åšå½’ä¸€åŒ–ã€åˆåšåœ¨çº¿éš¾åº¦è°ƒåº¦å™¨ï¼ˆé›¶é¢å¤–å¼€é”€ï¼‰ï¼›ä¸ Goldilocks ç‹¬ç«‹æ”¶æ•›åˆ°åŒä¸€è§„å¾‹ï¼ˆä¸­é—´éš¾åº¦æœ€ä¼˜ï¼‰ï¼›ICML 2026 æŠ•ç¨¿ï¼ˆarXiv:2602.12642ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/VAM-Verbalized-Action-Masking-Exploration|VAM]] â€” Within-state æ¢ç´¢å¡Œç¼©çš„è¯Šæ–­ä¸ä¿®å¤ï¼šè¯­è¨€åŒ– action masking å¼ºåˆ¶ group å†… rollout è¦†ç›–ä¸åŒ action åˆ†æ”¯ï¼›â˜…â˜…â˜…â˜†ï¼ˆarXiv:2602.16833ï¼Œå›½é™…è±¡æ£‹åœºæ™¯ï¼‰
- [[AI/LLM/RL/Other-Algorithms/STAPO-Spurious-Token-Aware-Policy-Optimization|STAPO]] â€” 0.01% spurious tokens æºå¸¦è™šå‡æ¢¯åº¦æ˜¯ RL è®­ç»ƒå´©æºƒæ ¹æºï¼›mask æ‰å³å¯ç¨³å®šè®­ç»ƒï¼›æ¸…å+æ»´æ»´ï¼ˆarXiv:2602.15620ï¼‰â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/Stable-Asynchrony-VCPO-Off-Policy-RL|Stable Asynchrony (VCPO)]] â€” å¼‚æ­¥ off-policy RL çš„æ–¹å·®çˆ†ç‚¸æ ¹å› ä¸ä¿®å¤ï¼šVariance-Controlled Policy Optimizationï¼Œè§£å†³ generation/training è§£è€¦åçš„ staleness é—®é¢˜ï¼›MIT HAN Labï¼ˆSong Hanï¼‰â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/SAPO-Soft-Adaptive-Policy-Optimization|SAPO]] â€” Qwen å›¢é˜Ÿï¼ˆQwen3-VL åœ¨ç”¨ï¼‰ï¼šsechÂ² è½¯é—¨æ§æ›¿ä»£ç¡¬è£å‰ªï¼Œä¸å¯¹ç§°æ¸©åº¦å¤„ç†æ­£è´Ÿ advantageï¼›åŒæ­¥ RL åœºæ™¯ä¸‹æ¯” GRPO/GSPO æ›´ç¨³å®šï¼›GSPOâ†’SAPO æ”¹è¿›é“¾æ¡ï¼ˆarXiv:2511.20347ï¼‰â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/RePO-Rephrasing-Policy-Optimization|RePO]] â€” Rephrasing Policy Optimizationï¼šOff-policy çŸ¥è¯†å˜æˆ On-policy å…¼å®¹è½¨è¿¹å†æ³¨å…¥è®­ç»ƒï¼Œè§£å†³ hard sample ä¸‰è§’å›°å¢ƒï¼ˆSFTé€€åŒ–/On-policyé‡‡ä¸åˆ°/Off-policyä¸ç¨³å®šï¼‰ï¼›â˜…â˜…â˜…ï¼ˆarXiv:2602.10819ï¼‰
- [[AI/LLM/RL/Other-Algorithms/MASPO-Mass-Adaptive-Soft-Policy-Optimization|MASPO]] â€” ç»Ÿä¸€æ¢¯åº¦åˆ©ç”¨+æ¦‚ç‡è´¨é‡+ä¿¡å·å¯é æ€§çš„ GRPO ä¸‰ç»´æ”¹è¿›ï¼šè½¯è£å‰ªæ›¿ä»£ç¡¬è£å‰ª + æ¦‚ç‡è´¨é‡æ ¡æ­£ + reward ä¿¡å·å¯é æ€§åŠ æƒï¼›å¾®è½¯äºšç ”ï¼ˆarXiv:2602.17550ï¼‰â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/RICOL-Retrospective-In-Context-Online-Learning|RICOL]] â­ â€” NeurIPS 2025ï¼ŒCMU+HKU+Stanfordï¼šTheorem 4.1 æ‰“é€š ICL=RL ç†è®ºç­‰ä»·æ€§â€”â€”ICL å‰å log-prob å·®æ­£æ¯”äº advantage functionï¼›critic-free sparse reward credit assignmentï¼›æ ·æœ¬æ•ˆç‡ PPO çš„ 3-5Ã—ï¼ˆarXiv:2602.17497ï¼‰â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Other-Algorithms/DEEP-GRPO-Deep-Dense-Exploration-Pivot-Resampling|DEEP-GRPO]] â€” Root Saturation é—®é¢˜æ ¹æ²»ï¼šPivot-Driven Resampling ä¸“æ”»æ·±å±‚ error-prone statesï¼›å¯¹æ¯” TreeRL/AttnRL æ¢ç´¢å¯å‘å¼çš„ç¼ºé™·ï¼›ICML æŠ•ç¨¿ï¼Œ2602.14169ï¼ˆâ˜…â˜…â˜…â˜…â˜†ï¼‰
- [[AI/LLM/RL/Other-Algorithms/VESPO-Variational-Sequence-Policy-Optimization|VESPO]] â­ â€” å˜åˆ†æ¨å¯¼é—­åˆå½¢å¼ soft kernel `Ï•(W)=W^Î±Â·exp(-Î»W)`ï¼Œç†è®ºä¸¥æ ¼è¶…è¶Šæ‰€æœ‰ heuristic clipï¼ˆGRPO/GSPO/SAPOï¼‰ï¼Œstaleness ratio 64Ã— å¼‚æ­¥è®­ç»ƒç¨³å®šï¼›â˜…â˜…â˜…â˜…â˜…ï¼ˆarXiv:2602.10693ï¼‰
- [[AI/LLM/RL/Other-Algorithms/AT-RL-Anchor-Token-Reinforcement-Learning-Multimodal|AT-RL]] â€” å¤šæ¨¡æ€ RLVRï¼šä»… 15% token æœ‰å¼ºè§†è§‰-æ–‡æœ¬è€¦åˆï¼ˆ"è§†è§‰é”šç‚¹"ï¼‰ï¼Œå›¾èšç±»è¯†åˆ«å¹¶é€‰æ‹©æ€§å¼ºåŒ–ï¼›32B æ¨¡å‹ MathVista 80.2 è¶…è¶Š 72B-Instructï¼›ä»… 1.2% å¼€é”€ï¼ˆarXiv:2602.11455ï¼‰â˜…â˜…â˜…â˜…

## è®­ç»ƒæ¡†æ¶ (Frameworks)
- [[AI/LLM/RL/Frameworks/Jet-RL-FP8-On-Policy-RL-Training|Jet-RL]] â€” NVIDIA+MIT HAN Labï¼šç»Ÿä¸€ FP8 on-policy RL è®­ç»ƒç²¾åº¦æµï¼Œè§£å†³ BF16-train/FP8-rollout åœ¨é•¿ rollout(>8K) æ—¶ç²¾åº¦å´©æºƒå’Œè®­ç»ƒå‘æ•£é—®é¢˜ï¼ˆarXiv:2601.14243ï¼‰â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Frameworks/QeRL-Quantization-Enhanced-RL|QeRL]] â­ â€” ICLR 2026ï¼ˆNVIDIA+MIT+HKU+THU+Song Hanï¼‰ï¼šé‡åŒ–å™ªå£°æ˜¯æœ‰ç›Šçš„â€”â€”4-bit é‡åŒ–+LoRA çš„ RL è®­ç»ƒä¸ä»… 1.5Ã— åŠ é€Ÿï¼Œåœ¨å¤šé¡¹åŸºå‡†ä¸Šè¿˜**è¶…è¶Š** 16-bit LoRAï¼›see-also: [[AI/LLM/RL/Frameworks/Jet-RL-FP8-On-Policy-RL-Training|Jet-RL]]ï¼ˆarXiv:2510.11696ï¼‰â˜…â˜…â˜…â˜…
- [[AI/RL/Slime RL Framework|Slime RL Framework]] â€” GLM-5 çš„å¼‚æ­¥ RL åŸºç¡€è®¾æ–½ï¼šè§£å†³ generation bottleneck >90%ï¼ŒAPRIL æ¡†æ¶ï¼ˆsee-also æŒ‡å‘æ·±åº¦ç‰ˆï¼‰

## ç»¼è¿°ä¸æ·±åº¦ç¬”è®°
- [[AI/LLM/RL/Theory/GRPO-Improvement-Panorama-2026|GRPO æ”¹è¿›å…¨æ™¯ 2026]] â­ â€” **ä¸ƒç»´æ¡†æ¶å…ƒåˆ†æ**ï¼ˆv2 æ›´æ–° 2026-02-21ï¼‰ï¼šDiversity/Token/Exploration/Sample/TrustRegion/Off-Policy/System ä¸ƒå±‚åˆ†ç±»ï¼ŒProGRPO+RePO è¡¥å…¥ Diversity ç»´åº¦ï¼›æ·±å±‚ç»Ÿä¸€è§†è§’ï¼šæ‰€æœ‰é—®é¢˜æŒ‡å‘åŒä¸€æ ¹å› ï¼ˆåºåˆ—çº§å¥–åŠ±è®­ç»ƒ token çº§å†³ç­–ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Theory/RLRR-Reference-Guided-Alignment-Non-Verifiable|RLRR]] â­ â€” Reference-Guided RL Alignment for Non-Verifiable Domainsï¼šç”¨é«˜è´¨é‡ reference + RefEval judge ä¸ºå¯¹é½ä»»åŠ¡é€ è½¯ verifierï¼ŒDPO æ€§èƒ½æ¥è¿‘ä¸“è®­ ArmoRMï¼›ICLR 2026ï¼ˆYale+Meta+Scale AI+Salesforceï¼ŒarXiv:2602.16802ï¼‰â˜…â˜…â˜…â˜…â˜†
- [[AI/LLM/RL/Theory/REMuL-CoT-Faithfulness-Multi-Listener-RL|REMuL]] â­ â€” CoT Faithfulness via Multi-Listener RLï¼šå®šä¹‰å¯æ“ä½œçš„ faithfulnessï¼ˆæ¨ç†é“¾å¯è¢«å…¶ä»–æ¨¡å‹"ç»§ç»­æ‰§è¡Œ"åˆ°ç›¸åŒç»“è®ºï¼‰ï¼Œä¸¤é˜¶æ®µè®­ç»ƒï¼ˆGRPO faithfulness RL â†’ masked SFT correctnessï¼‰ï¼Œ**å”¯ä¸€åŒæ—¶æå‡ faithfulness å’Œ accuracy çš„æ–¹æ³•**ï¼›UNC+Ciscoï¼ˆarXiv:2602.16154ï¼ŒICMLæŠ•ç¨¿ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Theory/RL-Training-Stability-2026-Unified-Analysis|RL è®­ç»ƒç¨³å®šæ€§ 2026 ç»Ÿä¸€åˆ†æ]] â­ â€” Scholar ç»¼åˆç¬”è®° v3ï¼šSTAPO/Goldilocks/VCPO/DEEP-GRPO/MASPO/DAPO/LACONIC å››ç»´æ‹“æ‰‘ï¼ˆToken/æ ·æœ¬/æ¢ç´¢/ç³»ç»Ÿï¼‰ï¼ŒæŒç»­æ›´æ–°ä¸­ï¼ˆ2026-02-20ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/Theory/MARS-Margin-Aware-Reward-Modeling-Self-Refinement|MARS]] â€” Fisher information è¯æ˜ä½ margin pair æä¾›æœ€å¤§è®­ç»ƒæ›²ç‡ï¼Œadaptive margin-aware æ•°æ®å¢å¼ºèšç„¦ decision boundary é™„è¿‘ï¼›ICMLæŠ•ç¨¿ï¼ˆarXiv:2602.17658ï¼‰â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/å¼ºåŒ–å­¦ä¹ ä¸RLHFåº”ç”¨-2026å…¨æ™¯|å¼ºåŒ–å­¦ä¹ ä¸RLHFåº”ç”¨ 2026 å…¨æ™¯]] â­ â€” é¢è¯•æ­¦å™¨ç‰ˆï¼Œ741è¡Œï¼Œç»å…¸RL(MDP/Bellman/PPO/GAE)â†’RLHF/DPO/GRPOå…¨é“¾è·¯ï¼Œä»åŸºç¡€åˆ°å‰æ²¿å®Œæ•´è¦†ç›– â˜…â˜…â˜…â˜…â˜…
- [[AI/LLM/RL/RLHF å…¨é“¾è·¯|RLHF å…¨é“¾è·¯]] â€” å®Œæ•´ RLHF ä¸‰é˜¶æ®µ
- [[AI/LLM/RL/RLHF-DPO-2026-æŠ€æœ¯å…¨æ™¯|RLHF/DPO 2026 æŠ€æœ¯å…¨æ™¯]] â€” é¢è¯•æ­¦å™¨ç‰ˆï¼Œ1147è¡Œï¼ŒRLHFâ†’RLAIFâ†’DPO å…¨é“¾è·¯ï¼ˆ2026-02-20ï¼‰
- [[AI/LLM/RL/å¯¹é½æŠ€æœ¯ç»¼è¿°|å¯¹é½æŠ€æœ¯ç»¼è¿°]] â€” RLHF â†’ DPO â†’ ORPO â†’ KTO â†’ SteerLM â†’ Constitutional AI
- [[AI/LLM/RL/RARL-Reward-Modeling-Survey|RARL Reward Modeling Survey]] â€” RL reasoning alignment ç»¼è¿°

## ç›¸å…³ MOC
- â†‘ ä¸Šçº§ï¼š[[AI/LLM/_MOC]]
- â†’ äº¤å‰ï¼š[[AI/Agent/_MOC]]ï¼ˆAgentic RLï¼‰
