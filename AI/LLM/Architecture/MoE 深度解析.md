---
title: "MoE (Mixture of Experts) æ·±åº¦è§£æ"
brief: "MoEï¼ˆMixture of Expertsï¼‰é€šè¿‡æ¡ä»¶è®¡ç®—å®ç°'å¤§å‚æ•°é‡ã€ä½è®¡ç®—é‡'â€”â€”æ¯ä¸ª token åªæ¿€æ´» Top-K ä¸ªä¸“å®¶ï¼Œæ¨ç† FLOPs è¿œä½äºç­‰å‚æ•° Dense æ¨¡å‹ï¼›æ ¸å¿ƒæŒ‘æˆ˜æ˜¯è´Ÿè½½å‡è¡¡ï¼ŒDeepSeek-V3 ç”¨ Auxiliary-Loss-Free Dynamic Bias ä¼˜é›…è§£å†³ï¼›ç†è§£ MoE æ˜¯è¯»æ‡‚ Mixtral/DeepSeek ç³»åˆ—æ¶æ„çš„å‰æ"
type: concept
domain: ai/llm/architecture
created: "2026-02-13"
updated: "2026-02-22"
tags:
  - ai/llm/architecture
  - ai/llm/moe
  - type/concept
  - interview/hot
status: complete
sources:
  - "Switch Transformers arXiv:2101.03961 (Fedus et al., 2021)"
  - "GShard arXiv:2006.16668 (Lepikhin et al., 2020)"
  - "DeepSeekMoE arXiv:2401.06066 (Dai et al., 2024)"
  - "DeepSeek-V2 arXiv:2405.04434"
  - "DeepSeek-V3 arXiv:2412.19437"
  - "Mixtral arXiv:2401.04088 (Jiang et al., 2024)"
related:
  - "[[AI/Foundations/DL-Basics/MoE åŸºç¡€]]"
  - "[[AI/LLM/Architecture/DeepSeek-R1]]"
  - "[[AI/LLM/Architecture/DeepSeek Engram]]"
  - "[[AI/LLM/Infra/åˆ†å¸ƒå¼è®­ç»ƒ]]"
  - "[[AI/LLM/Infra/DeepSpeed]]"
  - "[[AI/LLM/Inference/vLLM]]"
  - "[[AI/LLM/SFT/LoRA]]"
---

# MoE (Mixture of Experts) æ·±åº¦è§£æ

> **Brief**ï¼šMoE é€šè¿‡æ¡ä»¶è®¡ç®—ï¼ˆConditional Computationï¼‰å®ç°"å¤§å‚æ•°é‡ã€ä½è®¡ç®—é‡"â€”â€”æ¯ä¸ª token åªæ¿€æ´» Top-K ä¸ªä¸“å®¶ï¼Œæ¨ç† FLOPs è¿œä½äºç­‰å‚æ•° Dense æ¨¡å‹ã€‚æ ¸å¿ƒæŒ‘æˆ˜æ˜¯è´Ÿè½½å‡è¡¡ï¼ŒDeepSeek-V3 ç”¨ Auxiliary-Loss-Free Dynamic Bias ä¼˜é›…è§£å†³ã€‚
>
> æ¥æºï¼šSwitch Transformers arXiv:2101.03961; DeepSeek-V3 arXiv:2412.19437

---

## 1. MoE æ ¸å¿ƒæ€æƒ³

MoE çš„æ ¸å¿ƒæ˜¯**æ¡ä»¶è®¡ç®—ï¼ˆConditional Computationï¼‰**ï¼šä¸æ˜¯æ¯ä¸ªè¾“å…¥éƒ½æ¿€æ´»æ‰€æœ‰å‚æ•°ï¼Œè€Œæ˜¯æ ¹æ®è¾“å…¥åŠ¨æ€é€‰æ‹©ä¸€éƒ¨åˆ†"ä¸“å®¶"æ¥å¤„ç†ã€‚

> æ¥æºï¼šæ¡ä»¶è®¡ç®—çš„æ€æƒ³æœ€æ—©ç”± Bengio et al. (2013) æå‡ºï¼ŒMoE å±‚ç”± Shazeer et al. (2017, arXiv:1701.06538) å¼•å…¥ Transformerã€‚

### åŸºæœ¬ç»“æ„

```mermaid
flowchart LR
    A["Input Token"] --> B["Router (Gate)"]
    B --> C["Top-K é€‰æ‹©"]
    C --> E1["Expert 1"]
    C --> E2["Expert 2"]
    C -->|"..."| EN["Expert N"]
    E1 --> D["Weighted Sum"]
    E2 --> D
    EN --> D
    D --> F["Output"]

    style B fill:#f9f,stroke:#333,stroke-width:2px
    style C fill:#bbf,stroke:#333,stroke-width:2px
```

å…³é”®å…¬å¼ï¼š

$$y = \sum_{i=1}^{N} g_i(x) \cdot E_i(x)$$

å…¶ä¸­ $g_i(x)$ æ˜¯è·¯ç”±æƒé‡ï¼ˆå¤§éƒ¨åˆ†ä¸º 0ï¼Œåªæœ‰ Top-K éé›¶ï¼‰ï¼Œ$E_i(x)$ æ˜¯ç¬¬ $i$ ä¸ªä¸“å®¶çš„è¾“å‡ºã€‚

> æ¥æºï¼šShazeer et al. arXiv:1701.06538, Eq. (1)

### ä¼˜åŠ£åŠ¿

| ä¼˜åŠ¿ | åŠ£åŠ¿ |
|------|------|
| åŒç­‰ FLOPs ä¸‹å‚æ•°é‡æ›´å¤§ â†’ æ›´å¼ºè¡¨è¾¾åŠ› | éœ€è¦æ‰€æœ‰å‚æ•°åŠ è½½åˆ°æ˜¾å­˜ |
| é¢„è®­ç»ƒæ•ˆç‡é«˜ï¼ˆåŒè´¨é‡æ›´å¿«æ”¶æ•›ï¼‰ | å¾®è°ƒå®¹æ˜“è¿‡æ‹Ÿåˆ |
| æ¨ç†æ—¶åªæ¿€æ´»å­é›† â†’ FLOPs ä½ | è·¯ç”±ä¸å‡è¡¡å¯¼è‡´æ•ˆç‡ä¸‹é™ |
| å¯æ‰©å±•æ€§å¼º | é€šä¿¡å¼€é”€å¤§ï¼ˆåˆ†å¸ƒå¼è®­ç»ƒï¼‰ |

> ä»¥ Mixtral 8x7B ä¸ºä¾‹ï¼šæ€»å‚æ•° 47Bï¼ˆé 56Bï¼Œå› ä¸ºé FFN å±‚å…±äº«ï¼‰ï¼Œæ¯æ¬¡æ¨ç†åªæ¿€æ´»çº¦ 12B å‚æ•°ã€‚
> æ¥æºï¼šMixtral arXiv:2401.04088, Sec. 1
>
> å‚è§ [[AI/Foundations/DL-Basics/MoE åŸºç¡€|MoE åŸºç¡€]] äº†è§£æ›´å¤šåŸºç¡€æ¦‚å¿µã€‚

## 2. è·¯ç”±æœºåˆ¶æ¼”è¿›

### 2.1 ç»å…¸ Top-K è·¯ç”±

æœ€æ—©çš„ Switch Transformer (Google, 2021) ä½¿ç”¨ Top-1 è·¯ç”±ï¼š

```python
# ç»å…¸ Top-K Router
def top_k_route(x, W_gate, k=2):
    logits = x @ W_gate  # [batch, num_experts]
    topk_values, topk_indices = torch.topk(logits, k)
    weights = F.softmax(topk_values, dim=-1)
    return weights, topk_indices
```

é—®é¢˜ï¼šå®¹æ˜“å‡ºç°**è·¯ç”±åå¡Œï¼ˆRoute Collapseï¼‰**â€”â€”Router å€¾å‘äºæ€»æ˜¯é€‰æ‹©å°‘æ•°å‡ ä¸ªä¸“å®¶ã€‚

> æ¥æºï¼šSwitch Transformers arXiv:2101.03961, Sec. 2.2 è®¨è®ºäº†è·¯ç”±åå¡Œé—®é¢˜å¹¶å¼•å…¥ capacity factor ç¼“è§£

### 2.2 DeepSeek-V2ï¼šç»†ç²’åº¦ä¸“å®¶ + å…±äº«ä¸“å®¶

DeepSeek-V2 (2024) å¼•å…¥äº† **DeepSeekMoE** æ¶æ„çš„å…³é”®åˆ›æ–°ï¼š

> æ¥æºï¼šDeepSeekMoE arXiv:2401.06066; DeepSeek-V2 arXiv:2405.04434

#### ç»†ç²’åº¦ä¸“å®¶åˆ†å‰²
- ä¸åŒäº Mixtral çš„ 8 ä¸ªå¤§ä¸“å®¶ï¼ŒDeepSeek-V2 ä½¿ç”¨ **160 ä¸ªå°ä¸“å®¶**
- æ¯ä¸ª token æ¿€æ´» 6 ä¸ªè·¯ç”±ä¸“å®¶ + **2 ä¸ªå…±äº«ä¸“å®¶ï¼ˆShared Expertsï¼‰**
- å…±äº«ä¸“å®¶å§‹ç»ˆæ¿€æ´»ï¼Œå¤„ç†é€šç”¨çŸ¥è¯†ï¼›è·¯ç”±ä¸“å®¶å¤„ç†ç‰¹åŒ–çŸ¥è¯†

```mermaid
flowchart TD
    T["Token"] --> SE1["Shared Expert 1 (å§‹ç»ˆæ¿€æ´»)"]
    T --> SE2["Shared Expert 2 (å§‹ç»ˆæ¿€æ´»)"]
    T --> R["Router"]
    R -->|"Top-6"| RE["160 Routed Experts ä¸­é€‰ 6 ä¸ª"]
    SE1 --> SUM["åŠ æƒæ±‚å’Œ â†’ Output"]
    SE2 --> SUM
    RE --> SUM

    style SE1 fill:#afa,stroke:#333
    style SE2 fill:#afa,stroke:#333
    style R fill:#f9f,stroke:#333,stroke-width:2px
```

#### Multi-head Latent Attention (MLA)
- å‹ç¼© KV Cacheï¼šå°† Key-Value å‹ç¼©åˆ°ä½ç§©éšç©ºé—´
- æ˜¾å­˜ä»æ ‡å‡† MHA çš„ **100%** é™åˆ°çº¦ **5-10%**
- è¿™ä¹Ÿæ˜¯ [[AI/LLM/Inference/vLLM|vLLM]] ç­‰æ¨ç†æ¡†æ¶éœ€è¦é€‚é…çš„å…³é”®ç‰¹æ€§

> æ¥æºï¼šDeepSeek-V2 arXiv:2405.04434, Sec. 3.1 (Multi-head Latent Attention)

### 2.3 DeepSeek-V3ï¼šæ— è¾…åŠ©æŸå¤±è´Ÿè½½å‡è¡¡

DeepSeek-V3 (2024-12) åœ¨ MoE æ¶æ„ä¸Šåšå‡ºäº†çªç ´æ€§æ”¹è¿›ï¼š

> æ¥æºï¼šDeepSeek-V3 arXiv:2412.19437

#### æ¶æ„è§„æ ¼
- æ€»å‚æ•°ï¼š**671B**ï¼Œæ¯ token æ¿€æ´» **37B**
- 256 ä¸ªè·¯ç”±ä¸“å®¶ + 1 ä¸ªå…±äº«ä¸“å®¶
- æ¯ token æ¿€æ´» 8 ä¸ªè·¯ç”±ä¸“å®¶

#### Auxiliary-Loss-Free è´Ÿè½½å‡è¡¡

ä¼ ç»Ÿæ–¹æ³•ä½¿ç”¨è¾…åŠ©æŸå¤±ï¼ˆauxiliary lossï¼‰æƒ©ç½šä¸å‡è¡¡çš„è·¯ç”±ï¼Œä½†è¿™ä¼šæŸå®³æ¨¡å‹æ€§èƒ½ï¼š

```python
# ä¼ ç»Ÿæ–¹å¼ï¼šè¾…åŠ©æŸå¤±ï¼ˆSwitch Transformer æå‡ºï¼‰
L_balance = Î± * Î£(f_i * P_i)  # f_i=å®é™…åˆ†é…æ¯”ä¾‹, P_i=è·¯ç”±æ¦‚ç‡
# é—®é¢˜ï¼šÎ± çš„æƒè¡¡ â€”â€” å¤ªå¤§æŸå®³æ€§èƒ½ï¼Œå¤ªå°ä¸èµ·ä½œç”¨
```

> æ¥æºï¼šSwitch Transformers arXiv:2101.03961, Eq. (4)-(6) å®šä¹‰äº† auxiliary loss

DeepSeek-V3 çš„åˆ›æ–°â€”â€”**åŠ¨æ€åç½®é¡¹ï¼ˆDynamic Biasï¼‰**ï¼š

```python
# DeepSeek-V3: Auxiliary-Loss-Free Balancing
def route_with_bias(x, W_gate, bias):
    """æ¯ä¸ªä¸“å®¶ç»´æŠ¤ä¸€ä¸ªåç½®é¡¹ï¼ŒåŠ¨æ€è°ƒæ•´"""
    logits = x @ W_gate + bias  # bias ç”¨äºè·¯ç”±å†³ç­–
    # æ³¨æ„ï¼šbias åªå½±å“è·¯ç”±é€‰æ‹©ï¼Œä¸å½±å“æƒé‡è®¡ç®—
    topk_indices = torch.topk(logits, k=8).indices
    # æƒé‡è®¡ç®—æ—¶ä¸åŒ…å« bias
    weights = F.softmax((x @ W_gate)[topk_indices], dim=-1)
    return weights, topk_indices

# åŠ¨æ€æ›´æ–° biasï¼š
# å¦‚æœä¸“å®¶ i è¿‡è½½ â†’ é™ä½ bias_i
# å¦‚æœä¸“å®¶ i ç©ºé—² â†’ æé«˜ bias_i
```

æ ¸å¿ƒæ´å¯Ÿï¼š**å°†è·¯ç”±å†³ç­–ï¼ˆç”¨ bias è°ƒèŠ‚ï¼‰ä¸æƒé‡è®¡ç®—ï¼ˆä¸ç”¨ biasï¼‰è§£è€¦**ã€‚

> æ¥æºï¼šDeepSeek-V3 arXiv:2412.19437, Sec. 3.2 (Auxiliary-Loss-Free Load Balancing)

#### Multi-Token Prediction (MTP)
- æ¯ä¸ªä½ç½®é¢„æµ‹ä¸‹ä¸€ä¸ª token çš„åŒæ—¶ï¼Œé¢å¤–é¢„æµ‹åç»­ 1-2 ä¸ª token
- åœ¨è®­ç»ƒæ—¶æä¾›æ›´ä¸°å¯Œçš„ç›‘ç£ä¿¡å·
- æ¨ç†æ—¶å¯ç”¨äº [[AI/LLM/Inference/æ¨ç†ä¼˜åŒ–|Speculative Decoding]]

> æ¥æºï¼šDeepSeek-V3 arXiv:2412.19437, Sec. 3.4

### 2.4 DeepSeek-V3.2 çš„ RL è®­ç»ƒæ”¹è¿›

é’ˆå¯¹ MoE æ¨¡å‹çš„ RL è®­ç»ƒï¼š

- **Keep Routing**ï¼šåœ¨ rollout æ—¶è®°å½•ä¸“å®¶æ¿€æ´»æ¨¡å¼ï¼Œè®­ç»ƒæ—¶å¼ºåˆ¶ä½¿ç”¨ç›¸åŒè·¯ç”±
- ç¡®ä¿æ¢¯åº¦æ›´æ–°åªä½œç”¨äºå®é™…äº§ç”Ÿç­”æ¡ˆçš„ä¸“å®¶
- **Off-policy Sequence Masking**ï¼šä¸¢å¼ƒç­–ç•¥åç§»è¿‡å¤§çš„åºåˆ—

## 3. è´Ÿè½½å‡è¡¡è¯¦è§£

è´Ÿè½½å‡è¡¡æ˜¯ MoE å·¥ç¨‹ä¸­æœ€æ ¸å¿ƒçš„æŒ‘æˆ˜ï¼š

### 3.1 ä¸ºä»€ä¹ˆéœ€è¦è´Ÿè½½å‡è¡¡ï¼Ÿ

```mermaid
graph LR
    subgraph ä¸å‡è¡¡
        E1_bad["Expert 1: 80% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ"]
        E2_bad["Expert 2: 10% â–ˆâ–ˆ"]
        E3_bad["Expert 3: 5% â–ˆ"]
        E4_bad["Expert 4: 5% â–ˆ"]
    end

    subgraph ç†æƒ³çŠ¶æ€
        E1_good["Expert 1: 25% â–ˆâ–ˆâ–ˆâ–ˆ"]
        E2_good["Expert 2: 25% â–ˆâ–ˆâ–ˆâ–ˆ"]
        E3_good["Expert 3: 25% â–ˆâ–ˆâ–ˆâ–ˆ"]
        E4_good["Expert 4: 25% â–ˆâ–ˆâ–ˆâ–ˆ"]
    end

    ä¸å‡è¡¡ -->|"è´Ÿè½½å‡è¡¡ç­–ç•¥"| ç†æƒ³çŠ¶æ€

    style E1_bad fill:#f66,stroke:#333
    style E1_good fill:#6f6,stroke:#333
    style E2_good fill:#6f6,stroke:#333
    style E3_good fill:#6f6,stroke:#333
    style E4_good fill:#6f6,stroke:#333
```

ä¸å‡è¡¡åæœï¼šè¿‡è½½ä¸“å®¶æˆä¸ºç“¶é¢ˆï¼Œç©ºé—²ä¸“å®¶æœªå……åˆ†è®­ç»ƒã€‚

### 3.2 è´Ÿè½½å‡è¡¡ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | æ–¹æ³• | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¥æº |
|------|------|------|------|------|
| Auxiliary Loss | æ·»åŠ è´Ÿè½½å‡è¡¡æŸå¤±å‡½æ•° | ç®€å•ç›´æ¥ | è¶…å‚æ•æ„Ÿï¼ŒæŸå®³æ€§èƒ½ | Switch Transformer arXiv:2101.03961 |
| Expert Choice | ä¸“å®¶é€‰æ‹© token è€Œé token é€‰ä¸“å®¶ | å®Œç¾å‡è¡¡ | å› æœæ¨¡å‹ä¸é€‚ç”¨ | Zhou et al. arXiv:2202.09368 |
| Token Drop | è¶…è½½ä¸“å®¶ä¸¢å¼ƒ token | å¼ºåˆ¶å‡è¡¡ | ä¿¡æ¯ä¸¢å¤± | GShard arXiv:2006.16668 |
| **Dynamic Bias** (V3) | åç½®é¡¹åŠ¨æ€è°ƒæ•´ | æ— æ€§èƒ½æŸå®³ | å®ç°å¤æ‚ | DeepSeek-V3 arXiv:2412.19437 |
| Capacity Factor | é™åˆ¶æ¯ä¸ªä¸“å®¶å¤„ç†é‡ä¸Šé™ | å¯æ§ | éœ€è¦ä»”ç»†è°ƒå‚ | Switch Transformer arXiv:2101.03961 |

### 3.3 åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„ All-to-All é€šä¿¡

MoE çš„åˆ†å¸ƒå¼è®­ç»ƒéœ€è¦ **Expert Parallelism**ï¼š

```mermaid
flowchart LR
    subgraph GPU_0["GPU 0"]
        E0["Expert 0"]
        E1["Expert 1"]
    end
    subgraph GPU_1["GPU 1"]
        E2["Expert 2"]
        E3["Expert 3"]
    end

    GPU_0 <-->|"All-to-All é€šä¿¡"| GPU_1

    style GPU_0 fill:#e8e8ff,stroke:#333
    style GPU_1 fill:#ffe8e8,stroke:#333
```

è·¯ç”±å token éœ€è¦å‘é€åˆ°å¯¹åº” GPU ä¸Šçš„ä¸“å®¶å¤„ç†ã€‚

DeepSeek-V3 çš„é€šä¿¡ä¼˜åŒ–ï¼š
- **FP8 é‡åŒ–é€šä¿¡**ï¼šå°† All-to-All é€šä¿¡æ•°æ®ä» FP16 â†’ FP8ï¼Œå¸¦å®½å‡åŠ
- **é‡å è®¡ç®—ä¸é€šä¿¡**ï¼šåœ¨è®¡ç®—å½“å‰å±‚æ—¶é¢„å–ä¸‹ä¸€å±‚çš„é€šä¿¡æ•°æ®
- è®­ç»ƒæˆæœ¬ä»… **$5.58M**ï¼ˆ2048 Ã— H800, 2ä¸ªæœˆï¼‰ï¼Œæè‡´å·¥ç¨‹ä¼˜åŒ–

> æ¥æºï¼šDeepSeek-V3 arXiv:2412.19437, Sec. 4 (Training Infrastructure)

## 4. å·¥ç¨‹å®è·µè¦ç‚¹

### 4.1 MoE è®­ç»ƒ Checklist

```yaml
è®­ç»ƒé…ç½®:
  num_experts: 256          # DeepSeek-V3 è§„æ¨¡
  num_shared_experts: 1     # å…±äº«ä¸“å®¶
  top_k: 8                  # æ¯ token æ¿€æ´»æ•°
  expert_capacity_factor: 1.25  # å®¹é‡å› å­
  load_balance_strategy: "dynamic_bias"  # V3 æ–¹å¼

é€šä¿¡ä¼˜åŒ–:
  expert_parallel_size: 8   # EP å¹¶è¡Œåº¦
  communication_dtype: "fp8" # é‡åŒ–é€šä¿¡
  overlap_comm_compute: true
```

### 4.2 æ¨ç†éƒ¨ç½²æ³¨æ„äº‹é¡¹

- **æ˜¾å­˜éœ€æ±‚**ï¼šå³ä½¿åªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶ï¼Œæ‰€æœ‰å‚æ•°éƒ½è¦åŠ è½½
- DeepSeek-V3 671B â†’ éœ€è¦çº¦ **350GB+ æ˜¾å­˜**ï¼ˆFP8 é‡åŒ–åçº¦ ~170GBï¼‰
- [[AI/LLM/Inference/vLLM|vLLM]] å·²æ”¯æŒ DeepSeek-V3 çš„ MoE æ¨ç†
- Expert offloadingï¼šå†·é—¨ä¸“å®¶å¯ä»¥ offload åˆ° CPU/SSD

## 5. é¢è¯•å¸¸è§é—®é¢˜

**Q1: MoE å’Œ Dense æ¨¡å‹ï¼Œç›¸åŒ FLOPs ä¸‹è°æ›´å¼ºï¼Ÿ**
A: MoE æ›´å¼ºã€‚å› ä¸º MoE å¯ä»¥åœ¨ç›¸åŒè®¡ç®—é‡ä¸‹æ‹¥æœ‰æ›´å¤§çš„å‚æ•°é‡ï¼ˆæ›´å¤šçŸ¥è¯†å®¹é‡ï¼‰ï¼Œåªæ˜¯æ¯æ¬¡æ¨ç†åªç”¨ä¸€éƒ¨åˆ†ã€‚Switch Transformer è®ºæ–‡ï¼ˆarXiv:2101.03961ï¼‰å®éªŒè¡¨æ˜ MoE åœ¨åŒç­‰ FLOPs ä¸‹é¢„è®­ç»ƒæ”¶æ•›æ›´å¿«ã€æ€§èƒ½æ›´å¼ºã€‚

**Q2: DeepSeek-V3 çš„ auxiliary-loss-free è´Ÿè½½å‡è¡¡æ€ä¹ˆå·¥ä½œçš„ï¼Ÿ**
A: ç»™æ¯ä¸ªä¸“å®¶åŠ ä¸€ä¸ª bias é¡¹ï¼Œå½±å“è·¯ç”±å†³ç­–ä½†ä¸å½±å“æƒé‡è®¡ç®—ã€‚å¦‚æœä¸“å®¶è¿‡è½½å°±é™ biasã€ç©ºé—²å°±å‡ biasã€‚å…³é”®æ˜¯è·¯ç”±å†³ç­–å’Œæƒé‡è®¡ç®—çš„è§£è€¦ã€‚ï¼ˆæ¥æºï¼šDeepSeek-V3 arXiv:2412.19437, Sec. 3.2ï¼‰

**Q3: ä¸ºä»€ä¹ˆ DeepSeek ç”¨ç»†ç²’åº¦ä¸“å®¶ï¼ˆ160/256ä¸ªå°ä¸“å®¶ï¼‰è€Œä¸æ˜¯ Mixtral é‚£æ ·çš„ 8 ä¸ªå¤§ä¸“å®¶ï¼Ÿ**
A: ç»†ç²’åº¦ä¸“å®¶æä¾›æ›´çµæ´»çš„çŸ¥è¯†ç»„åˆæ–¹å¼ã€‚8 ä¸ªå¤§ä¸“å®¶ â†’ æ¯æ¬¡é€‰ 2 ä¸ªåªæœ‰ $C(8,2)=28$ ç§ç»„åˆï¼›160 ä¸ªå°ä¸“å®¶é€‰ 6 ä¸ª â†’ ç»„åˆæ•° $C(160,6) \approx 2.1 \times 10^{10}$ï¼Œè¡¨è¾¾åŠ›è¿œè¶…ã€‚ï¼ˆæ¥æºï¼šDeepSeekMoE arXiv:2401.06066, Sec. 2ï¼‰

**Q4: MoE å¾®è°ƒä¸ºä»€ä¹ˆå®¹æ˜“è¿‡æ‹Ÿåˆï¼Ÿ**
A: å› ä¸ºæ¯ä¸ªä¸“å®¶çœ‹åˆ°çš„æ•°æ®é‡å°‘ï¼ˆæ€»æ•°æ®è¢« $K/N$ ç¨€é‡Šï¼‰ï¼Œè€Œå‚æ•°é‡åˆå¤§ã€‚è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ [[AI/LLM/SFT/LoRA|LoRA]] å¾®è°ƒã€å†»ç»“ Routerã€å¢åŠ æ­£åˆ™åŒ–ã€‚

**Q5: è·¯ç”±åå¡Œï¼ˆRoute Collapseï¼‰æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•æ£€æµ‹å’Œè§£å†³ï¼Ÿ**
A: è·¯ç”±å™¨æŒç»­åªé€‰å°‘æ•°ä¸“å®¶ï¼Œå…¶ä»–ä¸“å®¶å¾—ä¸åˆ°è®­ç»ƒã€‚æ£€æµ‹ï¼šç›‘æ§æ¯ä¸ªä¸“å®¶çš„ token æ¥æ”¶é‡åˆ†å¸ƒã€‚è§£å†³ï¼šè´Ÿè½½å‡è¡¡æŸå¤±/Dynamic Bias/Expert Choice Routingã€‚

---

## ğŸ”§ è½åœ°åº”ç”¨

### ä»€ä¹ˆæ—¶å€™ç”¨ MoEï¼Ÿ
- **é¢„è®­ç»ƒå¤§è§„æ¨¡æ¨¡å‹**ï¼šå¸Œæœ›æ¨¡å‹å®¹é‡å¤§ä½†æ¨ç† FLOPs å¯æ§ï¼ŒMoE æ˜¯é¦–é€‰æ¶æ„ï¼ˆMixtral 8x7Bã€DeepSeek-V3 éƒ½æ˜¯ MoEï¼‰
- **å¤šä»»åŠ¡/å¤šé¢†åŸŸæ¨¡å‹**ï¼šä¸åŒä¸“å®¶å¯ä»¥è‡ªç„¶ç‰¹åŒ–ä¸ºä¸åŒé¢†åŸŸï¼ˆä»£ç ä¸“å®¶ã€æ•°å­¦ä¸“å®¶ã€è¯­è¨€ä¸“å®¶ï¼‰ï¼Œæ¯” Dense æ›´é«˜æ•ˆ
- **æ¨ç†æˆæœ¬æ•æ„Ÿåœºæ™¯**ï¼šç›¸åŒè´¨é‡ä¸‹ MoE æ¨ç† FLOPs æ›´ä½ï¼ˆDeepSeek-V3 671B å‚æ•°ä½†åªæ¿€æ´» 37Bï¼‰

### ä»€ä¹ˆæ—¶å€™ä¸ç”¨ MoEï¼Ÿ
- **æ˜¾å­˜å—é™**ï¼šMoE æ€»å‚æ•°é‡å¤§ï¼Œæ‰€æœ‰ä¸“å®¶éƒ½è¦åŠ è½½ï¼Œå¯¹æ¨ç†æ˜¾å­˜ä¸å‹å¥½
- **å°æ¨¡å‹åœºæ™¯**ï¼š7B ä»¥ä¸‹æ¨¡å‹ç”¨ MoE æ”¶ç›Šä¸æ˜æ˜¾ï¼Œè·¯ç”±å¼€é”€å æ¯”å¤ªé«˜
- **å¾®è°ƒåœºæ™¯**ï¼šMoE å¾®è°ƒå®¹æ˜“è¿‡æ‹Ÿåˆï¼Œä¼˜å…ˆè€ƒè™‘ LoRA æˆ–å†»ç»“è·¯ç”±å±‚

### éƒ¨ç½²å·¥ç¨‹è¦ç‚¹
- **Expert Parallelism (EP)**ï¼šå¤§ MoE æ¨¡å‹å¿…é¡»è·¨ GPU åˆ†å¸ƒä¸“å®¶ï¼Œéœ€è¦é«˜é€Ÿ All-to-All é€šä¿¡
- **KV Cache ä¼˜åŒ–**ï¼šMLAï¼ˆDeepSeek-V2/V3ï¼‰å¤§å¹…å‹ç¼© KV Cacheï¼ŒvLLM éœ€è¦ä¸“é—¨é€‚é…
- **Expert Offloading**ï¼šå¯¹æ¨ç†å»¶è¿Ÿä¸æ•æ„Ÿçš„åœºæ™¯ï¼Œå¯ä»¥æŠŠå†·é—¨ä¸“å®¶ offload åˆ° CPU/SSDï¼ŒèŠ‚çœ GPU æ˜¾å­˜
- **é‡åŒ–éƒ¨ç½²**ï¼šFP8 é‡åŒ–å¯å°† DeepSeek-V3 ä» ~350GB å‹ç¼©åˆ° ~170GBï¼Œé€‚é… 8Ã—H100

### é¢è¯•åŠ åˆ†é¡¹
- èƒ½ç”»å‡º MoE çš„ Router â†’ Expert â†’ Weighted Sum æµç¨‹
- èƒ½è§£é‡Š Auxiliary Loss vs Dynamic Bias çš„ trade-off
- èƒ½è¯´æ˜ Expert Parallelism ä¸­ All-to-All é€šä¿¡çš„ç“¶é¢ˆå’Œä¼˜åŒ–

---

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿ
MoE è¯æ˜äº†ä¸€ä¸ªæ·±åˆ»çš„è®¾è®¡å“²å­¦ï¼š**ä¸æ˜¯æ‰€æœ‰çŸ¥è¯†éƒ½éœ€è¦åŒæ—¶å‚ä¸è®¡ç®—**ã€‚è¿™å’Œäººè„‘çš„å·¥ä½œæ–¹å¼ç±»ä¼¼â€”â€”é¢å¯¹æ•°å­¦é¢˜æ—¶ï¼Œä½ ä¸éœ€è¦æ¿€æ´»è¯­è¨€ç¿»è¯‘åŒºåŸŸçš„ç¥ç»å…ƒã€‚DeepSeek-V3 ç”¨ 671B å‚æ•°ä½†åªæ¿€æ´» 37B å°±åœ¨å¤šä¸ªåŸºå‡†ä¸Šè¶…è¶Š GPT-4oï¼Œè¿™è¯´æ˜"ç¨€ç–æ¿€æ´» + å¤§å®¹é‡"å¯èƒ½æ˜¯æ¯”"å¯†é›†è®¡ç®— + å°å®¹é‡"æ›´ä¼˜çš„ scaling è·¯çº¿ã€‚

å¯¹è€æ¿çš„å¯ç¤ºï¼šåœ¨ç³»ç»Ÿè®¾è®¡ä¸­ï¼Œ"æ¡ä»¶è®¡ç®—"æ˜¯ä¸€ä¸ªé€šç”¨æ€è·¯â€”â€”ä¸è¦è®©æ‰€æœ‰æ¨¡å—å¤„ç†æ‰€æœ‰è¯·æ±‚ï¼Œè€Œæ˜¯æ ¹æ®è¯·æ±‚ç±»å‹è·¯ç”±åˆ°ä¸“é—¨çš„æ¨¡å—ã€‚è¿™ä¸ªæ€è·¯å¯ä»¥ç”¨åœ¨ Agent æ¶æ„ã€å¾®æœåŠ¡è·¯ç”±ç­‰åœºæ™¯ã€‚

### å±€é™ä¸æœªè§£é—®é¢˜
- **æ¨ç†æ˜¾å­˜ç“¶é¢ˆ**ï¼šMoE çš„æ€»å‚æ•°é‡å¤§ï¼Œå³ä½¿åªæ¿€æ´»éƒ¨åˆ†ä¸“å®¶ä¹Ÿéœ€è¦å…¨éƒ¨åŠ è½½ï¼Œè¿™é™åˆ¶äº†ç«¯ä¾§éƒ¨ç½²
- **ä¸“å®¶ç‰¹åŒ–çš„å¯è§£é‡Šæ€§**ï¼šè·¯ç”±å™¨åˆ°åº•å­¦ä¼šäº†ä»€ä¹ˆï¼Ÿæ¯ä¸ªä¸“å®¶åˆ°åº•ç‰¹åŒ–äº†ä»€ä¹ˆçŸ¥è¯†ï¼Ÿç›®å‰ç¼ºä¹ç³»ç»Ÿæ€§ç ”ç©¶
- **MoE + RL çš„ç¨³å®šæ€§**ï¼šåœ¨ RL è®­ç»ƒä¸­ä¸“å®¶è·¯ç”±å¯èƒ½éœ‡è¡ï¼ŒDeepSeek-V3.2 çš„ Keep Routing æ˜¯ä¸€ä¸ª workaroundï¼Œä½†ä¸æ˜¯æ ¹æœ¬è§£å†³
- **é•¿ä¸Šä¸‹æ–‡ä¸­çš„è·¯ç”±ä¸€è‡´æ€§**ï¼šä¸€æ®µè¿è´¯çš„è®ºè¿°ä¸­ä¸åŒ token å¯èƒ½è·¯ç”±åˆ°ä¸åŒä¸“å®¶ï¼Œè¿™ä¼šå½±å“ä¸€è‡´æ€§å—ï¼Ÿ

### è„‘æš´æ‹“å±•
- MoE çš„"ä¸“å®¶ + è·¯ç”±"æ€è·¯èƒ½å¦ç”¨åœ¨ Agent ç³»ç»Ÿä¸­ï¼Ÿå¤šä¸ªä¸“ä¸š Agent + ä¸€ä¸ª Router Agent = Agent MoEï¼Ÿ
- å¦‚æœæ¯ä¸ªä¸“å®¶é™„åŠ ä¸€ä¸ª LoRAï¼Œèƒ½å¦å®ç° MoE-LoRA æ··åˆå¾®è°ƒï¼Ÿï¼ˆå·²æœ‰ç›¸å…³å·¥ä½œï¼šMoLoRAï¼‰
- DeepSeek çš„ Dynamic Bias æ˜¯å¦å¯ä»¥è¿ç§»åˆ°æ¨èç³»ç»Ÿçš„æµé‡åˆ†é…ï¼Ÿ

> ğŸ”— See also:
> - [[AI/Foundations/DL-Basics/MoE åŸºç¡€]] â€” å…¥é—¨æ¦‚å¿µ
> - [[AI/LLM/Architecture/DeepSeek-R1]] â€” æ¨ç†èƒ½åŠ›ï¼ŒGRPO è®­ç»ƒ
> - [[AI/LLM/Architecture/DeepSeek Engram]] â€” æ¡ä»¶è®°å¿†æ–°ç»´åº¦
> - [[AI/LLM/Infra/åˆ†å¸ƒå¼è®­ç»ƒ]] â€” Expert Parallelism çš„åˆ†å¸ƒå¼å®ç°
> - [[AI/LLM/Infra/DeepSpeed]] â€” MoE è®­ç»ƒæ”¯æŒ

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961) â€” MoE åœ¨ Transformer ä¸­çš„é‡Œç¨‹ç¢‘å·¥ä½œï¼ŒTop-1 è·¯ç”± + Capacity Factorï¼Œå¿…è¯» â­â­â­â­â­
- [GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668) â€” Google çš„å¤§è§„æ¨¡ MoE å·¥ç¨‹åŒ–æ–¹æ¡ˆï¼ŒToken Drop æ€è·¯ â­â­â­â­
- [DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models](https://arxiv.org/abs/2401.06066) â€” ç»†ç²’åº¦ä¸“å®¶ + å…±äº«ä¸“å®¶çš„æå‡º â­â­â­â­â­
- [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437) â€” 671B MoE çš„å®Œæ•´å·¥ç¨‹ç»†èŠ‚ï¼ŒDynamic Bias + MTP + FP8 è®­ç»ƒ â­â­â­â­â­

### æ·±åº¦è§£è¯»
- [Mixture of Experts Explained (HuggingFace Blog)](https://huggingface.co/blog/moe) â€” æœ€ä½³å…¥é—¨å‘è§£è¯»ï¼Œå«ä»£ç  â­â­â­â­â­
- [Mixtral of Experts (Mistral AI)](https://arxiv.org/abs/2401.04088) â€” Mixtral 8x7B æŠ€æœ¯æŠ¥å‘Šï¼Œå¼€æº MoE çš„ä»£è¡¨ â­â­â­â­

### å®è·µèµ„æº
- [Megablocks](https://github.com/databricks/megablocks) â€” Databricks çš„é«˜æ•ˆ MoE è®­ç»ƒåº“ â­â­â­â­
- [DeepSpeed MoE](https://www.deepspeed.ai/tutorials/mixture-of-experts/) â€” å¾®è½¯ DeepSpeed çš„ MoE è®­ç»ƒæ•™ç¨‹ â­â­â­â­
- [vLLM MoE æ¨ç†](https://docs.vllm.ai/) â€” æ”¯æŒ Mixtral/DeepSeek MoE æ¨ç†çš„æ¡†æ¶ â­â­â­â­
