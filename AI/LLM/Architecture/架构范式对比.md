---
tags: [LLM, Transformer, 架构设计, GPT, BERT, T5]
created: 2026-02-14
status: draft
---

# Decoder-Only vs Encoder-Decoder 架构范式对比

在 Transformer 发展史中，出现了多种架构范式。从早期的 Encoder-Decoder 到后来统治 LLM 领域的 Decoder-Only，每种范式都有其独特的设计理念和适用场景。本文将全面对比这些架构范式，解释为什么 GPT 路线最终胜出。

## 四大架构范式

### 1. Encoder-Only（BERT 系列）

专注于理解和表征学习，双向注意力。

```python
class EncoderOnlyTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers, max_seq_len):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(max_seq_len, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # 分类头
        self.classifier = nn.Linear(d_model, 2)  # 二分类示例
    
    def forward(self, input_ids, attention_mask=None):
        seq_len = input_ids.size(1)
        pos_ids = torch.arange(seq_len, device=input_ids.device)
        
        # 双向注意力 - 能看到整个序列
        x = self.embedding(input_ids) + self.pos_embedding(pos_ids)
        encoded = self.encoder(x, src_key_padding_mask=~attention_mask)
        
        # [CLS] token 或池化用于下游任务
        return self.classifier(encoded[:, 0])  # 取第一个位置
```

**特点**：
- **双向注意力**：每个 token 能看到前后全部上下文
- **并行训练**：所有位置同时计算
- **理解导向**：特别适合分类、问答等理解任务

### 2. Encoder-Decoder（T5、BART）

经典的序列到序列架构。

```python
class EncoderDecoderTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        
        # Encoder：双向注意力
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # Decoder：因果注意力 + 交叉注意力
        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, batch_first=True)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)
        
        self.output_proj = nn.Linear(d_model, vocab_size)
    
    def forward(self, src_ids, tgt_ids):
        # Encoder 处理输入
        src_emb = self.embedding(src_ids)
        memory = self.encoder(src_emb)
        
        # Decoder 生成输出
        tgt_emb = self.embedding(tgt_ids)
        # 因果掩码 - decoder 只能看到之前的token
        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_ids.size(1))
        
        decoded = self.decoder(
            tgt_emb, 
            memory, 
            tgt_mask=tgt_mask  # 因果掩码
        )
        
        return self.output_proj(decoded)
```

**注意力模式**：
- **Encoder 自注意力**：双向，理解输入语义
- **Decoder 自注意力**：单向因果，保证生成顺序
- **交叉注意力**：Decoder 关注 Encoder 输出

### 3. Decoder-Only（GPT 系列）

纯生成式架构，统一的语言建模目标。

```python
class DecoderOnlyTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers, max_seq_len):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(max_seq_len, d_model)
        
        # 所有层都是相同的 decoder layer
        self.layers = nn.ModuleList([
            DecoderBlock(d_model, nhead) for _ in range(num_layers)
        ])
        
        self.ln_f = nn.LayerNorm(d_model)
        self.output_proj = nn.Linear(d_model, vocab_size)
    
    def forward(self, input_ids, past_key_values=None):
        seq_len = input_ids.size(1)
        pos_ids = torch.arange(seq_len, device=input_ids.device)
        
        # 因果掩码 - 只能看到之前的token
        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        
        x = self.embedding(input_ids) + self.pos_embedding(pos_ids)
        
        for layer in self.layers:
            x = layer(x, causal_mask, past_key_values)
        
        x = self.ln_f(x)
        logits = self.output_proj(x)
        
        return logits

class DecoderBlock(nn.Module):
    def __init__(self, d_model, nhead):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)
        self.ln2 = nn.LayerNorm(d_model)
        self.mlp = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model)
        )
    
    def forward(self, x, causal_mask, past_key_values=None):
        # Pre-Norm + Causal Self-Attention
        normed = self.ln1(x)
        attn_out, _ = self.attn(
            normed, normed, normed,
            attn_mask=causal_mask,
            need_weights=False
        )
        x = x + attn_out
        
        # Pre-Norm + MLP
        x = x + self.mlp(self.ln2(x))
        return x
```

### 4. Prefix LM（PaLM、GLM）

混合双向和单向注意力的方案。

```python
def create_prefix_mask(seq_len, prefix_len):
    """
    创建 Prefix LM 的注意力掩码
    prefix 部分双向，suffix 部分因果
    """
    mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)
    
    # Prefix 部分：双向注意力
    mask[:prefix_len, :prefix_len] = False  # 可以互相看到
    
    # Suffix 部分：因果注意力
    for i in range(prefix_len, seq_len):
        mask[i, :i+1] = False  # 只能看到前面的（包括整个prefix）
        mask[i, i+1:] = True   # 不能看到后面的
    
    return mask

# 使用示例
seq_len, prefix_len = 10, 4
mask = create_prefix_mask(seq_len, prefix_len)
print("Prefix LM Mask:")
print(mask.int())
# 输出：
# 0 0 0 0 1 1 1 1 1 1  <- position 0 can see prefix
# 0 0 0 0 1 1 1 1 1 1  <- position 1 can see prefix  
# 0 0 0 0 1 1 1 1 1 1  <- position 2 can see prefix
# 0 0 0 0 1 1 1 1 1 1  <- position 3 can see prefix
# 0 0 0 0 0 1 1 1 1 1  <- position 4 causal
# 0 0 0 0 0 0 1 1 1 1  <- position 5 causal
# ...
```

## 注意力掩码对比

不同架构的注意力模式差异巨大：

| 架构 | 注意力模式 | 训练并行性 | 生成方式 |
|------|-----------|------------|----------|
| **Encoder-Only** | 全双向 | 完全并行 | 不直接生成 |
| **Encoder-Decoder** | Encoder双向 + Decoder因果 | 部分并行 | 自回归 |
| **Decoder-Only** | 全因果 | 完全并行 | 自回归 |
| **Prefix LM** | 混合 | 完全并行 | 自回归 |

### 可视化注意力模式

```python
import matplotlib.pyplot as plt
import numpy as np

def visualize_attention_patterns():
    seq_len = 8
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 1. Encoder-Only (BERT)
    encoder_mask = np.zeros((seq_len, seq_len))  # 全部可见
    axes[0,0].imshow(encoder_mask, cmap='RdBu', vmin=-1, vmax=1)
    axes[0,0].set_title('Encoder-Only\n(BERT)')
    
    # 2. Decoder-Only (GPT) 
    decoder_mask = np.triu(np.ones((seq_len, seq_len)), k=1)  # 上三角
    axes[0,1].imshow(decoder_mask, cmap='RdBu', vmin=-1, vmax=1)
    axes[0,1].set_title('Decoder-Only\n(GPT)')
    
    # 3. Encoder-Decoder (T5) - 只显示decoder部分
    encdec_mask = np.triu(np.ones((seq_len, seq_len)), k=1)
    axes[1,0].imshow(encdec_mask, cmap='RdBu', vmin=-1, vmax=1)
    axes[1,0].set_title('Encoder-Decoder\n(Decoder部分)')
    
    # 4. Prefix LM
    prefix_len = 3
    prefix_mask = np.triu(np.ones((seq_len, seq_len)), k=1)
    prefix_mask[:prefix_len, :prefix_len] = 0  # prefix双向
    axes[1,1].imshow(prefix_mask, cmap='RdBu', vmin=-1, vmax=1)
    axes[1,1].set_title('Prefix LM\n(prefix=3)')
    
    for ax in axes.flat:
        ax.set_xlabel('Key Position')
        ax.set_ylabel('Query Position')
    
    plt.tight_layout()
    plt.show()

# 运行可视化
visualize_attention_patterns()
```

## 为什么 GPT 路线胜出？

### 1. Scaling Laws 优势

**统一目标函数**：Decoder-Only 用单一的语言建模损失：

$$\mathcal{L} = -\sum_{i=1}^{T} \log P(x_i | x_{<i})$$

这种统一性带来了巨大优势：

```python
# GPT 的简洁训练循环
def train_step(model, batch):
    input_ids = batch['input_ids']  # [batch, seq_len]
    
    # 输入是 x[:-1]，目标是 x[1:]
    inputs = input_ids[:, :-1]
    targets = input_ids[:, 1:]
    
    logits = model(inputs)  # [batch, seq_len-1, vocab]
    
    # 简单的交叉熵损失
    loss = F.cross_entropy(
        logits.reshape(-1, logits.size(-1)), 
        targets.reshape(-1),
        ignore_index=-1
    )
    
    return loss
```

**对比 Encoder-Decoder 的复杂性**：
- 需要设计输入/输出格式
- 多个损失函数（重构、分类等）
- 数据预处理更复杂

### 2. 参数效率

Decoder-Only 避免了架构复杂性：

| 组件 | Encoder-Decoder | Decoder-Only |
|------|----------------|--------------|
| **Encoder 层** | N 层 | 0 层 |
| **Decoder 层** | N 层 | N 层 |
| **交叉注意力** | N × nhead | 0 |
| **总参数** | ~2N × d² | ~N × d² |

```python
# 参数量对比
def count_parameters():
    d_model, nhead, num_layers = 768, 12, 12
    vocab_size = 50000
    
    # Encoder-Decoder
    enc_dec_params = (
        2 * num_layers * (4 * d_model**2) +  # Self-attn + FFN × 2
        num_layers * (d_model**2) +          # Cross-attn
        2 * vocab_size * d_model             # Embedding × 2
    )
    
    # Decoder-Only  
    decoder_params = (
        num_layers * (4 * d_model**2) +      # Self-attn + FFN
        vocab_size * d_model                 # Embedding
    )
    
    print(f"Encoder-Decoder: {enc_dec_params:,} 参数")
    print(f"Decoder-Only: {decoder_params:,} 参数")
    print(f"节省: {(1 - decoder_params/enc_dec_params)*100:.1f}%")

count_parameters()
# Encoder-Decoder: 122,112,000 参数
# Decoder-Only: 76,416,000 参数  
# 节省: 37.4%
```

### 3. 涌现能力的扩展性

**Few-shot Learning**：GPT-3 展示了 Decoder-Only 的涌现能力：

```python
# In-Context Learning 示例
prompt = """
# 翻译任务示例
English: Hello, how are you?
Chinese: 你好，你怎么样？

English: The weather is nice today.
Chinese: 今天天气很好。

English: I love machine learning.
Chinese:"""

# GPT 能直接理解任务并执行，无需微调
response = model.generate(prompt)
# 输出：我喜欢机器学习。
```

**任务泛化**：单一模型处理多种任务：
- 文本生成
- 问答
- 翻译  
- 代码生成
- 推理

### 4. 工程优势

**简化架构**：
```python
# Decoder-Only 的简洁性
class SimpleGPT(nn.Module):
    def __init__(self, vocab_size, d_model, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.layers = nn.ModuleList([
            DecoderBlock(d_model) for _ in range(num_layers)
        ])
        self.output = nn.Linear(d_model, vocab_size)
    
    def forward(self, x):
        x = self.embedding(x)
        for layer in self.layers:
            x = layer(x)
        return self.output(x)

# 相同功能的 Encoder-Decoder 需要更多代码
```

**推理效率**：
- **KV Cache**：只需缓存一个方向
- **并行度**：无交叉注意力瓶颈
- **内存占用**：架构更简单

## 代表模型对比

### Encoder-Only 代表

| 模型 | 参数量 | 特点 | 适用任务 |
|------|--------|------|----------|
| **BERT-Base** | 110M | 双向理解 | 分类、问答、NER |
| **RoBERTa** | 125M | 优化训练策略 | 理解任务 |
| **DeBERTa** | 134M | 解耦注意力 | GLUE 榜首 |

### Encoder-Decoder 代表

| 模型 | 参数量 | 特点 | 适用任务 |
|------|--------|------|----------|
| **T5-Base** | 220M | Text-to-Text | 序列转换 |
| **BART** | 140M | 去噪预训练 | 摘要、对话 |
| **UL2** | 20B | 统一语言学习 | 多任务 |

### Decoder-Only 代表

| 模型 | 参数量 | 特点 | 适用任务 |
|------|--------|------|----------|
| **GPT-3** | 175B | Few-shot Learning | 通用文本 |
| **PaLM** | 540B | 推理能力 | 复杂推理 |
| **GPT-4** | ~1.7T | 多模态 | AGI 方向 |
| **LLaMA** | 7B-65B | 开源高效 | 研究/应用 |

## 现代趋势与选择建议

### 选择指南

| 场景 | 推荐架构 | 理由 |
|------|----------|------|
| **理解任务**（分类、抽取） | Encoder-Only | 双向理解，效率高 |
| **序列转换**（翻译、摘要） | Encoder-Decoder | 明确输入输出分离 |
| **通用 AI**（对话、推理） | Decoder-Only | 统一范式，可扩展 |
| **研究原型** | Decoder-Only | 社区支持，资源丰富 |

### 未来方向

1. **Decoder-Only 继续主导**：GPT-4、Claude 等证明了路径正确性
2. **架构创新**：MoE、RoPE、ALiBi 等改进
3. **多模态融合**：视觉、音频统一到 Decoder-Only
4. **效率优化**：[[Mamba]]、[[RetNet]] 等替代方案

## 面试常见问题

### Q1：为什么 Decoder-Only 能够统治 LLM 领域？

**答案**：
1. **统一目标**：单一的语言建模损失，简化训练
2. **参数效率**：避免 Encoder 和交叉注意力的冗余
3. **扩展性**：Scaling Laws 表现更好，涌现能力更强
4. **工程简洁**：架构简单，易于优化和部署
5. **任务泛化**：In-Context Learning 能力，无需微调适应新任务

### Q2：Encoder-Decoder 在什么场景下仍然有优势？

**答案**：
1. **明确的输入输出分离**：机器翻译、文档摘要
2. **计算资源受限**：小模型场景下更高效
3. **特定领域优化**：针对性设计的任务（如代码生成的 CodeT5）
4. **可解释性要求**：Encoder 表示可以单独分析
5. **混合任务**：同时需要理解和生成的复杂场景

### Q3：不同架构的注意力掩码是如何影响模型能力的？

**答案**：
**双向注意力（BERT）**：
- 优势：完整上下文理解，适合理解任务
- 劣势：无法直接生成，需要额外解码头

**单向因果注意力（GPT）**：
- 优势：自然生成能力，统一训练目标
- 劣势：理解任务可能次优（但大模型下差距很小）

**混合模式（T5）**：
- 优势：结合两者优点
- 劣势：架构复杂，参数效率低

### Q4：如何选择合适的 Transformer 架构？

**答案**：
**考虑因素**：
1. **任务类型**：生成 vs 理解 vs 转换
2. **数据规模**：大数据倾向 Decoder-Only
3. **计算预算**：小模型可考虑 Encoder-Decoder
4. **部署要求**：推理效率、内存限制
5. **开发周期**：Decoder-Only 生态更成熟

**决策树**：
```
if 任务 == "纯理解" and 模型规模 < 1B:
    return "Encoder-Only"
elif 任务 == "序列转换" and 输入输出差异大:
    return "Encoder-Decoder"  
else:
    return "Decoder-Only"  # 默认选择
```

### Q5：Pre-training 目标如何影响架构选择？

**答案**：
**BERT 式掩码语言模型**：
- 需要双向上下文，必须用 Encoder
- 适合理解任务，但生成能力有限

**GPT 式自回归生成**：
- 因果约束，适合 Decoder-Only
- 统一各种任务为生成问题

**T5 式 Span 重构**：
- 需要 Encoder-Decoder 处理损坏和重构
- 在理解和生成间取平衡

**现代趋势**：
统一到自回归生成，因为：
1. 简化了预训练流程
2. 下游任务适配更灵活
3. 扩展性更好

---

**相关链接**：
- [[GPT 系列解析]]
- [[BERT 架构深度]]
- [[T5 统一框架]]
- [[Attention Mechanism]]
- [[Layer Normalization]]
- [[Scaling Laws]]