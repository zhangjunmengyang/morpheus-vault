---
title: "预训练数据工程"
brief: "LLM 预训练数据工程完整体系：去重（MinHash/SimHash/SemDedupe）、质量过滤（Gopher规则/KenLM困惑度/分类器）、域配比优化（DoReMi/D4/DOREMI-Pile）、合成数据飞轮；核心认知：预训练=数据质量映射，Phi-1/FineWeb/Llama-3 的竞争壁垒均在数据工程。"
tags: [LLM, Pretraining, DataEngineering, Deduplication, QualityFilter, DomainMix]
created: 2026-02-27
status: permanent
rating: ★★★★
sources:
  - "Penedo et al. *FineWeb: Decanting the Web for the Finest Text Data at Scale* arXiv:2406.17557 (NeurIPS 2024 Datasets Track)"
  - "Lee et al. *Deduplicating Training Data Makes Language Models Better* arXiv:2107.06499 (ACL 2022)"
  - "Xie et al. *DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining* arXiv:2305.10429 (NeurIPS 2023)"
  - "Tirumala et al. *D4: Improving LLM Pretraining via Document De-Duplication and Diversification* arXiv:2308.12284 (NeurIPS 2023)"
  - "Meta AI. *Llama 3 Technical Report* arXiv:2407.21783 (2024)"
related:
  - "[[AI/3-LLM/Pretraining/预训练原理]]"
  - "[[AI/3-LLM/Pretraining/小规模训练手册]]"
  - "[[AI/3-LLM/SFT/训练数据构建]]"
---

# 预训练数据工程

> **核心认知**：预训练 = 数据质量的映射。模型能力上限由数据决定，训练方法只是趋近这个上限的效率手段。

---

## 一、为什么数据工程是独立的知识域

预训练目标函数、Scaling Law、并行策略已经成熟标准化。真正的竞争壁垒在**数据工程**：

- **Phi-1（2023）**：7B 参数接近 CodeLlama 34B 性能 → 原因：高质量合成"教科书级"代码数据
- **Llama 3（2024）**：从 2T 到 15T token，同时大幅提升过滤强度 → 数量×质量双重提升
- **FineWeb（2024，NeurIPS Datasets Track）**：70+ 消融实验证明，相同 CommonCrawl 来源，精心设计的过滤 pipeline 大幅超越 C4 和 RefinedWeb

**核心矛盾**：Web 数据天然充满噪声，但预训练需要万亿 token 规模。工程问题是：如何在保证规模的同时提升信息密度？

---

## 二、去重（Deduplication）

### 2.1 为什么去重是第一优先级

Lee et al.（ACL 2022）"Deduplicating Training Data Makes Language Models Better" 实证：
- 去重后困惑度显著下降
- 减少记忆化（memorization），提升泛化
- 防止测试集污染——重复内容可能包含 benchmark 答案

**规模问题**：CommonCrawl 文档级重复率高达 30-60%，近似重复更高。

### 2.2 三类去重方案

#### 精确匹配（Exact Dedup）
- **文档级**：MD5/SHA256 哈希，O(n) 时间，只能发现完全相同文档
- **子串级（Suffix Array）**：将所有文档拼接，构建后缀数组，找所有长度 > k 的重复子串删除
  - 优点：能发现段落/句子级精确重复
  - 缺点：内存 O(n)，万亿 token 规模需要分桶流式工程

#### 近似匹配（Near-dedup，主流）

**MinHash + LSH**（最常用，Llama 3 / FineWeb 均采用）：
```
1. 将文档 tokenize 为 n-gram（通常 n=5）
2. 计算 MinHash 签名（128-256 个 hash 函数）
3. 将签名分 b 个 band，每 band r 行（b×r = 总签名数）
4. 同一 band 内哈希相同 → 候选对（LSH Blocking）
5. 候选对计算精确 Jaccard → 超过阈值（0.7-0.9）判为重复
```
- Llama 3：文档级 MinHash（阈值 0.8）+ 行级精确去重

**SimHash**：文档映射为 64-bit 指纹，汉明距离 < 3 视为近似重复。快，精度略低于 MinHash；Google 早期网页去重采用。

#### 语义级去重（新兴）
- 用小模型（MiniLM 等）编码 embedding，余弦相似度 > 0.95 的聚类去重
- 能发现语义相同但表面不同的重复（改写/翻译后重复）
- 计算成本高，Llama 3 用于高风险语料子集

### 2.3 去重粒度层次

| 粒度 | 方法 | 主要目标 |
|------|------|---------|
| 数据集间 | 哈希/MinHash 跨语料 | 防同一文档在多数据集重复计数 |
| 文档级 | MinHash/SimHash | 主力方案 |
| 段落/句子级 | 后缀数组/行级精确匹配 | 处理"大部分相同"的文档 |
| n-gram 级 | 滑动窗口统计 | 过滤模板、样板文本（法律/广告） |

**FineWeb 发现（NeurIPS 2024）**：
- URL 级去重优先于内容级（同一 URL 多次爬取）
- 行级重复率是单文档质量的强信号
- 15T token 规模，去重单独贡献 +2-4% 下游任务

---

## 三、质量过滤（Quality Filtering）

### 3.1 两层过滤架构

```
原始爬取数据
    ↓
[Stage 1] 启发式过滤（速度优先，规则引擎）
    ↓ 保留约 60-80%
[Stage 2] 模型基过滤（质量优先，模型推理）
    ↓ 保留约 20-40%
高质量训练集
```

### 3.2 启发式过滤规则体系

**Llama 3 使用的规则**：
- **文档长度**：< 100 词或 > 100K 词丢弃
- **重复 n-gram 覆盖率（Duplicate n-gram Coverage Ratio）**：文档内 n-gram 自重复率高 → 系统日志/模板页 → 丢弃
- **脏词计数（Dirty Word Count）**：NSFW 内容过滤
- **Token 分布 KL 散度**：与语料整体 token 分布做 KL 散度，离群文档（罕见字符组合）丢弃
- **符号/数字比例**：特殊字符 > 30% 或数字 > 50% → 丢弃

**C4 的规则集（T5 训练数据，典型案例）**：
- 去掉不以标点符号结尾的行
- 去掉包含 175 个"bad words"词汇的页面
- 去掉包含"javascript"字符串的行（说明是脚本注入）
- 去掉引用政策文件的行（privacy policy / terms of service）
- 去掉 < 5 句话的文档
- 保留英语（语言过滤）

### 3.3 模型基过滤

#### 困惑度过滤（Perplexity Filter）
- 用 KenLM 在高质量语料（Wikipedia + 书籍）训练，计算每文档困惑度
- 高困惑度 = 远离高质量分布 → 丢弃
- **局限**：对代码、医学、法律等专业领域不公平——领域 token 分布天然不同

#### 分类器过滤（Classifier-based Filter）

**参考模型分类器（FineWeb 路线）**：
- 正样本：Wikipedia、StackExchange 高赞、书籍
- 负样本：随机 Web 文档
- 训练轻量分类器（fastText），规模化打分

**FineWeb-Edu（2024）**——LLM 标注 + 分类器扩展：
```
1. 用 LLaMA-3-70B 对 500K CommonCrawl 文档按"教育价值"打 0-5 分
2. 基于这 500K 标注训 fastText 分类器（可扩展到万亿 token）
3. 保留评分 ≥ 3 的文档 → 15T 中约 1.3T token（FineWeb-Edu）
4. FineWeb-Edu 在 MMLU/ARC 等任务大幅超越全量 FineWeb
```
这个范式的本质是：用强模型（昂贵）标注少量数据 → 训轻量分类器（便宜）→ 大规模推理。

**质量分类器的陷阱**（arXiv:2510.00866，2025）：
- "Data-Quality Illusion"：分类器高分 ≠ 训练效果好
- 分类器偏差（特定写作风格偏好）会被放大进训练数据
- 建议：分类器过滤 + 消融测试验证，不盲信分数

---

## 四、Domain Mix（数据配比策略）

### 4.1 为什么配比至关重要

Gopher（DeepMind 2021）：将书籍比例从 0% 提升到 22%，模型性能显著提升。
不同域的信息密度差异极大：
- 1 token 学术论文 >> 1 token 网页（知识密度差）
- 代码对逻辑推理能力的激活效果远超自然语言

### 4.2 主流大模型配比参考

**LLaMA 1（2023，1.4T token）**：
```
CommonCrawl: 67%（CCNet 过滤）
C4: 15%
GitHub: 4.5%
Wikipedia: 4.5%
Books: 4.5%（Project Gutenberg + Books3）
ArXiv: 2.5%
StackExchange: 2%
```

**Llama 3（2024，15.6T token）**：
- 代码/推理相关 ~ 8-10%（相比 Llama 1 大幅提升）
- 关键变化：每类来源都经历了更严格的过滤；最后阶段（Annealing，约 40B token）大幅提升高质量数据比例

**规律**：代码数据对 coding + reasoning 的贡献 >> 体积比例，这是 Llama 3 和 DeepSeek 系列都选择提升代码比例的原因。

### 4.3 DoReMi：自动化配比优化（NeurIPS 2023，arXiv:2305.10429）

**问题**：手工调配比 = 凭经验 + 大量 ablation，成本极高且依赖下游任务知识

**核心算法（Group DRO + Proxy Model）**：
```
1. 用基准权重训练小代理模型（280M参数）
2. 在代理模型上跑 Group DRO：
   - 每个 domain 是一个"group"
   - 目标：最小化最坏域的 excess loss
     excess_loss[d] = current_loss[d] - reference_loss[d]
   - DRO 自动增大"学得不够好"的域权重
3. 小模型学到的域权重 → 直接用于训练 8B 大模型
```

**直觉**：DRO 让训练均衡覆盖所有域，而不是让数据量大的域（网页）主导学习。

**结果**：
- 280M proxy 找到的权重，8B 大模型有效（跨规模迁移性强）
- 无下游任务知识仍能匹配下游调优权重性能
- 用更少 token 达到相同性能

**DoReMi 的局限**：
- 两阶段 overhead（先训 proxy）
- domain 粒度定义敏感（粗粒度好，细粒度可能过拟合）
- 未解决域内质量差异

### 4.4 D4：去重 + 多样性联合（NeurIPS 2023，arXiv:2308.12284）

**核心 insight**：MinHash 去重只减冗余，不保证多样性。

**D4 算法**：
1. MinHash 去重（减冗余）
2. Density Sampling（增多样性）：将文档 embed → 在 embedding 空间按密度逆比例采样（稀疏区域多采）

**结果**：相同 token 预算，D4 显著优于纯去重。
**本质**：多样性（覆盖更多概念空间）和去重（减少冗余）是不同但互补的目标，需要同时优化。

---

## 五、数据飞轮与合成数据

### 5.1 合成数据预训练——Phi 系列的极端案例

**Phi-1（2023）**：
- GPT-3.5 生成"教科书级" Python 练习 + 解答，仅 7B token
- 1.3B 参数，HumanEval 67.7% 超越 CodeLLama 34B（53.7%）

**Phi-2（2023）**：
- 合成"教科书"自然语言 + 代码混合，2.7B 参数
- 多数 benchmark 超越 7B 级别模型

**关键 insight**：
- 合成数据在**技能获取**（coding / math reasoning）效果极好——技能转移不依赖原始文档多样性
- 但用于**知识注入**效果有限——合成知识受限于 teacher model，循环放大 model bias
- 合成数据 → 执行验证（运行代码 / 数学验算）= 过滤 → 大幅提升质量

### 5.2 课程学习（Curriculum / Annealing）

**Llama 3 Annealing 阶段**：最后约 40B token，大幅提升高质量数据比例（代码、数学、合成推理）。
**规律**：训练后期的数据对最终能力贡献不成比例地大（"Long Tail" token 效应）。

**Minerva（Google）**：先训通用语料 → 后期加大数学/科学比例（2:8 阶段切换），专攻科学推理。

---

## 六、工程实现：大规模 Pipeline

### 6.1 HuggingFace datatrove（FineWeb 背后工具链）

```python
# FineWeb pipeline 示意（datatrove 框架）
pipeline = [
    WarcReader(data_folder),           # 读取 CommonCrawl WARC 文件
    URLFilter(exclusion_writer),        # URL 黑名单过滤
    Trafilatura(favour_recall=True),    # HTML → 纯文本提取
    LanguageFilter(languages=["en"]),   # 语言过滤（fastText LID）
    GopherQualityFilter(),              # Gopher 启发式规则
    GopherRepetitionFilter(),           # 重复内容规则
    FineWebQualityFilter(),             # FineWeb 专有规则
    MinhashDeduplicate(threshold=0.7),  # MinHash 近似去重
    TokensCounter(),                    # 统计 token 数
    DocumentJsonlWriter(output)         # 输出 JSONL
]
```

### 6.2 关键工程挑战

**规模化去重**：
- 1T token ≈ 1B 文档，全量两两比较不可行
- LSH Blocking 降为候选对 → 精确 Jaccard 验证
- 分布式：Spark/Ray，每 batch ~100M 文档

**流式 pipeline**：
- 避免全量落盘再处理（I/O 瓶颈）
- 每个过滤规则是 transform，流式过滤直接进下一步

**数据版本控制与消融**：
- 不同过滤参数 = 不同版本数据集
- 记录每步过滤的文档保留率（debug 关键指标）
- FineWeb：70+ 消融实验，量化每个过滤步骤的边际贡献

---

## 七、面试核心问答

### Q1: 预训练数据工程的三大核心步骤？

去重（Deduplication）→ 质量过滤（Quality Filtering）→ 配比优化（Domain Mixing）。
去重通常先于质量过滤（先减冗余，再准确统计质量分布）。

### Q2: MinHash 去重如何调参？

- `num_perm` = 128-256（越大越准确）
- `threshold` = 0.7-0.85（越低越激进）
- `ngram_size` = 5 字符 n-gram（短文档鲁棒）
- band 数 b 和每 band 行数 r：b×r = num_perm；b 大 → 召回高、精度降

### Q3: 为什么不只用 perplexity 过滤？

① 域偏移：KenLM 训在 Wikipedia，代码/医学文档天然高困惑度 → 误删专业数据；② 计算成本高；③ 对 repetition 不敏感（重复内容困惑度可能很低）。实践：启发式规则先过 → perplexity / 分类器精过。

### Q4: DoReMi 的核心直觉是什么？

用小模型（280M proxy）在各 domain 跑 DRO 找到"最坏情况最优"的域权重：DRO 自动加大"现在学得不够好"的域权重，使学习均衡。这个权重跨规模有效（proxy 280M → main 8B），且不需要下游任务知识。

### Q5: 合成数据什么时候有效，什么时候无效？

**有效**：技能获取（coding、math reasoning）——技能转移不依赖原始文档多样性，合成的练习题 + 验证执行 = 高密度训练信号。
**无效/危险**：知识注入——合成知识受 teacher model 知识范围限制，循环放大 model bias，且难以验证事实正确性。

### Q6: 什么是 Annealing 阶段的作用？

预训练最后阶段（Llama 3：约 40B token）大幅提升高质量数据比例（代码、数学、合成推理）。Long Tail Token 效应：训练末尾的数据对最终能力贡献不成比例地大，因为此时模型已经有基础能力，高质量数据能精准提升"最后一公里"。

---

## 关键引用

| 论文 | 核心贡献 | 评分 |
|------|---------|------|
| Lee et al. ACL 2022, "Deduplicating Training Data" | 去重 → 更好模型，系统实证 | ★★★★ |
| Xie et al. NeurIPS 2023, DoReMi (2305.10429) | Group DRO 自动化域配比 | ★★★★★ |
| Yu et al. NeurIPS 2023, D4 (2308.12284) | 去重 + 多样性联合优化 | ★★★★ |
| Penedo et al. NeurIPS 2024, FineWeb (2406.17557) | 70+ 消融的 15T 数据集，方法论标杆 | ★★★★★ |
| Gunasekar et al. 2023, Phi-1 | 合成教科书数据，小模型超大模型 | ★★★★★ |
| Meta 2024, Llama 3 Tech Report | 工业级 15T pipeline 实践 | ★★★★★ |

---

## See Also

- [[AI/6-应用/Synthetic-Data/Synthetic-Data|合成数据（Synthetic Data）]] — 预训练数据工程的重要一环：Self-Play/Backtranslation/Persona-based 生成策略
- [[AI/3-LLM/Pretraining/预训练原理]] — 目标函数 / Scaling Law / 训练稳定性（与本文互补：原理 vs 工程）
- [[AI/3-LLM/Pretraining/小规模训练手册]] — 实验规模数据工程参考
- [[AI/3-LLM/SFT/训练数据构建]] — SFT 数据工程（对比：web-scale 无监督 vs curated pair 有监督，流程正交）
- [[AI/3-LLM/RL/Theory/GRPO-改进七维框架分析]] — 预训练数据质量决定 RL 微调上限；data flywheel（模型→生成→过滤→再训练）是 RL 与预训练的连接机制
- [[AI/3-LLM/Architecture/Mamba-SSM]] — 架构选择影响数据需求（SSM 对序列长度敏感，Transformer 对数据多样性敏感）

## 推荐阅读

1. **FineWeb**（arXiv:2406.17557）— 70+ 消融实验，最系统的 CommonCrawl 过滤研究，NeurIPS 2024 Best Datasets
2. **DoReMi**（arXiv:2305.10429）— 无需 held-out loss 的自动化域配比，NeurIPS 2023
3. **Llama 3 Tech Report**（arXiv:2407.21783）— 15T token 工业级 pipeline，最有价值的工程参考
4. **D4**（arXiv:2308.12284）— 去重+多样性联合优化的理论框架
- [[AI/3-LLM/Pretraining/LLM数据工程2026技术全景|LLM数据工程全景]] — 工业级全景：Dedup/质量过滤/DoReMi/D4/FineWeb-Edu/合成数据飞轮（本文的深度扩展版）
