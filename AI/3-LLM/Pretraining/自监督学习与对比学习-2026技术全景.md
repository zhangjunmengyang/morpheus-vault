---
title: 自监督学习与对比学习 — 2026 技术全景
brief: 自监督学习（SSL）四大范式全景：对比学习（SimCLR/MoCo/CLIP）、掩码建模（BERT/MAE/BEiT）、生成式（GPT/Diffusion）、预测式（BYOL/DINO）；含 InfoNCE 理论推导、视觉/NLP/多模态前沿，以及 2026 年 SSL 与 RLHF 融合趋势。面试武器版，含公式推导和高频题参考答案。
type: survey
domain: ai/llm/pretraining
date: 2026-02-22
updated: 2026-02-22
tags:
  - self-supervised-learning
  - contrastive-learning
  - pretraining
  - CLIP
  - MAE
  - DINO
  - survey
  - interview-prep
status: complete
sources:
  - Chen et al., SimCLR arXiv:2002.05709 (2020)
  - He et al., MoCo arXiv:1911.05722 (2019)
  - Radford et al., CLIP arXiv:2103.00020 (2021)
  - He et al., MAE arXiv:2111.06377 (2021)
  - Caron et al., DINO arXiv:2104.14294 (2021)
  - Devlin et al., BERT arXiv:1810.04805 (2018)
  - Grill et al., BYOL arXiv:2006.07733 (2020)
  - Zbontar et al., Barlow Twins arXiv:2103.03230 (2021)
  - Bardes et al., VICReg arXiv:2105.04906 (2021)
related:
  - "[[LLM-预训练与分布式训练-2026-全景|预训练与分布式训练 2026 全景]]"
  - "[[LLM-数据工程-2026-技术全景|数据工程 2026 全景]]"
  - "[[多模态大模型-2026-技术全景|多模态大模型 2026 全景]]"
  - "[[Transformer架构深度解析-2026技术全景|Transformer 架构深度解析]]"
  - "[[预训练原理|预训练原理]]"
---

# 自监督学习与对比学习 — 2026 技术全景

> **定位**：面试深度笔记 · 覆盖自监督学习四大范式 + 对比学习理论与实践 + 多模态前沿
> **风格**：公式推导 + 代码示例 + 面试高频题，拒绝泛泛而谈
> **最后更新**：2026-02

---

## 目录

1. [自监督学习范式总览](#一自监督学习范式总览)
2. [对比学习理论基础](#二对比学习理论基础)
3. [视觉对比学习进化](#三视觉对比学习进化)
4. [掩码图像建模（MIM）](#四掩码图像建模mim)
5. [视觉自监督 Transformer](#五视觉自监督-transformer)
6. [NLP 自监督学习](#六nlp-自监督学习)
7. [多模态对比学习](#七多模态对比学习)
8. [数据增强策略](#八数据增强策略)
9. [下游任务迁移](#九下游任务迁移)
10. [2026 前沿趋势](#十2026-前沿趋势)
11. [面试高频题精选](#十一面试高频题精选)
12. [参考文献](#十二参考文献)

---

## 一、自监督学习范式总览

### 1.1 为什么需要自监督学习

监督学习依赖大量人工标注数据——成本高、覆盖窄、泛化差。自监督学习（Self-Supervised Learning, SSL）从 **无标注数据** 中构造监督信号，让模型学习通用表征。

**核心思想**：将数据本身的结构/冗余作为监督信号，设计 pretext task 使模型在解决该任务时学到可迁移的特征。

截至 2026 年，SSL 已成为几乎所有 SOTA 模型的预训练范式：

| 范式 | 核心思路 | 代表方法 | 适用模态 |
|------|---------|---------|---------|
| **对比学习** | 拉近正样本对，推远负样本对 | SimCLR, MoCo, CLIP | CV, 多模态 |
| **掩码建模** | 遮住部分输入，预测被遮部分 | BERT, MAE, BEiT | NLP, CV |
| **生成式** | 建模数据分布，自回归/扩散生成 | GPT, DALL-E, Diffusion | NLP, CV |
| **预测式** | 预测输入某部分的属性/关系 | BYOL, DINO, VICReg | CV |

### 1.2 四大范式详解

#### 1.2.1 对比学习（Contrastive Learning）

```
正样本对 (x, x⁺): 同一样本的不同增强视图
负样本对 (x, x⁻): 不同样本

目标: sim(f(x), f(x⁺)) >> sim(f(x), f(x⁻))
```

**关键组件**：
- **数据增强**：构造正样本对（裁剪、颜色抖动、翻转等）
- **编码器**：提取特征表示
- **投影头**：将特征映射到对比空间
- **损失函数**：InfoNCE / NT-Xent

**优势**：学到的表征具有强判别性，下游任务迁移效果好
**劣势**：依赖大量负样本，训练成本高，对数据增强策略敏感

#### 1.2.2 掩码建模（Masked Modeling）

```
输入: The [MASK] sat on the [MASK]
目标: 预测 [MASK] → cat, mat

输入: 遮住 75% 的图像 patch
目标: 重建被遮住的像素/token
```

**NLP 掩码建模**：BERT 的 MLM（随机遮住 15% token）
**CV 掩码建模**：MAE（随机遮住 75% patch，像素级重建）

**优势**：不需要负样本，计算效率高（只处理可见 patch）
**劣势**：重建目标的选择影响表征质量（像素 vs. token vs. 特征）

#### 1.2.3 生成式自监督（Generative SSL）

```
自回归: p(x_t | x_{<t})  → GPT 系列
扩散:   p(x_{t-1} | x_t)  → DDPM, Stable Diffusion
VAE:    p(x | z) · p(z)    → VQ-VAE, DALL-E
```

通过建模数据的联合分布或条件分布来学习表征。GPT 系列的因果语言建模（CLM）是最成功的案例。

**优势**：天然适合生成任务，表征蕴含丰富的语义信息
**劣势**：自回归推理慢，表征可能偏向低层纹理

#### 1.2.4 预测式自监督（Predictive SSL）

不需要负样本，也不需要重建输入，而是通过 **预测一个视图来学习另一个视图的信息**。

```
BYOL:     online network 预测 target network 的输出
DINO:     student 匹配 teacher 的输出分布
VICReg:   直接约束表征的方差/不变性/协方差
```

**关键问题**：如何防止表征坍缩（collapse）？
- Stop-gradient（BYOL, SimSiam）
- EMA teacher（DINO, MoCo v3）
- 正则化约束（VICReg, Barlow Twins）

### 1.3 范式融合趋势

2024-2026 年的趋势是 **范式边界模糊化**：

| 方法 | 融合方式 |
|------|---------|
| iBOT | 掩码建模 + 自蒸馏（DINO 风格） |
| CoCa | 对比学习 + 生成式（captioning） |
| EVA-02 | 掩码建模 + CLIP 特征蒸馏 |
| DINOv2 | 自蒸馏 + iBOT 掩码建模 + 对比学习 |
| data2vec 2.0 | 跨模态统一掩码预测 |

**面试要点**：当被问"对比学习和掩码建模哪个更好"时，答案是 **取决于下游任务和数据规模**：
- 判别任务（分类、检测）：对比学习通常更好
- 稠密预测（分割、深度估计）：掩码建模更好
- 大规模预训练：两者融合（DINOv2）表现最佳

---

## 二、对比学习理论基础

### 2.1 InfoNCE 损失

InfoNCE（Noise Contrastive Estimation）由 van den Oord et al. (2018) 在 CPC 论文中提出，是对比学习最核心的损失函数。

#### 数学推导

给定 anchor $z_i$，正样本 $z_j$，以及 $N-1$ 个负样本 $\{z_k\}_{k \neq i}$：

$$\mathcal{L}_{\text{InfoNCE}} = -\log \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1}^{N} \exp(\text{sim}(z_i, z_k) / \tau)}$$

其中：
- $\text{sim}(u, v) = \frac{u^\top v}{\|u\| \cdot \|v\|}$ 是余弦相似度
- $\tau$ 是温度参数（temperature）
- 分母包含 1 个正样本 + $(N-1)$ 个负样本

#### InfoNCE 与互信息的关系

InfoNCE 是互信息 $I(X; Y)$ 的下界：

$$I(X; Y) \geq \log(N) - \mathcal{L}_{\text{InfoNCE}}$$

即 InfoNCE 越小，互信息下界越紧。当 $N \to \infty$，InfoNCE 趋近于真实互信息。

**直觉理解**：最小化 InfoNCE = 最大化正样本对之间的互信息 = 让编码器保留两个视图共有的信息（语义信息）而丢弃视图独有的信息（噪声/增强引入的变化）。

#### 代码实现

```python
import torch
import torch.nn.functional as F

def info_nce_loss(z_i: torch.Tensor, z_j: torch.Tensor, temperature: float = 0.07):
    """
    InfoNCE loss for contrastive learning.
    
    Args:
        z_i: (B, D) 第一个视图的表征
        z_j: (B, D) 第二个视图的表征（正样本对）
        temperature: 温度参数
    
    Returns:
        loss: scalar
    """
    batch_size = z_i.shape[0]
    
    # L2 归一化
    z_i = F.normalize(z_i, dim=1)
    z_j = F.normalize(z_j, dim=1)
    
    # 拼接所有表征: (2B, D)
    z = torch.cat([z_i, z_j], dim=0)
    
    # 计算相似度矩阵: (2B, 2B)
    sim_matrix = torch.mm(z, z.T) / temperature
    
    # 正样本对的 mask
    # 对于 z_i[k]，正样本是 z_j[k]（索引 k + B）
    # 对于 z_j[k]，正样本是 z_i[k]（索引 k）
    labels = torch.cat([
        torch.arange(batch_size, 2 * batch_size),
        torch.arange(batch_size)
    ], dim=0).to(z_i.device)
    
    # 排除自身相似度（对角线置为 -inf）
    mask = torch.eye(2 * batch_size, dtype=torch.bool, device=z_i.device)
    sim_matrix.masked_fill_(mask, float('-inf'))
    
    # Cross-entropy loss
    loss = F.cross_entropy(sim_matrix, labels)
    return loss
```

### 2.2 NT-Xent（Normalized Temperature-Scaled Cross Entropy）

SimCLR 使用的损失函数，本质上是 InfoNCE 的对称版本：

$$\ell_{i,j} = -\log \frac{\exp(\text{sim}(z_i, z_j) / \tau)}{\sum_{k=1}^{2N} \mathbb{1}_{[k \neq i]} \exp(\text{sim}(z_i, z_k) / \tau)}$$

$$\mathcal{L}_{\text{NT-Xent}} = \frac{1}{2N} \sum_{k=1}^{N} [\ell_{2k-1, 2k} + \ell_{2k, 2k-1}]$$

与 InfoNCE 的区别：
1. **对称**：$i \to j$ 和 $j \to i$ 都计算
2. **排除自身**：分母中 $k \neq i$
3. **batch 内负样本**：$2(N-1)$ 个负样本来自同一 batch

### 2.3 对齐性（Alignment）与均匀性（Uniformity）

Wang & Isola (2020) 将对比学习的目标分解为两个独立的性质：

#### 对齐性（Alignment）

正样本对的表征应该 **接近**：

$$\mathcal{L}_{\text{align}} = \mathbb{E}_{(x, x^+) \sim p_{\text{pos}}} \left[ \|f(x) - f(x^+)\|_2^\alpha \right]$$

通常 $\alpha = 2$，即正样本对的欧氏距离的期望应该最小。

#### 均匀性（Uniformity）

所有表征应该在超球面上 **均匀分布**：

$$\mathcal{L}_{\text{uniform}} = \log \mathbb{E}_{(x, y) \sim p_{\text{data}}^2} \left[ e^{-t \|f(x) - f(y)\|_2^2} \right]$$

$t > 0$ 是一个标量参数（通常 $t = 2$）。

**直觉**：
- 只优化对齐性 → 所有表征坍缩到同一点（trivial solution）
- 只优化均匀性 → 正负样本不可分
- 两者平衡 → 好的表征空间

```python
def alignment_loss(z_i, z_j, alpha=2):
    """正样本对的对齐损失"""
    z_i = F.normalize(z_i, dim=1)
    z_j = F.normalize(z_j, dim=1)
    return (z_i - z_j).norm(dim=1).pow(alpha).mean()

def uniformity_loss(z, t=2):
    """表征均匀性损失"""
    z = F.normalize(z, dim=1)
    sq_pdist = torch.pdist(z, p=2).pow(2)
    return sq_pdist.mul(-t).exp().mean().log()

# 联合优化
loss = alignment_loss(z_i, z_j) + lambda_u * uniformity_loss(torch.cat([z_i, z_j]))
```

### 2.4 温度参数 τ 的深入分析

温度参数 $\tau$ 是对比学习中最关键的超参数之一。

#### 数学分析

考虑 softmax 中的温度：

$$p_{ij} = \frac{\exp(s_{ij} / \tau)}{\sum_k \exp(s_{ik} / \tau)}$$

- **$\tau \to 0$**：分布趋于 one-hot，只关注最相似的负样本（hard negative）
- **$\tau \to \infty$**：分布趋于均匀，所有负样本同等对待
- **适中的 $\tau$**：平衡 hard 和 easy negatives

#### 梯度分析

对 $\tau$ 求偏导：

$$\frac{\partial \mathcal{L}}{\partial s_{ij}} = \frac{1}{\tau} \left( p_{ij} - \mathbb{1}_{[j = j^+]} \right)$$

小 $\tau$：梯度幅度 $\propto 1/\tau$，对 hard negatives 贡献更大的梯度
大 $\tau$：梯度更均匀，训练更稳定但收敛更慢

#### 经验值

| 方法 | 温度 $\tau$ | 说明 |
|------|------------|------|
| SimCLR | 0.1 | 经过大量调参 |
| MoCo v2 | 0.07 | 较低温度，更关注 hard negatives |
| CLIP | 可学习（初始化 0.07） | `log_temperature` 作为可训练参数 |
| SigLIP | N/A（改用 sigmoid） | 避免 softmax 的 temperature 敏感性 |

**面试要点**：温度太低会导致训练不稳定（梯度爆炸），温度太高会让正负样本难以区分。实践中 $\tau \in [0.05, 0.2]$ 是安全范围。

### 2.5 负采样策略

#### 负样本数量的影响

SimCLR 的实验表明，**更多负样本 = 更好的表征**（直到某个饱和点）：

| Batch Size | Neg per sample | Top-1 Acc (ImageNet linear) |
|-----------|---------------|---------------------------|
| 256       | 510           | 64.5%                     |
| 2048      | 4094          | 69.1%                     |
| 8192      | 16382         | 70.4%                     |

为什么更多负样本有帮助？
- InfoNCE 是互信息的下界，$N$ 越大，下界越紧
- 更多负样本提供更丰富的对比信号
- 降低了"假负样本"（同类但不同增强）的影响

#### Hard Negative Mining

不是所有负样本同等有用。**Hard negatives**（与 anchor 语义相近但确实不同的样本）提供更多信息：

```python
def hard_negative_mining(sim_matrix, labels, top_k=10):
    """选择 top-k hardest negatives"""
    batch_size = sim_matrix.shape[0]
    mask_pos = (labels.unsqueeze(0) == labels.unsqueeze(1))  # 正样本 mask
    
    # 将正样本的相似度置为 -inf，找最 hard 的负样本
    sim_neg = sim_matrix.clone()
    sim_neg[mask_pos] = float('-inf')
    
    # 每个 anchor 选 top-k 最相似的负样本
    hard_neg_indices = sim_neg.topk(top_k, dim=1).indices
    return hard_neg_indices
```

#### 解耦负采样：MoCo 的队列机制

MoCo 通过维护一个动量更新的 **队列（queue）** 解决负样本数量问题：

```python
class MoCoQueue:
    def __init__(self, dim=128, queue_size=65536):
        self.queue = F.normalize(torch.randn(dim, queue_size), dim=0)
        self.ptr = 0
    
    @torch.no_grad()
    def enqueue_dequeue(self, keys):
        """将新的 key 入队，旧的出队"""
        batch_size = keys.shape[0]
        self.queue[:, self.ptr:self.ptr + batch_size] = keys.T
        self.ptr = (self.ptr + batch_size) % self.queue.shape[1]
```

**核心洞察**：queue 使负样本数量与 batch size 解耦。即使 batch size = 256，也能有 65536 个负样本。

### 2.6 表征坍缩（Representation Collapse）

对比学习面临的核心理论问题：为什么模型不会学到 trivial solution（所有输入映射到同一点）？

#### 坍缩的类型

1. **完全坍缩**：$f(x) = c$ 对所有 $x$，常向量
2. **维度坍缩**：表征只使用少数维度，大部分维度为 0
3. **聚类坍缩**：所有样本映射到少数几个离散点

#### 防止坍缩的机制

| 机制 | 方法 | 原理 |
|------|------|------|
| 负样本排斥 | SimCLR, MoCo | 负样本之间的排斥力防止坍缩 |
| Stop-gradient | BYOL, SimSiam | 切断梯度传播的对称性 |
| EMA teacher | DINO, MoCo v3 | 慢更新的 teacher 提供稳定目标 |
| 特征正则化 | VICReg, Barlow Twins | 显式约束表征的方差/协方差 |
| Centering | DINO | 减去全局均值，防止某一维度主导 |
| Sharpening | DINO | teacher 输出用低温度 softmax |

#### SimSiam 的坍缩分析（Chen & He 2021）

SimSiam 没有负样本、没有动量编码器，仅靠 stop-gradient 工作。论文通过 **EM 算法** 分析：

```
# SimSiam 前向传播
z1, z2 = encoder(aug1(x)), encoder(aug2(x))
p1, p2 = predictor(z1), predictor(z2)

# 对称损失 + stop-gradient
loss = -0.5 * (cos_sim(p1, z2.detach()) + cos_sim(p2, z1.detach()))
```

**关键洞察**：stop-gradient 使得优化等价于交替执行 E 步和 M 步，predictor 近似了 expectation 操作。如果移除 stop-gradient → 立即坍缩。

---

## 三、视觉对比学习进化

### 3.1 SimCLR（2020）

**Simple Framework for Contrastive Learning of Visual Representations**

#### 架构

```
                    Data Augmentation
                    ┌─────────────────┐
Input x ──────────>│ aug_1    aug_2   │
                    └──┬──────────┬───┘
                       │          │
                    ┌──v──┐   ┌──v──┐
                    │  f  │   │  f  │   共享权重的 Encoder (ResNet-50)
                    └──┬──┘   └──┬──┘
                       │ h_i      │ h_j
                    ┌──v──┐   ┌──v──┐
                    │  g  │   │  g  │   Projection Head (MLP)
                    └──┬──┘   └──┬──┘
                       │ z_i      │ z_j
                       └─── NT-Xent ───┘
```

#### 核心发现

1. **数据增强的组合至关重要**：Random Crop + Color Jitter 效果最好；单独任一增强效果差
2. **投影头（Projection Head）提升巨大**：加 2 层 MLP 提升 ~10%。原因：投影头可以丢弃增强特定的信息
3. **更大的 batch size 更好**：4096 以上显著改善
4. **更长的训练更好**：800 epochs > 200 epochs

#### 关键代码

```python
class SimCLR(nn.Module):
    def __init__(self, backbone, proj_dim=128):
        super().__init__()
        self.encoder = backbone  # ResNet-50, 去掉最后的 FC
        self.projector = nn.Sequential(
            nn.Linear(2048, 2048),
            nn.ReLU(),
            nn.Linear(2048, proj_dim),
        )
    
    def forward(self, x1, x2):
        h1, h2 = self.encoder(x1), self.encoder(x2)
        z1, z2 = self.projector(h1), self.projector(h2)
        loss = nt_xent_loss(z1, z2, temperature=0.1)
        return loss
```

#### SimCLR v2 改进

- 更深的投影头（3 层 MLP）
- 用 SK（Selective Kernel）替换 ResNet 的 3x3 conv
- 引入知识蒸馏：大模型→小模型迁移
- 半监督学习：1% 标签 + SSL pretraining

### 3.2 MoCo 系列

#### MoCo v1（2020）

**Momentum Contrast for Unsupervised Visual Representation Learning**

核心创新：**动量对比**——解耦 dictionary size 和 batch size。

```
Query encoder (f_q):  正常梯度更新
Key encoder (f_k):    动量更新 θ_k ← m·θ_k + (1-m)·θ_q

Queue: 存储最近 K 个 mini-batch 的 key 表征
```

$$\theta_k \leftarrow m \cdot \theta_k + (1 - m) \cdot \theta_q, \quad m = 0.999$$

**为什么需要动量更新？**
如果 key encoder 和 query encoder 完全一样（共享权重），queue 中的 key 来自不同 epoch 的 encoder → 不一致 → 训练不稳定。动量更新使 key encoder 缓慢变化，保证 queue 中 key 的一致性。

```python
class MoCo(nn.Module):
    def __init__(self, encoder, dim=128, K=65536, m=0.999, T=0.07):
        super().__init__()
        self.K = K  # queue size
        self.m = m  # momentum
        self.T = T  # temperature
        
        self.encoder_q = encoder(num_classes=dim)
        self.encoder_k = encoder(num_classes=dim)
        
        # Key encoder 不需要梯度
        for param_k in self.encoder_k.parameters():
            param_k.requires_grad = False
        
        # 初始化 queue
        self.register_buffer("queue", torch.randn(dim, K))
        self.queue = F.normalize(self.queue, dim=0)
        self.register_buffer("queue_ptr", torch.zeros(1, dtype=torch.long))
    
    @torch.no_grad()
    def _momentum_update_key_encoder(self):
        for param_q, param_k in zip(self.encoder_q.parameters(), 
                                      self.encoder_k.parameters()):
            param_k.data = self.m * param_k.data + (1 - self.m) * param_q.data
    
    def forward(self, x_q, x_k):
        q = F.normalize(self.encoder_q(x_q), dim=1)  # (B, dim)
        
        with torch.no_grad():
            self._momentum_update_key_encoder()
            k = F.normalize(self.encoder_k(x_k), dim=1)  # (B, dim)
        
        # 正样本对: (B, 1)
        l_pos = torch.einsum('nc,nc->n', [q, k]).unsqueeze(-1)
        # 负样本对: (B, K)
        l_neg = torch.einsum('nc,ck->nk', [q, self.queue.clone().detach()])
        
        # logits: (B, 1+K)
        logits = torch.cat([l_pos, l_neg], dim=1) / self.T
        labels = torch.zeros(logits.shape[0], dtype=torch.long, device=q.device)
        
        loss = F.cross_entropy(logits, labels)
        self._dequeue_and_enqueue(k)
        return loss
```

#### MoCo v2（2020）

在 MoCo v1 基础上吸收 SimCLR 的发现：
1. **MLP 投影头**（替换 fc）→ +2.4%
2. **更强的数据增强**（加 blur + color jitter）→ +1.0%
3. **余弦学习率调度** → +0.6%

总计：MoCo v2 以 256 batch 达到 SimCLR 8192 batch 的效果。

#### MoCo v3（2021）

**An Empirical Study of Training Self-Supervised Vision Transformers**

MoCo v3 将 MoCo 框架适配到 ViT：
- 去掉 queue，回归 SimCLR 式 batch 内对比
- 保留动量 encoder
- 关键发现：ViT SSL 训练不稳定，**patch projection 层冻结** 可以解决

```
训练不稳定 → loss 突然跳升 → 梯度在 patch embed 层异常
解决方案: 随机初始化 patch embed，训练全程冻结
```

### 3.3 BYOL（2020）

**Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning**

**革命性贡献**：首次证明 **不需要负样本** 也能做好对比学习。

#### 架构

```
                         aug_1(x)          aug_2(x)
                            │                  │
                    ┌───────v────────┐  ┌──────v────────┐
                    │ Online Network │  │ Target Network │
                    │  f_θ → g_θ    │  │   f_ξ → g_ξ   │
                    │  → q_θ (pred) │  │                │
                    └───────┬───────┘  └──────┬─────────┘
                            │ p_θ             │ z_ξ
                            └──── MSE Loss ───┘
                            
Target: ξ ← τ·ξ + (1-τ)·θ   (EMA, τ=0.996→1.0)
```

$$\mathcal{L}_{\text{BYOL}} = \| \bar{q}_\theta(z_\theta^1) - \bar{z}_\xi^2 \|_2^2 + \| \bar{q}_\theta(z_\theta^2) - \bar{z}_\xi^1 \|_2^2$$

其中 $\bar{v} = v / \|v\|_2$。

#### 为什么不会坍缩？

这是 BYOL 最核心的问题。几种解释：

1. **BatchNorm 隐式负样本**（Richemond et al. 2020）：BN 层计算 batch 统计量，隐式地引入了样本间的对比。但后来证明没有 BN 也能工作。

2. **EMA + Predictor 的交互**（Tian et al. 2021）：
   - EMA target 提供缓慢变化的学习目标
   - Predictor 打破对称性
   - 两者的结合使得表征空间持续扩展而非收缩

3. **Spectral 分析**（Tian et al. 2021）：BYOL 隐式地最大化了表征的谱熵（spectral entropy），即鼓励表征使用更多维度。

### 3.4 SimSiam（2021）

**Exploring Simple Siamese Representation Learning**

SimSiam 是 BYOL 的极简化版本——**去掉动量编码器**，仅靠 stop-gradient 工作。

```python
# SimSiam 伪代码（极简）
z1, z2 = encoder(aug1(x)), encoder(aug2(x))
p1, p2 = predictor(z1), predictor(z2)

# stop-gradient 是唯一防坍缩的机制
loss = -0.5 * (F.cosine_similarity(p1, z2.detach()).mean()
              + F.cosine_similarity(p2, z1.detach()).mean())
```

#### 消融实验

| 配置 | Top-1 Acc | 坍缩？ |
|------|----------|--------|
| 完整 SimSiam | 68.1 | ❌ |
| 去掉 stop-gradient | 0.1 | ✅ 立即坍缩 |
| 去掉 predictor | 0.1 | ✅ 立即坍缩 |
| 去掉 stop-gradient + 加负样本 | 67.2 | ❌（回退到 SimCLR） |

### 3.5 Barlow Twins（2021）

**Barlow Twins: Self-Supervised Learning via Redundancy Reduction**

受 Barlow 冗余减少原则启发，通过约束 **交叉相关矩阵** 来学习表征。

#### 损失函数

$$\mathcal{L}_{\text{BT}} = \sum_i (1 - C_{ii})^2 + \lambda \sum_i \sum_{j \neq i} C_{ij}^2$$

其中 $C$ 是两个视图表征的交叉相关矩阵：

$$C_{ij} = \frac{\sum_b z_{b,i}^A z_{b,j}^B}{\sqrt{\sum_b (z_{b,i}^A)^2} \sqrt{\sum_b (z_{b,j}^B)^2}}$$

- 第一项（不变性项）：对角线趋向 1 → 同一维度在两个视图中应一致
- 第二项（冗余减少项）：非对角线趋向 0 → 不同维度应去相关

```python
def barlow_twins_loss(z_a, z_b, lambda_coeff=0.005):
    """
    z_a, z_b: (B, D) 两个视图的表征
    """
    batch_size = z_a.shape[0]
    
    # 标准化
    z_a = (z_a - z_a.mean(0)) / z_a.std(0)
    z_b = (z_b - z_b.mean(0)) / z_b.std(0)
    
    # 交叉相关矩阵: (D, D)
    c = (z_a.T @ z_b) / batch_size
    
    # 损失
    on_diag = (1 - c.diagonal()).pow(2).sum()
    off_diag = c.flatten()[:-1].view(c.shape[0] - 1, c.shape[1] + 1)[:, 1:].flatten().pow(2).sum()
    
    loss = on_diag + lambda_coeff * off_diag
    return loss
```

#### 优势

- 不需要大 batch（256 即可）
- 不需要 EMA/stop-gradient
- 表征维度可以很高（8192），效果更好（因为去相关约束是维度级别的）

### 3.6 VICReg（2022）

**Variance-Invariance-Covariance Regularization**

VICReg 将 Barlow Twins 的思想显式分解为三个独立的正则化项。

#### 三个目标

$$\mathcal{L} = \lambda \cdot s(Z^A, Z^B) + \mu \cdot [v(Z^A) + v(Z^B)] + \nu \cdot [c(Z^A) + c(Z^B)]$$

1. **不变性（Invariance）** $s$：正样本对的 MSE 距离
$$s(Z^A, Z^B) = \frac{1}{N} \sum_i \| z_i^A - z_i^B \|_2^2$$

2. **方差（Variance）** $v$：每个维度的标准差应大于阈值 $\gamma$（通常 $\gamma = 1$）
$$v(Z) = \frac{1}{D} \sum_j \max(0, \gamma - \sqrt{\text{Var}(z_{:,j}) + \epsilon})$$

3. **协方差（Covariance）** $c$：不同维度应去相关
$$c(Z) = \frac{1}{D} \sum_{i \neq j} [\text{Cov}(Z)]_{ij}^2$$

```python
def vicreg_loss(z_a, z_b, sim_coeff=25.0, std_coeff=25.0, cov_coeff=1.0):
    # Invariance
    sim_loss = F.mse_loss(z_a, z_b)
    
    # Variance
    std_z_a = torch.sqrt(z_a.var(dim=0) + 1e-4)
    std_z_b = torch.sqrt(z_b.var(dim=0) + 1e-4)
    var_loss = (torch.relu(1 - std_z_a).mean() + torch.relu(1 - std_z_b).mean()) / 2
    
    # Covariance
    z_a = z_a - z_a.mean(dim=0)
    z_b = z_b - z_b.mean(dim=0)
    cov_z_a = (z_a.T @ z_a) / (z_a.shape[0] - 1)
    cov_z_b = (z_b.T @ z_b) / (z_b.shape[0] - 1)
    cov_loss = (off_diagonal(cov_z_a).pow(2).sum() / z_a.shape[1] +
                off_diagonal(cov_z_b).pow(2).sum() / z_b.shape[1])
    
    return sim_coeff * sim_loss + std_coeff * var_loss + cov_coeff * cov_loss
```

### 3.7 方法对比总结

| 方法 | 年份 | 负样本 | EMA | Pred | 投影头维度 | Batch | IN-1K Linear |
|------|------|--------|-----|------|-----------|-------|-------------|
| SimCLR | 2020 | ✅ | ❌ | ❌ | 128 | 4096 | 69.3 |
| MoCo v2 | 2020 | ✅(queue) | ✅ | ❌ | 128 | 256 | 71.1 |
| BYOL | 2020 | ❌ | ✅ | ✅ | 256 | 4096 | 74.3 |
| SimSiam | 2021 | ❌ | ❌ | ✅ | 2048 | 256 | 71.3 |
| Barlow Twins | 2021 | ❌ | ❌ | ❌ | 8192 | 2048 | 73.2 |
| VICReg | 2022 | ❌ | ❌ | ❌ | 8192 | 2048 | 73.2 |
| MoCo v3 (ViT-B) | 2021 | ❌(batch) | ✅ | ✅ | 256 | 4096 | 76.7 |

**趋势**：从依赖大量负样本 → 不需要负样本 → 正则化替代负样本。

---

## 四、掩码图像建模（MIM）

### 4.1 MAE（2022）

**Masked Autoencoders Are Scalable Self-Supervised Learners**

He et al. 提出的 MAE 是 MIM 领域的里程碑——简单、高效、可扩展。

#### 核心设计

```
输入图像 → 分割为 patch → 随机遮住 75% → 只对可见 25% 编码 → 解码重建所有 patch

关键: encoder 只处理可见 patch → 计算量减少 75%!
```

$$\mathcal{L}_{\text{MAE}} = \frac{1}{|\mathcal{M}|} \sum_{i \in \mathcal{M}} \| x_i - \hat{x}_i \|_2^2$$

其中 $\mathcal{M}$ 是被遮住的 patch 集合，只在 masked patch 上计算 MSE 损失。

#### 架构细节

```python
class MAE(nn.Module):
    def __init__(self, encoder, decoder, mask_ratio=0.75):
        super().__init__()
        self.encoder = encoder      # ViT-Large
        self.decoder = decoder      # 轻量 ViT（8 层 512 维）
        self.mask_ratio = mask_ratio
    
    def random_masking(self, x, mask_ratio):
        """
        x: (B, N, D) patch embeddings
        返回: 可见 patch, mask, 还原索引
        """
        B, N, D = x.shape
        len_keep = int(N * (1 - mask_ratio))
        
        noise = torch.rand(B, N, device=x.device)
        ids_shuffle = torch.argsort(noise, dim=1)
        ids_restore = torch.argsort(ids_shuffle, dim=1)
        
        # 保留前 len_keep 个 patch
        ids_keep = ids_shuffle[:, :len_keep]
        x_visible = torch.gather(x, dim=1, 
                                  index=ids_keep.unsqueeze(-1).expand(-1, -1, D))
        
        # 生成 mask: 1 = masked, 0 = visible
        mask = torch.ones([B, N], device=x.device)
        mask[:, :len_keep] = 0
        mask = torch.gather(mask, dim=1, index=ids_restore)
        
        return x_visible, mask, ids_restore
    
    def forward(self, imgs):
        # Patch embedding
        patches = self.encoder.patch_embed(imgs)
        patches = patches + self.encoder.pos_embed[:, 1:, :]
        
        # Random masking
        visible, mask, ids_restore = self.random_masking(patches, self.mask_ratio)
        
        # Encoder: 只处理可见 patch
        latent = self.encoder.forward_encoder(visible)
        
        # Decoder: 填入 mask token, 重建所有 patch
        pred = self.decoder(latent, ids_restore)
        
        # 只在 masked patch 计算损失
        loss = (pred - target) ** 2
        loss = (loss * mask.unsqueeze(-1)).sum() / mask.sum()
        return loss
```

#### 关键发现

1. **75% 的 masking ratio 最优**：不同于 BERT 的 15%，图像冗余度远高于文本
2. **简单的像素级 MSE 重建**：无需 tokenizer（vs. BEiT），效果一样好
3. **轻量解码器**：8 层、512 维、仅用于预训练，fine-tune 时丢弃
4. **高效训练**：3x 加速（因为 encoder 只处理 25% patch）
5. **ViT-Huge 在 ImageNet 达到 87.8%**（fine-tune），超越当时所有监督方法

### 4.2 BEiT（2022）

**BERT Pre-Training of Image Transformers**

BEiT 是将 BERT 的 MLM 思想引入视觉的先驱。

#### 与 MAE 的关键区别：离散 visual token

```
步骤 1: 训练 dVAE (discrete VAE) 将图像 patch → visual token
步骤 2: 掩码建模，预测 masked patch 的 visual token（分类）而非像素（回归）
```

$$\mathcal{L}_{\text{BEiT}} = -\sum_{i \in \mathcal{M}} \log p(v_i | x_{\setminus \mathcal{M}})$$

其中 $v_i$ 是由 dVAE 编码的离散 token。

**为什么用 visual token 而不是像素？**
- 像素级重建偏向低层纹理信息
- Visual token 包含更高层的语义信息
- 分类损失比回归损失更容易优化

#### BEiT v2 & v3

- **BEiT v2**（2023）：用 VQ-KD（向量量化知识蒸馏）替代 dVAE，tokenizer 质量更好
- **BEiT-3**（2023）：统一的多模态预训练（图像/文本/图文对），Multiway Transformer

### 4.3 iBOT（2022）

**Image BERT Pre-Training with Online Tokenizer**

iBOT 融合了 MIM（BEiT 风格）和自蒸馏（DINO 风格）：

```
Student: 处理 masked image → 预测 masked patch 的 teacher 输出
Teacher: 处理 full image → EMA 更新

损失 = MIM loss (masked patch prediction) + [CLS] distillation loss (DINO 风格)
```

**核心创新**：Online Tokenizer——不需要预训练的 dVAE，teacher 网络本身就是 tokenizer。

$$\mathcal{L}_{\text{iBOT}} = \underbrace{\sum_{i \in \mathcal{M}} H(P_t^i, P_s^i)}_{\text{MIM loss}} + \underbrace{H(P_t^{\text{[CLS]}}, P_s^{\text{[CLS]}})}_{\text{Self-distillation loss}}$$

### 4.4 SdAE（2022）

**Self-distilled Masked AutoEncoder**

SdAE 是 MAE 的自蒸馏版本：重建目标不是原始像素，而是 teacher encoder 的特征。

```
目标: ||decoder(masked_visible) - teacher_encoder(full_image)||²
      而不是: ||decoder(masked_visible) - pixels||²
```

这使得重建目标包含更丰富的语义信息。

### 4.5 EVA 系列

#### EVA（2023）

**Exploring the Limits of Masked Visual Representation Learning at Scale**

核心思想：用 **CLIP 的视觉特征** 作为 MIM 的重建目标。

$$\mathcal{L}_{\text{EVA}} = \frac{1}{|\mathcal{M}|} \sum_{i \in \mathcal{M}} \| f_{\text{student}}(x_i) - f_{\text{CLIP}}(x_i) \|_2^2$$

**为什么 CLIP 特征好？**
- CLIP 用 4 亿图文对训练，特征包含丰富的语义
- 比像素/dVAE token 更好的监督信号
- EVA ViT-g/14（10亿参数）在 ImageNet 达到 89.6%

#### EVA-02（2024）

- 更大规模：ViT-E（43亿参数）
- 改进的 MIM + CLIP 蒸馏目标
- 用于后续 InternVL 等模型的视觉主干

### 4.6 MIM 方法对比

| 方法 | 重建目标 | Tokenizer | Masking | IN-1K FT |
|------|---------|-----------|---------|----------|
| MAE | 像素 | 无 | Random 75% | 87.8 (ViT-H) |
| BEiT | dVAE token | dVAE (offline) | Random 40% | 86.2 (ViT-L) |
| iBOT | Teacher feat | EMA (online) | Block 40% | 86.6 (ViT-L) |
| SdAE | Teacher feat | EMA | Random 75% | 87.4 (ViT-L) |
| EVA | CLIP feat | CLIP (frozen) | Random 40% | 89.6 (ViT-g) |

**面试要点**：
- MAE 的计算效率最高（只编码可见 patch）
- BEiT 的离散 token 捕获更高层语义
- EVA 利用 CLIP 先验取得最佳效果但依赖预训练 CLIP
- 重建目标的选择（像素/token/特征）决定了学到的表征层次

---

## 五、视觉自监督 Transformer

### 5.1 DINO（2021）

**Self-Distillation with No Labels**

DINO 发现自监督的 ViT 具有惊人的涌现性质——**attention map 自动关注物体轮廓**。

#### 架构

```
                  Student network (θ_s)          Teacher network (θ_t)
                         │                              │
        ┌────── global crop 1,2 ──────┐    ┌── global + local crops ──┐
        │                             │    │                          │
        v                             v    v                          v
    [CLS] token → softmax(z_s/τ_s)       [CLS] token → softmax(z_t/τ_t)
                         │                              │
                         └─── Cross-Entropy Loss ───────┘

Teacher: θ_t ← λ·θ_t + (1-λ)·θ_s   (EMA)
Teacher 温度: τ_t < τ_s (sharpening)
Centering: c ← m·c + (1-m)·mean(z_t)
```

#### 损失函数

$$\mathcal{L} = \sum_{x \in \{x_1^g, x_2^g\}} \sum_{x' \neq x} -P_t(x') \log P_s(x)$$

$$P_s(x) = \text{softmax}(g_{\theta_s}(x) / \tau_s), \quad P_t(x) = \text{softmax}((g_{\theta_t}(x) - c) / \tau_t)$$

注意：
- Student 接收所有视图（2 global + N local）
- Teacher 只接收 global 视图
- Teacher 输出做 centering + sharpening

#### Multi-crop 策略

```
2 个 global crops: 224×224，覆盖 > 50% 原图
N 个 local crops: 96×96，覆盖 < 50% 原图

Student 处理所有 crops，Teacher 只处理 global crops
→ 鼓励 local 视图的表征与 global 视图一致
→ 学习局部到全局的语义对应
```

#### 涌现性质

DINO ViT 的 self-attention 展现出令人惊讶的涌现能力：
1. **语义分割**：attention map 自动分割前景/背景，无需标注
2. **物体定位**：attention head 关注不同物体部件
3. **K-NN 分类**：不用线性探测，直接用 KNN 就能达到 74.5% Top-1

### 5.2 DINOv2（2023）

**DINOv2: Learning Robust Visual Features without Supervision**

Meta AI 推出的 DINOv2 是截至 2024 年最强的视觉自监督基础模型。

#### 与 DINO 的区别

| 维度 | DINO | DINOv2 |
|------|------|--------|
| 损失 | 自蒸馏 | 自蒸馏 + iBOT MIM + KoLeo 正则 |
| 数据 | ImageNet-1K | LVD-142M（自动策展的 1.42 亿图像） |
| 模型 | ViT-S/B | ViT-S/B/L/g |
| 训练 | 标准 | Flash Attention + FSDP |

#### 核心改进

1. **自动数据策展**：
   ```
   原始数据池（12 亿图像）
   → 图像去重（cosine sim > 0.6）
   → 用 ImageNet 做 retrieval 选高质量子集
   → LVD-142M（1.42 亿高质量多样图像）
   ```

2. **融合损失**：
   $$\mathcal{L} = \mathcal{L}_{\text{DINO}} + \mathcal{L}_{\text{iBOT}} + \lambda_{\text{KoLeo}} \mathcal{L}_{\text{KoLeo}}$$
   
   KoLeo 正则化鼓励表征的均匀分布（Kozachenko-Leonenko 熵估计）。

3. **知识蒸馏**：先训练 ViT-g，再蒸馏到 ViT-S/B/L

#### 性能

DINOv2 ViT-g 的线性探测结果：

| 任务 | 数据集 | DINOv2 | OpenCLIP | MAE |
|------|-------|--------|----------|-----|
| 分类 | ImageNet | 86.5 | 83.8 | 76.4 |
| 分割 | ADE20K | 49.0 mIoU | 39.3 | 46.1 |
| 深度 | NYUd | 0.34 RMSE | 0.42 | 0.36 |
| 检索 | Oxford | 80.1 mAP | 75.6 | 54.2 |

**面试要点**：DINOv2 是"通用视觉特征"的最佳实践——一个模型，多个任务，不需微调只做线性探测就能达到很好效果。

### 5.3 Vision Transformer 的 Registers（2024）

**Vision Transformers Need Registers**

DINOv2 团队发现 ViT 的 attention map 中存在 **伪影（artifacts）**——某些 patch token 的 attention 异常高（称为"register" patch），它们不对应任何语义区域。

#### 问题

```
正常 ViT attention: 均匀分布在物体上
DINO/DINOv2: 某些 patch 的 norm 异常高，吸收了大量 attention
→ 这些 patch 被模型当作"寄存器"，存储全局信息
→ 影响 attention map 的可解释性和稠密预测任务
```

#### 解决方案

添加 $R$ 个可学习的 **register token**（如 $R=4$），类似 [CLS] token 但不用于输出：

```python
class ViTWithRegisters(nn.Module):
    def __init__(self, num_registers=4):
        self.register_tokens = nn.Parameter(torch.randn(1, num_registers, dim))
    
    def forward(self, x):
        # x: (B, N, D) patch tokens
        cls = self.cls_token.expand(B, -1, -1)
        reg = self.register_tokens.expand(B, -1, -1)
        
        x = torch.cat([cls, reg, x], dim=1)  # (B, 1+R+N, D)
        x = self.transformer(x)
        
        # 输出时去掉 register tokens
        cls_out = x[:, 0]
        patch_out = x[:, 1 + self.num_registers:]
        return cls_out, patch_out
```

添加 register 后：
- Attention map 更干净，伪影消失
- 稠密预测任务性能提升
- 计算开销可忽略（仅多几个 token）

### 5.4 SigLIP（2023）

**Sigmoid Loss for Language Image Pre-Training**

SigLIP 将 CLIP 的 softmax 对比损失替换为 **sigmoid 损失**，带来多项优势。

#### 核心改变

CLIP 的 softmax 损失：
$$\mathcal{L}_{\text{CLIP}} = -\frac{1}{B} \sum_{i=1}^B \log \frac{\exp(s_{ii}/\tau)}{\sum_j \exp(s_{ij}/\tau)}$$

SigLIP 的 sigmoid 损失：
$$\mathcal{L}_{\text{SigLIP}} = -\frac{1}{B^2} \sum_{i,j} \left[ y_{ij} \log \sigma(s_{ij}) + (1-y_{ij}) \log (1-\sigma(s_{ij})) \right]$$

其中 $y_{ij} = \mathbb{1}[i=j]$，$s_{ij} = t \cdot \langle z_i^{\text{img}}, z_j^{\text{text}} \rangle + b$。

#### 优势

1. **不需要全局 softmax**：sigmoid 是 pairwise 计算，不需要跨 GPU all-gather
2. **更好的分布式扩展**：可以 chunk-wise 计算，内存友好
3. **性能相当或更好**：SigLIP ViT-L 在 zero-shot ImageNet 上达到 82.1%（vs. CLIP 80.2%）
4. **温度问题简化**：sigmoid 对温度不那么敏感

---

## 六、NLP 自监督学习

### 6.1 经典 Pretext Tasks

#### MLM（Masked Language Modeling）

BERT 的核心预训练任务：

```
输入: The cat [MASK] on the [MASK]
目标: 预测 sat, mat
遮盖策略: 15% token → 80% [MASK] + 10% random + 10% 保留
```

$$\mathcal{L}_{\text{MLM}} = -\sum_{i \in \mathcal{M}} \log p(x_i | x_{\setminus \mathcal{M}}; \theta)$$

**为什么不遮 100%？** 因为要让模型看到上下文。**为什么不只用 [MASK]？** 因为 fine-tune 时没有 [MASK] token，需要缩小预训练和微调的分布差异。

#### CLM（Causal Language Modeling）

GPT 的预训练任务——自回归预测下一个 token：

$$\mathcal{L}_{\text{CLM}} = -\sum_{t=1}^T \log p(x_t | x_{<t}; \theta)$$

```python
# GPT-style CLM
logits = model(input_ids)  # (B, T, V)
shift_logits = logits[:, :-1, :]  # 预测 t+1
shift_labels = input_ids[:, 1:]   # 目标
loss = F.cross_entropy(shift_logits.view(-1, V), shift_labels.view(-1))
```

**MLM vs CLM**：
- MLM 是双向的，更适合理解任务（NLU）
- CLM 是单向的，天然适合生成任务（NLG）
- 大模型时代，CLM 统一了理解和生成

#### NSP（Next Sentence Prediction）

BERT 的辅助任务：判断两个句子是否连续。

```
输入: [CLS] Sentence A [SEP] Sentence B [SEP]
标签: IsNext / NotNext (50/50)
```

**后来被证明效果有限**：RoBERTa 去掉 NSP 后效果反而更好，因为负样本（随机拼接的句子）太简单。

#### STS（Sentence Textual Similarity）

对比学习在 NLP 的应用，用于学习句子级别的表征：

```
正样本: 同一句子的不同 dropout mask（SimCSE）
负样本: batch 内其他句子
```

### 6.2 BERT（2018）

| 配置 | 值 |
|------|-----|
| 架构 | Transformer Encoder |
| 预训练任务 | MLM + NSP |
| 参数量 | 110M (Base) / 340M (Large) |
| 数据 | BooksCorpus + English Wikipedia (~16GB) |
| 输入长度 | 512 tokens |

#### BERT 的表征特点

- **[CLS] token 的表征**：聚合全句信息，但原始 BERT 的 [CLS] 表征质量不高（anisotropy 问题）
- **各层表征不同**：底层 → 语法信息，高层 → 语义信息
- **各向异性（Anisotropy）问题**：表征聚集在一个窄锥体中，余弦相似度普遍偏高

### 6.3 GPT 系列

```
GPT-1 (2018): 117M, CLM, BooksCorpus
GPT-2 (2019): 1.5B, CLM, WebText (40GB)
GPT-3 (2020): 175B, CLM, 570GB 混合数据
GPT-4 (2023): MoE, 估计 1.8T 参数（未公开）
GPT-4o (2024): 多模态原生
```

**关键洞察**：从 GPT-3 开始，CLM 在足够大的模型和数据上展现出 **in-context learning** 能力——不需要微调，只通过 prompt 就能完成各种任务。

### 6.4 T5（2020）

**Text-to-Text Transfer Transformer**

T5 将所有 NLP 任务统一为 text-to-text 格式：

```
翻译: "translate English to German: That is good" → "Das ist gut"
摘要: "summarize: <article>" → "<summary>"
分类: "cola sentence: The course is jumping well" → "not acceptable"
```

预训练任务：**Span Corruption**（类似 MLM 但遮的是连续 span）

```
输入: "Thank you <extra_id_0> me to your party <extra_id_1> week"
目标: "<extra_id_0> for inviting <extra_id_1> last <extra_id_2>"
```

### 6.5 ELECTRA（2020）

**Efficiently Learning an Encoder that Classifies Token Replacements Accurately**

ELECTRA 用 **替换检测** 替代 MLM，效率更高：

```
Generator (小模型): 对 masked token 生成替换词
Discriminator (主模型): 判断每个 token 是否被替换

原始: "the chef cooked the meal"
Generator输出: "the chef ate the meal"  (cooked→ate)
Discriminator: [original, original, replaced, original, original]
```

$$\mathcal{L}_{\text{ELECTRA}} = \sum_{t=1}^T -[\mathbb{1}(x_t = x_t^{\text{orig}}) \log D(x_t) + \mathbb{1}(x_t \neq x_t^{\text{orig}}) \log(1-D(x_t))]$$

**优势**：
- 对 **所有** token 计算损失（MLM 只对 15% 的 masked token 计算）
- 训练效率提高 4x（小模型达到 BERT-Large 效果）
- 学到的表征更鲁棒

### 6.6 DeBERTa（2021）

**Decoding-enhanced BERT with Disentangled Attention**

DeBERTa 的两大创新：

#### 1. 解耦注意力（Disentangled Attention）

标准 Transformer 将内容和位置编码相加后计算 attention。DeBERTa 将它们 **解耦**：

$$A_{ij} = \underbrace{H_i^c {H_j^c}^\top}_{\text{content-to-content}} + \underbrace{H_i^c {P_{i|j}^r}^\top}_{\text{content-to-position}} + \underbrace{P_{j|i}^r {H_j^c}^\top}_{\text{position-to-content}}$$

注意：没有 position-to-position 项（因为没有语义信息）。

#### 2. Enhanced Mask Decoder

在最后的 MLM 预测层引入绝对位置信息（之前的层只用相对位置）：

```
标准 BERT: 所有层都用绝对位置编码
DeBERTa:  除最后一层外用相对位置，最后一层加入绝对位置
```

这使得模型能利用相对位置的灵活性，同时在需要时（如 MLM 预测位置敏感的 token）使用绝对位置。

#### DeBERTa v3（2023）

结合了 ELECTRA 的替换检测 + DeBERTa 的解耦注意力，是截至 2024 年最强的 encoder-only 模型之一。

### 6.7 LLM 时代的自监督演进

2023-2026 年，NLP 自监督学习的趋势：

| 趋势 | 说明 |
|------|------|
| **CLM 统一** | GPT-4/Claude/Gemini 统一用 CLM，理解+生成一体 |
| **超长上下文** | 从 512 → 128K → 1M+，位置编码是关键（RoPE/ALiBi） |
| **多阶段预训练** | 基础预训练 → 持续预训练（领域数据）→ 指令微调 → 对齐 |
| **Mixture of Experts** | Mixtral/DeepSeek-V3 用 MoE 提高参数量但控制计算 |
| **数据质量 > 数据量** | Phi 系列证明高质量小数据可以训出强模型 |
| **Curriculum learning** | 数据排序/混合比例动态调整 |
| **合成数据增强** | 用强模型生成训练数据（Cosmopedia, Orca） |

**面试要点**：被问"BERT 和 GPT 的区别"时，不要只说"双向 vs 单向"。更深层的区别：
- BERT 适合表征学习，GPT 适合生成
- BERT 需要 fine-tune 才能做下游任务，GPT-3+ 可以 in-context learning
- 大模型时代 CLM 的 scaling law 更好，这是 GPT 路线胜出的根本原因

---

## 七、多模态对比学习

### 7.1 CLIP（2021）

**Learning Transferable Visual Models From Natural Language Supervision**

CLIP 是多模态对比学习的标杆——用 4 亿图文对训练，实现了图像和文本的统一表征。

#### 架构

```
Image Encoder (ViT-L/14)          Text Encoder (Transformer)
        │                                   │
        v                                   v
   I_1, I_2, ..., I_N                T_1, T_2, ..., T_N
        │                                   │
        └──────── Contrastive Loss ─────────┘

相似度矩阵: S[i,j] = I_i · T_j / τ
对角线应最大（正样本对），其余应最小（负样本对）
```

#### 损失函数

$$\mathcal{L}_{\text{CLIP}} = \frac{1}{2} (\mathcal{L}_{i2t} + \mathcal{L}_{t2i})$$

$$\mathcal{L}_{i2t} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp(s_{ii}/\tau)}{\sum_{j=1}^N \exp(s_{ij}/\tau)}$$

$$\mathcal{L}_{t2i} = -\frac{1}{N} \sum_{i=1}^N \log \frac{\exp(s_{ii}/\tau)}{\sum_{j=1}^N \exp(s_{ji}/\tau)}$$

```python
def clip_loss(image_features, text_features, temperature):
    """
    image_features: (B, D) L2-normalized
    text_features: (B, D) L2-normalized
    """
    logits = image_features @ text_features.T / temperature  # (B, B)
    labels = torch.arange(logits.shape[0], device=logits.device)
    
    loss_i2t = F.cross_entropy(logits, labels)
    loss_t2i = F.cross_entropy(logits.T, labels)
    
    return (loss_i2t + loss_t2i) / 2
```

#### CLIP 的 Zero-Shot 推理

```
# 对每个类别构造文本 prompt
prompts = ["a photo of a {class}" for class in classes]
text_features = text_encoder(prompts)  # (C, D)

# 对输入图像编码
image_feature = image_encoder(image)  # (1, D)

# 计算相似度，选最大的
similarities = image_feature @ text_features.T  # (1, C)
prediction = similarities.argmax(dim=1)
```

#### CLIP 的局限性

1. **数据噪声**：4 亿图文对来自互联网，噪声大
2. **图文对齐粒度粗**：只有全局对齐，缺少区域/token 级对齐
3. **组合理解差**："a red cube on a blue sphere" 等空间/属性关系
4. **文本编码器能力有限**：只有 63M 参数的 Transformer
5. **否定理解差**：对"not"等否定词不敏感

### 7.2 ALIGN（2021）

**Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision**

Google 的 ALIGN 用 **18 亿** 噪声图文对训练：

```
CLIP: 4 亿图文对，经过仔细过滤
ALIGN: 18 亿图文对，最少过滤（只做频率阈值过滤）
→ 证明"规模可以弥补噪声"
```

ALIGN 使用 EfficientNet 作为 image encoder，性能与 CLIP 相当，但训练数据策略完全不同。

### 7.3 SigLIP（2023）

在第五章已介绍核心思想。这里补充多模态方面的影响：

```
CLIP: softmax 需要全局归一化 → all-gather 所有 GPU 的表征
SigLIP: sigmoid pairwise → 可以 chunk-wise 分布式计算

实际影响:
- CLIP batch size 32K 需要 128 GPU all-gather
- SigLIP 同样 batch size 可以更高效地分片
```

SigLIP 被 Google 的 PaLI 系列多模态模型广泛采用。

### 7.4 CoCa（2022）

**Contrastive Captioners are Image-Text Foundation Models**

CoCa 融合了 **对比学习 + 生成式** 两种目标：

```
Image Encoder
     │
     ├──→ [CLS] ──→ Contrastive Loss (与 text [CLS] 对比)
     │
     └──→ All tokens ──→ Cross-Attention ──→ Captioning Loss (自回归生成文本)
```

$$\mathcal{L}_{\text{CoCa}} = \mathcal{L}_{\text{contrastive}} + \lambda \cdot \mathcal{L}_{\text{captioning}}$$

**优势**：
- 对比损失保证好的全局对齐
- 生成损失提供细粒度的语义理解
- 一个模型同时支持 zero-shot 分类和图像描述

### 7.5 Florence（2022）

**Florence: A New Foundation Model for Computer Vision**

Microsoft 的 Florence 强调 **多粒度、多任务** 的统一视觉基础模型：

```
数据: 9 亿图文对 (FLD-900M)
预训练: UniCL (统一对比学习，处理 image-text 和 image-label 混合数据)
扩展: 从图像级 → 区域级 → 像素级
任务: 分类、检测、分割、caption、VQA
```

**UniCL 的创新**：统一处理图文对和图像-标签数据

$$s_{ij} = \begin{cases} \cos(v_i, l_j) & \text{if label data} \\ \cos(v_i, t_j) & \text{if image-text pair} \end{cases}$$

### 7.6 InternVL 系列（2024-2026）

InternVL 是上海 AI Lab 的多模态基础模型系列，持续进化：

#### InternVL 1.0

- 视觉编码器：InternViT-6B（EVA-02 架构，60亿参数）
- 文本编码器：InternLM
- 预训练：对比学习 + 生成式

#### InternVL 1.5 → 2.0 → 2.5

```
核心改进路径:
1. 动态分辨率（dynamic resolution）: 适应不同尺寸的输入
2. 更强的 LLM backbone: InternLM-2 → Qwen2.5
3. 数据质量: 更好的图文对过滤和合成
4. 训练策略: 多阶段预训练（contrastive → generative → instruction）
```

#### InternVL 2.5（2025）

- 支持 8K 分辨率动态输入
- 多模态 Chain-of-Thought
- 在多个 benchmark 上达到 GPT-4V 级别

### 7.7 多模态对比学习对比

| 方法 | 年份 | 数据规模 | Image Enc | Text Enc | IN-1K ZS |
|------|------|---------|-----------|----------|----------|
| CLIP | 2021 | 400M | ViT-L/14 | Transformer 63M | 75.3 |
| ALIGN | 2021 | 1.8B | EfficientNet | BERT-Large | 76.4 |
| Florence | 2022 | 900M | CoSwin-H | UniCL | 83.7 |
| CoCa | 2022 | ~1B | ViT-g | Decoder | 86.3 |
| SigLIP | 2023 | ~1B | ViT-L | Transformer | 82.1 |
| EVA-CLIP | 2024 | 2B+ | EVA-02-E | - | 82.0 |
| InternVL 2.5 | 2025 | 5B+ | InternViT-6B | InternLM | 88.2+ |

---

## 八、数据增强策略

数据增强是自监督学习成功的关键——它决定了正样本对共享什么信息（语义）、丢弃什么信息（增强引入的变化）。

### 8.1 CV 数据增强

#### SimCLR 的增强组合实验

SimCLR 论文系统地研究了各种增强的组合效果：

| 增强组合 | Top-1 (Linear) | 贡献 |
|---------|---------------|------|
| Random Crop only | 53.2 | 基线 |
| + Color Jitter | 64.5 | +11.3 |
| + Gaussian Blur | 66.6 | +2.1 |
| + Grayscale | 67.0 | +0.4 |
| Full (上述全部) | 69.3 | — |

**核心发现**：Random Crop + Color Jitter 是最重要的组合，其他增强是锦上添花。

#### 增强详解

```python
import torchvision.transforms as T

def simclr_augmentation(image_size=224):
    return T.Compose([
        # 1. Random Resized Crop: 最关键的增强
        # 随机裁剪 8%-100% 面积，resize 到 224
        T.RandomResizedCrop(image_size, scale=(0.08, 1.0)),
        
        # 2. Random Horizontal Flip
        T.RandomHorizontalFlip(p=0.5),
        
        # 3. Color Jitter: 第二关键
        T.RandomApply([
            T.ColorJitter(
                brightness=0.4,
                contrast=0.4, 
                saturation=0.4,
                hue=0.1
            )
        ], p=0.8),
        
        # 4. Random Grayscale
        T.RandomGrayscale(p=0.2),
        
        # 5. Gaussian Blur
        T.RandomApply([T.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0))], p=0.5),
        
        # 6. 标准化
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
```

#### 为什么 Random Crop + Color Jitter 如此重要？

直觉解释：
- **Random Crop**：迫使模型关注局部语义（而非全局布局），学习 **位置不变性**
- **Color Jitter**：迫使模型忽略颜色信息，学习 **颜色不变性**

如果只用 crop 不用 color：模型可以通过匹配颜色直方图来"作弊"
如果只用 color 不用 crop：模型可以通过匹配空间布局来"作弊"

#### DINO 的 Multi-Crop

```python
def dino_multi_crop(image_size=224, local_size=96):
    # 2 个 global crops (224×224, 覆盖 >50% 原图)
    global_transform = T.Compose([
        T.RandomResizedCrop(image_size, scale=(0.4, 1.0)),
        # ... 其他增强
    ])
    
    # N 个 local crops (96×96, 覆盖 <50% 原图)
    local_transform = T.Compose([
        T.RandomResizedCrop(local_size, scale=(0.05, 0.4)),
        # ... 其他增强（更弱的 color jitter）
    ])
    
    return global_transform, local_transform
```

Multi-crop 的计算开销很小（local crop 只有 96×96），但效果提升显著（+2-3%）。

#### MIM 的增强策略

MAE/BEiT 的数据增强明显 **更弱**：

```
MAE: 只用 Random Resized Crop + Horizontal Flip
原因: 掩码本身就是一种强增强，不需要额外的颜色/模糊增强
```

### 8.2 NLP 数据增强

NLP 的增强比 CV 困难得多——离散的 token 空间不像连续的像素空间那样容易扰动。

#### Token 级增强

| 方法 | 操作 | 示例 |
|------|------|------|
| 随机删除 | 删除部分 token | "I love this movie" → "I this movie" |
| 随机插入 | 插入随机 token | "I love this" → "I really love this" |
| 随机替换 | 用同义词替换 | "good movie" → "great movie" |
| 随机交换 | 交换相邻 token | "I love" → "love I" |

#### Dropout 增强（SimCSE 的发现）

SimCSE 发现最简单有效的 NLP 增强：**同一句子通过两次不同的 dropout 得到的表征作为正样本对**。

```python
def simcse_forward(model, input_ids, attention_mask):
    # 同一句子过两次 encoder（不同的 dropout mask）
    z1 = model(input_ids, attention_mask)  # dropout mask 1
    z2 = model(input_ids, attention_mask)  # dropout mask 2 (自动不同)
    
    # z1 和 z2 是正样本对
    loss = info_nce_loss(z1, z2, temperature=0.05)
    return loss
```

**为什么 dropout 有效？**
- Dropout 本质是一种 **数据增强**，每次前向传播随机丢弃不同的神经元
- 两次 dropout 得到的表征保留了语义信息，但有微小差异
- 这种差异恰好是对比学习需要的"增强"

#### 回译（Back Translation）

用于构造平行语料的增强：

```
原文 (EN): "The weather is nice today"
→ 翻译 (ZH): "今天天气很好"
→ 回译 (EN): "The weather is great today"
→ 原文和回译构成正样本对
```

### 8.3 多模态数据增强

#### 图像侧增强

通常沿用 CV 的增强策略（crop, color jitter, flip 等）。

#### 文本侧增强

1. **Prompt engineering**：同一图像配多种描述模板
   ```
   "a photo of a cat"
   "an image showing a cat"  
   "a cat in a photograph"
   ```

2. **Text cropping**：对长 caption 截取不同子串

3. **CLIP 的 80 种 prompt 模板**（用于 zero-shot）：
   ```
   "a photo of a {class}"
   "a blurry photo of a {class}"
   "a sculpture of a {class}"
   "a tattoo of a {class}"
   ...
   → 最终用 80 个模板的平均 text feature
   ```

#### 跨模态增强

1. **Mixup**：对图像和文本同时做插值
2. **CutMix**：混合两张图的 patch + 混合对应文本
3. **Modal dropout**：随机丢弃一个模态的信息（鼓励跨模态推理）

---

## 九、下游任务迁移

### 9.1 线性探测（Linear Probing）

**最严格的表征质量评估**：冻结 encoder，只训练一个线性分类头。

```python
class LinearProbe(nn.Module):
    def __init__(self, encoder, num_classes):
        super().__init__()
        self.encoder = encoder
        for param in self.encoder.parameters():
            param.requires_grad = False  # 冻结 encoder
        self.head = nn.Linear(encoder.embed_dim, num_classes)
    
    def forward(self, x):
        with torch.no_grad():
            features = self.encoder(x)  # (B, D)
        return self.head(features)
```

**线性探测 vs. fine-tune 的含义差异**：
- 线性探测好 = 预训练表征本身就含有丰富的判别信息
- fine-tune 好但线性探测差 = 预训练表征是好的初始化，但需要适配
- 对比学习通常线性探测更好，MIM 通常 fine-tune 更好

#### 为什么有这个差异？

```
对比学习: 显式优化判别性（正负样本分离）
→ 表征空间已经具有线性可分性
→ 线性探测效果好

MIM: 优化重建能力（预测 pixel/token）
→ 表征编码了丰富的底层信息，但未显式优化判别性
→ 需要 fine-tune 来提取判别信息
```

### 9.2 全量微调（Full Fine-Tuning）

解冻所有参数，端到端训练：

```python
# 关键超参数差异
fine_tune_config = {
    "lr": 1e-4,           # 比从零训练低 10-100x
    "weight_decay": 0.05,
    "layer_decay": 0.75,  # 底层学习率更低（层级学习率衰减）
    "warmup_epochs": 5,
    "epochs": 100,
    "drop_path": 0.1,     # ViT 的随机深度
}
```

#### 层级学习率衰减（Layer-wise LR Decay）

底层特征更通用，高层特征更任务相关。底层用更小的学习率保护通用特征：

$$\text{lr}_l = \text{lr}_{\text{base}} \times \gamma^{L - l}$$

其中 $L$ 是总层数，$l$ 是当前层，$\gamma \in [0.65, 0.85]$ 是衰减因子。

```python
def get_layer_wise_lr(model, base_lr, layer_decay, num_layers):
    """为每一层设置不同的学习率"""
    param_groups = []
    for name, param in model.named_parameters():
        layer_id = get_layer_id(name, num_layers)
        lr = base_lr * (layer_decay ** (num_layers - layer_id))
        param_groups.append({"params": [param], "lr": lr})
    return param_groups
```

### 9.3 Zero-Shot 迁移

不需要任何任务特定的训练数据，通过文本 prompt 直接推理。

```python
# CLIP zero-shot classification
def zero_shot_classify(model, image, class_names, templates):
    """
    templates: ["a photo of a {}", "a drawing of a {}", ...]
    class_names: ["cat", "dog", "bird", ...]
    """
    # 1. 为每个类别生成文本特征（多模板平均）
    text_features = []
    for name in class_names:
        texts = [t.format(name) for t in templates]
        text_tokens = tokenize(texts)
        feat = model.encode_text(text_tokens)  # (T, D)
        feat = feat.mean(dim=0)  # (D,) 多模板平均
        feat = F.normalize(feat, dim=0)
        text_features.append(feat)
    text_features = torch.stack(text_features)  # (C, D)
    
    # 2. 编码图像
    image_feature = model.encode_image(image)  # (1, D)
    image_feature = F.normalize(image_feature, dim=1)
    
    # 3. 计算相似度
    logits = image_feature @ text_features.T  # (1, C)
    return logits.argmax(dim=1)
```

**Prompt Engineering 对 Zero-Shot 的影响巨大**：

| Prompt | IN-1K Top-1 |
|--------|------------|
| "{class}" | 68.3 |
| "a photo of a {class}" | 72.1 |
| 80 模板集成 | 75.3 |
| CuPL (GPT-4 生成描述) | 77.2 |

### 9.4 Few-Shot 迁移

只用少量标注样本（1-shot, 5-shot, 16-shot）进行适配。

#### 方法 1：Tip-Adapter（Training-free）

```python
# 无需训练的 few-shot 适配
def tip_adapter(image_feature, train_features, train_labels, alpha=1.0, beta=5.5):
    """
    train_features: (K*C, D) few-shot 样本的特征
    train_labels: (K*C,) one-hot 标签
    """
    affinity = image_feature @ train_features.T  # (1, K*C)
    affinity = (-beta * (1 - affinity)).exp()      # 相似度 → 权重
    
    # 加权投票
    cache_logits = affinity @ train_labels  # (1, C)
    
    # 结合 zero-shot logits
    zero_shot_logits = image_feature @ text_features.T
    
    return zero_shot_logits + alpha * cache_logits
```

#### 方法 2：Linear Probe Few-Shot

在 few-shot 数据上训练线性头，但用正则化防止过拟合。

#### 方法 3：CoOp / CoCoOp（Prompt Learning）

不微调模型参数，而是学习 **soft prompt**：

```python
# CoOp: 学习 prompt 的连续向量
class CoOp(nn.Module):
    def __init__(self, clip_model, n_ctx=16, n_classes=100):
        self.ctx = nn.Parameter(torch.randn(n_ctx, 512))  # 可学习的 prompt token
        self.class_names = ...  # 类别名
    
    def forward(self):
        # 构造 prompt: [ctx1, ctx2, ..., ctx16, CLASS]
        prompts = torch.cat([self.ctx.unsqueeze(0).expand(n_classes, -1, -1),
                            class_embeddings.unsqueeze(1)], dim=1)
        text_features = clip_model.encode_text(prompts)
        return text_features
```

### 9.5 Prompt-Based 迁移

#### Visual Prompt Tuning（VPT）

在 ViT 的输入中插入可学习的 visual prompt token：

```python
class VPT(nn.Module):
    def __init__(self, vit, num_tokens=10, prompt_dim=768):
        self.vit = vit
        for p in self.vit.parameters():
            p.requires_grad = False  # 冻结 ViT
        
        # VPT-Shallow: 只在第一层加 prompt
        self.prompt_tokens = nn.Parameter(torch.randn(1, num_tokens, prompt_dim))
    
    def forward(self, x):
        B = x.shape[0]
        x = self.vit.patch_embed(x)  # (B, N, D)
        prompts = self.prompt_tokens.expand(B, -1, -1)  # (B, P, D)
        x = torch.cat([x[:, :1], prompts, x[:, 1:]], dim=1)  # [CLS, prompts, patches]
        return self.vit.forward_features(x)
```

#### Adapter 方法

在 Transformer 层之间插入轻量 adapter 模块：

```python
class Adapter(nn.Module):
    def __init__(self, d_model, bottleneck=64):
        self.down = nn.Linear(d_model, bottleneck)
        self.act = nn.GELU()
        self.up = nn.Linear(bottleneck, d_model)
        self.scale = nn.Parameter(torch.ones(1) * 0.1)
    
    def forward(self, x):
        return x + self.scale * self.up(self.act(self.down(x)))
```

### 9.6 迁移方法对比

| 方法 | 可训练参数 | 需要数据 | 效果 | 场景 |
|------|----------|---------|------|------|
| Zero-shot | 0 | 0 | 中 | 无标注数据 |
| Linear Probe | ~0.1M | 1K+ | 中高 | 评估表征质量 |
| Few-shot (Tip) | 0 | 1-16/类 | 中高 | 标注极少 |
| Prompt Tuning | ~0.1M | 100+ | 高 | 模型不可修改 |
| LoRA | ~1-10M | 1K+ | 高 | 高效微调 |
| Full Fine-Tune | All | 10K+ | 最高 | 充足数据 |

---

## 十、2026 前沿趋势

### 10.1 Video SSL

视频比图像多了 **时间维度**，提供更丰富的自监督信号。

#### VideoMAE（2022→v2 2023）

```
核心思想: 视频版 MAE
- 将视频分为 3D patch (时间×空间)
- 随机遮住 90%（视频冗余度比图像更高）
- 用 ViT 重建
```

VideoMAE v2 的关键改进：
- 双掩码策略（encoder 高掩码率 + decoder 低掩码率）
- 10亿参数的 ViT-g 在 Kinetics-400 达到 SOTA
- 用未标注视频预训练，大幅减少标注依赖

#### V-JEPA（2024, Meta/LeCun）

Yann LeCun 推动的 **Joint-Embedding Predictive Architecture** 在视频上的应用：

```
核心: 在特征空间预测，而非像素空间
- 不重建被遮住的 patch 的像素
- 而是预测被遮住的 patch 的特征表示

优势:
- 避免了像素级重建的高频噪声
- 学到更抽象的时空特征
- 更好的下游迁移性能
```

#### 2025-2026 Video SSL 趋势

1. **长视频理解**：从短 clip（16帧）→ 长视频（几分钟到几小时）
2. **时序对比学习**：利用视频的时间顺序作为自监督信号
3. **视频-文本对比学习**：类似 CLIP 但用视频-文本对
4. **视频世界模型**：通过预测未来帧学习物理规律（Sora 的思路）

### 10.2 Audio SSL

#### wav2vec 2.0（2020）/ HuBERT（2021）

```
wav2vec 2.0:
- 对比学习 + 量化
- CNN encoder → Transformer → 对比损失
- 正样本: 同一时间步的量化表示
- 负样本: 其他时间步

HuBERT:
- 掩码预测（类似 BERT/BEiT）
- 用 k-means 聚类标签作为预测目标
- 迭代训练: 特征→聚类→训练→更好的特征→更好的聚类
```

#### Whisper（2023）的弱监督

Whisper 虽然不是严格的 SSL，但展示了一种实用的弱监督方法：
- 用 68 万小时的弱标注音频（互联网字幕）训练
- 多任务: 语种检测 + 转录 + 翻译
- 规模带来的鲁棒性超过了精心设计的 SSL 方法

#### 2025-2026 Audio SSL 趋势

1. **音频-视觉对齐**：学习音频和视觉的联合表征（AudioCLIP）
2. **音乐理解**：MusicLM/Jukebox 风格的音乐 SSL
3. **环境音理解**：用于机器人感知的通用音频表征

### 10.3 3D SSL

#### Point-MAE / Point-BERT（2022）

将 MIM 思想引入 3D 点云：

```
Point-MAE:
- 将点云分为 patch（FPS + KNN 分组）
- 随机遮住 60-80% 的 patch
- 用 Transformer 重建被遮住点的坐标

Point-BERT:
- 用 dVAE 将点云 patch 量化为 token
- 掩码预测 token（类似 BEiT）
```

#### 3D-视觉-语言对齐

```
ULIP (2023): 3D 点云 + 图像 + 文本 三模态对齐
OpenShape (2023): 开放世界 3D 理解（借助 CLIP）
Point-Bind (2024): 将 3D 绑定到 ImageBind 的统一表征空间
```

### 10.4 Robotics SSL

机器人学是 SSL 的重要应用领域——真实环境中标注数据极其昂贵。

#### RT-2 / Octo / π0

```
RT-2 (2023, Google): 
- 用 VLM (PaLM-E) 直接输出机器人动作
- 预训练的视觉-语言表征 transfer 到机器人控制

Octo (2024, UC Berkeley):
- 通用机器人策略
- 在 80 万条轨迹上用 Transformer 预训练
- 迁移到新机器人/新任务只需少量微调

π0 (2024, Physical Intelligence):
- 流匹配（flow matching）训练机器人策略
- 多任务、多机器人统一模型
```

#### 机器人 SSL 的核心挑战

1. **数据多样性**：不同机器人、不同环境、不同任务
2. **动作空间异质**：关节角 vs. 末端执行器 vs. 导航指令
3. **安全约束**：机器人不能像 NLP 那样随意探索
4. **实时性要求**：推理延迟要求低（<50ms）

### 10.5 自监督 Scaling Laws

#### 视觉 SSL 的 Scaling Law

DINOv2 和 EVA 系列的实验揭示了视觉 SSL 的 scaling behavior：

```
性能 ∝ log(模型参数) × log(数据量) × log(计算量)

具体观察:
- ViT-S (22M) → ViT-B (86M) → ViT-L (307M) → ViT-g (1.1B) → ViT-G (1.8B)
  ImageNet linear: 77.0 → 82.1 → 84.5 → 86.5 → 87.0
  
- 数据量 scaling: 1M → 14M → 142M
  DINOv2 ViT-g: 85.2 → 86.0 → 86.5
```

#### 对比学习的 Scaling Law（特有规律）

1. **Batch Size Scaling**：对比学习比 MIM 更依赖大 batch
   - SimCLR: 256 → 8192 = +7% 绝对提升
   - MAE: 256 → 4096 = +0.5% 几乎无影响

2. **温度与 Batch 的耦合**：更大的 batch 需要更低的温度

3. **投影维度**：Barlow Twins/VICReg 中维度需要足够大（8192+）

#### LLM 的 Scaling Law

Chinchilla scaling law：

$$L(N, D) = \frac{A}{N^\alpha} + \frac{B}{D^\beta} + E$$

其中 $N$ 是参数量，$D$ 是训练 token 数。Chinchilla 建议 $N$ 和 $D$ 以相同比例增长。

**2025-2026 更新**：
- DeepSeek-V3 等模型证明 MoE 改变了 scaling law
- 数据质量的影响可能大于数据量（Phi-3 的启示）
- Inference-time compute（OpenAI o1 风格）开辟了新的 scaling 维度

### 10.6 其他前沿方向

#### 1. World Models（世界模型）

通过自监督预测来建模世界的动态：

```
JEPA (LeCun 2022 构想):
- 不预测像素（太底层、太难）
- 在潜在空间预测（abstract prediction）
- 层次化的预测（hierarchical JEPA）

实际进展:
- Sora: 视频生成暗含世界模型
- Genie (DeepMind): 从视频学交互式世界
- V-JEPA: 在视频特征空间的预测
```

#### 2. 自监督 + 强化学习

```
用 SSL 学习的表征作为 RL 的状态表示:
- CURL (2020): 对比学习 + RL
- SPR (2021): 预测未来状态的 SSL + RL
- VIP (2023): 视频预训练的价值函数
```

#### 3. 多模态统一表征

```
ImageBind (2023, Meta):
- 用图像作为"锚点"
- 将 6 种模态 (图像/文本/音频/深度/IMU/热图) 绑定到统一空间
- 只需要 image-X 的配对数据，不需要全部两两配对

4M (2024, EPFL):
- 任意模态到任意模态的转换
- 统一 tokenization + 自回归生成
```

---

## 十一、面试高频题精选

### Q1：对比学习中温度参数 τ 的作用是什么？太大/太小会怎样？

**答案要点**：

温度 $\tau$ 控制 softmax 分布的 **锐度（sharpness）**。

- **$\tau$ 较小（如 0.01）**：
  - softmax 趋近 argmax，只关注最 hard 的负样本
  - 梯度幅度大（$\propto 1/\tau$），训练不稳定
  - 容易受噪声样本干扰

- **$\tau$ 较大（如 1.0）**：
  - softmax 趋近均匀分布，所有负样本同等对待
  - 梯度信号弱，收敛慢
  - 不够关注 hard negatives

- **最优区间**：通常 $\tau \in [0.05, 0.2]$
- CLIP 的做法：$\tau$ 作为可学习参数（`log_temperature`），初始化为 0.07，让模型自动调节

### Q2：BYOL 为什么不会表征坍缩？

**答案要点**：

BYOL 没有负样本，理论上所有输出相同就能使 MSE 损失为 0（坍缩），但实际不会坍缩。原因：

1. **EMA target 的慢更新**：
   - Target network 是 online network 的 EMA 版本
   - 缓慢变化的 target 提供了"移动目标"
   - Online 需要不断"追赶" target，而 target 又慢慢跟随 online → 形成不断扩展表征空间的动态

2. **Predictor 打破对称性**：
   - 没有 predictor，两个网络的梯度是对称的 → 坍缩到不动点
   - 有 predictor → 打破对称性 → 非平凡解

3. **BatchNorm 的隐式对比**（辅助作用）：
   - BN 层引入了 batch 内样本的统计量
   - 但后来证明去掉 BN 换成 LN，BYOL 仍然工作

**关键实验**：
- 去掉 stop-gradient → 立即坍缩
- 去掉 predictor → 立即坍缩
- 去掉 EMA（SimSiam 风格）→ 仍然工作（但需要 stop-gradient + predictor）

### Q3：MAE 为什么用 75% 的高掩码率？与 BERT 的 15% 有什么区别？

**答案要点**：

**根本原因：信息冗余度不同**

- **文本**：高信息密度，每个 token 都有独特含义。遮住 15% 就足够困难
- **图像**：高空间冗余，相邻 patch 高度相关。遮住 30% 太简单（可以从邻居插值）

**实验证据**（MAE 论文 Figure 6）：

| 掩码率 | Top-1 (Fine-Tune) | Top-1 (Linear) |
|--------|-------------------|----------------|
| 25% | 84.1 | 60.1 |
| 50% | 85.2 | 66.3 |
| 75% | 85.9 | 67.8 |
| 90% | 85.3 | 49.3 |

75% 是 sweet spot：
- 低于 75%：任务太简单，模型学不到高层语义
- 高于 85%：信息太少，重建任务无法完成

**额外好处**：75% 掩码 → encoder 只处理 25% 的 patch → **3x 训练加速**

### Q4：CLIP 的 zero-shot 能力来自哪里？有什么局限性？

**答案要点**：

**能力来源**：
1. **大规模图文对预训练**：4 亿图文对覆盖了海量视觉概念
2. **自然语言作为开放标签空间**：不像分类器限定在 N 个类别，文本可以描述任何概念
3. **对比学习的判别性**：图文表征空间中的相似度直接反映语义匹配度

**局限性**：
1. **组合理解差**：不理解 "a red circle to the left of a blue square"（属性绑定问题）
2. **否定理解差**：对 "not" 等否定词不敏感
3. **细粒度区分差**：相似子类（如鸟类物种）的区分能力弱
4. **分布外泛化有限**：训练集中少见的概念效果差
5. **Prompt 敏感**：不同的 prompt 模板可导致 ±5% 的性能差异
6. **粒度粗**：只有全局对齐，缺少区域/像素级理解

### Q5：对比学习和掩码建模，下游任务迁移有什么区别？

**答案要点**：

| 维度 | 对比学习 | 掩码建模 |
|------|---------|---------|
| 线性探测 | ⭐ 强 | 弱 |
| 全量微调 | 强 | ⭐ 更强 |
| 分类任务 | ⭐ 强 | 好 |
| 稠密预测 | 一般 | ⭐ 更好 |
| Few-shot | ⭐ 强 | 一般 |
| 训练效率 | 低（大 batch） | ⭐ 高（MAE） |

**原因分析**：
- 对比学习 **显式优化判别性** → 表征天然线性可分 → 线性探测强
- MIM **编码丰富的空间信息** → 细粒度特征好 → 稠密预测强
- MIM 需要 fine-tune 来"激活"判别能力，对比学习已经内置

**最佳实践**：DINOv2 融合两者（自蒸馏 + iBOT MIM），兼得优势。

### Q6：SimCLR 中投影头（Projection Head）为什么有用？丢弃后用底层特征反而更好？

**答案要点**：

这是对比学习最反直觉的发现之一。

**现象**：投影头输出 $z$ 用于计算对比损失，但 $z$ 的线性探测效果差于投影头之前的 $h$。

**解释**（Chen et al. 2020）：
1. 对比损失要求表征对数据增强 **不变**
2. 投影头 **吸收了增强相关的信息**（颜色、裁剪位置等）
3. $z$ 丢弃了增强信息 → 这些信息在下游任务中可能有用
4. $h$ 保留了更多信息 → 更适合迁移

**类比**：
```
h = 编码了所有信息的"完整表征"
g (投影头) = "信息过滤器"，去掉增强相关的噪声
z = g(h) = "对比学习优化的表征"（牺牲部分信息换取增强不变性）
```

### Q7：DINOv2 为什么被称为"通用视觉特征"？它是如何训练的？

**答案要点**：

**"通用"的含义**：一个冻结的 DINOv2 encoder + 简单线性头，就能在分类/分割/深度估计/检索等多种任务上取得接近 SOTA 的效果。

**训练流程**：

1. **数据策展**：
   - 从 12 亿原始图像中策展 1.42 亿（LVD-142M）
   - 去重 + 以 ImageNet 为 anchor 的 retrieval 筛选

2. **融合损失**：
   ```
   L = L_DINO (自蒸馏, [CLS] token)
     + L_iBOT (掩码建模, patch token)
     + L_KoLeo (均匀性正则化)
   ```

3. **训练配置**：
   - ViT-g (1.1B 参数), Flash Attention + FSDP
   - batch size 2048, 训练 ~625K iterations

4. **知识蒸馏**：
   - 先训 ViT-g
   - 再蒸馏到 ViT-S/B/L（更小模型也能用）

### Q8：解释 Barlow Twins 的冗余减少原则。

**答案要点**：

灵感来自神经科学家 Horace Barlow 的 **冗余减少假说**：感知系统的目标是对感官输入去除冗余，得到高效编码。

**数学实现**：约束两个视图表征的交叉相关矩阵 $C$ 趋向单位矩阵 $I$。

$$\mathcal{L} = \underbrace{\sum_i (1 - C_{ii})^2}_{\text{不变性}} + \lambda \underbrace{\sum_{i \neq j} C_{ij}^2}_{\text{去相关}}$$

- **对角线 = 1**：同一维度在两个视图中保持一致（不变性）
- **非对角线 = 0**：不同维度之间去相关（冗余减少）

**与其他方法的关系**：
- VICReg 将 Barlow Twins 分解为 3 个显式项（方差/不变性/协方差）
- 和 PCA/白化的联系：去相关本质上是在特征空间做白化

### Q9：为什么 CLIP 使用 sigmoid 损失（SigLIP）比 softmax 更好？

**答案要点**：

**Softmax 损失的问题**：
1. 需要 **全局归一化**：softmax 的分母需要所有样本的相似度之和
2. 分布式训练时需要 **all-gather**：把所有 GPU 的表征收集起来
3. 内存开销：$O(B^2)$ 的相似度矩阵，B=32K 时巨大
4. 对温度参数敏感

**Sigmoid 损失的优势**：
1. **Pairwise 计算**：每对样本独立计算 $\sigma(s_{ij})$，不需要全局归一化
2. **分布式友好**：可以分块（chunk-wise）计算，不需要 all-gather 所有表征
3. **内存高效**：可以流式处理
4. **温度不敏感**：sigmoid 的饱和特性使其对温度变化更鲁棒

**性能**：SigLIP ViT-L 在 zero-shot ImageNet 上 82.1%（vs. CLIP 80.2%），训练效率更高。

### Q10：自监督学习中"pretext task"的设计原则是什么？

**答案要点**：

好的 pretext task 应满足：

1. **任务不可平凡解（non-trivial）**：
   - 不能通过低层次的捷径解决
   - 例：旋转预测可以通过检测重力方向"作弊" → 不够好
   - 例：MAE 75% 掩码 → 不能简单插值 → 强制学语义

2. **解决任务需要高层语义理解**：
   - 对比学习：区分同一图片的不同视图 vs 不同图片 → 需要理解内容
   - MLM：根据上下文预测缺失词 → 需要理解语法和语义

3. **与下游任务的表征需求对齐**：
   - 判别任务需要类别级特征 → 对比学习
   - 稠密预测需要空间细粒度特征 → MIM

4. **计算效率合理**：
   - MAE 只编码 25% patch → 效率极高
   - SimCLR 需要大 batch → 效率较低

5. **数据增强不引入错误监督信号**：
   - 水平翻转 OK（语义不变）
   - 垂直翻转可能不 OK（人脸颠倒后语义变化大）

### Q11：请解释 VICReg 的三个目标项及其作用。

**答案要点**：

VICReg（Variance-Invariance-Covariance Regularization）用三个显式正则化项替代负样本：

1. **Invariance（不变性）**：$s(Z^A, Z^B) = \frac{1}{N}\sum_i \|z_i^A - z_i^B\|^2$
   - 正样本对的表征应接近
   - 类似对比学习的"拉近正样本"

2. **Variance（方差）**：$v(Z) = \frac{1}{D}\sum_j \max(0, 1 - \sqrt{\text{Var}(z_{:,j})+\epsilon})$
   - 每个维度的标准差应大于 1
   - **防止完全坍缩**（所有样本映射到同一点）
   - 类似于 BatchNorm 的效果

3. **Covariance（协方差）**：$c(Z) = \frac{1}{D}\sum_{i\neq j} \text{Cov}(z_{:,i}, z_{:,j})^2$
   - 不同维度之间应去相关
   - **防止维度坍缩**（只用少数维度编码信息）
   - 鼓励信息分散在所有维度中

**直觉**：不变性=学什么，方差=不坍缩，协方差=用好每个维度。

### Q12：比较 DINOv2 和 CLIP 作为视觉 backbone 的优劣。

**答案要点**：

| 维度 | DINOv2 | CLIP |
|------|--------|------|
| 训练数据 | 纯图像 (LVD-142M) | 图文对 (400M+) |
| 训练信号 | 自蒸馏 + MIM | 图文对比 |
| Zero-Shot | ❌ 不支持 | ✅ 天然支持 |
| 线性探测 (IN-1K) | 86.5% ⭐ | 83.8% |
| 稠密预测 | ⭐ 非常强 (分割/深度) | 一般 |
| 多模态任务 | 需要加 text encoder | ✅ 原生支持 |
| 表征通用性 | ⭐ 多任务表现均匀 | 偏向分类/检索 |
| 部署成本 | ViT-g 较大 | 多种大小可选 |

**选择指导**：
- 需要 zero-shot 或多模态 → CLIP/SigLIP
- 需要通用视觉特征（分割/深度等）→ DINOv2
- 2025+ 的趋势：InternVL 等方法尝试融合两者优势

### Q13：请解释 SimCSE 的无监督和有监督版本。

**答案要点**：

**无监督 SimCSE**：
- 正样本对：同一句子的两次不同 dropout mask 的输出
- 负样本：batch 内其他句子
- 损失：InfoNCE
- 关键洞察：dropout 是一种最小侵入性的数据增强

```python
# 无监督 SimCSE
z1 = model(sentence, dropout_rate=0.1)  # dropout mask 1
z2 = model(sentence, dropout_rate=0.1)  # dropout mask 2（自动不同）
loss = info_nce(z1, z2, temperature=0.05)
```

**有监督 SimCSE**：
- 正样本对：NLI 数据集中的蕴含对 (entailment)
- 硬负样本：NLI 中的矛盾对 (contradiction)
- 效果更好（+3-5%），因为 NLI 数据提供了更有意义的语义正/负样本

**核心贡献**：
1. 揭示了预训练语言模型表征的 **各向异性问题**
2. 对比学习可以显著改善句子表征的均匀性和对齐性
3. 简单到极致的方法（dropout 即增强）反而效果最好

### Q14：为什么 Vision Transformer 需要 Register Token？

**答案要点**：

**问题发现**（Darcet et al. 2024）：
在 DINO/DINOv2 训练的 ViT 中，某些 patch token 的 norm 异常大（称为 "artifact tokens"）。这些 token：
- 出现在低信息量的背景区域
- 吸收了大量 attention（高 attention weight）
- 充当了"全局信息寄存器"的角色

**危害**：
- Attention map 不再可解释（热力图出现异常亮点）
- 稠密预测任务受影响（分割/深度质量下降）

**解决方案**：
添加 $R$ 个可学习的 register token（如 R=4），让模型有显式的"寄存器"来存储全局信息。

**结果**：
- Attention map 更干净，伪影消失
- 稠密预测性能提升
- 计算开销可忽略（仅多 4 个 token）

**深层洞察**：这揭示了 Transformer 的一个有趣行为——当缺少显式的全局信息存储机制时，模型会自发地"征用"某些 patch token 作为寄存器。

### Q15：对比学习在 LLM 时代还有什么用？

**答案要点**：

虽然 LLM 主流使用 CLM（自回归），但对比学习在以下场景仍然关键：

1. **Embedding 模型**：
   - text-embedding-3 (OpenAI)、bge-m3 (BAAI) 等
   - 用对比学习训练句子级表征
   - 用于 RAG 检索、语义搜索

2. **多模态对齐**：
   - CLIP/SigLIP 仍是多模态模型的视觉 backbone
   - InternVL、LLaVA 等 MLLM 的视觉编码器

3. **偏好学习（RLHF/DPO 的隐式对比）**：
   - DPO 损失在数学上等价于 pairwise 对比损失
   - 正样本 = preferred response，负样本 = rejected response

4. **表征学习**：
   - 推荐系统的用户/物品表征
   - 代码相似度（CodeSearchNet）
   - 分子/蛋白质相似度

5. **数据质量评估**：
   - 用对比学习的表征做数据去重、相似度过滤

---

## 十二、参考文献

### 对比学习基础

1. van den Oord, A., Li, Y., & Vinyals, O. (2018). "Representation Learning with Contrastive Predictive Coding." *arXiv:1807.03748*
2. Chen, T., Kornblith, S., Norouzi, M., & Hinton, G. (2020). "A Simple Framework for Contrastive Learning of Visual Representations." *ICML 2020*. (SimCLR)
3. Chen, T., Kornblith, S., Swersky, K., Norouzi, M., & Hinton, G. (2020). "Big Self-Supervised Models are Strong Semi-Supervised Learners." *NeurIPS 2020*. (SimCLR v2)
4. He, K., Fan, H., Wu, Y., Xie, S., & Girshick, R. (2020). "Momentum Contrast for Unsupervised Visual Representation Learning." *CVPR 2020*. (MoCo)
5. Chen, X., Fan, H., Girshick, R., & He, K. (2020). "Improved Baselines with Momentum Contrastive Learning." *arXiv:2003.04297*. (MoCo v2)
6. Chen, X., Xie, S., & He, K. (2021). "An Empirical Study of Training Self-Supervised Vision Transformers." *ICCV 2021*. (MoCo v3)

### 非对比方法

7. Grill, J.-B., et al. (2020). "Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning." *NeurIPS 2020*. (BYOL)
8. Chen, X. & He, K. (2021). "Exploring Simple Siamese Representation Learning." *CVPR 2021*. (SimSiam)
9. Zbontar, J., Jing, L., Misra, I., LeCun, Y., & Deny, S. (2021). "Barlow Twins: Self-Supervised Learning via Redundancy Reduction." *ICML 2021*.
10. Bardes, A., Ponce, J., & LeCun, Y. (2022). "VICReg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning." *ICLR 2022*.

### 掩码图像建模

11. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., & Girshick, R. (2022). "Masked Autoencoders Are Scalable Self-Supervised Learners." *CVPR 2022*. (MAE)
12. Bao, H., Dong, L., Piao, S., & Wei, F. (2022). "BEiT: BERT Pre-Training of Image Transformers." *ICLR 2022*.
13. Zhou, J., et al. (2022). "iBOT: Image BERT Pre-Training with Online Tokenizer." *ICLR 2022*.
14. Fang, Y., et al. (2023). "EVA: Exploring the Limits of Masked Visual Representation Learning at Scale." *CVPR 2023*.
15. Fang, Y., et al. (2024). "EVA-02: A Visual Representation for Neon Genesis." *IJCV 2024*.

### 视觉自监督 Transformer

16. Caron, M., et al. (2021). "Emerging Properties in Self-Supervised Vision Transformers." *ICCV 2021*. (DINO)
17. Oquab, M., et al. (2023). "DINOv2: Learning Robust Visual Features without Supervision." *TMLR 2024*.
18. Darcet, T., Oquab, M., Mairal, J., & Bojanowski, P. (2024). "Vision Transformers Need Registers." *ICLR 2024*.

### NLP 自监督

19. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding." *NAACL 2019*.
20. Radford, A., et al. (2019). "Language Models are Unsupervised Multitask Learners." OpenAI. (GPT-2)
21. Brown, T., et al. (2020). "Language Models are Few-Shot Learners." *NeurIPS 2020*. (GPT-3)
22. Raffel, C., et al. (2020). "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer." *JMLR 2020*. (T5)
23. Clark, K., et al. (2020). "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators." *ICLR 2020*.
24. He, P., et al. (2021). "DeBERTa: Decoding-enhanced BERT with Disentangled Attention." *ICLR 2021*.
25. Gao, T., Yao, X., & Chen, D. (2021). "SimCSE: Simple Contrastive Learning of Sentence Embeddings." *EMNLP 2021*.

### 多模态对比学习

26. Radford, A., et al. (2021). "Learning Transferable Visual Models From Natural Language Supervision." *ICML 2021*. (CLIP)
27. Jia, C., et al. (2021). "Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision." *ICML 2021*. (ALIGN)
28. Zhai, X., et al. (2023). "Sigmoid Loss for Language Image Pre-Training." *ICCV 2023*. (SigLIP)
29. Yu, J., et al. (2022). "CoCa: Contrastive Captioners are Image-Text Foundation Models." *TMLR 2022*.
30. Yuan, L., et al. (2022). "Florence: A New Foundation Model for Computer Vision." *arXiv:2111.11432*.
31. Chen, Z., et al. (2024). "InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks." *CVPR 2024*.

### 前沿方向

32. Tong, Z., et al. (2022). "VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training." *NeurIPS 2022*.
33. Bardes, A., Garrido, Q., Ponce, J., Chen, X., Rabbat, M., LeCun, Y., Assran, M., & Balestriero, R. (2024). "V-JEPA: Latent Video Prediction for Visual Representation Learning." *arXiv:2404.08471*.
34. Baevski, A., et al. (2020). "wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations." *NeurIPS 2020*.
35. Girdhar, R., et al. (2023). "ImageBind: One Embedding Space To Bind Them All." *CVPR 2023*.

### 理论分析

36. Wang, T. & Isola, P. (2020). "Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere." *ICML 2020*.
37. Tian, Y., Chen, X., & Ganguli, S. (2021). "Understanding Self-Supervised Learning Dynamics without Contrastive Pairs." *ICML 2021*.
38. Hoffmann, J., et al. (2022). "Training Compute-Optimal Large Language Models." *NeurIPS 2022*. (Chinchilla)

---

> **导航**：[[预训练原理|预训练原理]] | [[LLM-预训练与分布式训练-2026-全景|LLM 预训练与分布式训练]] | [[Transformer架构深度解析-2026技术全景|Transformer 架构深度解析]] | [[计算机视觉基础与前沿-2026技术全景|CV 2026 技术全景]]