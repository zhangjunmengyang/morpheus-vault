---
brief: "LLM 微调实战 2026 技术全景——LoRA/QLoRA/Full Fine-tuning/DPO 完整对比；从数据准备到部署的工程全链路；面试武器库标注，包含设计题参考答案（为什么用 LoRA 不用全量/QLoRA 的 NF4 原理/SFT 数据质量 vs 数量取舍）。"
title: "LLM 微调实战 - 2026 技术全景"
date: 2026-02-21
type: landscape
domain: ai/llm/finetuning
rating: ★★★★★
tags:
  - ai/llm/finetuning
  - interview/weapon
  - peft
  - lora
  - qlora
  - rlhf
  - dpo
  - sft
status: active
---

# LLM 微调实战 — 2026 技术全景

> "Pre-training gives you the world model; fine-tuning gives you the interface."
> 预训练赋予世界模型，微调赋予交互界面。

本笔记面向 AI 算法工程师面试，全栈覆盖 LLM 微调技术：从全参数微调到参数高效方法（LoRA/QLoRA/DoRA），从 SFT 数据工程到 RLHF/DPO 对齐，从单卡训练到分布式微调，从经典方法到 2026 前沿（MoLoRA/ReFT/LoRA Merging）。每个主题附带核心公式、关键论文、面试题深度答案与实战 tips。末尾附 18 道难度递进面试题 + 完整附录。

---

## 目录

1. [全参数微调 vs 参数高效微调 (PEFT)](#1-全参数微调-vs-参数高效微调-peft)
2. [LoRA 深度解析](#2-lora-深度解析)
3. [QLoRA：4-bit 量化微调](#3-qlora4-bit-量化微调)
4. [LoRA 变体家族](#4-lora-变体家族)
5. [Prefix Tuning / Prompt Tuning / P-Tuning 系列](#5-prefix-tuning--prompt-tuning--p-tuning-系列)
6. [Adapter 方法](#6-adapter-方法)
7. [SFT 实战：数据工程与训练策略](#7-sft-实战数据工程与训练策略)
8. [RLHF 与偏好对齐](#8-rlhf-与偏好对齐)
9. [对齐税与灾难性遗忘](#9-对齐税与灾难性遗忘)
10. [训练框架对比](#10-训练框架对比)
11. [分布式微调](#11-分布式微调)
12. [2026 前沿技术](#12-2026-前沿技术)
13. [面试题集（18 道，难度递进）](#13-面试题集)
14. [附录](#14-附录)
15. [参考文献](#15-参考文献)

---

## 1. 全参数微调 vs 参数高效微调 (PEFT)

### 1.1 全参数微调 (Full Fine-Tuning, FFT)

**核心思想**：更新模型所有参数 $\theta$，使用任务数据 $\mathcal{D}$ 最小化损失函数。

$$\theta^* = \arg\min_\theta \sum_{(x,y) \in \mathcal{D}} \mathcal{L}(f_\theta(x), y)$$

**优点**：
- 理论上界最高，能充分适配下游任务
- 无结构限制，可以改变模型任何表示
- 当训练数据足够大（>100K 高质量样本）时效果最优

**缺点**：
- 显存需求巨大：7B 模型 FFT 需要 ~56GB（AdamW 优化器需要参数 + 梯度 + 两个动量状态 = 4× 参数量，BF16 下 7B × 2B × 4 = 56GB）
- 每个下游任务需要存储完整模型副本
- 灾难性遗忘风险高——模型偏离预训练分布
- 训练成本高：7B 模型 FFT 单次训练 8×A100 约需 2-4 小时（取决于数据量）

### 1.2 参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT)

**核心思想**：冻结大部分预训练参数 $\theta_0$，仅训练少量额外参数 $\Delta\theta$。

$$f_{\theta_0 + \Delta\theta}(x), \quad |\Delta\theta| \ll |\theta_0|$$

**PEFT 方法分类**：

| 类别 | 方法 | 可训练参数占比 | 代表 |
|------|------|--------------|------|
| 加法 (Additive) | Adapter / Prefix Tuning | 0.1-3% | Houlsby 2019 |
| 重参数化 (Reparameterization) | LoRA / DoRA | 0.1-1% | Hu et al. 2021 |
| 选择性 (Selective) | BitFit / Diff Pruning | 0.05-0.5% | Zaken et al. 2022 |
| 混合 (Hybrid) | MAM Adapter / UniPELT | 0.5-2% | He et al. 2022 |

### 1.3 选择指南：FFT vs PEFT

```
决策流程：
1. 训练数据 > 100K 且有充足算力？ → 考虑 FFT
2. 单卡 24GB/48GB？ → LoRA / QLoRA
3. 需要多任务部署？ → LoRA（热插拔 adapter）
4. 推理零延迟要求？ → LoRA（可合并进基座）
5. 极限低资源 (<16GB)？ → QLoRA 4-bit
6. 预训练数据领域差异大？ → 考虑 FFT 或 大秩 LoRA
```

**关键论文**：
- [1] Houlsby et al., "Parameter-Efficient Transfer Learning for NLP," ICML 2019
- [2] Ding et al., "Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models," 2022

**实战 Tips**：
- 即使选 PEFT，也建议先做小规模 FFT 基线对比
- 7B 模型 LoRA(r=16) 通常能达到 FFT 95%+ 的效果
- 超大规模模型（>70B）几乎只能 PEFT，FFT 成本不可接受

---

## 2. LoRA 深度解析

### 2.1 数学原理

**核心思想**：预训练权重矩阵 $W_0 \in \mathbb{R}^{d \times k}$ 的更新量 $\Delta W$ 具有低秩结构，可分解为两个小矩阵的乘积。

$$h = W_0 x + \Delta W x = W_0 x + \frac{\alpha}{r} B A x$$

其中：
- $A \in \mathbb{R}^{r \times k}$：降维矩阵（初始化为 Kaiming/Gaussian）
- $B \in \mathbb{R}^{d \times r}$：升维矩阵（初始化为零矩阵）
- $r \ll \min(d, k)$：秩（rank），通常 4-64
- $\alpha$：缩放因子（scaling factor）
- $\frac{\alpha}{r}$：实际缩放比例

**为什么 B 初始化为零？**
- 训练开始时 $\Delta W = BA = 0$，模型行为等同预训练模型
- 避免随机初始化破坏预训练表示
- 等价于从预训练权重出发做增量学习

**参数效率计算**：

$$\text{LoRA 参数量} = 2 \times r \times d \times L_{\text{target}}$$

以 LLaMA-7B 为例（d=4096, 32 layers, 对 Q/K/V/O 四个矩阵加 LoRA）：
$$2 \times 16 \times 4096 \times 4 \times 32 = 16,777,216 \approx 16.7M \quad (0.24\% \text{ of } 7B)$$

### 2.2 秩（Rank）选择

秩 $r$ 是 LoRA 最关键的超参数。低秩假设的核心是：**微调所需的权重更新存在于低维子空间中**。

| 秩 (r) | 适用场景 | 参数占比 (7B) | 典型效果 |
|---------|---------|--------------|---------|
| 4-8 | 简单分类/NER | ~0.06-0.12% | 基线 |
| 16 | 通用 SFT | ~0.24% | 甜蜜点 |
| 32-64 | 复杂推理/数学 | 0.5-1% | 逼近 FFT |
| 128-256 | 跨域迁移/长文本 | 2-4% | 接近 FFT |

**秩选择经验法则**：
1. **任务复杂度**：越复杂 → 秩越高。简单情感分类 r=4 足够，数学推理可能需要 r=64+
2. **领域差异**：预训练数据与下游数据差异越大 → 秩越高
3. **数据量**：数据少时低秩有正则化效果，数据多时可以提高秩
4. **Aggarwal et al. (2024)** 的实验表明：r=16 是大多数 NLP 任务的 Pareto 最优

### 2.3 Alpha (α) 调参

$\alpha$ 控制 LoRA 更新的幅度：

$$\Delta W = \frac{\alpha}{r} BA$$

**关键理解**：
- $\alpha/r$ 是有效缩放因子，而非 $\alpha$ 本身
- 常见设置：$\alpha = 2r$（即有效缩放 = 2）或 $\alpha = r$（有效缩放 = 1）
- $\alpha$ 过大 → 训练不稳定，过小 → 收敛慢

**实战调参策略**：
```python
# 经验公式：先设 alpha = 2 * r，再微调
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,      # alpha = 2r
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                     "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
)
```

**α 与学习率的关系**：
- 实际更新幅度 ∝ $\frac{\alpha}{r} \times \text{lr}$
- 改变 r 时，若保持 α 不变，需同比调整 lr
- 更好的做法：固定 α/r 比例，独立调 lr

### 2.4 目标模块选择

| 目标模块 | 效果 | 推荐度 |
|---------|------|--------|
| Q, V only | 原论文默认 | ★★★ |
| Q, K, V, O | 更好效果 | ★★★★ |
| Q, K, V, O + MLP (gate, up, down) | 全层最优 | ★★★★★ |
| Embedding + LM Head | 词表适配 | 新语言/领域时加 |

**实验结论（2024-2025 年社区共识）**：
- 对所有线性层加 LoRA（attention + MLP）效果最优
- 边际收益最大的是从 Q/V → 扩展到 MLP 层
- Embedding 层的 LoRA 对新领域（如代码、数学公式）特别有效

### 2.5 实现细节与注意事项

**LoRA Dropout**：
- 在 A 矩阵输入前加 dropout
- 推荐 0.05-0.1，起正则化作用
- 数据量大时可设为 0

**合并（Merge）与推理**：
```python
# 训练完成后合并到基座模型
merged_model = model.merge_and_unload()
# 合并后推理速度 = 原始模型，零额外延迟
# W_new = W_0 + (alpha/r) * B @ A
```

**多 LoRA 服务**：
- 基座模型共享，不同任务加载不同 LoRA adapter
- vLLM / SGLang 已原生支持多 LoRA 并发服务
- 内存开销 = 基座 + N × LoRA_size（通常 10-50MB/adapter）

**关键论文**：
- [3] Hu et al., "LoRA: Low-Rank Adaptation of Large Language Models," ICLR 2022
- [4] Dettmers et al., "QLoRA: Efficient Finetuning of Quantized LLMs," NeurIPS 2023

**面试题**：见 [§13 Q1-Q4](#q1-lora-的数学原理是什么为什么有效)

---

## 3. QLoRA：4-bit 量化微调

### 3.1 核心创新

QLoRA 通过三个关键技术实现 4-bit 量化下的高效微调：

**技术一：NormalFloat4 (NF4) 量化**

NF4 是一种信息论最优的 4-bit 数据类型，假设权重服从正态分布 $\mathcal{N}(0, \sigma^2)$。

量化过程：
1. 将正态分布的 CDF 均匀分成 $2^4 = 16$ 个分位数
2. 每个量化区间包含等概率的权重值
3. 量化值 = 各区间的中点

$$q_i = \Phi^{-1}\left(\frac{2i+1}{2 \times 2^b}\right), \quad i = 0, 1, \ldots, 2^b - 1$$

**NF4 vs INT4**：NF4 对正态分布数据的量化误差更小（理论最优），INT4 是均匀量化。

**技术二：双重量化 (Double Quantization)**

对量化常数（每 64 个权重一个 FP32 scale factor）再做一次 8-bit 量化：
- 原始：每 64 个权重 → 1 个 FP32 scale = 0.5 bit/weight 开销
- 双重量化后：开销降至 ~0.127 bit/weight
- 对 7B 模型节省约 0.4GB 显存

**技术三：分页优化器 (Paged Optimizers)**

利用 NVIDIA 的统一内存（Unified Memory）：
- 优化器状态在 GPU 内存不足时自动迁移到 CPU 内存
- 序列长度波动大时避免 OOM
- 类似操作系统的虚拟内存分页机制

### 3.2 显存分析

以 LLaMA-7B 为例：

| 组件 | FFT (BF16) | LoRA (BF16) | QLoRA (NF4) |
|------|-----------|-------------|-------------|
| 模型参数 | 14 GB | 14 GB | **3.5 GB** |
| LoRA 参数 | - | ~33 MB | ~33 MB |
| 梯度 | 14 GB | ~33 MB | ~33 MB |
| 优化器状态 | 28 GB | ~66 MB | ~66 MB |
| **总计** | **~56 GB** | **~14.1 GB** | **~3.6 GB** |

> QLoRA 使 7B 模型微调可在**单张 RTX 4090 (24GB)** 上完成，甚至 **RTX 3090** 也可行。

### 3.3 QLoRA 训练流程

```python
from transformers import AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# Step 1: 4-bit 加载
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",        # NF4 量化
    bnb_4bit_compute_dtype=torch.bfloat16,  # 计算精度
    bnb_4bit_use_double_quant=True,    # 双重量化
)

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B",
    quantization_config=bnb_config,
    device_map="auto",
)

# Step 2: 准备 k-bit 训练
model = prepare_model_for_kbit_training(model)
# 关键操作：启用梯度检查点 + 冻结量化层 + 上投射到 BF16 计算

# Step 3: 加 LoRA
lora_config = LoraConfig(r=16, lora_alpha=32, ...)
model = get_peft_model(model, lora_config)

# Step 4: 正常训练
trainer = SFTTrainer(model=model, ...)
trainer.train()
```

### 3.4 QLoRA 的局限性

1. **训练速度下降 20-30%**：4-bit → BF16 的反量化计算有开销
2. **效果略微下降**：量化误差会传播到 LoRA 梯度（通常 <1% 差距）
3. **不支持多卡分片**：bitsandbytes 的 4-bit 量化与 FSDP 不兼容（截至 2025 年底）
4. **合并复杂**：需要先反量化基座权重，再合并 LoRA，再重新量化

**关键论文**：
- [4] Dettmers et al., "QLoRA: Efficient Finetuning of Quantized LLMs," NeurIPS 2023
- [5] Dettmers & Zettlemoyer, "The case for 4-bit precision: k-bit Inference Scaling Laws," ICML 2023

**实战 Tips**：
- `bnb_4bit_compute_dtype` 必须设为 `bfloat16`，用 `float16` 会有数值问题
- QLoRA 训练时 batch size 设小（因为省了模型显存，但 activation 没省）
- Unsloth 库对 QLoRA 做了内核优化，速度提升 2-4×
- 长序列场景用 gradient checkpointing 进一步省显存

---

## 4. LoRA 变体家族

### 4.1 DoRA (Weight-Decomposed Low-Rank Adaptation)

**核心思想**：将权重分解为**方向 (Direction)** 和**大小 (Magnitude)** 两个分量，分别学习。

$$W = m \cdot \frac{W_0 + BA}{\|W_0 + BA\|_c}$$

其中：
- $m \in \mathbb{R}^{1 \times k}$：可训练的大小向量（列归一化后的缩放）
- $\frac{W_0 + BA}{\|W_0 + BA\|_c}$：方向分量（列归一化）
- $\|\cdot\|_c$：逐列 L2 范数

**灵感来源**：Weight Normalization (Salimans & Kingma, 2016) 的思想——解耦权重的方向和大小可以改善优化景观。

**为什么 DoRA 比 LoRA 更好？**
- FFT 的更新中，方向变化远大于大小变化
- LoRA 将两者耦合学习，低秩约束会限制表达力
- DoRA 单独用一个向量 m 学大小（参数量极小），低秩部分专注学方向变化
- 实验：DoRA 在 r=16 时接近 LoRA r=64 的效果

**参数增加量**：仅多一个 $d$-维向量（negligible，如 4096 个参数）

**关键论文**：
- [6] Liu et al., "DoRA: Weight-Decomposed Low-Rank Adaptation," ICML 2024

### 4.2 LoRA+ (Differential Learning Rates)

**核心思想**：给 A 和 B 矩阵设置不同的学习率。

$$\eta_B = \lambda \cdot \eta_A, \quad \lambda \gg 1$$

**理论依据**：
- A 矩阵（降维）需要学习输入特征的选择/投影
- B 矩阵（升维）需要学习输出空间的映射
- 最优学习率比例 $\lambda$ 取决于模型宽度 $d$
- 推荐 $\lambda = 16$（B 的学习率是 A 的 16 倍）

**效果**：
- 在 RoBERTa/LLaMA 上比标准 LoRA 提升 1-2%
- 几乎零额外成本，只是改变学习率配置

**关键论文**：
- [7] Hayou et al., "LoRA+: Efficient Low Rank Adaptation of Large Models," ICML 2024

### 4.3 rsLoRA (Rank-Stabilized LoRA)

**核心思想**：修正 LoRA 的缩放因子，使不同秩的配置具有可比性。

标准 LoRA：$\Delta W = \frac{\alpha}{r} BA$

rsLoRA：$\Delta W = \frac{\alpha}{\sqrt{r}} BA$

**理论依据**：
- 当 $r$ 增大时，$BA$ 的输出方差随 $r$ 线性增长
- $\frac{1}{r}$ 缩放在不同 $r$ 之间不具有一致性
- $\frac{1}{\sqrt{r}}$ 缩放保持输出方差恒定（类比 attention 中的 $\frac{1}{\sqrt{d_k}}$）

**实际意义**：
- 使用 rsLoRA 后，增加秩不需要重新调学习率
- 高秩（r=64+）场景下效果提升更明显

**关键论文**：
- [8] Kalajdzievski, "A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA," 2023

### 4.4 AdaLoRA (Adaptive Budget Allocation)

**核心思想**：不同层/模块对微调的贡献不同，**自适应分配秩预算**。

$$\Delta W = P \Lambda Q$$

其中 $P$, $Q$ 是正交矩阵，$\Lambda$ 是对角矩阵（奇异值）。

通过 importance score 动态剪枝小奇异值：
$$S_i = s_i + \beta \cdot \frac{\partial \mathcal{L}}{\partial s_i}$$

低重要性的奇异值被置零，对应秩减少，预算重新分配给重要的层。

**效果**：
- 相同总参数量下比均匀分配秩的 LoRA 效果更好
- 自动发现"哪些层需要更多表达力"
- 缺点：训练更复杂，需要维护 SVD 参数化

**关键论文**：
- [9] Zhang et al., "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning," ICLR 2023

### 4.5 变体对比总结

| 方法 | 核心改进 | 额外开销 | 效果提升 | 推荐度 |
|------|---------|---------|---------|--------|
| LoRA | 基线 | 基线 | 基线 | ★★★★ |
| DoRA | 方向/大小解耦 | +0.01% 参数 | +1-3% | ★★★★★ |
| LoRA+ | 差异学习率 | 零 | +1-2% | ★★★★★ |
| rsLoRA | 秩稳定缩放 | 零 | 高秩时明显 | ★★★★ |
| AdaLoRA | 自适应秩分配 | SVD 参数化 | +1-2% | ★★★ |
| VeRA | 共享随机矩阵 | 参数量极低 | 略低于 LoRA | ★★★ |

**实战建议**：2026 年新项目推荐 **DoRA + LoRA+** 组合，零额外成本，效果稳定提升。

---

## 5. Prefix Tuning / Prompt Tuning / P-Tuning 系列

### 5.1 Prefix Tuning

**核心思想**：在每一层 Transformer 的 Key/Value 前拼接可训练的前缀向量。

$$\text{head}_i = \text{Attn}(xW_Q, [\underbrace{P_K^{(i)}}_{\text{prefix}}; xW_K], [\underbrace{P_V^{(i)}}_{\text{prefix}}; xW_V])$$

- 每层有独立的前缀参数 $P_K^{(i)}, P_V^{(i)} \in \mathbb{R}^{l_p \times d}$
- $l_p$：前缀长度（通常 10-100 tokens）
- 通过 MLP 重参数化训练，训练后丢弃 MLP

**参数量**：$2 \times l_p \times d \times L$（每层 K/V 各一组）

以 LLaMA-7B, $l_p=20$：$2 \times 20 \times 4096 \times 32 = 5.24M \approx 0.075\%$

**关键论文**：
- [10] Li & Liang, "Prefix-Tuning: Optimizing Continuous Prompts for Generation," ACL 2021

### 5.2 Prompt Tuning

**核心思想**：仅在输入嵌入层添加可训练的 soft prompt，而非每层。

$$h_0 = [\underbrace{P}_{\text{soft prompt}}; \text{Emb}(x)]$$

- 参数量 = $l_p \times d$（极少，如 20 × 4096 = 81K）
- 当模型规模 >10B 时，效果逼近 FFT（"Prompt Tuning 的缩放定律"）
- 小模型（<3B）上效果差距明显

**Prompt Tuning v2 (Deep Prompt Tuning)**：
- 在每层都加 soft prompt（等价于 Prefix Tuning 的简化版）
- 效果介于 Prompt Tuning 和 Prefix Tuning 之间

**关键论文**：
- [11] Lester et al., "The Power of Scale for Parameter-Efficient Prompt Tuning," EMNLP 2021

### 5.3 P-Tuning v2

**核心思想**：Deep Prompt Tuning 的实现变体，在每层添加可训练前缀，并针对 NLU 任务优化。

与 Prefix Tuning 的区别：
- P-Tuning v2 去掉了 MLP 重参数化（直接训练前缀）
- 更适合分类/NLU 任务
- 在小模型（330M-10B）上效果更好

**关键论文**：
- [12] Liu et al., "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks," ACL 2022

### 5.4 系列对比

| 方法 | 修改位置 | 参数量 | 推理延迟 | 小模型效果 | 大模型效果 |
|------|---------|--------|---------|-----------|-----------|
| Prompt Tuning | 仅输入层 | 极少 | 极低 | 差 | 好 |
| Prefix Tuning | 每层 K/V | 少 | 中（前缀占 attention） | 中 | 好 |
| P-Tuning v2 | 每层前缀 | 少 | 中 | 好 | 好 |
| LoRA | 权重矩阵 | 少 | **零**（可合并） | 好 | 好 |

**2026 共识**：Prefix/Prompt 系列在 LoRA 面前已基本退出主流。LoRA 的优势在于可合并导致推理零延迟，且效果更稳定。Prefix 类方法的唯一优势是参数量更小（对需要 serve 数千个 adapter 的多租户场景有价值）。

**实战 Tips**：
- 如果面试官问"为什么不用 Prefix Tuning"，关键论点是：推理延迟 + 效果不如 LoRA
- Prefix 类方法需要占用 context window（前缀占 token 位置），这在 context 宝贵的场景下是硬伤

---

## 6. Adapter 方法

### 6.1 Bottleneck Adapter (Houlsby Adapter)

**核心思想**：在 Transformer 每层中插入小型瓶颈模块（bottleneck）。

```
Adapter 结构：
Input (d) → Down-project (d→r) → Nonlinearity (ReLU/GELU) → Up-project (r→d) → Residual Add
```

$$\text{Adapter}(x) = x + f(xW_{\text{down}})W_{\text{up}}$$

- 参数量：$2 \times d \times r + r$（下投影 + 上投影 + bias）
- 原始论文在 Self-Attention 和 FFN **之后各插一个** Adapter
- 后续工作发现只在 FFN 后插一个效果相当

**关键论文**：
- [1] Houlsby et al., "Parameter-Efficient Transfer Learning for NLP," ICML 2019

### 6.2 Parallel Adapter

**核心思想**：将 Adapter 与原始层**并行**而非串行连接。

$$h = \text{FFN}(x) + s \cdot \text{Adapter}(x)$$

$s$ 是可学习的缩放因子。

**优势**：
- 避免串行 Adapter 的梯度消失问题
- 并行计算效率更高
- 实验表明并行优于串行

**关键论文**：
- [13] He et al., "Towards a Unified View of Parameter-Efficient Transfer Learning," ICLR 2022

### 6.3 Adapter vs LoRA

| 维度 | Adapter | LoRA |
|------|---------|------|
| 推理延迟 | 有（额外前向路径） | 零（可合并） |
| 结构修改 | 增加新模块 | 不改变架构 |
| 表达力 | 有非线性 | 纯线性 |
| 多任务服务 | 需要额外计算 | 仅需加载权重 |
| 2026 流行度 | 低 | 高 |

**面试要点**：Adapter 方法的非线性是其理论优势（比 LoRA 的线性变换表达力更强），但推理延迟的硬伤使其在工业界几乎被 LoRA 取代。

---

## 7. SFT 实战：数据工程与训练策略

### 7.1 SFT 的本质

SFT (Supervised Fine-Tuning) 是将预训练 LLM 从"文本补全器"变为"指令遵循者"的关键步骤。

$$\mathcal{L}_{\text{SFT}} = -\sum_{t=1}^{T} \mathbb{1}[t \in \text{response}] \cdot \log p_\theta(y_t | x, y_{<t})$$

注意：**只在 response 部分计算 loss**，instruction/system prompt 部分被 mask 掉。

### 7.2 数据格式与 Chat Template

**标准指令格式**：
```json
{
  "messages": [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "解释量子计算"},
    {"role": "assistant", "content": "量子计算是..."}
  ]
}
```

**Chat Template 的关键性**：
```
# LLaMA 3 格式
<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are a helpful assistant.<|eot_id|>
<|start_header_id|>user<|end_header_id|>
解释量子计算<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>
量子计算是...<|eot_id|>
```

**常见错误**：
1. ❌ 用错 Chat Template → 模型输出乱码/性能骤降
2. ❌ 在 instruction 部分也计算 loss → 模型学会复述问题而非回答
3. ❌ 忘记加 EOS token → 模型不知道何时停止生成
4. ❌ 多轮对话没有正确 mask → 模型学到"偷看"后续轮次

### 7.3 数据质量 vs 数量

**LIMA 定律 (Zhou et al., 2023)**：
> "1000 条精心标注的高质量数据 > 100K 条低质量数据"

**关键实验数据**：
- LIMA：仅 1000 条高质量数据微调 LLaMA-65B，效果超越 Alpaca (52K)
- Phi-1：1.3B 参数 + 高质量教科书数据 → 在代码任务上超越 10× 大的模型
- "Less Is More for Alignment" (Chen et al., 2024)：质量过滤后的 6K 条数据效果最优

**数据质量评估维度**：

| 维度 | 说明 | 检查方法 |
|------|------|---------|
| 准确性 | 回答事实正确 | 人工抽检 + LLM 评估 |
| 多样性 | 覆盖多种任务类型 | 聚类分析分布 |
| 复杂度 | 包含推理链、多步骤 | 回答长度分布 + 标注 |
| 格式一致性 | 统一的格式规范 | 正则匹配 + 自动化检查 |
| 无毒无害 | 不含有害内容 | Reward Model 打分 |

### 7.4 数据构建实战

**高质量 SFT 数据构建流程**：

```
1. 种子数据收集（人工标注 500-1000 条种子）
     ↓
2. LLM 辅助扩展（用 GPT-4/Claude 生成变体）
     ↓
3. 质量过滤
   - Reward Model 打分 → 取 top-k
   - Dedup（MinHash / SemDeDup）→ 去重
   - 多样性采样（Facility Location / k-DPP）
     ↓
4. 人工验证（随机抽检 10-20%）
     ↓
5. 数据混合
   - 通用指令 60% + 领域数据 30% + 安全数据 10%
   - 保留少量预训练数据防止遗忘（replay buffer）
```

**数据量经验值**：
| 场景 | 推荐数据量 | 训练 Epoch |
|------|-----------|-----------|
| 简单任务适配 | 1K-5K | 3-5 |
| 通用 Chat 能力 | 10K-50K | 1-3 |
| 领域专家（医疗/法律） | 50K-200K | 1-2 |
| 多能力模型 | 100K-1M | 1 |

### 7.5 训练超参数推荐

```python
training_args = TrainingArguments(
    # 学习率
    learning_rate=2e-5,          # LoRA 时可用 1e-4 ~ 3e-4
    lr_scheduler_type="cosine",
    warmup_ratio=0.03,           # 3% warmup
    
    # Batch size
    per_device_train_batch_size=4,
    gradient_accumulation_steps=8,  # 有效 batch = 32
    
    # 训练轮次
    num_train_epochs=3,          # 小数据集 3-5，大数据集 1
    
    # 精度
    bf16=True,                   # A100/H100 必开
    
    # 正则化
    weight_decay=0.01,
    max_grad_norm=1.0,
    
    # 序列长度
    max_seq_length=4096,         # 根据数据分布设置
    
    # 保存与评估
    save_strategy="steps",
    eval_strategy="steps",
    eval_steps=100,
    save_steps=100,
    save_total_limit=3,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
)
```

**关键论文**：
- [14] Zhou et al., "LIMA: Less Is More for Alignment," NeurIPS 2023
- [15] Chen et al., "AlpaGasus: Training A Better Alpaca with Fewer Data," ICLR 2024

**实战 Tips**：
- SFT 的第一步是**确保 Chat Template 正确**——这比任何超参调优都重要
- 学习率对 LoRA SFT 影响巨大：太高→过拟合/遗忘，太低→欠拟合
- 过拟合信号：train loss 持续下降但 eval loss 上升 → 减少 epoch 或增加 dropout
- 小数据集（<5K）推荐 LoRA r=8-16；大数据集可以用 r=32-64
- 混合高质量通用数据（如 OpenHermes, UltraChat）能提升泛化

---

## 8. RLHF 与偏好对齐

### 8.1 RLHF 经典流程

```
Stage 1: SFT         → 指令遵循能力
Stage 2: Reward Model → 人类偏好建模
Stage 3: RL (PPO)     → 用 RM 信号优化策略
```

**Stage 2: Reward Model 训练**

Bradley-Terry 偏好模型：

$$p(y_w \succ y_l | x) = \sigma(r_\theta(x, y_w) - r_\theta(x, y_l))$$

损失函数：

$$\mathcal{L}_{\text{RM}} = -\mathbb{E}_{(x, y_w, y_l)} [\log \sigma(r_\theta(x, y_w) - r_\theta(x, y_l))]$$

**Stage 3: PPO 优化**

$$\mathcal{L}_{\text{PPO}} = -\mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta} \left[ r_\phi(x, y) - \beta \cdot D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}}) \right]$$

KL 散度项防止策略偏离 SFT 基线过远（reward hacking 的关键防线）。

### 8.2 DPO：Direct Preference Optimization

**核心突破**：消除显式 Reward Model，直接用偏好数据优化策略。

**理论推导**：

从 RLHF 的最优策略出发：

$$\pi^*(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta} r^*(x,y)\right)$$

反解出隐式 reward：

$$r^*(x,y) = \beta \log \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x)$$

代入 Bradley-Terry 模型，得到 DPO 损失：

$$\mathcal{L}_{\text{DPO}} = -\mathbb{E}_{(x, y_w, y_l)} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]$$

**DPO 的优势**：
- 无需训练 Reward Model（省掉一整个阶段）
- 无需 RL 训练循环（无 PPO 的复杂性）
- 训练稳定性远好于 PPO
- 计算成本仅为 PPO 的 1/3-1/5

### 8.3 DPO 变体

**IPO (Identity Preference Optimization)**：

$$\mathcal{L}_{\text{IPO}} = \mathbb{E} \left[ \left( \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} - \frac{1}{2\beta} \right)^2 \right]$$

- 解决 DPO 的过拟合问题（DPO 的 sigmoid 会将 chosen/rejected 概率推向极端）
- 更鲁棒的损失函数，对噪声偏好数据更稳健

**KTO (Kahneman-Tversky Optimization)**：

$$\mathcal{L}_{\text{KTO}} = \mathbb{E}_{y_w}[\lambda_w \sigma(r_{\text{ref}} - \beta \cdot \text{KL}_w)] + \mathbb{E}_{y_l}[\lambda_l \sigma(\beta \cdot \text{KL}_l - r_{\text{ref}})]$$

- **不需要成对偏好数据**！只需要 "好" 和 "不好" 的标签
- 基于前景理论（Prospect Theory）的损失不对称性
- 数据收集成本比 DPO 低得多（不需要同一 prompt 的 pair）

**SimPO (Simple Preference Optimization)**：

$$\mathcal{L}_{\text{SimPO}} = -\log \sigma \left( \frac{\beta}{|y_w|} \log \pi_\theta(y_w|x) - \frac{\beta}{|y_l|} \log \pi_\theta(y_l|x) - \gamma \right)$$

- 去掉 reference model（进一步简化）
- 用长度归一化的 log-probability 代替 KL
- $\gamma$：target margin，确保 chosen 和 rejected 有足够差距

**GRPO (Group Relative Policy Optimization)**：

- DeepSeek 提出，在推理增强场景效果显著
- 不需要 Critic Model，用组内相对排序代替绝对 reward
- 详见 [[GRPO-Improvement-Panorama-2026|GRPO 改进全景]]

### 8.4 偏好对齐方法选择

```
决策流程：
1. 有成对偏好数据？
   - Yes → DPO (推荐) / IPO (噪声多时)
   - No → 只有 thumbs up/down → KTO
2. 需要最强效果？ → RLHF (PPO) 仍是天花板
3. 推理增强？ → GRPO
4. 最简化流程？ → SimPO / ORPO
5. 在线迭代？ → Online DPO (自我生成 → 自我偏好排序)
```

**关键论文**：
- [16] Ouyang et al., "Training Language Models to Follow Instructions with Human Feedback," NeurIPS 2022
- [17] Rafailov et al., "Direct Preference Optimization: Your Language Model is Secretly a Reward Model," NeurIPS 2023
- [18] Azar et al., "A General Theoretical Paradigm to Understand Learning from Human Feedback," AISTATS 2024 (IPO)
- [19] Ethayarajh et al., "KTO: Model Alignment as Prospect Theoretic Optimization," ICML 2024
- [20] Meng et al., "SimPO: Simple Preference Optimization with a Reference-Free Reward," NeurIPS 2024

---

## 9. 对齐税与灾难性遗忘

### 9.1 对齐税 (Alignment Tax)

**定义**：模型在对齐（RLHF/DPO）后，在某些基准测试（如数学、编码、知识问答）上性能下降的现象。

**原因**：
- KL 约束不足 → 策略过度偏离预训练分布
- Reward Model 的偏差被放大（reward hacking）
- 训练数据的分布偏移（alignment 数据通常是安全/有帮助的对话）

**量化**：
$$\text{Alignment Tax} = \text{Perf}_{\text{SFT}}(\text{benchmark}) - \text{Perf}_{\text{aligned}}(\text{benchmark})$$

典型值：MMLU 下降 1-3%，GSM8K 下降 2-5%

### 9.2 灾难性遗忘 (Catastrophic Forgetting)

**定义**：微调后模型丧失预训练获得的通用能力。

**表现**：
- 微调后模型在目标任务上很好，但在其他任务上严重退化
- 语言流畅度下降、知识遗忘、推理能力减弱

### 9.3 工程解法

**方法一：数据混合 (Data Mixing / Replay Buffer)**
```python
# 在 SFT 数据中混入预训练数据
dataset = concatenate_datasets([
    sft_data,           # 70%：目标任务数据
    pretrain_data,      # 20%：预训练数据（网页文本）
    general_instruct,   # 10%：通用指令数据
])
```
- 最简单有效，工业界标配
- 预训练数据比例 10-30% 足以显著缓解遗忘

**方法二：EWC (Elastic Weight Consolidation)**

$$\mathcal{L}_{\text{EWC}} = \mathcal{L}_{\text{task}} + \lambda \sum_i F_i (\theta_i - \theta_0^i)^2$$

- $F_i$：Fisher 信息矩阵对角元素（衡量参数对预训练任务的重要性）
- 重要参数少动，不重要参数多动
- 缺点：需要计算 Fisher 矩阵，计算量大

**方法三：LoRA 天然防遗忘**
- LoRA 冻结基座权重，只训练增量 → 本质上就是一种正则化
- 低秩约束限制了更新幅度 → 不会大幅偏离预训练
- 这是 LoRA 被广泛采用的一个隐性优势

**方法四：渐进式微调 (Progressive Fine-Tuning)**
```
阶段 1：低学习率 + 大 batch → 稳定适应
阶段 2：常规学习率 → 任务学习
阶段 3：学习率衰减 → 精细调整
```

**方法五：NEFTune (Noisy Embedding Fine-Tuning)**

$$\tilde{e} = e + \frac{\alpha}{\sqrt{Ld}} \cdot \epsilon, \quad \epsilon \sim \mathcal{U}(-1, 1)$$

- 在输入嵌入上加均匀噪声
- 简单但有效：提升泛化、减少过拟合
- 几行代码即可实现

**关键论文**：
- [21] Kirkpatrick et al., "Overcoming Catastrophic Forgetting in Neural Networks," PNAS 2017
- [22] Jang et al., "Exploring the Benefits of Training Expert Language Models over Instruction Tuning," ICML 2023

**实战 Tips**：
- **首选 LoRA**：它天然就是最好的防遗忘方案
- 如果必须 FFT，务必混入 10-20% 预训练数据
- 监控多个基准测试（不只看目标任务），设置遗忘报警阈值
- DPO 的 $\beta$ 参数直接控制与 reference model 的距离——$\beta$ 太小会加剧遗忘

---

## 10. 训练框架对比

### 10.1 主流框架概览

| 框架 | 定位 | LoRA 支持 | 分布式 | 易用性 | 2026 活跃度 |
|------|------|----------|--------|--------|-----------|
| HF TRL | 官方 RLHF/DPO 工具包 | ✅ via PEFT | ✅ DeepSpeed/FSDP | ★★★ | 🔥🔥🔥🔥🔥 |
| Axolotl | YAML 驱动微调 | ✅ | ✅ DeepSpeed | ★★★★ | 🔥🔥🔥🔥 |
| LLaMA-Factory | 中文社区最流行 | ✅ | ✅ DeepSpeed | ★★★★★ | 🔥🔥🔥🔥🔥 |
| Unsloth | 极致速度优化 | ✅ | ❌ 仅单卡 | ★★★★★ | 🔥🔥🔥🔥 |
| torchtune | PyTorch 原生 | ✅ | ✅ FSDP | ★★★ | 🔥🔥🔥 |

### 10.2 详细对比

**Hugging Face TRL**：
- 最完整的对齐训练工具包：SFTTrainer / DPOTrainer / PPOTrainer / ORPOTrainer
- 与 PEFT / transformers / datasets 深度集成
- 支持 Online DPO / KTO / GRPO 等最新方法
- 缺点：PPO 实现复杂度高，debug 困难

**Axolotl**：
- YAML 配置驱动，几乎零代码微调
- 内置数据格式转换、chat template 自动适配
- 支持多种量化方案（bitsandbytes / GPTQ / AWQ）
- 缺点：YAML 配置复杂，自定义灵活度有限

**LLaMA-Factory**：
- WebUI 可视化训练——极低门槛
- 支持 100+ 模型和数据集
- 集成 SFT / RLHF / DPO / PPO / ORPO
- 中文社区活跃度最高
- 缺点：深度定制需要改源码

**Unsloth**：
- 单卡速度之王：Triton 内核优化，比 HF 快 2-5×
- 显存优化：比标准实现节省 40-60%
- 支持 QLoRA + Flash Attention 2 融合
- 缺点：仅支持单卡，不支持分布式

**torchtune**：
- Meta 官方 PyTorch 原生方案
- 无外部依赖，纯 PyTorch 实现
- FSDP2 原生支持
- 缺点：功能尚不如 TRL 完善

### 10.3 框架选择指南

```
单卡快速实验？       → Unsloth（速度最快）
零代码开箱即用？     → LLaMA-Factory（WebUI）
生产级 RLHF/DPO？   → TRL（最完整）
YAML 配置管理？      → Axolotl
PyTorch 原生主义？   → torchtune
```

**实战 Tips**：
- 快速验证想法：Unsloth → 确认方案后迁移到 TRL/Axolotl 做分布式
- LLaMA-Factory 适合团队协作（非 ML 工程师也能用 WebUI 训练）
- TRL 的 `SFTTrainer` 是目前最标准的 SFT 实现，面试代码题推荐用它

---

## 11. 分布式微调

### 11.1 DeepSpeed ZeRO

**核心思想**：将训练状态（参数、梯度、优化器状态）分片到多个 GPU。

| ZeRO 阶段 | 分片内容 | 显存节省 | 通信开销 |
|-----------|---------|---------|---------|
| ZeRO-1 | 优化器状态 | ~4× | 低 |
| ZeRO-2 | + 梯度 | ~8× | 中 |
| ZeRO-3 | + 参数 | ~N× (N=GPU数) | 高 |
| ZeRO-Offload | + CPU 卸载 | 极高 | 极高 |

**ZeRO-3 通信分析**：
- 前向传播：需要 all-gather 收集完整参数
- 反向传播：all-gather 参数 + reduce-scatter 梯度
- 总通信量 = 3× 参数量（比 DDP 的 1× 多 2×）
- 但每卡显存降为 $\frac{1}{N}$

**配置示例 (ZeRO-2, 推荐用于 LoRA)**：
```json
{
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {"device": "none"},
    "allgather_partitions": true,
    "allgather_bucket_size": 5e8,
    "reduce_scatter": true,
    "reduce_bucket_size": 5e8,
    "overlap_comm": true,
    "contiguous_gradients": true
  },
  "bf16": {"enabled": true},
  "gradient_accumulation_steps": 4,
  "gradient_clipping": 1.0,
  "train_micro_batch_size_per_gpu": 4
}
```

**LoRA + DeepSpeed 的最佳配置**：
- LoRA 可训练参数少 → ZeRO-2 足够（ZeRO-3 的通信开销不值得）
- FFT → ZeRO-3 必需
- ZeRO-Offload 在 LoRA 场景下意义不大（参数已经很小了）

### 11.2 FSDP (Fully Sharded Data Parallel)

**核心思想**：PyTorch 原生的参数全分片方案（类似 ZeRO-3）。

```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP

model = FSDP(
    model,
    sharding_strategy=ShardingStrategy.FULL_SHARD,
    mixed_precision=MixedPrecision(
        param_dtype=torch.bfloat16,
        reduce_dtype=torch.bfloat16,
        buffer_dtype=torch.bfloat16,
    ),
    auto_wrap_policy=transformer_auto_wrap_policy,
    use_orig_params=True,  # LoRA 必须设为 True
)
```

**FSDP vs DeepSpeed**：

| 维度 | FSDP | DeepSpeed |
|------|------|-----------|
| 集成 | PyTorch 原生 | 第三方库 |
| 配置 | Python API | JSON/Python |
| LoRA 兼容性 | 需要 `use_orig_params=True` | 开箱即用 |
| QLoRA 兼容性 | FSDP2 支持 | bitsandbytes 限制 |
| 生态 | 较新 | 成熟 |
| 2026 趋势 | FSDP2 上升 | 依然主流 |

### 11.3 Pipeline Parallelism

**适用场景**：单张 GPU 放不下模型参数时（不考虑 ZeRO-3 的情况）。

```
GPU 0: Layer 0-7     ─┐
GPU 1: Layer 8-15     │ 流水线
GPU 2: Layer 16-23    │
GPU 3: Layer 24-31   ─┘
```

**Micro-batch 流水线**：
- GPipe：同步流水线，简单但 bubble 大
- 1F1B：交替前向/反向，减少 50% 的流水线气泡
- Interleaved 1F1B：更细粒度，气泡进一步减少

**流水线气泡比例**：
$$\text{Bubble ratio} = \frac{p - 1}{m + p - 1}$$
其中 $p$ = 流水线阶段数，$m$ = micro-batch 数。$m \gg p$ 时气泡可忽略。

**关键论文**：
- [23] Rajbhandari et al., "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models," SC 2020
- [24] Zhao et al., "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel," VLDB 2023

**实战 Tips**：
- LoRA 微调 7B → 2×A100 + ZeRO-2 足够
- LoRA 微调 70B → 4-8×A100 + ZeRO-3 或 FSDP
- FFT 7B → 8×A100 + ZeRO-2/3
- FFT 70B → 64+ A100 + ZeRO-3 + Pipeline Parallelism
- 调试分布式问题：先单卡确认代码正确，再扩展到多卡

---

## 12. 2026 前沿技术

### 12.1 Mixture of LoRA Experts (MoLoRA)

**核心思想**：将 MoE（Mixture of Experts）的思想引入 LoRA——多个 LoRA adapter 作为 experts，通过路由器动态选择。

$$\Delta W(x) = \sum_{i=1}^{E} g_i(x) \cdot \frac{\alpha}{r} B_i A_i$$

其中 $g_i(x) = \text{Top-K}(\text{Softmax}(W_{\text{router}} \cdot h(x)))$ 是路由权重。

**优势**：
- 不同输入激活不同的 LoRA experts → 更强的表达力
- 总参数量 = E × LoRA_params，但每次只激活 Top-K
- 天然支持多任务：不同任务可以自动路由到不同 expert

**挑战**：
- 路由器训练不稳定（负载均衡问题）
- 推理时无法简单合并到基座模型（依赖输入的动态路由）
- 需要支持 MoE 的 serving 基础设施

### 12.2 LoRA Merging

**核心思想**：将多个独立训练的 LoRA adapter 合并成一个模型。

**方法一：线性合并 (Task Arithmetic)**

$$\theta_{\text{merged}} = \theta_0 + \sum_{i=1}^{N} \lambda_i \cdot \Delta\theta_i$$

$\lambda_i$ 是各任务的权重系数。

**方法二：TIES-Merging**
1. Trim：去掉小的参数变化
2. Elect Sign：对冲突参数取多数投票的符号
3. Disjoint Merge：只合并不冲突的参数

**方法三：DARE (Drop And REscale)**
1. 随机 drop 大部分 delta 参数（保留 1-10%）
2. Rescale 保留的参数以保持期望不变
3. 合并后效果接近于完整合并

**实际应用**：
- 将代码能力 LoRA + 数学能力 LoRA + 中文能力 LoRA 合并
- 无需联合训练，独立训练后合并
- 开源社区大量使用（HuggingFace mergekit 工具）

**关键论文**：
- [25] Yadav et al., "TIES-Merging: Resolving Interference When Merging Models," NeurIPS 2023
- [26] Yu et al., "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch," ICML 2024 (DARE)

### 12.3 Representation Fine-Tuning (ReFT)

**核心思想**：不修改模型权重，而是在隐层表示（representations）上做干预。

$$h'_l = h_l + R_l \cdot \phi(s_l)$$

其中：
- $h_l$：第 $l$ 层的隐层表示
- $R_l \in \mathbb{R}^{d \times r}$：低秩干预矩阵
- $s_l$：可学习的干预参数
- $\phi$：非线性变换

**LoReFT (Low-Rank Linear Subspace ReFT)**：
$$h'_l = h_l + R_l(R_l^T h_l - s_l)$$

- 在 $R_l$ 定义的低秩子空间中编辑表示
- 参数量比 LoRA 少 10-50×（极端参数效率）
- 效果在某些任务上接近 LoRA

**理论意义**：
- 连接了 PEFT 和 Representation Engineering 两个领域
- 为"模型如何存储知识"提供了可操作的视角
- 未来方向：结合 mechanistic interpretability 的定向编辑

**关键论文**：
- [27] Wu et al., "ReFT: Representation Finetuning for Language Models," NeurIPS 2024

### 12.4 其他前沿方向

**GaLore (Gradient Low-Rank Projection)**：
- 投影梯度到低秩子空间，减少优化器状态的显存
- 不同于 LoRA 投影权重，GaLore 投影梯度
- 效果接近 FFT，显存接近 LoRA

**LoRA-XS (Extremely Small LoRA)**：
- 利用冻结的 SVD 分解作为骨架
- 仅训练中间的小矩阵（$r \times r$）
- 参数量降低 100×

**Spectrum (SNR-based Layer Freezing)**：
- 根据各层的信噪比(SNR)决定是否冻结
- 高 SNR 层：已学好 → 冻结
- 低 SNR 层：需要适配 → 训练
- 自动化的 layer-wise 选择性微调

---

## 13. 面试题集

> 18 道面试题，从基础到专家级递进。每题附深度答案。

---

### 基础级（Q1-Q6）

#### Q1: LoRA 的数学原理是什么？为什么有效？

**答案**：

LoRA 的核心假设是微调所需的权重更新 $\Delta W$ 具有低秩结构。将 $\Delta W$ 分解为两个小矩阵的乘积 $\Delta W = BA$，其中 $B \in \mathbb{R}^{d \times r}$, $A \in \mathbb{R}^{r \times k}$, $r \ll \min(d, k)$。

前向传播：$h = W_0 x + \frac{\alpha}{r} BAx$

有效性的三层理解：
1. **经验证据**：Aggarwal et al. (2024) 分析了微调后的 $\Delta W$ 矩阵，发现其有效秩远低于全秩——前 1% 的奇异值解释了 90%+ 的方差
2. **信息论视角**：微调只需"告诉"模型如何在预训练子空间中调整方向，不需要学习全新的表示
3. **正则化效果**：低秩约束相当于隐式正则化，减少过拟合风险——这在小数据集场景下是优势

初始化策略：B=0, A=Kaiming，保证训练开始时 $\Delta W = 0$，模型从预训练状态出发。

---

#### Q2: QLoRA 的三个核心技术是什么？它如何实现在消费级 GPU 上微调 7B 模型？

**答案**：

三个核心技术：
1. **NF4 量化**：基于正态分布假设的 4-bit 量化，对每个量化 bucket 内权重等概率分区，量化误差信息论最优
2. **双重量化**：对量化 scale factor（FP32）再做 INT8 量化，进一步节省 ~0.4GB/7B
3. **分页优化器**：利用 NVIDIA 统一内存，优化器状态在 GPU OOM 时自动换入 CPU

显存计算：7B 模型 NF4 量化后仅 3.5GB（vs BF16 的 14GB），加上 LoRA 参数 ~33MB + 梯度 ~33MB + 优化器状态 ~66MB ≈ 3.6GB。加上 activation 和 buffer，总需 ~6-10GB，RTX 4090 (24GB) 甚至 RTX 3090 (24GB) 都能训练。

关键注意：`bnb_4bit_compute_dtype` 设为 `bfloat16` 而非 `float16`，后者在梯度缩放时有数值问题。

---

#### Q3: LoRA 中的 alpha 和 rank 分别是什么？如何调参？

**答案**：

- **rank (r)**：低秩矩阵的秩，决定表达能力的上限。r 越大表达力越强，但参数量线性增长
- **alpha (α)**：缩放因子，实际缩放比例为 α/r

调参策略：
1. **rank**：先从 r=16 开始（大多数任务的甜蜜点），任务复杂/跨域时增至 32-64
2. **alpha**：常设 α=2r（有效缩放=2），也有人直接设 α=r
3. **核心原则**：改变 r 时，如果保持 α 不变，有效缩放 α/r 会变化，需要同步调整学习率
4. **rsLoRA 的改进**：用 α/√r 替代 α/r，使不同 rank 之间更具可比性，免去重新调参

实用技巧：先用 r=16, α=32 做基线，再根据 eval loss 曲线微调。如果欠拟合 → 加大 r；过拟合 → 减小 r 或增加 dropout。

---

#### Q4: 全参数微调和 LoRA 微调的显存需求分别怎么估算？

**答案**：

**全参数微调 (BF16 + AdamW)**：
$$\text{显存} \approx 16 \times P \text{ bytes}$$

拆解：
- 参数 (BF16)：$2P$
- 梯度 (BF16)：$2P$
- AdamW 状态 (FP32 ×2)：$4P + 4P = 8P$（第一动量 + 第二动量）
- 主权重 FP32 副本：$4P$
- 总计：$\approx 16P$（7B → ~112GB，加上 activation 需要 8×A100）

**LoRA 微调 (BF16 基座 + BF16 LoRA)**：
$$\text{显存} \approx 2P + 16 \times P_{\text{LoRA}}$$

- 冻结基座 (BF16)：$2P$（仅存参数，无梯度/优化器）
- LoRA 参数 + 梯度 + 优化器：$16 \times P_{\text{LoRA}}$（$P_{\text{LoRA}} \ll P$）
- 7B + LoRA(r=16, all layers) → ~14GB + ~270MB ≈ 14.3GB

**QLoRA 微调 (NF4 基座 + BF16 LoRA)**：
$$\text{显存} \approx 0.5P + 16 \times P_{\text{LoRA}}$$

- NF4 基座：$0.5P$（4bit = 0.5 bytes/param）
- 7B → ~3.5GB + ~270MB ≈ 3.8GB

**Activation 显存**（序列长度相关）：
$$\text{Activation} \approx 2 \times b \times s \times h \times L \text{ bytes (with gradient checkpointing)}$$

---

#### Q5: SFT 训练中为什么只对 response 计算 loss？不 mask 会怎样？

**答案**：

只对 response 计算 loss（instruction 部分设 `labels=-100` 被 cross-entropy 忽略）的原因：

1. **目标对齐**：我们要训练模型"给出好的回答"，而非"复述问题"。在 instruction 上计算 loss 等于教模型背题目，浪费学习容量
2. **防止分布混淆**：instruction 来自用户，response 来自标注者，两者的文本分布不同。混合计算 loss 会让模型困惑"应该模仿谁"
3. **实验证据**：不 mask 的模型倾向于在回答开头复述问题（parroting），且在生成质量基准上显著下降

不 mask 的后果：
- 模型学到"重复用户问题"的模式
- 回答质量下降，尤其在开放式生成任务上
- 等价于把模型当做"语言模型续写"而非"指令执行者"来训练

**例外情况**：
- 预训练式 SFT（如 continued pretraining）可以不 mask
- 某些框架的默认行为可能是全序列 loss——务必检查！

---

#### Q6: DPO 和 RLHF(PPO) 的核心区别是什么？各自的优缺点？

**答案**：

**RLHF (PPO)**：
- 流程：SFT → 训练 Reward Model → PPO 优化策略
- 优点：理论天花板最高，可以迭代优化
- 缺点：训练复杂（4个模型同时在 GPU 上）、超参敏感、reward hacking

**DPO**：
- 流程：SFT → 直接用偏好数据优化策略（跳过 RM 和 RL）
- 数学本质：通过变量替换，将 RLHF 的目标函数改写为直接对策略的监督学习
- 隐式 reward：$r(x,y) = \beta \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)} + C$

| 维度 | PPO | DPO |
|------|-----|-----|
| 训练复杂度 | 极高（4 模型） | 低（2 模型） |
| GPU 需求 | 4× | 2× |
| 训练稳定性 | 差 | 好 |
| 效果天花板 | 最高 | 略低 |
| 迭代优化 | 支持 Online | 需要 Online DPO |
| 工程门槛 | 高 | 低 |

2026 趋势：DPO 及其变体（SimPO/KTO/ORPO）已成为对齐的主流选择，PPO 主要用于需要 Online RL 的场景（如推理增强）。

---

### 进阶级（Q7-Q12）

#### Q7: DoRA 相比 LoRA 的改进是什么？为什么 Direction + Magnitude 的解耦有效？

**答案**：

DoRA 将权重分解为方向（Direction）和大小（Magnitude）：$W = m \cdot \frac{W_0 + BA}{\|W_0 + BA\|_c}$

改进本质：
1. **FFT 的更新模式分析**：研究者发现 FFT 中的权重更新主要是方向变化，大小变化很小
2. **LoRA 的局限**：标准 LoRA 将方向和大小耦合在 BA 中学习，低秩约束同时限制了两者
3. **DoRA 的解决**：将大小用一个 $d$-维向量 $m$ 单独学习（几乎不增加参数），让低秩部分 BA 专注于学方向变化

效果：
- DoRA(r=16) ≈ LoRA(r=64) 的效果，参数量却只有后者的 1/4
- 在 commonsense reasoning、VQA 等任务上持续优于 LoRA
- 额外参数量：仅一个 $d$-维向量（如 4096 个参数 = 8KB），negligible

灵感来源是 Weight Normalization：将权重写成方向和大小的分离形式可以改善优化景观（更平滑的 loss surface）。

---

#### Q8: 对比 Adapter、Prefix Tuning 和 LoRA 三类 PEFT 方法的设计哲学和适用场景

**答案**：

**设计哲学差异**：

| 方法 | 哲学 | 修改什么 | 隐喻 |
|------|------|---------|------|
| Adapter | 加新模块 | 网络结构 | 给模型装外挂 |
| Prefix Tuning | 加新上下文 | 注意力机制 | 给模型加系统提示词 |
| LoRA | 改原权重 | 权重矩阵 | 给模型的突触微调 |

**关键区分维度**：

1. **推理延迟**：LoRA 可合并→零延迟；Adapter 和 Prefix 有额外计算
2. **表达力**：Adapter 有非线性→理论表达力最强；LoRA 和 Prefix 是线性/软注意力
3. **多任务服务**：LoRA 最适合（加载不同权重即可）；Prefix 次之（不同前缀）；Adapter 需要完整前向路径
4. **统一理论 (He et al. 2022)**：三者都可视为在隐层表示上做加法修改，只是作用位置和参数化方式不同

适用场景：
- 2026 年几乎所有场景都推荐 LoRA/DoRA
- 超多租户场景（数千个微任务）→ Prefix Tuning 的参数量优势可能有价值
- 理论研究/Ablation → Adapter 的非线性可作为 LoRA 的上界对比

---

#### Q9: 如何设计一个高质量的 SFT 数据集？从 0 到 1 的完整流程是什么？

**答案**：

**6 步流程**：

**Step 1: 任务定义与种子设计**
- 明确目标任务的输入/输出规范
- 人工撰写 200-500 条高质量种子样本（golden set）
- 制定标注指南（annotation guideline）

**Step 2: 数据扩展**
- LLM 辅助生成：用 GPT-4/Claude 基于种子生成变体
- Self-Instruct：让模型自己生成新的 instruction
- 真实用户数据：从产品日志中采样（需脱敏）
- 目标：10K-100K 候选样本

**Step 3: 质量过滤（最关键）**
- Reward Model 打分 → 取 top 70-80%
- 规则过滤：长度、格式、关键词黑名单
- LLM-as-Judge：用强模型评分（Likert scale 1-5）
- 去重：MinHash（表面去重）+ Embedding 聚类（语义去重）

**Step 4: 多样性保证**
- 任务类型分布分析（问答/总结/翻译/编码/推理...）
- Embedding 聚类 → 从每个簇采样保证覆盖
- 避免 majority class 的统治

**Step 5: 格式标准化**
- 统一 Chat Template（必须与目标模型一致！）
- 验证 special tokens 正确添加
- 多轮对话的 turn 边界检查

**Step 6: 数据混合**
- 通用指令数据 50-60%（保底泛化）
- 目标领域数据 30-40%（核心能力）
- 安全/拒绝数据 5-10%（对齐兜底）
- 少量预训练数据 5%（防遗忘 replay）

**LIMA 定律的工程启示**：与其花时间扩大数据量，不如花时间提升数据质量。1000 条精品 > 100K 条噪声。

---

#### Q10: 灾难性遗忘在 LLM 微调中的具体表现是什么？如何在工程上解决？

**答案**：

**具体表现**：
1. **能力退化**：微调后的数学/编码能力下降（"对齐税"）
2. **知识丢失**：微调前能回答的世界知识问题微调后不能了
3. **语言退化**：英文模型微调中文后，英文能力下降
4. **格式僵化**：过度拟合训练数据的格式，丧失灵活性

**5 个工程解法**（按推荐度排序）：

1. **LoRA / PEFT**（★★★★★）
   - 冻结基座权重天然防遗忘
   - 低秩约束限制更新幅度
   - 可以热切换不同 adapter 实现多任务

2. **数据混合 Replay**（★★★★★）
   - 混入 10-30% 预训练/通用数据
   - 简单有效，工业界标配

3. **学习率控制**（★★★★）
   - 使用较小学习率（FFT: 1e-5~5e-6, LoRA: 1e-4~3e-4）
   - Cosine schedule + 低终止学习率
   - 少 epoch（1-3 epoch 通常足够）

4. **NEFTune 噪声嵌入**（★★★）
   - 输入嵌入加噪声，一行代码实现
   - 提升泛化，减少过拟合到微调数据

5. **EWC / L2 正则化**（★★★）
   - 通过 Fisher 信息矩阵保护重要参数
   - 实现复杂，通常 LoRA + Replay 就够了

**监控策略**：
- 维护一组多维评估基准（不只看目标任务）
- 设置遗忘警报：任一基准下降 >3% 立即排查
- 定期对比微调前后的模型在通用基准上的表现

---

#### Q11: DeepSpeed ZeRO-2 和 ZeRO-3 的核心区别是什么？LoRA 微调应该选哪个？

**答案**：

**核心区别**：

| 维度 | ZeRO-2 | ZeRO-3 |
|------|--------|--------|
| 分片内容 | 优化器状态 + 梯度 | 优化器状态 + 梯度 + **参数** |
| 每卡显存 | 模型参数完整 + O+G 分片 | 全部分片（≈总量/N） |
| 通信量 | 1× 参数量 (reduce-scatter) | 3× 参数量 (all-gather × 2 + reduce-scatter) |
| 吞吐量 | 高 | 较低（通信瓶颈） |

**LoRA 微调选择**：

**首选 ZeRO-2**。原因：
1. LoRA 的可训练参数极少（<1%），优化器状态和梯度的分片需求低
2. 基座模型冻结 → 不需要分片冻结参数（ZeRO-3 对冻结参数的 all-gather 是纯浪费）
3. ZeRO-2 通信量低，吞吐量更高

**何时用 ZeRO-3**：
- 冻结的基座模型单卡放不下（如 70B 模型在 A100-40GB 上）
- FFT 大模型必须用 ZeRO-3

**实战经验**：
- 7B LoRA: 2× A100 + ZeRO-2 → 完美
- 70B LoRA: 8× A100 + ZeRO-3 → 可以但通信开销大
- 70B QLoRA: 4× A100 + ZeRO-2 → 推荐（NF4 量化后模型小了）

---

#### Q12: 对比 DPO / IPO / KTO / SimPO 四种偏好对齐方法

**答案**：

| 方法 | 需要的数据 | Reference Model | 核心改进 | 适用场景 |
|------|-----------|----------------|---------|---------|
| DPO | 成对偏好 (w,l) | 需要 | 隐式 reward | 通用 |
| IPO | 成对偏好 (w,l) | 需要 | 防过拟合（正则化） | 噪声数据多 |
| KTO | 独立标签 (good/bad) | 需要 | 非成对，前景理论 | 数据难成对时 |
| SimPO | 成对偏好 (w,l) | **不需要** | 长度归一化 | 极简流程 |

**深度对比**：

1. **DPO 的问题**：sigmoid 损失会将 chosen/rejected 概率推向极端 → 过拟合偏好数据
2. **IPO 的解决**：用 MSE 损失替代 sigmoid，加了隐式正则化 → 更鲁棒
3. **KTO 的突破**：不需要同一 prompt 的 (w,l) pair，只需要 "这个回答好/不好" → 大幅降低数据收集成本
4. **SimPO 的简化**：去掉 reference model（训练时不需要额外显存），用长度归一化的 log-prob 代替 KL → 训练最简

**2026 推荐**：
- 有高质量成对数据 → DPO 或 SimPO
- 数据噪声大 → IPO
- 只有单条评分数据 → KTO
- 推理增强场景 → GRPO

---

### 专家级（Q13-Q18）

#### Q13: 从 LoRA 的低秩假设出发，分析什么场景下 LoRA 会失效？如何诊断和解决？

**答案**：

**LoRA 失效的场景**：

1. **任务需要高秩更新**
   - 当下游任务与预训练任务差异极大（如英文模型微调阿拉伯文）
   - $\Delta W$ 的有效秩远超设定的 r → 低秩近似误差大
   - 诊断：比较不同 r 的 eval loss，若 r=64 仍有大幅提升 → 低秩假设可能不成立
   - 解决：增大 r 至 128-256，或回退到 FFT

2. **Embedding/LM Head 需要大量更新**
   - 新词表/新语言需要改变 embedding 空间
   - Embedding 矩阵的更新通常不是低秩的（不同新 token 的更新方向正交）
   - 解决：对 Embedding 层做 FFT，其他层用 LoRA

3. **数据极少 + 模型极小**
   - <100 条数据 + <1B 模型：LoRA 的参数量太低，无法学到有意义的变化
   - 此时 Prompt Tuning 或 few-shot ICL 可能更好

4. **多目标冲突**
   - 一个 LoRA adapter 同时学互相矛盾的任务（如"总是同意" + "指出错误"）
   - 低秩空间无法同时表达冲突的方向
   - 解决：MoLoRA（多 expert 路由）或分开训练再 merge

**诊断清单**：
- [ ] eval loss 是否随 r 增大持续下降？（高秩需求信号）
- [ ] 特定层的 LoRA 权重范数是否异常大？（该层需要更多容量）
- [ ] 合并后推理效果 vs 未合并是否一致？（数值精度问题）

---

#### Q14: 如果让你从零设计一个 LoRA serving 系统，支持数千个 adapter 的并发推理，你会怎么设计？

**答案**：

**架构设计**：

```
Layer 1: Request Router
  - 解析请求 → 确定需要哪个 LoRA adapter
  - Adapter 注册表（ID → metadata + 权重路径）

Layer 2: Base Model Pool
  - 共享基座模型实例（如 1-2 个 LLaMA-70B 推理实例）
  - Continuous Batching（不同 adapter 的请求可以 batch 在一起）

Layer 3: LoRA Adapter Cache
  - LRU 缓存热门 adapter（GPU HBM）
  - 冷 adapter 存在 CPU/SSD，按需加载
  - 预算：每个 adapter 10-50MB，1000 个 = 10-50GB

Layer 4: Kernel Fusion
  - SGMV (Scatter-Gather Matrix-Vector) 内核
  - 单次 GEMM 中同时计算多个不同 LoRA 的输出
  - vLLM 的 Punica 内核 / S-LoRA 的 unified paging
```

**关键技术挑战**：

1. **Batching 不同 adapter**
   - 问题：同一 batch 中不同请求用不同 LoRA，无法做标准 GEMM
   - 解决：SGMV 内核——将不同 LoRA 的 A/B 矩阵打包，用自定义 CUDA kernel 并行计算
   - S-LoRA 论文：支持数千 adapter 并发，吞吐量仅下降 4%

2. **内存管理**
   - Adapter Paging：类似 PagedAttention 的思想，按需加载 adapter 到 GPU
   - Adapter 共享：相同 adapter 的并发请求共享权重
   - 预取策略：基于历史请求模式预加载热门 adapter

3. **延迟控制**
   - 冷启动：adapter 首次加载需要 CPU→GPU 传输（~5-10ms for 50MB adapter）
   - 解决：预热池 + 分层缓存（GPU HBM → CPU RAM → SSD）

**参考实现**：vLLM 和 SGLang 已原生支持多 LoRA serving。

---

#### Q15: LoRA Merging 中的冲突问题（task interference）如何解决？TIES-Merging 和 DARE 的核心思路？

**答案**：

**冲突问题的本质**：

当合并多个 LoRA adapter 时，不同任务的权重更新可能在同一参数上方向相反，直接相加会互相抵消。

$$\Delta W_{\text{merged}} = \sum_i \lambda_i \Delta W_i$$

若 $\Delta W_1^{(j)} > 0$ 但 $\Delta W_2^{(j)} < 0$，合并后 $\Delta W_{\text{merged}}^{(j)} \approx 0$ → 两个任务的信息都丢失了。

**TIES-Merging 三步解决**：

1. **Trim（修剪）**：对每个 $\Delta W_i$，将小于阈值 $\tau$ 的参数置零
   - 大部分微小更新是噪声，去除后减少冲突
   - 保留 top-k% 最大的参数变化

2. **Elect Sign（选举符号）**：对冲突参数，取多数投票决定最终符号
   $$\text{sign}_j = \text{sign}\left(\sum_i \text{sign}(\Delta W_i^{(j)}) \right)$$
   - 少数服从多数，解决符号冲突

3. **Disjoint Merge（不相交合并）**：
   - 只合并与选举符号一致的参数
   - 不一致的参数忽略（避免强行相加造成的干扰）

**DARE 的核心思路**：

1. **Drop**：对每个 $\Delta W_i$，随机 drop 90-99% 的参数（保留 1-10%）
   - 灵感来自 Dropout 的正则化效果
   - 大部分参数的微小变化是冗余的
2. **Rescale**：对保留的参数乘以 $\frac{1}{p}$（$p$ = 保留概率）
   - 保持期望不变：$\mathbb{E}[\tilde{\Delta W}] = \Delta W$
3. **Merge**：在稀疏化后的参数上做加权平均
   - 稀疏化大幅降低了冲突概率

**实战经验**：DARE + TIES 组合效果最佳；mergekit 工具可一键完成。

---

#### Q16: 解释 Representation Fine-Tuning (ReFT) 的核心思想。它与 LoRA 的本质区别是什么？

**答案**：

**ReFT 核心思想**：

传统 PEFT（包括 LoRA）修改的是模型的**权重参数**。ReFT 不改权重，而是在推理时对隐层**表示（representations）** 做干预。

$$h'_l = h_l + R_l(R_l^T h_l - s_l)$$

- $h_l$：第 $l$ 层的原始隐层表示
- $R_l \in \mathbb{R}^{d \times r}$：低秩子空间的基（冻结或可训练）
- $s_l \in \mathbb{R}^r$：干预向量（可训练）
- 含义：在 $R_l$ 定义的子空间中，将表示编辑为目标值 $s_l$

**与 LoRA 的本质区别**：

| 维度 | LoRA | ReFT |
|------|------|------|
| 修改对象 | 权重 $W$ | 表示 $h$ |
| 修改时机 | 训练时学习，推理时固化 | 训练和推理时都动态干预 |
| 可合并性 | ✅ 合并到基座 | ❌ 需要运行时干预 |
| 参数量 | 少（~0.1-1%） | 极少（~0.01-0.1%） |
| 理论连接 | 矩阵分解 | Representation Engineering |
| 可解释性 | 低 | 高（可分析干预了什么表示方向） |

**ReFT 的独特价值**：
1. 参数效率极致：比 LoRA 少 10-50× 参数
2. 连接了 mechanistic interpretability：可以精确知道"模型的哪个表示维度被编辑了"
3. 为"定向编辑模型行为"提供了可操作的工具

**局限性**：无法合并到基座，推理有额外计算；在复杂任务上效果仍略逊于 LoRA。

---

#### Q17: 在 70B 模型上做 QLoRA 微调，你遇到了 loss 不收敛的问题，如何排查？

**答案**：

**系统排查框架（按可能性排序）**：

**1. Chat Template 错误 (50% 的问题出在这里)**
```python
# 排查：打印 tokenize 后的结果
tokenized = tokenizer.apply_chat_template(example, return_tensors="pt")
print(tokenizer.decode(tokenized[0]))
# 确认：special tokens 正确、role 标记正确、EOS 存在
```

**2. Loss Masking 错误**
- 检查 labels 是否正确 mask 了 instruction 部分
- 常见 bug：全序列都计算 loss → loss 下降但实际没学到
- 验证：`(labels != -100).sum()` 应该只包含 response tokens

**3. 学习率过高**
- 70B + QLoRA 的推荐 lr: 1e-4 ~ 2e-4
- lr > 5e-4 几乎必定不稳定
- 检查 loss 曲线：如果剧烈震荡 → lr 过高

**4. NF4 量化精度问题**
- `bnb_4bit_compute_dtype` 是否设为 `bfloat16`？
- `float16` + 4bit 的梯度可能有数值下溢
- 检查 gradient norm 是否为 NaN/Inf

**5. 数据问题**
- 空样本或超长样本导致 loss 异常
- 数据中有 special tokens 冲突（如 `<|im_end|>` 出现在正文中）
- 排查：打印 loss 最高的样本，人工检查

**6. Batch Size / Gradient Accumulation**
- 有效 batch size 太小 → 梯度噪声大 → 不稳定
- 推荐有效 batch ≥ 16
- 检查 `per_device_batch_size × gradient_accumulation × num_gpus`

**7. 模型加载问题**
- 权重是否完整加载？（检查 warning 信息）
- `prepare_model_for_kbit_training` 是否调用？
- LoRA 是否正确 attach？（打印 `model.print_trainable_parameters()`）

**快速诊断脚本**：
```python
# 第一步：用 1 条数据过拟合测试
trainer.train()  # 期望：loss 应该在 20 步内降到 <0.5
# 如果不降 → 模型/数据/配置有严重问题
```

---

#### Q18: 展望 2026-2027，LLM 微调技术的发展趋势是什么？哪些方向可能被颠覆？

**答案**：

**趋势一：微调与推理的融合 (Test-Time Training, TTT)**
- 推理时针对每个输入做即时微调
- TTT Layer / Self-Training at Inference
- 打破"训练时学习 → 推理时固定"的范式

**趋势二：Mixture of LoRA Experts 成为标配**
- 从"一个 adapter 服务所有请求"到"动态路由不同 adapter"
- 类似 MoE 的思想在 PEFT 层面复现
- 需要 serving 基础设施跟进（vLLM 已开始支持）

**趋势三：合成数据 + 自我对齐闭环**
- Self-Play / Constitutional AI / SPIN
- 减少对人类标注的依赖
- 模型自己生成训练数据 → 自己评估 → 自己优化

**趋势四：LoRA 与 Continual Learning 的深度结合**
- 动态增长 LoRA 池（新任务不覆盖旧 adapter）
- Progressive LoRA Merging（定期合并 + 新增）
- 解决"微调不能停"的工业需求

**趋势五：ReFT 和 Representation Engineering 崛起**
- 从"改权重"到"改表示"的范式转移
- 与 Mechanistic Interpretability 深度结合
- 实现更精准的行为编辑（如消除特定偏见）

**可能被颠覆的假设**：
- "微调需要梯度回传" → Retrieval + ICL 可能在某些场景取代微调
- "一个模型一种人格" → 动态 LoRA 切换可能实现多人格/多风格
- "对齐需要人类反馈" → AI 反馈 (RLAIF) + 过程奖励可能超越人类标注

---

## 14. 附录

### 附录 A：微调方法选择决策树

```
                        ┌─── 是否需要微调？
                        │
              ┌─────────┴─────────┐
              │ Yes               │ No → Few-shot ICL / RAG
              │
        ┌─────┴─────┐
        │ 算力充足？ │
        │           │
   ┌────┴────┐  ┌───┴────┐
   │ Yes     │  │ No     │
   │         │  │        │
   │ 数据>100K?│ └→ QLoRA (单卡)
   │         │
 ┌─┴──┐  ┌──┴──┐
 │Yes │  │ No  │
 │    │  │     │
 │FFT │  │LoRA/│
 │    │  │DoRA │
 │    │  │     │
 └────┘  └─────┘

         ┌─── 需要对齐？
         │
    ┌────┴────┐
    │ Yes     │ No → 只做 SFT
    │         │
    │ 有成对偏好数据？
    │         │
  ┌─┴──┐  ┌──┴──┐
  │Yes │  │ No  │
  │DPO/│  │KTO  │
  │IPO │  │     │
  └────┘  └─────┘
```

### 附录 B：常见报错排查

| 报错信息 | 原因 | 解决方案 |
|---------|------|---------|
| `OutOfMemoryError` | 显存不足 | 减小 batch_size / 开 gradient_checkpointing / 用 QLoRA |
| `RuntimeError: expected scalar type BFloat16` | dtype 不匹配 | 设 `bnb_4bit_compute_dtype=torch.bfloat16` |
| `ValueError: Target modules not found` | LoRA 目标模块名写错 | 打印 `model.named_modules()` 检查实际名称 |
| `loss=nan` | 学习率过高 / 数据异常 | 降 lr / 检查数据 / 开 gradient_clipping |
| `CUDA error: device-side assert triggered` | 标签越界 / tokenizer 问题 | 设 `CUDA_LAUNCH_BLOCKING=1` 查看具体位置 |
| `Tokenizer chat_template not set` | 缺少 chat template | 手动设置 `tokenizer.chat_template` |
| `RuntimeError: element 0 of tensors does not require grad` | 冻结层梯度问题 | `prepare_model_for_kbit_training()` 或检查 requires_grad |
| `DeepSpeed: All-reduce error` | 分布式通信问题 | 检查 NCCL 版本 / 网络配置 / `NCCL_DEBUG=INFO` |
| `RuntimeError: FSDP + QLoRA` | 不兼容 | FSDP2 或 换 DeepSpeed ZeRO-2 |
| `Eval loss 上升, Train loss 下降` | 过拟合 | 减少 epoch / 增加 dropout / 减小 r / 增加数据 |

### 附录 C：显存估算公式

**通用公式**：

$$\text{GPU Memory (bytes)} = M_{\text{params}} + M_{\text{grad}} + M_{\text{optim}} + M_{\text{act}}$$

**各组件估算**：

| 组件 | FFT (BF16+AdamW) | LoRA (BF16) | QLoRA (NF4) |
|------|-----------------|-------------|-------------|
| 参数 | $2P$ | $2P + 2P_L$ | $0.5P + 2P_L$ |
| 梯度 | $2P$ | $2P_L$ | $2P_L$ |
| 优化器 | $12P$* | $12P_L$ | $12P_L$ |
| Activation | $\sim 2bshL$** | $\sim 2bshL$ | $\sim 2bshL$ |

*AdamW: FP32 master weights (4P) + first moment (4P) + second moment (4P) = 12P
**With gradient checkpointing; b=batch, s=seq_len, h=hidden, L=layers

**快速估算表**（LoRA r=16, 所有线性层, seq_len=4096, batch=4）：

| 模型 | FFT 显存 | LoRA 显存 | QLoRA 显存 |
|------|---------|----------|-----------|
| 1.3B | ~21 GB | ~3.5 GB | ~2 GB |
| 7B | ~112 GB | ~16 GB | ~7 GB |
| 13B | ~208 GB | ~28 GB | ~12 GB |
| 34B | ~544 GB | ~70 GB | ~25 GB |
| 70B | ~1120 GB | ~145 GB | ~48 GB |

> 注：以上为理论估算，实际因框架实现、CUDA 内核缓存等会有 10-30% 浮动。

### 附录 D：微调 Checklist

```
□ 数据
  □ Chat Template 正确且与目标模型匹配
  □ Response-only loss masking 生效
  □ EOS token 正确添加
  □ 数据去重 + 质量过滤完成
  □ 数据分布分析（长度/任务类型/语言）
  
□ 模型配置
  □ LoRA target_modules 覆盖所有线性层
  □ alpha/rank 比例合理（推荐 alpha=2r）
  □ QLoRA: compute_dtype=bfloat16
  
□ 训练
  □ 学习率合理（LoRA: 1e-4~3e-4）
  □ Gradient checkpointing 开启
  □ Warmup ratio 3-10%
  □ 有效 batch size ≥ 16
  
□ 评估
  □ 目标任务评估集
  □ 通用能力评估集（防遗忘监控）
  □ 1 条数据过拟合测试通过
  
□ 部署
  □ LoRA 合并测试通过
  □ 合并前后推理结果一致
  □ 推理框架兼容性确认
```

---

## 15. 参考文献

1. Houlsby, N., et al. "Parameter-Efficient Transfer Learning for NLP." ICML 2019.
2. Ding, N., et al. "Delta Tuning: A Comprehensive Study of Parameter Efficient Methods for Pre-trained Language Models." Nature Machine Intelligence, 2023.
3. Hu, E. J., et al. "LoRA: Low-Rank Adaptation of Large Language Models." ICLR 2022.
4. Dettmers, T., et al. "QLoRA: Efficient Finetuning of Quantized LLMs." NeurIPS 2023.
5. Dettmers, T. & Zettlemoyer, L. "The Case for 4-bit Precision: k-bit Inference Scaling Laws." ICML 2023.
6. Liu, S.-Y., et al. "DoRA: Weight-Decomposed Low-Rank Adaptation." ICML 2024.
7. Hayou, S., et al. "LoRA+: Efficient Low Rank Adaptation of Large Models." ICML 2024.
8. Kalajdzievski, D. "A Rank Stabilization Scaling Factor for Fine-Tuning with LoRA." 2023.
9. Zhang, Q., et al. "AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning." ICLR 2023.
10. Li, X. L. & Liang, P. "Prefix-Tuning: Optimizing Continuous Prompts for Generation." ACL 2021.
11. Lester, B., et al. "The Power of Scale for Parameter-Efficient Prompt Tuning." EMNLP 2021.
12. Liu, X., et al. "P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks." ACL 2022.
13. He, J., et al. "Towards a Unified View of Parameter-Efficient Transfer Learning." ICLR 2022.
14. Zhou, C., et al. "LIMA: Less Is More for Alignment." NeurIPS 2023.
15. Chen, L., et al. "AlpaGasus: Training A Better Alpaca with Fewer Data." ICLR 2024.
16. Ouyang, L., et al. "Training Language Models to Follow Instructions with Human Feedback." NeurIPS 2022.
17. Rafailov, R., et al. "Direct Preference Optimization: Your Language Model is Secretly a Reward Model." NeurIPS 2023.
18. Azar, M. G., et al. "A General Theoretical Paradigm to Understand Learning from Human Feedback." AISTATS 2024.
19. Ethayarajh, K., et al. "KTO: Model Alignment as Prospect Theoretic Optimization." ICML 2024.
20. Meng, Y., et al. "SimPO: Simple Preference Optimization with a Reference-Free Reward." NeurIPS 2024.
21. Kirkpatrick, J., et al. "Overcoming Catastrophic Forgetting in Neural Networks." PNAS 2017.
22. Jang, J., et al. "Exploring the Benefits of Training Expert Language Models over Instruction Tuning." ICML 2023.
23. Rajbhandari, S., et al. "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models." SC 2020.
24. Zhao, Y., et al. "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel." VLDB 2023.
25. Yadav, P., et al. "TIES-Merging: Resolving Interference When Merging Models." NeurIPS 2023.
26. Yu, L., et al. "Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch." ICML 2024.
27. Wu, Z., et al. "ReFT: Representation Finetuning for Language Models." NeurIPS 2024.
28. Sheng, Y., et al. "S-LoRA: Serving Thousands of Concurrent LoRA Adapters." MLSys 2024.
29. Aggarwal, A., et al. "LoRA Learns Less and Forgets Less." TMLR 2024.
30. Jang, E., et al. "NEFTune: Noisy Embeddings Improve Instruction Finetuning." ICLR 2024.
31. Zhao, J., et al. "GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection." ICML 2024.
32. Wang, L., et al. "LoRA-XS: Low-Rank Adaptation with Extremely Small Number of Parameters." 2024.

---

*最后更新：2026-02-21*
*本笔记为面试武器级文档，建议配合实际训练经验反复修订。*

---

## See Also（馆长补充）

-  — LLM 知识域全索引
- [[AI面试速查手册|AI 面试速查手册]] — 速查层（同系列面试武器）
- [[RLHF-DPO-2026-技术全景|RLHF DPO 2026 技术全景]] — 本文 RLHF/DPO 章节的深度版（对齐技术完整链路）
- [[AI/3-LLM/Training/LLM预训练与分布式训练2026全景|LLM 预训练与分布式训练 2026]] — 上游：微调的前提是预训练，两文合读构成训练全链路
- [[知识蒸馏与模型压缩-2026技术全景|知识蒸馏与模型压缩 2026]] — 下游：微调后的部署优化方向
