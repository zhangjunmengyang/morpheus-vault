---
brief: "SFT å®æˆ˜æŒ‡å—â€”â€”ä»æ•°æ®å·¥ç¨‹åˆ°æ¨¡å‹éƒ¨ç½²çš„ SFT å®Œæ•´æ“ä½œæ‰‹å†Œï¼›æ•°æ®è´¨é‡è¯„ä¼°/è¶…å‚é€‰æ‹©/è¿‡æ‹Ÿåˆåˆ¤æ–­/è¯„ä¼°æŒ‡æ ‡çš„å·¥ç¨‹ç»éªŒï¼›interview/hotï¼ŒSFT å·¥ç¨‹å¸ˆé¢è¯•çš„æ ¸å¿ƒå‚è€ƒææ–™ã€‚"
title: "SFT å®æˆ˜æŒ‡å—ï¼šä»æ•°æ®åˆ°éƒ¨ç½²"
date: 2026-02-13
tags:
  - ai/llm/training
  - ai/llm/sft
  - ai/fine-tuning
  - type/practice
  - interview/hot
status: active
---

# SFT å®æˆ˜æŒ‡å—ï¼šä»æ•°æ®åˆ°éƒ¨ç½²

> Supervised Fine-Tuning æ˜¯ post-training çš„ç¬¬ä¸€æ­¥â€”â€”ç”¨é«˜è´¨é‡æŒ‡ä»¤æ•°æ®è®©é¢„è®­ç»ƒæ¨¡å‹å­¦ä¼š"å¬è¯"

## 1. SFT åœ¨ Post-Training Pipeline ä¸­çš„ä½ç½®

```
Pre-training â†’ SFT â†’ Preference Alignment (DPO/RLHF) â†’ éƒ¨ç½²
                â†‘
           æœ¬æ–‡é‡ç‚¹
```

SFT çš„ç›®æ ‡ä¸æ˜¯æ•™æ¨¡å‹æ–°çŸ¥è¯†ï¼ˆé‚£æ˜¯ pre-training/CPT çš„äº‹ï¼‰ï¼Œè€Œæ˜¯ï¼š
- å°†é¢„è®­ç»ƒçŸ¥è¯†**å¯¹é½**åˆ°æŒ‡ä»¤-å›ç­”æ ¼å¼
- æ¿€æ´»æ¨¡å‹çš„**æŒ‡ä»¤éµå¾ª**èƒ½åŠ›
- å»ºç«‹**é£æ ¼å’Œå®‰å…¨è¾¹ç•Œ**

å‚è§ [[SFT åŸç†]] äº†è§£ç†è®ºåŸºç¡€ï¼Œæœ¬æ–‡èšç„¦å·¥ç¨‹å®è·µã€‚

## 2. æ•°æ®å‡†å¤‡â€”â€”è´¨é‡ > æ•°é‡

### æ•°æ®é‡çš„åç›´è§‰

```
LIMA è®ºæ–‡ (2023): ä»… 1000 æ¡é«˜è´¨é‡æ•°æ® â†’ åª²ç¾ GPT-4 æ—©æœŸç‰ˆæœ¬
Alpaca:         52K æ¡ GPT-3.5 ç”Ÿæˆæ•°æ® â†’ æ•ˆæœä¸€èˆ¬
WizardLM:       é€æ­¥å¢åŠ å¤æ‚åº¦çš„ 70K æ•°æ® â†’ æ˜¾è‘—æå‡

ç»éªŒæ³•åˆ™:
  - é€šç”¨ SFT: 5K-50K é«˜è´¨é‡æ•°æ®è¶³å¤Ÿ
  - é¢†åŸŸ SFT: 1K-10K é¢†åŸŸæ•°æ® + æ··å…¥é€šç”¨æ•°æ®é˜²é—å¿˜
  - æ•°æ®è´¨é‡ç¿»å€ > æ•°æ®æ•°é‡ç¿»åå€
```

### æ•°æ®è´¨é‡æ£€æŸ¥æ¸…å•

```
âœ… å›ç­”å‡†ç¡®æ€§ï¼šäº‹å®æ­£ç¡®ã€é€»è¾‘ä¸€è‡´
âœ… æŒ‡ä»¤å¤šæ ·æ€§ï¼šè¦†ç›–ç›®æ ‡åœºæ™¯çš„å„ç§ pattern
âœ… å›ç­”é£æ ¼ä¸€è‡´ï¼šç»Ÿä¸€çš„è¯­æ°”ã€æ ¼å¼ã€è¯¦ç•¥ç¨‹åº¦
âœ… å»é‡ï¼šè¯­ä¹‰å»é‡ï¼ˆä¸ä»…ä»…æ˜¯å­—é¢å»é‡ï¼‰
âœ… é•¿åº¦åˆ†å¸ƒåˆç†ï¼šé¿å…å…¨æ˜¯çŸ­å›ç­”æˆ–å…¨æ˜¯é•¿å›ç­”
âœ… éš¾åº¦æ¢¯åº¦ï¼šåŒ…å«ç®€å•/ä¸­ç­‰/å›°éš¾çš„æŒ‡ä»¤
âŒ é¿å…ï¼šä½è´¨é‡ GPT ç”Ÿæˆæ•°æ®ã€æ ¼å¼æ··ä¹±ã€è‡ªç›¸çŸ›ç›¾
```

### æ•°æ®æ¸…æ´— Pipeline

```python
from datasets import load_dataset
import hashlib

def clean_sft_data(dataset):
    """SFT æ•°æ®æ¸…æ´—æµæ°´çº¿"""
    seen_hashes = set()
    cleaned = []
    
    for item in dataset:
        # 1. åŸºæœ¬è¿‡æ»¤
        if len(item["instruction"]) < 10:
            continue  # æŒ‡ä»¤å¤ªçŸ­
        if len(item["output"]) < 20:
            continue  # å›ç­”å¤ªçŸ­
        if len(item["output"]) > 8192:
            continue  # å›ç­”è¿‡é•¿ï¼ˆå¯èƒ½æ˜¯å™ªå£°ï¼‰
        
        # 2. è¯­ä¹‰å»é‡ (ç®€åŒ–ç‰ˆç”¨ hash)
        content_hash = hashlib.md5(
            (item["instruction"] + item["output"]).encode()
        ).hexdigest()
        if content_hash in seen_hashes:
            continue
        seen_hashes.add(content_hash)
        
        # 3. è´¨é‡è¿‡æ»¤ (å¯ç”¨ LLM-as-Judge)
        # score = judge_model.score(item)
        # if score < threshold: continue
        
        cleaned.append(item)
    
    return cleaned
```

## 3. æ•°æ®æ ¼å¼åŒ–

### ChatML æ ¼å¼ï¼ˆOpenAI é£æ ¼ï¼Œä¸»æµï¼‰

```
<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ AI åŠ©æ‰‹ã€‚<|im_end|>
<|im_start|>user
è§£é‡Šä»€ä¹ˆæ˜¯æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚<|im_end|>
<|im_start|>assistant
æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æ˜¯æŒ‡åœ¨æ·±åº¦ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­è¿‡ç¨‹ä¸­...<|im_end|>
```

### Alpaca æ ¼å¼

```
Below is an instruction that describes a task. Write a response.

### Instruction:
è§£é‡Šä»€ä¹ˆæ˜¯æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚

### Input:
ï¼ˆå¯é€‰çš„é¢å¤–è¾“å…¥ï¼‰

### Response:
æ¢¯åº¦æ¶ˆå¤±é—®é¢˜æ˜¯æŒ‡...
```

### ShareGPT / å¤šè½®å¯¹è¯æ ¼å¼

```json
{
  "conversations": [
    {"from": "system", "value": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„ AI åŠ©æ‰‹ã€‚"},
    {"from": "human", "value": "ä»€ä¹ˆæ˜¯ Transformerï¼Ÿ"},
    {"from": "gpt", "value": "Transformer æ˜¯ä¸€ç§åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„..."},
    {"from": "human", "value": "å®ƒå’Œ RNN çš„åŒºåˆ«å‘¢ï¼Ÿ"},
    {"from": "gpt", "value": "ä¸»è¦æœ‰ä¸‰ä¸ªåŒºåˆ«..."}
  ]
}
```

### Loss Masking çš„å…³é”®ç»†èŠ‚

```python
# SFT çš„ loss åªè®¡ç®— assistant å›ç­”éƒ¨åˆ†ï¼Œä¸ç®— instruction/system

# âŒ é”™è¯¯ï¼šå¯¹æ‰€æœ‰ token è®¡ç®— loss
loss = cross_entropy(logits, labels)

# âœ… æ­£ç¡®ï¼šmask æ‰é assistant éƒ¨åˆ†
labels[instruction_mask] = -100  # PyTorch ä¼šå¿½ç•¥ -100 çš„ token
loss = cross_entropy(logits, labels, ignore_index=-100)

# è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ chat template çš„æ­£ç¡®å¤„ç†å¦‚æ­¤é‡è¦ï¼
```

## 4. è¶…å‚é€‰æ‹©

### æ ¸å¿ƒè¶…å‚

```
å‚æ•°              æ¨èå€¼              è¯´æ˜
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Learning Rate    1e-5 ~ 2e-5        Full FT
                 1e-4 ~ 3e-4        LoRA
LR Schedule      Cosine decay       é…åˆ warmup
Warmup Steps     æ€» steps çš„ 3-10%  ç¨³å®šåˆå§‹è®­ç»ƒ
Batch Size       64-128             æœ‰æ•ˆ batch (å«æ¢¯åº¦ç´¯ç§¯)
Epochs           1-3                è¿‡å¤šä¼šè¿‡æ‹Ÿåˆ
Max Seq Length    2048-4096          æŒ‰éœ€è°ƒæ•´
Weight Decay     0.01-0.1           æ­£åˆ™åŒ–
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### ä½¿ç”¨ Unsloth + TRL çš„å®Œæ•´ç¤ºä¾‹

```python
from unsloth import FastLanguageModel
from trl import SFTTrainer, SFTConfig

# 1. åŠ è½½æ¨¡å‹
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2.5-7B-Instruct",
    max_seq_length=4096,
    load_in_4bit=True,  # QLoRA
)

# 2. é…ç½® LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r=32,               # LoRA rank
    lora_alpha=32,       # é€šå¸¸ç­‰äº r
    lora_dropout=0.05,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                     "gate_proj", "up_proj", "down_proj"],
    use_gradient_checkpointing="unsloth",  # å†…å­˜ä¼˜åŒ–
)

# 3. é…ç½®è®­ç»ƒ
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=dataset,
    args=SFTConfig(
        output_dir="./output",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=8,  # æœ‰æ•ˆ batch = 4Ã—8 = 32
        num_train_epochs=2,
        learning_rate=2e-4,
        lr_scheduler_type="cosine",
        warmup_ratio=0.05,
        bf16=True,
        logging_steps=10,
        save_strategy="steps",
        save_steps=200,
        max_seq_length=4096,
    ),
)

# 4. è®­ç»ƒ
trainer.train()

# 5. åˆå¹¶ LoRA æƒé‡å¹¶ä¿å­˜
model.save_pretrained_merged("./merged_model", tokenizer)
```

## 5. è¿‡æ‹Ÿåˆæ£€æµ‹ä¸é˜²å¾¡

### å…¸å‹ä¿¡å·

```
è¿‡æ‹Ÿåˆä¿¡å·:
  ğŸ“‰ è®­ç»ƒ loss æŒç»­ä¸‹é™ä½†éªŒè¯ loss å¼€å§‹ä¸Šå‡
  ğŸ¦œ ç”Ÿæˆæ–‡æœ¬å¼€å§‹"èƒŒè¯µ"è®­ç»ƒæ•°æ®ï¼ˆverbatim reproductionï¼‰
  ğŸ“‹ å›ç­”å˜å¾—æ¨¡æ¿åŒ–ï¼Œç¼ºä¹æ³›åŒ–
  ğŸ“Š éªŒè¯é›† BLEU/ROUGE ä¸‹é™

æ¬ æ‹Ÿåˆä¿¡å·:
  ğŸ“ˆ è®­ç»ƒ loss ä¸‹é™ç¼“æ…¢æˆ–ä¸æ”¶æ•›
  ğŸ¤· æ¨¡å‹ä»ç„¶ä¸éµå¾ªæŒ‡ä»¤æ ¼å¼
```

### é˜²å¾¡ç­–ç•¥

```
1. é™åˆ¶ epochs: é€šå¸¸ 1-3 epochs è¶³å¤Ÿï¼Œæ•°æ®å°‘æ—¶ 1 epoch
2. æ··å…¥é€šç”¨æ•°æ®: é¢†åŸŸæ•°æ® : é€šç”¨æ•°æ® = 1:1 åˆ° 1:3
3. æ—©åœ (Early Stopping): ç›‘æ§éªŒè¯ loss
4. æ­£åˆ™åŒ–: weight decay, dropout, LoRA dropout
5. æ•°æ®å¢å¼º: æ”¹å†™æŒ‡ä»¤ã€è°ƒæ¢é¡ºåº
6. è¯„ä¼°å¤šç»´åº¦: ä¸åªçœ‹ lossï¼Œåšäººå·¥è¯„æµ‹
```

## 6. LoRA vs Full Fine-Tuning é€‰å‹

```
ç»´åº¦               LoRA                    Full FT
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
è®­ç»ƒå‚æ•°é‡          0.1-1% å‚æ•°             100% å‚æ•°
æ˜¾å­˜éœ€æ±‚           ~16GB (7B+QLoRA)         ~120GB (7B, BF16+Adam)
è®­ç»ƒé€Ÿåº¦           å¿« 40-60%               åŸºå‡†
æ•ˆæœä¸Šé™           æ¥è¿‘ Full FT (90-95%)    æœ€é«˜
ç¾éš¾æ€§é—å¿˜         è¾ƒè½»                    è¾ƒé‡
å¤šä»»åŠ¡/å¤šå®¢æˆ·       å­˜å‚¨é«˜æ•ˆï¼ˆå¤šå¥— adapterï¼‰  éœ€è¦å¤šä»½å®Œæ•´æ¨¡å‹
é€‚ç”¨åœºæ™¯           èµ„æºæœ‰é™ / å¿«é€Ÿè¿­ä»£       è¿½æ±‚æè‡´æ•ˆæœ / å¤§è§„æ¨¡æ•°æ®
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### LoRA Rank é€‰æ‹©

```
r=8:   å¿«é€Ÿå®éªŒã€ç®€å•ä»»åŠ¡
r=16:  é€šç”¨æ¨è
r=32:  å¤æ‚ä»»åŠ¡ã€é¢†åŸŸé€‚é…
r=64:  æ¥è¿‘ Full FT æ•ˆæœ
r=128+: é€šå¸¸ä¸å¦‚ç›´æ¥ Full FT
```

### ä»€ä¹ˆæ—¶å€™å¿…é¡» Full FTï¼Ÿ

- è¯­è¨€/æ¨¡æ€æ‰©å±•ï¼ˆå¦‚è‹±æ–‡æ¨¡å‹é€‚é…ä¸­æ–‡ï¼‰
- å¤§è§„æ¨¡çŸ¥è¯†æ³¨å…¥ï¼ˆCPT + SFT pipelineï¼‰
- å¯¹è¾“å‡ºè´¨é‡æœ‰æè‡´è¦æ±‚çš„äº§å“

## 7. å¸¸è§å‘ä¸ Debug æŠ€å·§

```
å‘                              è§£å†³æ–¹æ¡ˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Loss ä¸ä¸‹é™                     æ£€æŸ¥ lrï¼ˆé€šå¸¸å¤ªå¤§ï¼‰ã€label maskingã€æ•°æ®æ ¼å¼
Loss ä¸‹é™ä½†ç”Ÿæˆè´¨é‡å·®            label masking å¯èƒ½æœ‰é—®é¢˜â€”â€”ç¡®è®¤åªå¯¹ response è®¡ç®— loss
å¤šè½®å¯¹è¯æ ¼å¼é”™è¯¯                ç”¨ tokenizer.apply_chat_template() è€Œéæ‰‹åŠ¨æ‹¼æ¥
ç”Ÿæˆé‡å¤/ä¹±ç                    æ£€æŸ¥ tokenizer çš„ special tokens æ˜¯å¦å¯¹é½
LoRA åˆå¹¶åæ•ˆæœä¸‹é™             æ£€æŸ¥ merge æ—¶çš„ dtypeï¼ˆé¿å…ç²¾åº¦æŸå¤±ï¼‰
è®­ç»ƒåæ¨¡å‹å˜"è¯ç—¨"              æ•°æ®ä¸­é•¿å›ç­”å¤ªå¤šï¼Œæ··å…¥ç®€æ´å›ç­”æˆ–æ§åˆ¶ max_new_tokens
```

## 8. é¢è¯•é«˜é¢‘é¢˜

### Q1: SFT å’Œ Pre-training çš„æ ¸å¿ƒåŒºåˆ«ï¼Ÿ
**ç­”**ï¼šPre-training æ˜¯åœ¨å¤§è§„æ¨¡æ— æ ‡æ³¨è¯­æ–™ä¸Šåš next token predictionï¼Œç›®æ ‡æ˜¯å­¦ä¹ è¯­è¨€çŸ¥è¯†å’Œä¸–ç•ŒçŸ¥è¯†ã€‚SFT æ˜¯åœ¨é«˜è´¨é‡æŒ‡ä»¤-å›ç­”å¯¹ä¸Šå¾®è°ƒï¼Œç›®æ ‡æ˜¯å°†çŸ¥è¯†å¯¹é½åˆ°ç”¨æˆ·æœŸæœ›çš„äº¤äº’æ ¼å¼ã€‚å…³é”®åŒºåˆ«ï¼š(1) æ•°æ®é‡ï¼šPT ä¸‡äº¿ token vs SFT å‡ ä¸‡æ¡ï¼›(2) Loss ç›®æ ‡ï¼šPT å¯¹æ‰€æœ‰ token è®¡ç®— lossï¼ŒSFT åªå¯¹ response éƒ¨åˆ†è®¡ç®—ï¼›(3) å­¦ä¹ æ•ˆç‡ï¼šSFT é€šå¸¸ 1-3 epochs å³å¯ï¼ŒPT å¯èƒ½è·‘ä¸åˆ° 1 epochã€‚

### Q2: ä¸ºä»€ä¹ˆ SFT æ•°æ®è´¨é‡æ¯”æ•°é‡é‡è¦ï¼Ÿ
**ç­”**ï¼šLIMA è®ºæ–‡è¯æ˜ 1000 æ¡é«˜è´¨é‡æ•°æ®å³å¯è·å¾—å¥½æ•ˆæœã€‚åŸå› æ˜¯ SFT ä¸æ˜¯æ•™æ¨¡å‹æ–°çŸ¥è¯†ï¼Œè€Œæ˜¯æ¿€æ´»å·²æœ‰èƒ½åŠ›â€”â€”æ¨¡å‹åœ¨ PT é˜¶æ®µå·²ç»è§è¿‡æµ·é‡æ–‡æœ¬ï¼ŒSFT åªéœ€ç»™å‡º"æ­£ç¡®ç¤ºèŒƒ"è®©æ¨¡å‹å­¦ä¼šè¾“å‡ºæ ¼å¼å’Œé£æ ¼ã€‚ä½è´¨é‡æ•°æ®åè€Œä¼šæ•™æ¨¡å‹åä¹ æƒ¯ï¼ˆå¹»è§‰ã€æ ¼å¼æ··ä¹±ï¼‰ã€‚å®è·µä¸­ï¼Œ1000 æ¡äººå·¥ç²¾æ ‡æ•°æ® > 50000 æ¡ GPT ç”Ÿæˆæ•°æ®ã€‚

### Q3: ChatML å’Œ Alpaca æ ¼å¼å¦‚ä½•é€‰æ‹©ï¼Ÿ
**ç­”**ï¼šå¦‚æœåŸºäºå·²æœ‰çš„ Instruct æ¨¡å‹å¾®è°ƒï¼Œ**å¿…é¡»ä½¿ç”¨è¯¥æ¨¡å‹çš„åŸç”Ÿ chat template**ï¼ˆå¦‚ Qwen ç”¨ ChatMLï¼ŒLLaMA 3 ç”¨å…¶ä¸“å± templateï¼‰ï¼Œå¦åˆ™ä¼šç ´åå·²æœ‰çš„å¯¹é½æ•ˆæœã€‚å¦‚æœä» Base æ¨¡å‹å¼€å§‹ SFTï¼ŒChatML æ˜¯æ›´å¥½çš„é€‰æ‹©â€”â€”å®ƒæœ‰æ˜ç¡®çš„è§’è‰²æ ‡è®°å’Œåˆ†éš”ç¬¦ï¼Œæ”¯æŒ system prompt å’Œå¤šè½®å¯¹è¯ã€‚Alpaca æ ¼å¼ç®€å•ä½†åªæ”¯æŒå•è½®ã€‚

### Q4: LoRA rank å¦‚ä½•é€‰æ‹©ï¼Ÿæœ‰ä»€ä¹ˆç†è®ºä¾æ®ï¼Ÿ
**ç­”**ï¼šLoRA çš„æ ¸å¿ƒå‡è®¾æ˜¯å¾®è°ƒæ—¶çš„æƒé‡å˜åŒ–æ˜¯ä½ç§©çš„ï¼ˆâˆ†W = BAï¼ŒBâˆˆR^{dÃ—r}, AâˆˆR^{rÃ—k}ï¼‰ã€‚rank r å†³å®šäº†èƒ½æ•è·çš„å˜åŒ–å¤æ‚åº¦ã€‚ç»éªŒæ³•åˆ™ï¼šç®€å•ä»»åŠ¡/å°‘é‡æ•°æ®ç”¨ r=8-16ï¼›å¤æ‚é¢†åŸŸé€‚é…ç”¨ r=32-64ï¼›r>64 æ—¶å‚æ•°é‡æ¥è¿‘ Full FTï¼Œä¸å¦‚ç›´æ¥ Full FTã€‚ç†è®ºä¸Šï¼Œr çš„é€‰æ‹©ä¸ä»»åŠ¡çš„"å†…åœ¨ç»´åº¦"ï¼ˆintrinsic dimensionï¼‰ç›¸å…³â€”â€”å¤šæ•°å¾®è°ƒä»»åŠ¡çš„å†…åœ¨ç»´åº¦è¿œä½äºæ¨¡å‹ç»´åº¦ã€‚

### Q5: å¦‚ä½•åˆ¤æ–­ SFT è®­ç»ƒæ˜¯å¦æˆåŠŸï¼ŸLoss æ›²çº¿è¯¥æ€ä¹ˆçœ‹ï¼Ÿ
**ç­”**ï¼š(1) è®­ç»ƒ loss åº”è¯¥åœ¨ç¬¬ä¸€ä¸ª epoch å†…å¿«é€Ÿä¸‹é™ç„¶åè¶‹äºå¹³ç¨³ï¼ˆå…¸å‹ 0.5-1.5 ä¹‹é—´ï¼‰ï¼›(2) å¦‚æœ loss < 0.5 ä¸”è¿˜åœ¨å¿«é€Ÿä¸‹é™ï¼Œå¯èƒ½åœ¨èƒŒè¯µæ•°æ®ï¼›(3) å¿…é¡»åŒæ—¶çœ‹éªŒè¯ lossï¼Œè®­ç»ƒå’ŒéªŒè¯ loss å·®è· > 0.3 è¯´æ˜è¿‡æ‹Ÿåˆï¼›(4) Loss åªæ˜¯å¿…è¦æ¡ä»¶ï¼Œä¸æ˜¯å……åˆ†æ¡ä»¶â€”â€”æœ€ç»ˆè¦åšäººå·¥è¯„æµ‹æˆ– LLM-as-Judge è¯„ä¼°å®é™…ç”Ÿæˆè´¨é‡ã€‚æ¨èç»´æŠ¤ä¸€ä¸ªå›ºå®šçš„ eval prompt setï¼Œæ¯ä¸ª checkpoint éƒ½è·‘ä¸€éå¯¹æ¯”ã€‚

---

**ç›¸å…³ç¬”è®°**ï¼š[[SFT åŸç†]] | [[LoRA]] | [[SFT-TRLå®è·µ]] | [[Post-Training Unified View è®ºæ–‡]]
