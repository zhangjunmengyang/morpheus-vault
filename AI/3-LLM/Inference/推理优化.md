---
title: "LLM 推理优化全景"
brief: "LLM 推理优化系统综述：从算子层（FlashAttention/算子融合）→ 调度层（Continuous Batching/PagedAttention）→ 模型层（量化/剪枝/Speculative Decoding）→ 系统层（PD分离/多GPU并行）的完整优化图谱。面试常考：各技术的适用场景和 trade-off。"
date: 2026-02-13
tags:
  - ai/llm/inference
  - ai/llm/optimization
  - type/concept
  - interview/hot
status: active
---

# LLM 推理优化全景

> Speculative Decoding、Continuous Batching、Quantization、TensorRT-LLM — 从延迟到吞吐的全方位优化

## 1. 推理性能指标

在展开优化技术之前，先明确核心指标：

| 指标 | 定义 | 优化方向 |
|------|------|----------|
| **TTFT** (Time To First Token) | 首 token 延迟 | Prefill 优化 |
| **TPOT** (Time Per Output Token) | 每个输出 token 耗时 | Decode 优化 |
| **Throughput** | tokens/s (全系统) | 批处理 + 并行 |
| **Latency** | 端到端响应时间 | 全链路 |

```
LLM 推理两阶段:
┌─────────────────┐    ┌──────────────────────────────┐
│    Prefill       │───→│         Decode               │
│ (Compute-bound) │    │     (Memory-bound)           │
│ 处理全部 prompt  │    │ 逐 token 生成，每步读全量 KV │
│ TTFT 在这里      │    │ TPOT × N = 主要延迟          │
└─────────────────┘    └──────────────────────────────┘
```

## 2. Speculative Decoding（投机解码）

### 2.1 核心思想

用一个**小模型（draft model）** 快速生成多个候选 token，再用**大模型一次性验证**。

```
传统解码 (N steps):
大模型: [t1] → [t2] → [t3] → [t4] → [t5]   共 5 次 forward

投机解码 (≈2 steps):
小模型: [t1, t2, t3, t4, t5]  快速 draft（便宜）
大模型: verify([t1, t2, t3, t4, t5])  一次 forward 验证全部
        → 接受 [t1, t2, t3]，拒绝 t4   → 大模型重新生成 t4
        → 继续 draft...
```

### 2.2 为什么 Speculative Decoding 有效？

- LLM decode 阶段是 **memory-bound**（每步只生成 1 token，但要读取全部模型权重）
- GPU 大量计算能力闲置
- 大模型验证 K 个 token 的成本 ≈ 生成 1 个 token（并行验证）
- **数学保证**：通过 rejection sampling，最终分布与大模型完全一致（无损！）

### 2.3 实现方式

```python
# Speculative Decoding 核心逻辑
def speculative_decode(draft_model, target_model, prompt, K=5):
    tokens = prompt
    while not done:
        # 1. Draft model 快速生成 K 个候选 token
        draft_tokens, draft_probs = draft_model.generate(tokens, max_new=K)
        
        # 2. Target model 一次 forward 验证所有候选
        target_probs = target_model.forward(tokens + draft_tokens)
        
        # 3. Rejection sampling: 逐个验证
        accepted = 0
        for i in range(K):
            r = random.random()
            if r < target_probs[i] / draft_probs[i]:  # 接受
                accepted += 1
            else:  # 拒绝，从 target 分布重采样
                new_token = sample_adjusted(target_probs[i], draft_probs[i])
                tokens.append(new_token)
                break
        
        if accepted == K:  # 全部接受，额外从 target 采样一个
            tokens.extend(draft_tokens)
            tokens.append(sample(target_probs[K]))
```

### 2.4 变体

| 方法 | Draft 来源 | 特点 |
|------|-----------|------|
| 标准 Speculative | 小模型 | 需要额外模型 |
| Self-Speculative | 同模型的早期层 | 无需额外模型 |
| Medusa | 多头预测 | 训练额外预测头 |
| Eagle | 自回归 draft head | 更高接受率 |
| DeepSeek MTP | Multi-Token Prediction | 训练时即内置 |
| Lookahead | N-gram 猜测 | 无需训练 |

DeepSeek-V3 的 **MTP（Multi-Token Prediction）** 是在训练时就加入了多 token 预测目标，推理时天然支持 speculative decoding。

## 3. Continuous Batching（连续批处理）

### 3.1 问题：静态批处理的浪费

```
静态 Batching:
Request 1: [████████████████]        完成!
Request 2: [██████████████████████]  完成!
Request 3: [████████]                完成! ← 短请求等长请求
Request 4: [████████████]            完成!
           ↑ 所有请求同时开始同时结束，短请求 GPU 空等 ↑

Continuous Batching:
Request 1: [████████████████]  完成 → Request 5 立刻加入!
Request 2: [██████████████████████]  还在继续...
Request 3: [████████] 完成 → Request 6 加入!
Request 4: [████████████] 完成 → Request 7 加入!
           ↑ 有位就插，GPU 持续满载 ↑
```

### 3.2 实现机制

```python
# Continuous Batching 调度伪代码
class ContinuousBatchScheduler:
    def __init__(self, max_batch_size, max_tokens):
        self.running = []    # 正在生成的请求
        self.waiting = []    # 等待队列
    
    def step(self):
        # 1. 移除已完成的请求
        self.running = [r for r in self.running if not r.is_done()]
        
        # 2. 从等待队列填充空位
        while len(self.running) < self.max_batch_size and self.waiting:
            new_req = self.waiting.pop(0)
            new_req.prefill()  # 预填充
            self.running.append(new_req)
        
        # 3. 对所有 running 请求执行一步 decode
        batch_decode(self.running)
```

### 3.3 Preemption（抢占）

当显存不足时，vLLM 支持两种抢占策略：
- **Swap**：将低优先级请求的 KV Cache 换出到 CPU
- **Recompute**：直接丢弃 KV Cache，需要时重新 prefill

## 4. Quantization（量化）

### 4.1 量化基础

将高精度（FP16/BF16）权重和激活压缩为低精度（INT8/INT4/FP8）：

```
FP16:  每个参数 2 bytes → 70B 模型 ≈ 140 GB
INT8:  每个参数 1 byte  → 70B 模型 ≈  70 GB
INT4:  每个参数 0.5 byte → 70B 模型 ≈  35 GB
```

### 4.2 主流量化方案对比

| 方案 | 类型 | 精度 | 速度 | 适用场景 |
|------|------|------|------|----------|
| **GPTQ** | Post-Training (Weight-only) | 高 | GPU 快 (需 Marlin) | GPU 部署 |
| **AWQ** | Post-Training (Weight-only) | 最高 | GPU 最快 | GPU 生产环境 |
| **GGUF** | Post-Training (Weight-only) | 中高 | CPU 友好 | 本地/CPU 部署 |
| **BitsAndBytes** | 动态量化 | 中 | 较慢 | 快速实验 |
| **FP8** | Hardware-native | 很高 | H100 最快 | H100/B200 |
| **SmoothQuant** | W8A8 (权重+激活) | 高 | 很快 | TensorRT-LLM |

### 4.3 GPTQ

基于 **OBQ (Optimal Brain Quantization)** 的逐层量化：

```python
# GPTQ 量化流程（概念）
for each layer in model:
    # 1. 收集校准数据的激活值
    activations = collect_activations(calibration_data, layer)
    
    # 2. 计算 Hessian 矩阵 H = X^T X
    H = activations.T @ activations
    
    # 3. 逐列量化权重，利用 Hessian 信息补偿误差
    for col in range(weight.shape[1]):
        w_q = quantize(weight[:, col])
        error = weight[:, col] - w_q
        # 关键：将误差分散到后续未量化的列
        weight[:, col+1:] -= error * H_inv[col, col+1:] / H_inv[col, col]
```

实操：
```bash
# 使用 AutoGPTQ
from auto_gptq import AutoGPTQForCausalLM
model = AutoGPTQForCausalLM.from_quantized("TheBloke/Llama-3-70B-GPTQ",
    device_map="auto", use_marlin=True)  # Marlin kernel 关键!
```

### 4.4 AWQ (Activation-Aware Weight Quantization)

核心洞察：**不是所有权重同等重要**。按照激活值的分布保护显著权重：

```python
# AWQ 核心思想
# 1. 找到 "salient" 权重（对应激活值大的通道）
scales = activation_magnitudes.max(dim=0)  # per-channel scale

# 2. 对显著通道的权重先放大（保护），量化后再缩回
weight_scaled = weight * scales  # 放大显著权重
weight_quantized = quantize(weight_scaled)  # 量化
# 推理时: output = (weight_quantized / scales) @ activation
```

**AWQ + Marlin kernel** 在 vLLM 中可达 **741 tok/s**，比未优化快 10x+。

### 4.5 GGUF

llama.cpp 生态的量化格式，CPU 推理的首选：

```bash
# 转换为 GGUF 格式
python convert_hf_to_gguf.py model_dir --outfile model.gguf --outtype q4_K_M

# 量化级别:
# Q2_K: 最激进，质量损失大
# Q4_K_M: 平衡点（推荐）
# Q5_K_M: 质量好，体积稍大
# Q8_0: 几乎无损
```

适用于 [[Ollama|Ollama]] 本地部署。

## 5. TensorRT-LLM

NVIDIA 官方的 LLM 推理优化引擎：

### 5.1 核心特性

```
TensorRT-LLM 优化栈:
┌──────────────────────────────┐
│     Python API (构建)         │
├──────────────────────────────┤
│  Graph Optimization          │
│  - Layer Fusion              │
│  - Kernel Auto-Tuning        │
│  - Memory Planning           │
├──────────────────────────────┤
│  Quantization                │
│  - SmoothQuant (W8A8)        │
│  - FP8 (Hopper native)       │
│  - AWQ / GPTQ support        │
├──────────────────────────────┤
│  Attention Optimization      │
│  - FlashAttention             │
│  - Paged KV Cache            │
│  - Multi-Block Mode           │
├──────────────────────────────┤
│  Parallelism                 │
│  - Tensor Parallel            │
│  - Pipeline Parallel          │
│  - Expert Parallel (MoE)      │
├──────────────────────────────┤
│  CUDA / cuBLAS / cuDNN        │
└──────────────────────────────┘
```

### 5.2 使用流程

```bash
# 1. 构建优化引擎
trtllm-build --checkpoint_dir ./checkpoint \
    --output_dir ./engine \
    --gemm_plugin float16 \
    --max_batch_size 64 \
    --max_input_len 2048 \
    --max_seq_len 4096

# 2. 运行推理
python run.py --engine_dir ./engine \
    --tokenizer meta-llama/Llama-3.1-70B-Instruct
```

### 5.3 2025 推理引擎对比

| 引擎 | 优势 | 劣势 | 适用场景 |
|------|------|------|----------|
| **vLLM** | 易用、PagedAttention、社区活跃 | 不如 TRT-LLM 极致 | 通用首选 |
| **TensorRT-LLM** | 极致性能、NVIDIA 深度优化 | 构建复杂、NVIDIA 锁定 | 追求极致延迟 |
| **TGI v3** | HuggingFace 生态、长对话优化 | 性能中等 | HF 生态用户 |
| **LMDeploy** | 4-bit 优化好、TurboMind | 社区较小 | Llama 系列量化 |
| **SGLang** | RadixAttention、前端灵活 | 新兴 | 复杂 prompting |
| **llama.cpp** | CPU 推理、GGUF | 不适合大规模 | 本地/边缘 |

## 6. 综合优化方案

### 6.1 生产级配置示例

```python
# vLLM 生产配置
from vllm import LLM, SamplingParams

llm = LLM(
    model="meta-llama/Llama-3.1-70B-Instruct",
    tensor_parallel_size=4,          # 4 GPU TP
    quantization="awq",              # AWQ 量化
    kv_cache_dtype="fp8",            # KV Cache FP8
    gpu_memory_utilization=0.9,
    enable_prefix_caching=True,      # Prefix Cache
    max_num_seqs=256,                # Continuous Batching
    speculative_model="meta-llama/Llama-3.1-8B-Instruct",
    num_speculative_tokens=5,        # Speculative Decoding
)
```

### 6.2 优化技术叠加效果

```
基线: 100 tokens/s (FP16, 无优化)
+ AWQ INT4:           200 tok/s  (2x)
+ Marlin kernel:      400 tok/s  (2x)
+ Continuous Batching: 800 tok/s  (2x throughput)
+ Speculative Decode:  延迟降低 2-3x
+ Prefix Caching:     TTFT 降低 40-70%（重复 prefix 时）
+ KV Cache FP8:       显存节省 50% → 更大 batch
```

## 7. 面试常见问题

**Q1: Speculative Decoding 会影响输出质量吗？**
A: 不会。通过 rejection sampling 数学保证最终分布与原始大模型完全一致。这是无损加速。

**Q2: Continuous Batching 和 Static Batching 的吞吐差多少？**
A: 通常 **2-8 倍**。差距取决于序列长度方差——长短差异越大，continuous batching 优势越明显。

**Q3: GPTQ 和 AWQ 选哪个？**
A: 生产环境推荐 **AWQ + Marlin kernel**，精度更好、推理更快。GPTQ 历史更久但需要 Marlin kernel 才能发挥速度。GGUF 只在 CPU 场景选择。

**Q4: FP8 量化和 INT4 量化如何选择？**
A: 有 H100/B200 → **FP8** 首选（硬件原生支持，精度损失极小）。消费级 GPU → INT4 (AWQ/GPTQ)。需要在精度和速度间权衡。

**Q5: 推理优化的优先级顺序是什么？**
A: 1) Quantization（最大的单项收益）→ 2) Continuous Batching（吞吐）→ 3) KV Cache 优化（显存/长序列）→ 4) Speculative Decoding（延迟）→ 5) Kernel 优化（FlashAttention/Marlin）。

## 相关链接

- [[AI/3-LLM/Inference/KV Cache|KV Cache 优化]] — PagedAttention & FlashAttention 细节
- [[vLLM|vLLM]] — 最流行的推理引擎
- [[Ollama|Ollama]] — 本地部署（GGUF）
- [[MoE 深度解析|MoE 深度解析]] — MoE 推理特殊性
- [[量化 & 显存预估|量化 & 显存预估]] — Unsloth 量化实践
