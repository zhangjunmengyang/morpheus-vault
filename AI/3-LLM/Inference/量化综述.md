---
title: "量化综述：GPTQ / AWQ / GGUF"
brief: "LLM 量化主流方案对比：GPTQ（PTQ，逐层最小化重建误差，4-bit）/ AWQ（激活值感知权重量化，保护重要通道）/ GGUF（llama.cpp 格式，CPU 友好分层量化）。面试重点：何时用哪种量化、精度-速度 trade-off、量化对模型的实际影响。"
date: 2026-02-13
tags:
  - ai/llm/inference
  - ai/llm/quantization
  - type/comparison
  - interview/hot
status: active
---

# 量化综述：GPTQ / AWQ / GGUF

> 把 70B 模型塞进消费级显卡——三大量化方案的原理、对比与选型

## 1. 为什么需要量化？

| 模型 | FP16 显存 | INT4 显存 | 可用硬件 |
|------|-----------|-----------|----------|
| Llama-3.1-7B | ~14 GB | ~4 GB | RTX 3060 (12GB) |
| Llama-3.1-70B | ~140 GB | ~35 GB | 2×A100 → 1×A100 |
| Llama-3.1-405B | ~810 GB | ~200 GB | 8×A100 → 2×A100 |

核心权衡：**精度 ↔ 速度 ↔ 显存**

```
FP32 (32bit) → FP16 (16bit) → INT8 (8bit) → INT4 (4bit)
  精度最高         通常训练        轻微损失         显著压缩
  4x 显存          2x 显存         1x 显存         0.5x 显存
```

## 2. 量化基础

### Post-Training Quantization (PTQ) vs QAT

- **PTQ**：训练后量化，不需要重新训练。GPTQ、AWQ、GGUF 都是 PTQ。
- **QAT**：量化感知训练，在训练中模拟量化误差。精度更好但需要训练资源。

### 权重量化的数学

将 FP16 权重映射到 INT4：

$$w_{int} = \text{round}\left(\frac{w_{fp} - z}{s}\right), \quad w_{fp} \approx s \cdot w_{int} + z$$

其中 $s$（scale）和 $z$（zero-point）是量化参数，通常按 **group** 计算（如每 128 个权重共享一组参数）。

参见 [[AI/3-LLM/Frameworks/Unsloth/量化|量化]] 和 [[AI/3-LLM/Frameworks/Unsloth/量化 & 显存预估|量化 & 显存预估]] 中的基础概念。

## 3. GPTQ (GPT Quantization)

### 原理

Frantar et al. (2022) 基于 **Optimal Brain Quantization**：逐列量化权重矩阵，利用 Hessian 信息补偿量化误差。

核心思想：量化第 i 列时，将产生的误差按 Hessian 逆 **分散到尚未量化的列**。

```
权重矩阵 W = [w1, w2, ..., wn]

Step 1: 量化 w1 → w1_q，计算误差 δ1
Step 2: 用 H^{-1} 将 δ1 补偿到 w2...wn
Step 3: 量化 w2 → w2_q，计算误差 δ2
...
关键: 每步的误差补偿让全局误差最小化
```

### 技术细节

```python
# 使用 auto-gptq 量化
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

quantize_config = BaseQuantizeConfig(
    bits=4,               # 量化位数
    group_size=128,       # 每组共享量化参数
    desc_act=True,        # 按激活值大小排序列（提高精度）
    damp_percent=0.01,    # Hessian 的阻尼因子
)

model = AutoGPTQForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-70B",
    quantize_config=quantize_config,
)

# 需要校准数据（128~256 条，用于计算 Hessian）
model.quantize(calibration_dataset)
model.save_quantized("Llama-3.1-70B-GPTQ-Int4")
```

### 特点

- ✅ 学术论文支撑，理论最严谨
- ✅ 配合 Marlin kernel 可达极高推理速度
- ✅ 支持 2/3/4/8 bit
- ❌ 量化过程慢（需要逐列 Hessian 计算）
- ❌ 不支持 CPU 推理
- ❌ `desc_act=True` 时不兼容部分推理引擎

## 4. AWQ (Activation-aware Weight Quantization)

### 原理

Lin et al. (2023)：**并非所有权重同等重要**。通过分析激活值分布，找到"显著"通道（salient channels），对其保留更高精度。

核心洞察：1% 的显著权重通道承载了大部分信息。跳过这些通道的量化（或用更高精度）可以显著降低误差。

```
传统量化: 所有权重统一 INT4 → 部分重要权重误差大
AWQ: 先找显著通道 → 对显著权重乘以缩放因子 s → 再统一量化
     效果: 显著权重的相对量化误差大幅降低
```

### 数学表示

对权重 $W$ 和激活 $X$，AWQ 找到缩放因子 $s$：

$$\min_s \| Q(W \cdot \text{diag}(s)) \cdot \text{diag}(s)^{-1} \cdot X - W \cdot X \|$$

```python
# 使用 autoawq 量化
from awq import AutoAWQForCausalLM
from transformers import AutoTokenizer

model = AutoAWQForCausalLM.from_pretrained("meta-llama/Llama-3.1-70B")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.1-70B")

quant_config = {
    "zero_point": True,
    "q_group_size": 128,
    "w_bit": 4,
    "version": "GEMM"  # 或 "GEMV"（batch=1 优化）
}

model.quantize(tokenizer, quant_config=quant_config)
model.save_quantized("Llama-3.1-70B-AWQ")
```

### 特点

- ✅ 量化速度快于 GPTQ（不需要 Hessian）
- ✅ 相同 bit 下精度通常优于 GPTQ
- ✅ 配合 Marlin kernel 推理速度极佳
- ✅ vLLM / TGI 原生支持
- ❌ 仅支持 4-bit（主流实现）
- ❌ GPU-only

## 5. GGUF (GPT-Generated Unified Format)

### 原理

GGUF 是 llama.cpp 社区开发的模型格式和量化方案。特点是 **CPU-friendly**，使用自定义的混合精度量化。

核心技术：按 block（通常 32 个权重）量化，不同 block 可用不同精度。**K-quant** 方案对不同层的重要性做自适应量化。

### 量化类型对比

| 量化类型 | 每权重 bit | 相对精度 | 说明 |
|----------|-----------|----------|------|
| Q2_K | 2.6 | ~85% | 极度压缩，精度损失明显 |
| Q3_K_M | 3.4 | ~90% | 内存极度受限时使用 |
| Q4_0 | 4.0 | ~91% | 基础 4-bit，速度快 |
| **Q4_K_M** | **4.8** | **~92%** | **推荐默认选择** |
| Q5_K_M | 5.7 | ~95% | 精度-大小平衡好 |
| Q6_K | 6.6 | ~97% | 接近 FP16 精度 |
| Q8_0 | 8.0 | ~99% | 几乎无损 |

```bash
# 使用 llama.cpp 量化
# 1. 转换 HF 模型为 GGUF
python convert_hf_to_gguf.py meta-llama/Llama-3.1-70B \
    --outfile llama-70b-f16.gguf

# 2. 量化
./llama-quantize llama-70b-f16.gguf \
    llama-70b-Q4_K_M.gguf Q4_K_M

# 3. 推理（Ollama 直接使用 GGUF）
ollama run llama3.1:70b-q4_K_M
```

### 特点

- ✅ **CPU 推理**：Mac (Metal) / x86 (AVX) 均支持
- ✅ Ollama / llama.cpp 生态
- ✅ 量化类型丰富，灵活选择精度-大小权衡
- ✅ 单文件格式，易分发
- ❌ GPU 推理速度不如 GPTQ/AWQ + Marlin
- ❌ 不被 vLLM 主力支持（有限兼容）
- ❌ 大模型 CPU 推理仍然很慢

参见 [[AI/3-LLM/Inference/Ollama|Ollama]] 中的本地部署。

## 6. 三方对比

### 精度对比（Llama-3.1-70B INT4，Perplexity 越低越好）

| 方法 | PPL (WikiText-2) | 相对 FP16 |
|------|-----------------|-----------|
| FP16 (baseline) | 3.32 | 100% |
| GPTQ-4bit-g128 | 3.48 | 95.4% |
| AWQ-4bit-g128 | 3.42 | 97.1% |
| GGUF-Q4_K_M | 3.51 | 94.6% |

### 推理速度（单张 A100-80G，batch=1，tok/s）

| 方法 | 无 Marlin | 有 Marlin |
|------|-----------|-----------|
| FP16 | 38 | — |
| GPTQ-4bit | 45 | 126 |
| AWQ-4bit | 42 | 130 |
| GGUF-Q4_K_M (GPU) | 35 | — |

> ⚠️ 关键发现（2026 JarvisLabs benchmark）：没有 Marlin kernel 时，GPTQ/AWQ 甚至可能比 FP16 更慢！Marlin 是 GPU 量化推理的关键加速器。

### 速度（M4 Max MacBook, CPU+GPU 混合）

| 方法 | tok/s |
|------|-------|
| GGUF-Q4_K_M (llama.cpp Metal) | 22 |
| GGUF-Q8_0 (llama.cpp Metal) | 14 |

## 7. 选型指南

```
你的硬件是什么？
├── NVIDIA GPU (A100/H100/4090)
│   ├── 用 vLLM/TGI 部署？→ AWQ（精度最好 + Marlin 加速）
│   └── 需要极致优化？→ GPTQ + Marlin（更多调参空间）
│
├── Apple Silicon (M1/M2/M3/M4)
│   └── llama.cpp / Ollama → GGUF Q4_K_M（性价比最高）
│
├── CPU only
│   └── GGUF（唯一选择）
│
└── 混合部署 (GPU + CPU offload)
    └── GGUF（原生支持层分配）
```

### 实践建议

1. **精度优先**：AWQ > GPTQ > GGUF（同 bit 数下）
2. **GPU 速度优先**：AWQ/GPTQ + Marlin kernel
3. **Mac 本地**：GGUF Q4_K_M 是黄金标准
4. **校准数据**：GPTQ/AWQ 都需要 128~256 条校准样本，用目标领域数据效果更好

## 8. 进阶话题

### 2-bit 量化

QuIP#、AQLM 等方案将量化推到 2-bit，通过向量量化（vector quantization）减少误差。精度损失仍然较大，但 405B 模型可以装入单机。

### FP8 量化

NVIDIA H100 原生支持 FP8，无需 PTQ 校准，几乎无损。正在成为大模型推理的新默认选择。

### 动态量化

BitsAndBytes 的 LLM.int8()：按 outlier 特征动态选择 FP16/INT8 混合。简单但速度不如静态量化。

## 9. 面试常见问题

1. **Q: GPTQ 和 AWQ 的核心区别？**
   A: GPTQ 基于 Hessian 信息做逐列误差补偿（OBQ 框架），AWQ 基于激活值感知找显著通道做缩放。AWQ 量化更快、精度略优，但 GPTQ 支持更多 bit 配置。

2. **Q: 为什么 GGUF 在 GPU 上比 GPTQ/AWQ 慢？**
   A: GGUF 的 kernel 优化针对 CPU（SIMD/Metal），没有像 Marlin 这样的 GPU 专用反量化 kernel。GPU 上 GPTQ/AWQ 可以用 Marlin kernel 融合反量化与矩阵乘法。

3. **Q: 量化会影响模型的哪些能力？**
   A: 低 bit 量化对以下能力影响较大：长尾知识（rare facts）、数学推理精度、代码细节。对通用对话、摘要等影响较小。4-bit 是实践中的"甜点"——损失可接受，压缩率 4x。

4. **Q: 校准数据怎么选？**
   A: 用目标领域数据最佳。通用场景用 C4/WikiText 子集。128~256 条通常足够，更多校准数据边际收益很小。

5. **Q: Group size 128 vs 64？**
   A: Group size 越小，每组量化参数越精确，精度越好，但存储开销越大（更多 scale/zero-point）。128 是精度-开销的甜点。

## 相关笔记

- [[AI/3-LLM/Frameworks/Unsloth/量化|量化]] — 量化基础概念
- [[AI/3-LLM/Frameworks/Unsloth/量化 & 显存预估|量化 & 显存预估]] — Unsloth 量化实践
- [[AI/3-LLM/Inference/推理优化|推理优化]] — 推理优化全景
- [[AI/3-LLM/Inference/KV Cache|KV Cache 优化]] — KV Cache 优化
- [[AI/3-LLM/Inference/vLLM|vLLM]] — vLLM 推理引擎
- [[AI/3-LLM/Inference/Ollama|Ollama]] — Ollama 本地部署
- [[AI/1-Foundations/ML-Basics/PPL 困惑度|PPL 困惑度]] — 困惑度评估
