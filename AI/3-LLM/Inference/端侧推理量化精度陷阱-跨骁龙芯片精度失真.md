---
title: "端侧推理量化精度陷阱：跨骁龙芯片精度失真"
brief: "工程避坑笔记：同一 INT4/INT8 量化模型在不同骁龙芯片（8 Gen 1/2/3）上输出精度不一致，原因是 QNN SDK 版本差异和 NPU 硬件 FP16 精度处理不同。记录 3 类精度失真场景、复现方法和标准 workaround（强制 CPU offload 的层/mixed precision 策略）。"
type: engineering-note
domain: ai/llm/inference
tags:
  - ai/llm
  - topic/inference
  - topic/quantization
  - on-device
  - snapdragon
  - npu
  - engineering
  - pitfall
rating: ★★★★☆
date: 2026-02-19
source: https://www.reddit.com/r/MachineLearning/comments/1r7ruu8/
archived_by: librarian
archived_date: 2026-02-20
---

# 端侧推理量化精度陷阱：跨骁龙芯片精度失真

> **TL;DR**: 相同权重、相同 ONNX 文件、相同 INT8 量化——跨 5 款骁龙 SoC 精度差达 20 个百分点。云端 benchmark 完全失真。必须在目标机型上跑真实精度测试。

---

## 问题现象

社区测试报告（r/MachineLearning, 2026-02-19）：

**测试条件**：
- 相同模型权重（INT8）
- 相同 ONNX 文件
- ONNX Runtime QNN ExecutionProvider
- 5 款骁龙 SoC，跨三代芯片

**精度测试结果**：

| 芯片 | 精度 | 相对云端基准 |
|------|------|------------|
| 骁龙 8 Gen 3（旗舰） | **91.8%** | -2.4% |
| 骁龙 8 Gen 2 | 87.3% | -6.9% |
| 骁龙 7 Gen 3 | 81.5% | -12.7% |
| 骁龙 6 Gen 3 | 77.4% | -16.8% |
| 骁龙 4 Gen 2（入门） | **71.2%** | **-23.0%** |
| **云端 benchmark（A100）** | **94.2%** | 基准 |

**结论**：同款模型，云端报 94.2%，但在入门骁龙机型上只有 71.2%——**差距超过 20 个百分点**。

---

## 根因分析

### 根因 1：NPU 对 INT8 算子的实现差异

不同代际骁龙的 Hexagon NPU 对 INT8 算子存在不同实现：

- **INT8 累加精度**：高端芯片用 INT32 累加，低端芯片可能用 INT16 累加→累积误差更大
- **Rounding Mode 差异**：round-half-up vs round-half-to-even 在边界值上产生不同结果
- **算子融合策略不同**：高端 NPU 支持更多 fused op（如 Conv+BN+ReLU 融合），低端分开执行→精度路径不同
- **权重布局**：不同 SoC 对 weight layout（NHWC vs NCHW）的硬件偏好不同，Transpose 可能引入精度损失

### 根因 2：ONNX Runtime QNN 抽象层的局限

ONNX Runtime 的 QNN ExecutionProvider 设计目标是跨设备兼容性，但：
- 硬件抽象层没有统一各 SoC 的低级算子行为
- 同一个 `onnx.MatMul` 在不同 SoC 上映射到不同 QNN 算子
- Quantization 的 zero_point 和 scale 在 QNN EP 下的解释可能因 SoC 而异

### 根因 3：量化方案本身的脆弱性

事后 INT8 量化（Post-Training Quantization, PTQ）的问题：
- Calibration data 在 GPU 上，不代表 NPU 执行路径
- 量化 scale 是为 FP32→INT8 设计的，没有考虑 NPU 的特定累加精度

---

## 后果：云端 benchmark 完全失真

这个现象说明：
1. **论文/厂商报告的量化精度基本是 GPU/CPU 上的数字**，与端侧 NPU 无关
2. **"INT8 精度损失 <2%"** 这类说法只在旗舰机型上成立
3. **边缘设备上的精度预算**（ability to absorb quantization error）远低于云端

---

## 并行信号：QAT 是解法方向

同期信号（2026-02-19）：**Qwen 3.5 官方即将发布 MXFP4 量化版本**（Qwen 团队负责人 Junyang Lin 确认），采用 Quantization-Aware Training（QAT），与 OpenAI GPT-o 系列和 Google Gemma 3 QAT 同路线。

**PTQ vs QAT 在端侧的关键区别**：

| 方案 | 量化时机 | 精度损失 | 端侧适配 |
|------|---------|---------|---------|
| PTQ（事后量化） | 训练后 | 中等（通常 2-5%，NPU 上可达 20%+）| 差，不感知硬件差异 |
| QAT（训练时感知）| 训练中 | 低（通常 <1%）| 好，模型学会适应低精度 |
| MXFP4（微软格式）| 训练时 | 极低 | 理论上好，但 NPU 支持有限 |

**行动建议**：端侧 AI 推理新项目，**等 QAT 版本**，不要用 bf16 后量化再 PTQ 的两步方案。

---

## 工程实践建议

### 测试原则
1. **必须在目标机型上跑真实精度测试**——云端数字仅供参考
2. **覆盖所有目标 SoC 代际**：不能只测旗舰，入门机型可能差 20%
3. **建立精度基准矩阵**：维护一个"芯片 × 量化方案 × 精度"的真实测试表

### 量化方案选择
1. **优先 QAT 方案**：等厂商发布官方 QAT 量化版（如 Qwen3.5 MXFP4）
2. **如果必须 PTQ**：在目标 SoC 上做 calibration，而非 GPU 上
3. **精度降级容忍设计**：应用层设计时预留"低端机型 fallback"逻辑

### NPU 算子兼容性
1. 在部署前运行 `onnxruntime_perf_test`，检查算子是否全部 NPU offload
2. 关注 `onnxruntime` QNN EP 的 release notes，各版本对算子支持变化较大
3. 考虑使用芯片厂商的原生 SDK（高通 QNN SDK）而非 ONNX Runtime 抽象层

---

## 相关背景：Hexagon NPU 代际差异

骁龙 Hexagon NPU 各代之间的 INT8 性能差异主要来自：

| 代际 | Hexagon 版本 | INT8 TOPS | 主要 INT8 改进 |
|------|-------------|----------|--------------|
| 8 Gen 3 | Hexagon 73 | 98 TOPS | Vector Extensions 增强，INT8 矩阵乘法硬件加速 |
| 8 Gen 2 | Hexagon 72 | 45 TOPS | — |
| 7 Gen 3 | Hexagon 71 | 15 TOPS | 低功耗设计优先 |
| 6/4 Gen | Hexagon 68/65 | <10 TOPS | 精度和性能均受限 |

低端 Hexagon 版本在处理 LLM 的 attention 算子时尤其容易出现精度问题——attention 的 softmax 数值稳定性对累加精度非常敏感。

---

## see-also

- [[AI/3-LLM/Inference/量化综述|量化综述]] — 量化方案综述（PTQ/QAT/GPTQ 等）
- [[AI/3-LLM/Inference/量化综述]] — 量化综述（另一版本）
- [[AI/3-LLM/Inference/Jet-RL-FP8-On-Policy-RL-Training|Jet-RL]] — 量化精度不一致问题在 RL 训练侧的体现（FP8 on-policy 精度统一）

---

*归档：馆长 · 2026-02-20 · 来源：r/MachineLearning + 2026-02-19 Sentinel 情报*
