---
title: GQA / MQA æ·±åº¦è§£æ
brief: Grouped Query Attention å’Œ Multi-Query Attention çš„åŸç†ã€å®ç°ä¸æ€§èƒ½å¯¹æ¯”ã€‚GQA æ˜¯ MHA å’Œ MQA çš„æ³›åŒ–ï¼Œé€šè¿‡å°† Q heads åˆ†ç»„å…±äº« KV å®ç° 75-87.5% çš„ KV Cache èŠ‚çœä¸”è´¨é‡æŸå¤± <0.5%ã€‚2025 å¹´ GQA (G=8) å·²æˆä¸ºå¼€æº LLM çš„äº‹å®æ ‡å‡†ã€‚
date: 2026-02-13
updated: 2026-02-22
tags:
  - ai/llm/architecture
  - ai/attention
  - ai/llm/inference
  - type/concept
  - interview/hot
status: complete
sources:
  - "Ainslie et al. GQA: Training Generalized Multi-Query Attention from Multi-Head Checkpoints. arXiv:2305.13245"
  - "Shazeer. Fast Transformer Decoding: One Write-Head is All You Need (MQA). arXiv:1911.02150"
  - Vaswani et al. Attention Is All You Need (MHA). arXiv:1706.03762
related:
  - "[[Attention å˜ä½“ç»¼è¿°|Attention å˜ä½“ç»¼è¿°]]"
  - "[[FlashAttention|FlashAttention]]"
  - "[[AI/3-LLM/Inference/KV Cache|KV Cache åŸç†ä¸ä¼˜åŒ–]]"
  - "[[Multi-Head Latent Attention|MLA è¯¦è§£]]"
  - "[[LLaMA|LLaMA]]"
---

# GQA / MQA æ·±åº¦è§£æ

> Grouped Query Attention & Multi-Query Attentionâ€”â€”KV Cache å¤§å°ä¸æ¨¡å‹è´¨é‡çš„ç²¾å¦™å¹³è¡¡

## 1. ä» MHA è¯´èµ·

### Multi-Head Attention (MHA)

æ ‡å‡† Transformer ä½¿ç”¨ MHAï¼šæ¯ä¸ª head æœ‰ç‹¬ç«‹çš„ Qã€Kã€V æŠ•å½±çŸ©é˜µï¼š

```
MHA (Multi-Head Attention):
  Q heads: h ä¸ªç‹¬ç«‹çš„ Q æŠ•å½±  â†’ h ä¸ª Q head
  K heads: h ä¸ªç‹¬ç«‹çš„ K æŠ•å½±  â†’ h ä¸ª K head
  V heads: h ä¸ªç‹¬ç«‹çš„ V æŠ•å½±  â†’ h ä¸ª V head

  æ¯ä¸ª head: Attention_i = softmax(Q_i @ K_i^T / âˆšd_k) @ V_i

ä¾‹: LLaMA-1 65B
  h = 64 heads, d_model = 8192, d_k = d_v = 128
  KV Cache per token per layer = 2 Ã— 64 Ã— 128 Ã— 2 bytes = 32 KB
  80 layers Ã— 32 KB = 2.56 MB per token
  4096 tokens Ã— 2.56 MB = 10.5 GB  â† ä»… KV Cacheï¼
```

**é—®é¢˜**ï¼šæ¨ç†æ—¶ KV Cache éšåºåˆ—é•¿åº¦çº¿æ€§å¢é•¿ï¼Œé•¿ä¸Šä¸‹æ–‡åœºæ™¯ä¸‹ KV Cache ç”šè‡³è¶…è¿‡æ¨¡å‹æƒé‡æœ¬èº«çš„æ˜¾å­˜å ç”¨ã€‚

## 2. Multi-Query Attention (MQA)

> æ¥æºï¼šShazeer, "Fast Transformer Decoding: One Write-Head is All You Need", arXiv:1911.02150

### æ ¸å¿ƒæ€æƒ³

**æ‰€æœ‰ Q head å…±äº«åŒä¸€ç»„ K å’Œ V**ï¼š

```
MQA:
  Q heads: h ä¸ªç‹¬ç«‹çš„ Q æŠ•å½±  â†’ h ä¸ª Q head  (ä¸å˜)
  K heads: 1 ä¸ªå…±äº« K æŠ•å½±   â†’ 1 ä¸ª K head   (h ä¸ª head å…±äº«!)
  V heads: 1 ä¸ªå…±äº« V æŠ•å½±   â†’ 1 ä¸ª V head   (h ä¸ª head å…±äº«!)

  æ‰€æœ‰ head: Attention_i = softmax(Q_i @ K^T / âˆšd_k) @ V
                                     â†‘å…±äº«    â†‘å…±äº«
```

### KV Cache èŠ‚çœ

```
MHA:  KV Cache = 2 Ã— h Ã— d_k Ã— n_layers Ã— seq_len
MQA:  KV Cache = 2 Ã— 1 Ã— d_k Ã— n_layers Ã— seq_len

èŠ‚çœæ¯”ä¾‹ = 1/h = 1/64 â‰ˆ 98.4% (å¯¹ h=64 çš„æ¨¡å‹)
```

### ä»£ç å®ç°

```python
import torch
import torch.nn as nn

class MultiQueryAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        # Q: å¤šä¸ª head (æ­£å¸¸)
        self.W_q = nn.Linear(d_model, d_model)
        # K, V: åªæœ‰ 1 ä¸ª head
        self.W_k = nn.Linear(d_model, self.d_k)    # d_model â†’ d_k (é d_model!)
        self.W_v = nn.Linear(d_model, self.d_k)
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x, kv_cache=None):
        B, N, _ = x.shape

        Q = self.W_q(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(B, N, 1, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(B, N, 1, self.d_k).transpose(1, 2)

        # K, V å¹¿æ’­åˆ°æ‰€æœ‰ head: (B, 1, N, d_k) â†’ (B, h, N, d_k)
        K = K.expand(-1, self.n_heads, -1, -1)
        V = V.expand(-1, self.n_heads, -1, -1)

        # æ ‡å‡† Attention
        scores = (Q @ K.transpose(-2, -1)) / (self.d_k ** 0.5)
        attn = torch.softmax(scores, dim=-1)
        out = (attn @ V).transpose(1, 2).contiguous().view(B, N, -1)
        return self.W_o(out)
```

### MQA çš„é—®é¢˜

- **è´¨é‡ä¸‹é™**ï¼šæ‰€æœ‰ head å…±äº« K/Vï¼Œè¡¨è¾¾èƒ½åŠ›å—é™
- åœ¨å¤§æ¨¡å‹ï¼ˆ> 30Bï¼‰ä¸Šä¸‹é™è¾ƒæ˜æ˜¾ï¼Œå°¤å…¶æ˜¯éœ€è¦å¤šç§ attention pattern çš„ä»»åŠ¡
- ä»£è¡¨æ¨¡å‹ï¼šPaLM (Google)ã€Falconã€StarCoder

## 3. Grouped Query Attention (GQA)

> æ¥æºï¼šAinslie et al., "GQA: Training Generalized Multi-Query Attention from Multi-Head Checkpoints", arXiv:2305.13245

### æ ¸å¿ƒæ€æƒ³

MQA å¤ªæ¿€è¿›ï¼ˆ1 ç»„ KVï¼‰ï¼ŒMHA å¤ªæµªè´¹ï¼ˆh ç»„ KVï¼‰ã€‚GQA å–æŠ˜ä¸­ï¼š**å°† h ä¸ª Q head åˆ†æˆ G ç»„ï¼Œæ¯ç»„å…±äº«ä¸€ç»„ KV**ï¼š

```
GQA (G ç»„):
  Q heads: h ä¸ªç‹¬ç«‹ Q headï¼Œåˆ†æˆ G ç»„
  K heads: G ä¸ª K headï¼Œæ¯ç»„ h/G ä¸ª Q head å…±äº«ä¸€ä¸ª K
  V heads: G ä¸ª V headï¼Œæ¯ç»„ h/G ä¸ª Q head å…±äº«ä¸€ä¸ª V

  h=32, G=8 æ—¶:
  æ¯ 4 ä¸ª Q head å…±äº« 1 ä¸ª K/V head

ç‰¹æ®Šæƒ…å†µ:
  G = h â†’ GQA é€€åŒ–ä¸º MHA (æ¯ä¸ª Q æœ‰ç‹¬ç«‹ KV)
  G = 1 â†’ GQA é€€åŒ–ä¸º MQA (æ‰€æœ‰ Q å…±äº«åŒä¸€ KV)
```

### ç›´è§‚ç†è§£

```mermaid
flowchart LR
    subgraph MHA["MHA (G=h=8)"]
        direction TB
        MHA_Q["Q: q1..q8\n8 ç‹¬ç«‹ Q heads"]
        MHA_KV["K: k1..k8\nV: v1..v8\n8 ç‹¬ç«‹ KV heads"]
        MHA_C["KV Cache: 100%"]
    end
    subgraph GQA4["GQA (h=8, G=4)"]
        direction TB
        G4_Q["Q: q1..q8\n8 Q heads, 4 ç»„"]
        G4_KV["K: k1 k2 k3 k4\nV: v1 v2 v3 v4\næ¯ç»„ 2Q å…±äº« 1KV"]
        G4_C["KV Cache: 50%"]
    end
    subgraph MQA["MQA (G=1)"]
        direction TB
        MQA_Q["Q: q1..q8\n8 Q heads"]
        MQA_KV["K: k1\nV: v1\nå…¨éƒ¨å…±äº« 1 KV"]
        MQA_C["KV Cache: 12.5%"]
    end
    MHA -->|"å‡å°‘KV heads"| GQA4 -->|"æç«¯å‹ç¼©"| MQA
```

**æ–‡æœ¬å¯¹ç…§**ï¼š

| é…ç½® | Q heads | KV heads | æ¯ç»„ Q å…±äº«æ•° | KV Cache å æ¯” |
|------|---------|----------|-------------|-------------|
| MHA (G=8) | 8 ç‹¬ç«‹ | 8 ç‹¬ç«‹ | 1:1 | 100% |
| GQA (G=4) | 8, åˆ† 4 ç»„ | 4 | 2:1 | 50% |
| GQA (G=2) | 8, åˆ† 2 ç»„ | 2 | 4:1 | 25% |
| MQA (G=1) | 8 | 1 å…±äº« | 8:1 | 12.5% |

### ä»£ç å®ç°

```python
class GroupedQueryAttention(nn.Module):
    def __init__(self, d_model, n_heads, n_kv_heads):
        super().__init__()
        self.n_heads = n_heads          # Q head æ•°
        self.n_kv_heads = n_kv_heads    # KV head æ•° (G)
        self.n_groups = n_heads // n_kv_heads  # æ¯ç»„ Q head æ•°
        self.d_k = d_model // n_heads

        self.W_q = nn.Linear(d_model, n_heads * self.d_k)
        self.W_k = nn.Linear(d_model, n_kv_heads * self.d_k)  # åªæœ‰ G ä¸ª head
        self.W_v = nn.Linear(d_model, n_kv_heads * self.d_k)
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x):
        B, N, _ = x.shape

        Q = self.W_q(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(B, N, self.n_kv_heads, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(B, N, self.n_kv_heads, self.d_k).transpose(1, 2)

        # å…³é”®: å°† KV æ‰©å±•åˆ°åŒ¹é… Q heads
        # (B, n_kv_heads, N, d_k) â†’ (B, n_heads, N, d_k)
        K = K.repeat_interleave(self.n_groups, dim=1)
        V = V.repeat_interleave(self.n_groups, dim=1)

        # æ ‡å‡† Attention
        scores = (Q @ K.transpose(-2, -1)) / (self.d_k ** 0.5)
        attn = torch.softmax(scores, dim=-1)
        out = (attn @ V).transpose(1, 2).contiguous().view(B, N, -1)
        return self.W_o(out)
```

## 4. ä¸»æµæ¨¡å‹çš„é€‰æ‹©

| æ¨¡å‹ | Attention ç±»å‹ | Q heads | KV heads (G) | ç»„æ¯” | KV Cache èŠ‚çœ |
|------|-------------|---------|-------------|------|-------------|
| GPT-3 175B | MHA | 96 | 96 | 1:1 | 0% |
| PaLM 540B | MQA | 48 | 1 | 48:1 | 97.9% |
| Falcon 40B | MQA | 64 | 1 | 64:1 | 98.4% |
| [[AI/3-LLM/Architecture/LLaMA|LLaMA]] 2 70B | GQA | 64 | 8 | 8:1 | **87.5%** |
| LLaMA 3 8B | GQA | 32 | 8 | 4:1 | **75%** |
| LLaMA 3 70B | GQA | 64 | 8 | 8:1 | **87.5%** |
| Mistral 7B | GQA | 32 | 8 | 4:1 | **75%** |
| DeepSeek-V3 | MLA | â€” | â€” | â€” | æ›´æ¿€è¿›å‹ç¼© |
| Qwen 2.5 72B | GQA | 64 | 8 | 8:1 | **87.5%** |

> **2025 å¹´è¶‹åŠ¿**ï¼šGQA å·²æˆä¸ºå¼€æº LLM çš„äº‹å®æ ‡å‡†ã€‚MQA å› è´¨é‡ä¸‹é™æ˜æ˜¾é€æ¸è¢«å¼ƒç”¨ã€‚DeepSeek çš„ MLAï¼ˆMulti-head Latent Attentionï¼‰ä»£è¡¨æ›´æ¿€è¿›çš„ KV å‹ç¼©æ–¹å‘ã€‚

## 5. KV Cache å†…å­˜è®¡ç®—

### å…¬å¼

```
KV Cache å¤§å° = 2 Ã— n_kv_heads Ã— d_k Ã— n_layers Ã— seq_len Ã— batch_size Ã— bytes_per_param

ç¤ºä¾‹: LLaMA 3 70B, seq_len=8192, batch=16, FP16
  = 2 Ã— 8 Ã— 128 Ã— 80 Ã— 8192 Ã— 16 Ã— 2
  = 2 Ã— 8 Ã— 128 Ã— 80 Ã— 8192 Ã— 16 Ã— 2
  = 34.36 GB

å¯¹æ¯” MHA ç‰ˆæœ¬ (KV heads=64):
  = 2 Ã— 64 Ã— 128 Ã— 80 Ã— 8192 Ã— 16 Ã— 2
  = 274.88 GB  â† ä¸å¯èƒ½æ”¾å…¥ä»»ä½•å•å¡ï¼

GQA èŠ‚çœ: 274.88 - 34.36 = 240.52 GB (87.5%)
```

### ä¸ [[AI/3-LLM/Inference/KV Cache|PagedAttention]] çš„ååŒ

GQA å‡å°‘ KV heads æ•°é‡ â†’ æ¯ä¸ª page æ›´å° â†’ PagedAttention ç®¡ç†æ›´é«˜æ•ˆ â†’ ç›¸åŒæ˜¾å­˜å¯æœåŠ¡æ›´å¤šå¹¶å‘è¯·æ±‚ã€‚

## 6. MHA â†’ GQA è½¬æ¢ï¼ˆUptrainingï¼‰

Google çš„ GQA è®ºæ–‡æå‡ºäº†å°†å·²è®­ç»ƒçš„ MHA æ¨¡å‹è½¬æ¢ä¸º GQA çš„æ–¹æ³•ï¼š

```
æ­¥éª¤:
1. å°† h ä¸ª KV head æŒ‰ç»„åˆ†ç»„
2. ç»„å†… KV æƒé‡å–å‡å€¼ (mean pooling)
3. ç”¨åŸå§‹æ•°æ®ç»§ç»­è®­ç»ƒ 5-10% çš„ tokens (uptraining)

æ•ˆæœ:
  MHA (h=64) â†’ GQA (G=8): ä»…éœ€åŸè®­ç»ƒ 5% çš„é¢å¤–è®­ç»ƒ
  è´¨é‡ä¸‹é™: < 1% (åœ¨å¤§å¤šæ•° benchmark ä¸Š)
```

## 7. æ€§èƒ½å¯¹æ¯”

### æ¨ç†ååé‡

```
æ¨¡å‹: 70B, A100 80GB Ã— 4, seq_len=4096

é…ç½®          | åå (tokens/s) | æœ€å¤§ batch | å»¶è¿Ÿ (ms/token)
MHA (G=64)    |     800         |     4      |     5.0
GQA (G=8)     |    3200         |    32      |     1.25
MQA (G=1)     |    4000         |    48      |     1.0

GQA vs MHA: ååæå‡ 4xï¼Œå› ä¸º KV Cache å° â†’ batch å¤§ â†’ å¹¶è¡Œåº¦é«˜
MQA vs GQA: ä»…é¢å¤– 25% æå‡ï¼Œä½†è´¨é‡æŸå¤±æ›´å¤§
```

### è´¨é‡å¯¹æ¯”

```
æ¨¡å‹: 30B scale, ç›¸åŒè®­ç»ƒæ•°æ®å’Œ tokens

           MMLU    HumanEval   GSM8K    å¹³å‡
MHA        68.5    42.1        54.2     54.9
GQA (G=8)  68.1    41.5        53.8     54.5  (-0.4)
MQA (G=1)  66.8    38.9        51.1     52.3  (-2.6)

GQA å‡ ä¹æ— æŸï¼ŒMQA ä¸‹é™æ˜æ˜¾
```

## 8. ä¸å…¶ä»–ä¼˜åŒ–çš„å…³ç³»

- **[[FlashAttention|FlashAttention]]**ï¼šGQA å‡å°‘ KV head â†’ æ¯ä¸ª head çš„ KV åºåˆ—ä¸å˜ï¼Œä½†æ€» KV å°‘ â†’ FlashAttention è®¡ç®—æ›´å¿«
- **[[AI/3-LLM/Inference/KV Cache|KV Cache ä¼˜åŒ–]]**ï¼šGQA æ˜¯ KV Cache ä¼˜åŒ–çš„ **æ¶æ„å±‚** æ–¹æ¡ˆï¼Œä¸ PagedAttentionï¼ˆç³»ç»Ÿå±‚ï¼‰äº’è¡¥
- **[[æ¨ç†ä¼˜åŒ–|æ¨ç†ä¼˜åŒ–]]**ï¼šGQA æ˜¯æ¨ç†ä¼˜åŒ–ä¸­æœ€é‡è¦çš„æ¶æ„è®¾è®¡é€‰æ‹©ä¹‹ä¸€
- **[[Continuous Batching|Continuous Batching]]**ï¼šKV Cache å° â†’ ç›¸åŒæ˜¾å­˜å¯å®¹çº³æ›´å¤šå¹¶å‘è¯·æ±‚ â†’ Continuous Batching æ•ˆç‡æ›´é«˜
- **[[é‡åŒ–ç»¼è¿°|é‡åŒ–]]**ï¼šGQA + INT4 é‡åŒ– = KV Cache åŒé‡å‹ç¼©

## é¢è¯•å¸¸è§é—®é¢˜

### Q1: GQA å’Œ MQA çš„æ ¸å¿ƒåŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿå„è‡ªé€‚åˆä»€ä¹ˆåœºæ™¯ï¼Ÿ

**GQA** å°† Q heads åˆ†æˆ G ç»„ï¼Œæ¯ç»„å…±äº«ä¸€å¥— KVï¼›**MQA** æ˜¯ G=1 çš„æç«¯æƒ…å†µï¼Œæ‰€æœ‰ Q heads å…±äº«åŒä¸€å¥— KVã€‚GQA åœ¨è´¨é‡å’Œæ•ˆç‡é—´å–å¾—æ›´å¥½å¹³è¡¡â€”â€”ä»¥ LLaMA 3 70B ä¸ºä¾‹ï¼Œ8 ä¸ª KV heads å·²èŠ‚çœ 87.5% KV Cacheï¼Œè´¨é‡ä¸‹é™ < 0.5%ï¼›è€Œ MQA è™½ç„¶èŠ‚çœ 98%+ï¼Œä½†è´¨é‡ä¸‹é™ 2-3%ã€‚**2025 å¹´ GQA å·²æˆä¸ºä¸šç•Œé»˜è®¤é€‰æ‹©**ã€‚

### Q2: GQA å¦‚ä½•èŠ‚çœ KV Cacheï¼Ÿå…·ä½“èŠ‚çœå¤šå°‘ï¼Ÿ

KV Cache å¤§å°æ­£æ¯”äº `n_kv_heads`ã€‚GQA å°† KV heads ä» h å‡å°‘åˆ° Gï¼ŒèŠ‚çœæ¯”ä¾‹ = `(h - G) / h`ã€‚ä¾‹å¦‚ LLaMA 3 70B (h=64, G=8)ï¼šèŠ‚çœ 87.5%ï¼Œ8192 é•¿åº¦ batch=16 æ—¶ä» ~275 GB é™è‡³ ~34 GBã€‚è¿™ä¸ä»…èŠ‚çœæ˜¾å­˜ï¼Œæ›´å…³é”®çš„æ˜¯å…è®¸æ›´å¤§ batch size â†’ æé«˜ååã€‚

### Q3: å¦‚ä½•å°†å·²è®­ç»ƒçš„ MHA æ¨¡å‹è½¬æ¢ä¸º GQAï¼Ÿ

Google çš„ uptraining æ–¹æ³•ï¼š(1) å°† KV heads æŒ‰ç›®æ ‡åˆ†ç»„ï¼›(2) ç»„å†… KV æƒé‡å–å‡å€¼ä½œä¸ºå…±äº«æƒé‡åˆå§‹åŒ–ï¼›(3) ç»§ç»­è®­ç»ƒçº¦ 5% çš„åŸå§‹è®­ç»ƒ token æ•°ã€‚æˆæœ¬è¿œä½äºä»å¤´è®­ç»ƒï¼Œè´¨é‡æ¥è¿‘åŸå§‹ MHA æ¨¡å‹ã€‚

### Q4: DeepSeek çš„ MLA å’Œ GQA æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

MLA (Multi-head Latent Attention) æ¯” GQA æ›´æ¿€è¿›ï¼šä¸æ˜¯ç®€å•å‡å°‘ KV head æ•°é‡ï¼Œè€Œæ˜¯å°† KV æŠ•å°„åˆ°ä¸€ä¸ªä½ç§©æ½œåœ¨ç©ºé—´ï¼Œç¼“å­˜å‹ç¼©åçš„ latent vectorï¼ˆç»´åº¦è¿œå°äºåŸå§‹ KVï¼‰ã€‚è§£ç æ—¶å†è§£å‹ç¼©ã€‚æ•ˆæœæ˜¯ KV Cache å‹ç¼©åˆ°åŸå§‹çš„ ~5-10%ï¼Œæ¯” GQA (12.5%) è¿˜å°ï¼Œä¸”è´¨é‡ä¸‹é™æ›´å°‘ã€‚ç¼ºç‚¹æ˜¯éœ€è¦é¢å¤–çš„å‹ç¼©/è§£å‹è®¡ç®—ã€‚

### Q5: GQA çš„ G å€¼å¦‚ä½•é€‰æ‹©ï¼Ÿ

ç»éªŒæ³•åˆ™ï¼š**G = h/8 åˆ° h/4** æ˜¯æœ€ä½³åŒºé—´ã€‚å¤ªå°ï¼ˆæ¥è¿‘ MQAï¼‰è´¨é‡ä¸‹é™æ˜æ˜¾ï¼›å¤ªå¤§ï¼ˆæ¥è¿‘ MHAï¼‰èŠ‚çœä¸å¤Ÿã€‚ä¸»æµé€‰æ‹©æ˜¯ G=8ï¼šLLaMA 2/3 70Bã€Qwen 2.5 72Bã€Mistral ç³»åˆ—éƒ½ç”¨ G=8ã€‚å¯¹å°æ¨¡å‹ï¼ˆ7B-13Bï¼‰ï¼ŒG=4ï¼ˆå¦‚ n_heads=32, n_kv_heads=8ï¼‰ä¹Ÿå¾ˆå¸¸è§ã€‚é€‰æ‹©è¿˜éœ€è€ƒè™‘ TP å¹¶è¡Œâ€”â€”G åº”è¯¥èƒ½è¢« TP degree æ•´é™¤ã€‚

---

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **æ¨ç†æœåŠ¡å®¹é‡è§„åˆ’**ï¼šKV Cache å…¬å¼ $\text{size} = 2 \times G \times d_k \times L \times s \times b \times \text{bytes}$ï¼Œç›´æ¥è®¡ç®—æœ€å¤§ batch size
- **æ¨¡å‹é€‰å‹**ï¼šåŒç­‰è§„æ¨¡ä¸‹ä¼˜å…ˆé€‰ GQA æ¨¡å‹ï¼ˆLLaMA 3 > LLaMA 1ï¼‰ï¼Œæ¨ç†ååå¯å·® 4x
- **MHAâ†’GQA è¿ç§»**ï¼šå¯¹å·²æœ‰ MHA æ¨¡å‹ï¼Œç”¨ Google çš„ uptraining æ–¹æ³•åªéœ€åŸè®­ç»ƒé‡ 5% å³å¯è½¬æ¢

### å·¥ç¨‹å®ç°è¦ç‚¹
- GQA å®ç°çš„å…³é”®æ˜¯ `repeat_interleave`ï¼šå°† G ä¸ª KV heads æ‰©å±•åˆ° h ä¸ª Q heads å¯¹é½
- TP å¹¶è¡Œæ—¶ G å¿…é¡»èƒ½è¢« TP degree æ•´é™¤ï¼Œå¦åˆ™éœ€è¦ KV heads replication
- GQA + [[FlashAttention|FlashAttention]] + [[AI/3-LLM/Inference/KV Cache|PagedAttention]] ä¸‰è€…ååŒæ˜¯ 2025 å¹´æ¨ç†ä¼˜åŒ–çš„æ ‡å‡† stack

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: GQA å’Œ MQA çš„æ•°å­¦å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ
  A: GQA æ˜¯æ³›åŒ–â€”â€”G=h æ—¶é€€åŒ–ä¸º MHAï¼ŒG=1 æ—¶é€€åŒ–ä¸º MQAã€‚KV Cache èŠ‚çœæ¯”ä¾‹ = $(h-G)/h$

---

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- GQA æ˜¯"å…è´¹çš„åˆé¤"â€”â€”å‡ ä¹ä¸æŸå¤±è´¨é‡å°±èƒ½å¤§å¹…æé«˜æ¨ç†æ•ˆç‡ã€‚ä»»ä½•æ–°é¡¹ç›®é€‰æ¨¡å‹æ—¶ï¼ŒGQA æ”¯æŒæ˜¯ç¡¬æ€§è¦æ±‚
- **KV Cache å¤§å°ç›´æ¥å†³å®šäº†ä½ çš„æ¨ç†æˆæœ¬**ï¼šGQA è®©åŒæ ·çš„ GPU é›†ç¾¤å¤šæœåŠ¡ 4-8 å€ç”¨æˆ·

### æœªè§£é—®é¢˜ä¸å±€é™
- GQA çš„ uptraining åªé€‚ç”¨äºåŒæ¶æ„çš„ MHAâ†’GQA è½¬æ¢ï¼Œè·¨æ¶æ„ï¼ˆå¦‚ MHAâ†’MLAï¼‰æ²¡æœ‰æˆç†Ÿæ–¹æ¡ˆ
- G å€¼çš„æœ€ä¼˜é€‰æ‹©ä»é ç»éªŒï¼Œæ²¡æœ‰ç†è®ºæŒ‡å¯¼ï¼ˆå–å†³äºä»»åŠ¡ã€æ¨¡å‹è§„æ¨¡ã€ç¡¬ä»¶é…ç½®çš„äº¤äº’ï¼‰

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- å¦‚æœ G å€¼å¯ä»¥ per-layer ä¸åŒï¼ˆæµ…å±‚ç”¨å° Gï¼Œæ·±å±‚ç”¨å¤§ Gï¼‰ï¼Œæ˜¯å¦èƒ½è¿›ä¸€æ­¥ä¼˜åŒ–è´¨é‡/æ•ˆç‡ tradeoffï¼Ÿç±»ä¼¼ [[MoE æ·±åº¦è§£æ|MoE]] çš„ per-layer expert æ•°é‡è°ƒæ•´
- MLA çš„æˆåŠŸè¯´æ˜ KV çš„ä½ç§©å‹ç¼©æ¯”ç®€å•çš„ head å…±äº«æ›´ä¼˜â€”â€”æœªæ¥å¯èƒ½å‡ºç°è‡ªé€‚åº”çš„ per-head å‹ç¼©ç‡

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [GQA: Training Generalized Multi-Query Attention from Multi-Head Checkpoints](https://arxiv.org/abs/2305.13245) â€” GQA åŸè®ºæ–‡ï¼Œå« uptraining æ–¹æ³•å’Œè¯¦ç»†æ¶ˆèå®éªŒ
- [Fast Transformer Decoding: One Write-Head is All You Need](https://arxiv.org/abs/1911.02150) â€” MQA åŸè®ºæ–‡ï¼ŒNoam Shazeer çš„ç»å…¸ä¹‹ä½œ

### æ·±åº¦è§£è¯»
- [Efficient Transformers: A Survey](https://arxiv.org/abs/2009.06732) â€” Attention æ•ˆç‡ä¼˜åŒ–çš„ç³»ç»Ÿæ€§ç»¼è¿° â­â­â­â­

### å®è·µèµ„æº
- [LLaMA 3 Technical Report](https://arxiv.org/abs/2407.21783) â€” GQA åœ¨ LLaMA 3 ä¸­çš„å®é™…åº”ç”¨
- [vLLM GitHub](https://github.com/vllm-project/vllm) â€” GQA + PagedAttention çš„å·¥ä¸šçº§æ¨ç†æ¡†æ¶

---

## See Also

> ğŸ”— See also: [[Attention å˜ä½“ç»¼è¿°|Attention å˜ä½“ç»¼è¿°]] â€” æœ¬æ–‡æ˜¯å…¶ GQA/MQA ç« èŠ‚çš„æ·±åº¦å±•å¼€
> ğŸ”— See also: [[FlashAttention|FlashAttention]] â€” GQA å‡å°‘ KV æ€»é‡ï¼ŒFlashAttention åŠ é€Ÿ Attention è®¡ç®—ï¼ŒäºŒè€…ååŒ
> ğŸ”— See also: [[AI/3-LLM/Inference/KV Cache|KV Cache]] â€” GQA æ˜¯ KV Cache æ¶æ„å±‚ä¼˜åŒ–çš„æ ¸å¿ƒï¼Œä¸ PagedAttentionï¼ˆç³»ç»Ÿå±‚ï¼‰äº’è¡¥
> ğŸ”— See also: [[Multi-Head Latent Attention|MLA è¯¦è§£]] â€” æ¯” GQA æ›´æ¿€è¿›çš„ KV å‹ç¼©è·¯çº¿
> ğŸ”— See also: [[LLaMA|LLaMA]] â€” GQA åœ¨ LLaMA 2/3 ç³»åˆ—ä¸­çš„å®é™…éƒ¨ç½²
