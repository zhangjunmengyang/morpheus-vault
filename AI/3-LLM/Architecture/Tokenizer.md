---
title: Tokenizer ä¸åˆ†è¯ï¼šç°ä»£ LLM çš„è¯­è¨€åˆ‡åˆ†è‰ºæœ¯
brief: åˆ†è¯æ˜¯ NLP çš„ç¬¬ä¸€æ­¥ä¹Ÿæ˜¯æœ€è¢«ä½ä¼°çš„ç¯èŠ‚ã€‚ä¸‰å¤§ä¸»æµç®—æ³•ï¼šBPEï¼ˆè´ªå¿ƒåˆå¹¶æœ€é¢‘ç¹å­—ç¬¦å¯¹ï¼ŒGPT/LLaMA é‡‡ç”¨ï¼‰ã€WordPieceï¼ˆåŸºäºè¯­è¨€æ¨¡å‹æ¦‚ç‡é€‰æ‹©åˆå¹¶ï¼ŒBERT é‡‡ç”¨ï¼‰ã€Unigramï¼ˆæ¦‚ç‡æ¨¡å‹å…¨å±€æœ€ä¼˜ï¼ŒSentencePiece é»˜è®¤ï¼‰ã€‚è¯è¡¨å¤§å°çš„é»„é‡‘æ³•åˆ™ï¼šè‹±æ–‡ 30-50Kï¼Œå¤šè¯­è¨€ 50-100Kã€‚Byte-level BPE å½»åº•è§£å†³ OOV é—®é¢˜ï¼Œæ˜¯ç°ä»£ LLM çš„ä¸»æµé€‰æ‹©ã€‚
type: concept
domain: ai/llm/architecture
created: 2026-02-14
updated: 2026-02-22
tags:
  - ai/llm/tokenizer
  - ai/llm/architecture
  - type/concept
status: complete
sources:
  - Sennrich et al. *Neural Machine Translation of Rare Words with Subword Units (BPE)* arXiv:1508.07909
  - "Kudo & Richardson. *SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing* arXiv:1808.06226"
  - "Kudo. *Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates* arXiv:1804.10959 (Unigram LM)"
  - OpenAI tiktoken æ–‡æ¡£ â€” https://github.com/openai/tiktoken
related:
  - "[[BERT|BERT]]"
  - "[[GPT|GPT]]"
  - "[[Qwen|Qwen]]"
  - "[[LLaMA|LLaMA]]"
---

# Tokenizer ä¸åˆ†è¯ï¼šç°ä»£ LLM çš„è¯­è¨€åˆ‡åˆ†è‰ºæœ¯

åˆ†è¯ï¼ˆTokenizationï¼‰æ˜¯ NLP çš„ç¬¬ä¸€æ­¥ï¼Œä¹Ÿæ˜¯æœ€å®¹æ˜“è¢«å¿½è§†çš„å…³é”®ç¯èŠ‚ã€‚ä»è¯çº§åˆ«åˆ°å­è¯å†åˆ°å­—èŠ‚çº§ç¼–ç ï¼Œåˆ†è¯æŠ€æœ¯çš„æ¼”è¿›ç›´æ¥å½±å“äº†æ¨¡å‹çš„æ€§èƒ½ä¸Šé™ã€‚æœ¬æ–‡å°†æ·±å…¥è§£æç°ä»£ LLM ä¸­çš„åˆ†è¯æŠ€æœ¯ï¼Œé‡ç‚¹å…³æ³¨ BPEã€WordPieceã€Unigram ç­‰ä¸»æµç®—æ³•ã€‚

## åˆ†è¯æŠ€æœ¯æ¼”è¿›å²

### ä¼ ç»Ÿæ–¹æ³•çš„å±€é™

```python
# ä¼ ç»Ÿç©ºæ ¼åˆ†è¯çš„é—®é¢˜
text = "I'm loving machine-learning!"

# ç®€å•ç©ºæ ¼åˆ†è¯
simple_split = text.split()
print(simple_split)  
# ['I'm', 'loving', 'machine-learning!']

# é—®é¢˜ï¼š
# 1. I'm -> åº”è¯¥åˆ†æˆ I + 'm
# 2. machine-learning -> è¿å­—ç¬¦å¤„ç†
# 3. æ ‡ç‚¹ç¬¦å·å¤„ç†
# 4. OOV (Out-of-Vocabulary) é—®é¢˜
```

### å­è¯åˆ†è¯çš„ä¼˜åŠ¿

å­è¯ï¼ˆSubwordï¼‰åˆ†è¯è§£å†³äº†ä¼ ç»Ÿæ–¹æ³•çš„æ ¸å¿ƒé—®é¢˜ï¼š

1. **å¼€æ”¾è¯è¡¨**ï¼šå¤„ç†æœªè§è¿‡çš„è¯
2. **å½¢æ€å­¦æ„ŸçŸ¥**ï¼šæ•è·è¯ç¼€ä¿¡æ¯
3. **è¯è¡¨å¤§å°å¯æ§**ï¼šå¹³è¡¡è¡¨è¾¾èƒ½åŠ›å’Œæ•ˆç‡
4. **å¤šè¯­è¨€å‹å¥½**ï¼šç»Ÿä¸€å¤„ç†ä¸åŒè¯­è¨€

## ä¸»æµç®—æ³•æ·±åº¦è§£æ

### 1. BPE (Byte Pair Encoding)

BPEï¼ˆarXiv:1508.07909ï¼‰æ˜¯æœ€å¹¿æ³›ä½¿ç”¨çš„å­è¯ç®—æ³•ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯è´ªå¿ƒåœ°åˆå¹¶æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹ã€‚

#### ç®—æ³•åŸç†

```python
from collections import defaultdict, Counter
import re

class BPETokenizer:
    def __init__(self, vocab_size=1000):
        self.vocab_size = vocab_size
        self.word_freqs = {}
        self.splits = {}
        self.merges = []
        
    def train(self, corpus):
        # 1. ç»Ÿè®¡è¯é¢‘
        self.word_freqs = Counter()
        for text in corpus:
            words = re.findall(r'\w+', text.lower())
            self.word_freqs.update(words)
        
        # 2. åˆå§‹åŒ–ï¼šæ¯ä¸ªè¯åˆ†è§£ä¸ºå­—ç¬¦
        alphabet = set()
        for word in self.word_freqs:
            alphabet.update(word)
        
        # åˆå§‹è¯è¡¨ï¼šå•å­—ç¬¦ + </w>
        vocab = list(alphabet) + ['</w>']
        
        # 3. åˆå§‹åˆ†å‰²ï¼šæ·»åŠ è¯å°¾æ ‡è®°
        self.splits = {
            word: [c for c in word[:-1]] + [word[-1] + '</w>']
            for word in self.word_freqs
        }
        
        # 4. è¿­ä»£åˆå¹¶
        while len(vocab) < self.vocab_size:
            # è®¡ç®—æ‰€æœ‰ç›¸é‚»pairçš„é¢‘ç‡
            pairs = defaultdict(int)
            for word, freq in self.word_freqs.items():
                split = self.splits[word]
                for i in range(len(split) - 1):
                    pairs[(split[i], split[i+1])] += freq
            
            if not pairs:
                break
                
            # æ‰¾åˆ°æœ€é¢‘ç¹çš„pair
            best_pair = max(pairs, key=pairs.get)
            
            # åˆå¹¶è¿™ä¸ªpair
            self.merges.append(best_pair)
            new_token = best_pair[0] + best_pair[1]
            vocab.append(new_token)
            
            # æ›´æ–°æ‰€æœ‰splits
            self.merge_vocab(best_pair)
    
    def merge_vocab(self, pair):
        """åˆå¹¶æŒ‡å®šçš„å­—ç¬¦å¯¹"""
        bigram = re.escape(' '.join(pair))
        p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
        
        for word in self.word_freqs:
            w_out = self.splits[word]
            # æŸ¥æ‰¾å¹¶åˆå¹¶
            new_split = []
            i = 0
            while i < len(w_out):
                if i < len(w_out) - 1 and (w_out[i], w_out[i+1]) == pair:
                    new_split.append(w_out[i] + w_out[i+1])
                    i += 2
                else:
                    new_split.append(w_out[i])
                    i += 1
            self.splits[word] = new_split
    
    def tokenize(self, text):
        """å¯¹æ–°æ–‡æœ¬è¿›è¡Œåˆ†è¯"""
        words = re.findall(r'\w+', text.lower())
        result = []
        
        for word in words:
            # åº”ç”¨å­¦åˆ°çš„merges
            split = [c for c in word[:-1]] + [word[-1] + '</w>']
            for pair in self.merges:
                # ä¾æ¬¡åº”ç”¨æ¯ä¸ªmergeè§„åˆ™
                split = self.apply_merge(split, pair)
            result.extend(split)
        return result
    
    def apply_merge(self, split, pair):
        new_split = []
        i = 0
        while i < len(split):
            if i < len(split) - 1 and (split[i], split[i+1]) == pair:
                new_split.append(split[i] + split[i+1])
                i += 2
            else:
                new_split.append(split[i])
                i += 1
        return new_split

# ä½¿ç”¨ç¤ºä¾‹
corpus = [
    "the quick brown fox jumps",
    "the fox is quick",
    "brown fox jumps high"
]

tokenizer = BPETokenizer(vocab_size=50)
tokenizer.train(corpus)
tokens = tokenizer.tokenize("the quickest fox")
print(f"åˆ†è¯ç»“æœ: {tokens}")
# å¯èƒ½è¾“å‡º: ['th', 'e</w>', 'qui', 'ck', 'est</w>', 'fox</w>']
```

#### BPE å˜ä½“ï¼šByte-level BPE

GPT-2/LLaMA ä½¿ç”¨çš„æ”¹è¿›ç‰ˆæœ¬ï¼š

```python
import json

class ByteLevelBPE:
    """GPT-2 é£æ ¼çš„å­—èŠ‚çº§ BPE"""
    
    def __init__(self):
        # å­—èŠ‚åˆ°Unicodeçš„æ˜ å°„ï¼ˆå¤„ç†æ‰€æœ‰å¯èƒ½å­—èŠ‚ï¼‰
        self.bytes_to_unicode = self._bytes_to_unicode()
        self.unicode_to_bytes = {v: k for k, v in self.bytes_to_unicode.items()}
        
    def _bytes_to_unicode(self):
        """åˆ›å»ºå­—èŠ‚åˆ°Unicodeå­—ç¬¦çš„æ˜ å°„"""
        bs = (
            list(range(ord("!"), ord("~") + 1)) +
            list(range(ord("Â¡"), ord("Â¬") + 1)) +
            list(range(ord("Â®"), ord("Ã¿") + 1))
        )
        cs = bs[:]
        n = 0
        for b in range(2**8):
            if b not in bs:
                bs.append(b)
                cs.append(2**8 + n)
                n += 1
        return dict(zip(bs, [chr(c) for c in cs]))
    
    def encode_text(self, text):
        """æ–‡æœ¬è½¬æ¢ä¸ºå­—èŠ‚åºåˆ—"""
        byte_encoded = text.encode('utf-8')
        return ''.join([self.bytes_to_unicode[b] for b in byte_encoded])
    
    def decode_text(self, tokens):
        """å­—èŠ‚åºåˆ—è½¬æ¢å›æ–‡æœ¬"""
        byte_string = ''.join(tokens)
        byte_array = bytes([self.unicode_to_bytes[c] for c in byte_string])
        return byte_array.decode('utf-8', errors='replace')

# ä½¿ç”¨ç¤ºä¾‹ï¼šå¤„ç†å¤šè¯­è¨€æ–‡æœ¬
byte_bpe = ByteLevelBPE()
text = "Hello ä¸–ç•Œ! ğŸ¤–"
encoded = byte_bpe.encode_text(text)
print(f"å­—èŠ‚ç¼–ç : {encoded}")
decoded = byte_bpe.decode_text(encoded)
print(f"è§£ç ç»“æœ: {decoded}")
```

### 2. WordPiece (BERT ä½¿ç”¨)

WordPiece ä¸ BPE ç±»ä¼¼ï¼Œä½†ä½¿ç”¨ä¸åŒçš„åˆå¹¶ç­–ç•¥ã€‚

```python
import math
from collections import defaultdict

class WordPieceTokenizer:
    def __init__(self, vocab_size=1000, unk_token='[UNK]'):
        self.vocab_size = vocab_size
        self.unk_token = unk_token
        self.vocab = {}
        
    def train(self, corpus):
        # 1. æ”¶é›†è¯é¢‘
        word_freqs = defaultdict(int)
        for text in corpus:
            words = text.split()
            for word in words:
                word_freqs[word] += 1
        
        # 2. åˆå§‹è¯è¡¨
        alphabet = set()
        for word in word_freqs:
            alphabet.update(word)
        
        vocab = {char: i for i, char in enumerate(sorted(alphabet))}
        vocab[self.unk_token] = len(vocab)
        
        # 3. å‡†å¤‡è®­ç»ƒæ•°æ®
        word_splits = {}
        for word, freq in word_freqs.items():
            word_splits[word] = [char for char in word]
        
        # 4. è¿­ä»£æ·»åŠ è¯ç‰‡æ®µ
        while len(vocab) < self.vocab_size:
            scores = {}
            
            # è®¡ç®—æ¯ä¸ªå¯èƒ½åˆå¹¶çš„åˆ†æ•°
            for word, freq in word_freqs.items():
                split = word_splits[word]
                for i in range(len(split) - 1):
                    pair = (split[i], split[i+1])
                    if pair not in scores:
                        scores[pair] = 0
                    scores[pair] += freq
            
            # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„pair
            if not scores:
                break
                
            best_pair = max(scores.items(), key=lambda x: x[1])
            pair, score = best_pair
            
            # åˆ›å»ºæ–°token
            new_token = pair[0] + pair[1]
            vocab[new_token] = len(vocab)
            
            # æ›´æ–°splits
            for word in word_freqs:
                new_split = []
                i = 0
                while i < len(word_splits[word]):
                    if (i < len(word_splits[word]) - 1 and 
                        word_splits[word][i] == pair[0] and 
                        word_splits[word][i+1] == pair[1]):
                        new_split.append(new_token)
                        i += 2
                    else:
                        new_split.append(word_splits[word][i])
                        i += 1
                word_splits[word] = new_split
        
        self.vocab = vocab
        
    def tokenize(self, text):
        """ä½¿ç”¨è´ªå¿ƒæœ€é•¿åŒ¹é…"""
        words = text.split()
        result = []
        
        for word in words:
            tokens = self._tokenize_word(word)
            result.extend(tokens)
        return result
    
    def _tokenize_word(self, word):
        """å¯¹å•ä¸ªè¯è¿›è¡ŒWordPieceåˆ†è¯"""
        if word in self.vocab:
            return [word]
        
        tokens = []
        start = 0
        
        while start < len(word):
            end = len(word)
            cur_substr = None
            
            # è´ªå¿ƒæ‰¾æœ€é•¿å­ä¸²
            while start < end:
                substr = word[start:end]
                if start > 0:
                    substr = "##" + substr  # WordPieceçš„å­è¯å‰ç¼€
                
                if substr in self.vocab:
                    cur_substr = substr
                    break
                end -= 1
            
            if cur_substr is None:
                return [self.unk_token]
            
            tokens.append(cur_substr)
            start = end
        
        return tokens

# ä½¿ç”¨ç¤ºä¾‹
wp = WordPieceTokenizer(vocab_size=100)
corpus = ["playing played player", "walking walked walker"]
wp.train(corpus)
tokens = wp.tokenize("walking player")
print(f"WordPieceåˆ†è¯: {tokens}")
# å¯èƒ½è¾“å‡º: ['walk', '##ing', 'play', '##er']
```

### 3. Unigram Language Model

SentencePiece çš„é»˜è®¤ç®—æ³•ï¼ˆKudo. *Subword Regularization* arXiv:1804.10959ï¼‰ï¼Œä½¿ç”¨æ¦‚ç‡æ¨¡å‹ã€‚

```python
import math
from collections import defaultdict, Counter

class UnigramTokenizer:
    def __init__(self, vocab_size=1000):
        self.vocab_size = vocab_size
        self.vocab = {}
        self.log_probs = {}
        
    def train(self, corpus, num_iterations=10):
        # 1. æ”¶é›†æ‰€æœ‰å¯èƒ½çš„å­ä¸²
        substrings = set()
        word_freqs = Counter()
        
        for text in corpus:
            words = text.split()
            for word in words:
                word_freqs[word] += 1
                # æ·»åŠ æ‰€æœ‰å¯èƒ½çš„å­ä¸²
                for i in range(len(word)):
                    for j in range(i + 1, len(word) + 1):
                        substrings.add(word[i:j])
        
        # 2. åˆå§‹åŒ–å¤§è¯è¡¨ï¼ˆä¿ç•™é«˜é¢‘å­ä¸²ï¼‰
        substr_counts = defaultdict(int)
        for word, freq in word_freqs.items():
            for substr in substrings:
                if substr in word:
                    substr_counts[substr] += freq * word.count(substr)
        
        # é€‰æ‹©åˆå§‹è¯è¡¨
        sorted_substrs = sorted(substr_counts.items(), 
                              key=lambda x: x[1], reverse=True)
        initial_vocab = dict(sorted_substrs[:self.vocab_size * 3])
        
        # 3. EMç®—æ³•è¿­ä»£ä¼˜åŒ–
        current_vocab = initial_vocab
        
        for iteration in range(num_iterations):
            # E-step: è®¡ç®—æœ€ä½³åˆ†å‰²
            word_splits = {}
            for word in word_freqs:
                word_splits[word] = self._best_split(word, current_vocab)
            
            # M-step: æ›´æ–°æ¦‚ç‡
            token_counts = defaultdict(int)
            total_tokens = 0
            
            for word, freq in word_freqs.items():
                for token in word_splits[word]:
                    token_counts[token] += freq
                    total_tokens += freq
            
            # è®¡ç®—å¯¹æ•°æ¦‚ç‡
            new_log_probs = {}
            for token, count in token_counts.items():
                new_log_probs[token] = math.log(count / total_tokens)
            
            # ä¿®å‰ªè¯è¡¨
            if len(token_counts) > self.vocab_size:
                sorted_tokens = sorted(token_counts.items(), 
                                     key=lambda x: x[1], reverse=True)
                current_vocab = dict(sorted_tokens[:self.vocab_size])
                self.log_probs = {k: new_log_probs[k] for k in current_vocab}
            else:
                current_vocab = token_counts
                self.log_probs = new_log_probs
        
        self.vocab = current_vocab
    
    def _best_split(self, word, vocab):
        """ä½¿ç”¨åŠ¨æ€è§„åˆ’æ‰¾æœ€ä½³åˆ†å‰²"""
        n = len(word)
        # dp[i] å­˜å‚¨ word[:i] çš„æœ€ä½³åˆ†å‰²çš„è´Ÿå¯¹æ•°æ¦‚ç‡
        dp = [float('inf')] * (n + 1)
        parent = [-1] * (n + 1)
        dp[0] = 0.0
        
        for i in range(n + 1):
            if dp[i] == float('inf'):
                continue
            for j in range(i + 1, n + 1):
                substr = word[i:j]
                if substr in vocab:
                    prob = self.log_probs.get(substr, -20.0)  # é»˜è®¤ä½æ¦‚ç‡
                    if dp[i] - prob < dp[j]:  # è´Ÿå¯¹æ•°æ¦‚ç‡ï¼Œè¶Šå°è¶Šå¥½
                        dp[j] = dp[i] - prob
                        parent[j] = i
        
        # å›æº¯æ„å»ºåˆ†å‰²
        result = []
        pos = n
        while pos > 0:
            start = parent[pos]
            result.append(word[start:pos])
            pos = start
        
        return result[::-1]
    
    def tokenize(self, text):
        words = text.split()
        result = []
        for word in words:
            tokens = self._best_split(word, self.vocab)
            result.extend(tokens)
        return result

# ä½¿ç”¨ç¤ºä¾‹
unigram = UnigramTokenizer(vocab_size=50)
corpus = ["machine learning", "deep learning", "neural network"]
unigram.train(corpus)
tokens = unigram.tokenize("deep neural")
print(f"Unigramåˆ†è¯: {tokens}")
```

### 4. SentencePieceï¼šç»Ÿä¸€æ¡†æ¶

Google çš„ SentencePieceï¼ˆarXiv:1808.06226ï¼‰æä¾›äº†ç»Ÿä¸€çš„æ¥å£ï¼š

```python
# å®‰è£…: pip install sentencepiece
import sentencepiece as spm

def train_sentencepiece(input_file, vocab_size=8000):
    """è®­ç»ƒ SentencePiece æ¨¡å‹"""
    spm.SentencePieceTrainer.train(
        input=input_file,
        model_prefix='sp_model',
        vocab_size=vocab_size,
        model_type='bpe',  # 'bpe', 'unigram', 'char', 'word'
        max_sentence_length=4192,
        shuffle_input_sentence=True,
        character_coverage=0.9995,
        # ç‰¹æ®Štoken
        pad_id=0,
        unk_id=1, 
        bos_id=2,
        eos_id=3,
        user_defined_symbols=['<mask>']
    )

# åŠ è½½å’Œä½¿ç”¨
sp = spm.SentencePieceProcessor()
sp.load('sp_model.model')

# ç¼–ç 
text = "SentencePiece is a great tokenizer!"
tokens = sp.encode(text, out_type=str)
print(f"Tokens: {tokens}")

ids = sp.encode(text, out_type=int)
print(f"IDs: {ids}")

# è§£ç 
decoded = sp.decode(ids)
print(f"Decoded: {decoded}")

# è¯è¡¨ä¿¡æ¯
print(f"Vocab size: {sp.vocab_size()}")
print(f"UNK token: {sp.id_to_piece(sp.unk_id())}")
```

## åˆ†è¯å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“

### 1. è¯è¡¨å¤§å°çš„æƒè¡¡

```python
import matplotlib.pyplot as plt
import numpy as np

def analyze_vocab_size_impact():
    """åˆ†æè¯è¡¨å¤§å°å¯¹å„é¡¹æŒ‡æ ‡çš„å½±å“"""
    vocab_sizes = [1000, 2000, 5000, 10000, 20000, 50000]
    
    # æ¨¡æ‹Ÿæ•°æ®ï¼ˆåŸºäºç»éªŒï¼‰
    compression_ratio = [0.3, 0.45, 0.6, 0.7, 0.75, 0.8]  # å‹ç¼©æ¯”
    training_speed = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5]       # è®­ç»ƒé€Ÿåº¦
    downstream_performance = [0.7, 0.8, 0.85, 0.9, 0.92, 0.93]  # ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½
    
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))
    
    # å‹ç¼©æ¯”
    ax1.plot(vocab_sizes, compression_ratio, 'b-o')
    ax1.set_xlabel('è¯è¡¨å¤§å°')
    ax1.set_ylabel('å‹ç¼©æ¯”')
    ax1.set_title('å‹ç¼©æ¯” vs è¯è¡¨å¤§å°')
    ax1.grid(True)
    
    # è®­ç»ƒé€Ÿåº¦
    ax2.plot(vocab_sizes, training_speed, 'r-s')
    ax2.set_xlabel('è¯è¡¨å¤§å°')
    ax2.set_ylabel('ç›¸å¯¹è®­ç»ƒé€Ÿåº¦')
    ax2.set_title('è®­ç»ƒé€Ÿåº¦ vs è¯è¡¨å¤§å°')
    ax2.grid(True)
    
    # ä¸‹æ¸¸æ€§èƒ½
    ax3.plot(vocab_sizes, downstream_performance, 'g-^')
    ax3.set_xlabel('è¯è¡¨å¤§å°')
    ax3.set_ylabel('ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½')
    ax3.set_title('æ¨¡å‹æ€§èƒ½ vs è¯è¡¨å¤§å°')
    ax3.grid(True)
    
    plt.tight_layout()
    plt.show()
    
    return vocab_sizes, compression_ratio, training_speed, downstream_performance

# è¿è¡Œåˆ†æ
analyze_vocab_size_impact()
```

### 2. ä¸åŒç®—æ³•çš„ç‰¹æ€§å¯¹æ¯”

| ç®—æ³• | å‹ç¼©æ•ˆç‡ | è®­ç»ƒé€Ÿåº¦ | OOVå¤„ç† | å¤šè¯­è¨€ | å®ç°å¤æ‚åº¦ |
|------|----------|----------|---------|--------|------------|
| **BPE** | é«˜ | å¿« | ä¼˜ç§€ | å¥½ | ç®€å• |
| **WordPiece** | ä¸­ | ä¸­ | ä¼˜ç§€ | ä¸­ | ä¸­ç­‰ |
| **Unigram** | ä¸­ | æ…¢ | æœ€ä¼˜ | æœ€å¥½ | å¤æ‚ |
| **SentencePiece** | é«˜ | å¿« | ä¼˜ç§€ | æœ€å¥½ | ç®€å•ï¼ˆåº“ï¼‰ |

### 3. çœŸå®æ¡ˆä¾‹ï¼šæ¨¡å‹å¯¹æ¯”

```python
def compare_tokenizers_on_text():
    """å¯¹æ¯”ä¸åŒåˆ†è¯å™¨çš„æ•ˆæœ"""
    test_texts = [
        "The quick brown fox jumps over the lazy dog.",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚",
        "I'm loving this new iPhone! ğŸ˜",
        "COVID-19 has significantly impacted the world.",
        "She said, 'Hello!' very enthusiastically."
    ]
    
    # æ¨¡æ‹Ÿä¸åŒåˆ†è¯å™¨çš„ç»“æœ
    results = {
        'GPT-2 (BPE)': {
            0: ['The', 'Ä quick', 'Ä brown', 'Ä fox', 'Ä jumps', 'Ä over', 'Ä the', 'Ä lazy', 'Ä dog', '.'],
            1: ['Ã¦', 'Ä£', 'Åƒ', 'Ã¥', 'Ä»', 'Ä¢', 'Ã¥', 'Ä¸', 'Åƒ', 'Ã¤', 'Å‚', 'Ä¢', 'Ã¦', 'Ä¹', 'Â¯', 'Ã¤ÂºÂº', 'Ã¥Â·Â¥'],
            2: ["I'm", 'Ä loving', 'Ä this', 'Ä new', 'Ä iPhone', '!', 'Ä ğŸ˜'],
        },
        'BERT (WordPiece)': {
            0: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.'],
            1: ['æœº', '##å™¨', '##å­¦', '##ä¹ ', '##æ˜¯', '##äºº', '##å·¥', '##æ™º', '##èƒ½', '##çš„'],
            2: ['i', "'", 'm', 'loving', 'this', 'new', 'iphone', '!', '[UNK]'],  # emoji OOV
        },
        'T5 (SentencePiece)': {
            0: ['â–The', 'â–quick', 'â–brown', 'â–fox', 'â–jumps', 'â–over', 'â–the', 'â–lazy', 'â–dog', '.'],
            1: ['â–æœºå™¨', 'å­¦ä¹ ', 'æ˜¯', 'äººå·¥', 'æ™ºèƒ½', 'çš„', 'ä¸€ä¸ª', 'é‡è¦', 'åˆ†æ”¯', 'ã€‚'],
            2: ['â–I', "'", 'm', 'â–loving', 'â–this', 'â–new', 'â–iPhone', '!', 'â–ğŸ˜'],
        }
    }
    
    for i, text in enumerate(test_texts[:3]):
        print(f"\næ–‡æœ¬ {i+1}: {text}")
        for tokenizer, tokenizations in results.items():
            if i in tokenizations:
                tokens = tokenizations[i]
                print(f"{tokenizer:20}: {tokens} ({len(tokens)} tokens)")

compare_tokenizers_on_text()
```

## å¤šè¯­è¨€åˆ†è¯æŒ‘æˆ˜

### ä¸­æ–‡åˆ†è¯ç‰¹æ®Šæ€§

```python
class ChineseTokenizationAnalysis:
    def __init__(self):
        self.examples = {
            'è¯è¾¹ç•Œæ¨¡ç³Š': ['ç ”ç©¶ç”Ÿå‘½çš„èµ·æº', 'ç ”ç©¶/ç”Ÿå‘½/çš„/èµ·æº vs ç ”ç©¶ç”Ÿ/å‘½/çš„/èµ·æº'],
            'è¯è¯­é•¿åº¦å˜åŒ–': ['äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•', 'è¯é•¿åº¦åˆ†å¸ƒä¸å‡åŒ€'],
            'æ–°è¯å‡ºç°': ['ChatGPTå¾ˆå‰å®³', 'æ–°è¯éœ€è¦åŠæ—¶å¤„ç†'],
        }
    
    def analyze_segmentation_ambiguity(self):
        """åˆ†æä¸­æ–‡åˆ†è¯æ­§ä¹‰"""
        text = "ç ”ç©¶ç”Ÿå‘½çš„èµ·æº"
        
        segmentations = [
            ['ç ”ç©¶', 'ç”Ÿå‘½', 'çš„', 'èµ·æº'],      # æ­£ç¡®
            ['ç ”ç©¶ç”Ÿ', 'å‘½', 'çš„', 'èµ·æº'],      # é”™è¯¯
            ['ç ”ç©¶', 'ç”Ÿ', 'å‘½', 'çš„', 'èµ·æº'],  # è¿‡åº¦åˆ†å‰²
        ]
        
        print("ä¸­æ–‡åˆ†è¯æ­§ä¹‰ç¤ºä¾‹:")
        for i, seg in enumerate(segmentations):
            print(f"æ–¹æ¡ˆ{i+1}: {' / '.join(seg)}")
        
        return segmentations
    
    def subword_advantages_for_chinese(self):
        """å­è¯åˆ†è¯å¯¹ä¸­æ–‡çš„ä¼˜åŠ¿"""
        examples = [
            {
                'word': 'äººå·¥æ™ºèƒ½',
                'char_level': ['äºº', 'å·¥', 'æ™º', 'èƒ½'],
                'subword': ['äººå·¥', 'æ™ºèƒ½'],
                'advantage': 'ä¿ç•™è¯­ä¹‰å•å…ƒ'
            },
            {
                'word': 'ChatGPT',
                'char_level': ['C', 'h', 'a', 't', 'G', 'P', 'T'],
                'subword': ['Chat', 'GPT'],
                'advantage': 'å¤„ç†è‹±æ–‡æ··åˆ'
            }
        ]
        
        for ex in examples:
            print(f"\nè¯è¯­: {ex['word']}")
            print(f"å­—ç¬¦çº§: {ex['char_level']}")
            print(f"å­è¯çº§: {ex['subword']}")
            print(f"ä¼˜åŠ¿: {ex['advantage']}")

# åˆ†æä¸­æ–‡åˆ†è¯
chinese_analysis = ChineseTokenizationAnalysis()
chinese_analysis.analyze_segmentation_ambiguity()
chinese_analysis.subword_advantages_for_chinese()
```

### è·¨è¯­è¨€ç»Ÿä¸€å¤„ç†

```python
def multilingual_tokenization_strategy():
    """å¤šè¯­è¨€åˆ†è¯ç»Ÿä¸€ç­–ç•¥"""
    
    # ä¸åŒè¯­è¨€çš„ç‰¹ç‚¹
    language_features = {
        'è‹±æ–‡': {
            'ç‰¹ç‚¹': ['ç©ºæ ¼åˆ†éš”', 'å½¢æ€å˜åŒ–ä¸°å¯Œ', 'å¤§å°å†™æ•æ„Ÿ'],
            'æŒ‘æˆ˜': ['ç¼©å†™å¤„ç†', 'å¤åˆè¯', 'æ–°è¯'],
            'ç­–ç•¥': 'BPE with byte-level encoding'
        },
        'ä¸­æ–‡': {
            'ç‰¹ç‚¹': ['æ— æ˜æ˜¾åˆ†éš”', 'å­—ç¬¦å¯†é›†', 'è¯­ä¹‰ç»„åˆ'],
            'æŒ‘æˆ˜': ['åˆ†è¯æ­§ä¹‰', 'æ–°è¯è¯†åˆ«', 'å¤æ±‰è¯­'],
            'ç­–ç•¥': 'SentencePiece Unigram'
        },
        'æ—¥æ–‡': {
            'ç‰¹ç‚¹': ['å¤šç§æ–‡å­—æ··åˆ', 'æ— ç©ºæ ¼', 'åŠ©è¯ä¸°å¯Œ'],
            'æŒ‘æˆ˜': ['å‡åæ±‰å­—æ··åˆ', 'è¯­è¨€å˜ä½“'],
            'ç­–ç•¥': 'SentencePiece with character coverage adjustment'
        },
        'é˜¿æ‹‰ä¼¯æ–‡': {
            'ç‰¹ç‚¹': ['ä»å³åˆ°å·¦', 'è¿å†™', 'å˜éŸ³ç¬¦å·'],
            'æŒ‘æˆ˜': ['å­—å½¢å˜åŒ–', 'æ–¹å‘æ€§'],
            'ç­–ç•¥': 'Byte-level BPE with normalization'
        }
    }
    
    print("å¤šè¯­è¨€åˆ†è¯ç­–ç•¥:")
    for lang, info in language_features.items():
        print(f"\n{lang}:")
        print(f"  ç‰¹ç‚¹: {', '.join(info['ç‰¹ç‚¹'])}")
        print(f"  æŒ‘æˆ˜: {', '.join(info['æŒ‘æˆ˜'])}")
        print(f"  æ¨èç­–ç•¥: {info['ç­–ç•¥']}")
    
    # ç»Ÿä¸€å¤„ç†æ–¹æ¡ˆ
    unified_approach = {
        'character_coverage': 0.9995,  # è¦†ç›–99.95%çš„å­—ç¬¦
        'vocab_size': 32000,           # å¹³è¡¡å„è¯­è¨€éœ€æ±‚
        'model_type': 'unigram',       # æœ€é€‚åˆå¤šè¯­è¨€
        'normalization': True,         # è§„èŒƒåŒ–è¾“å…¥
        'byte_fallback': True,         # å­—èŠ‚çº§åå¤‡
    }
    
    print(f"\nç»Ÿä¸€é…ç½®: {unified_approach}")

multilingual_tokenization_strategy()
```

## ç°ä»£ LLM çš„åˆ†è¯é€‰æ‹©

### ä¸»æµæ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | åˆ†è¯ç®—æ³• | è¯è¡¨å¤§å° | ç‰¹æ®Šè®¾è®¡ |
|------|----------|----------|----------|
| **GPT-2/3** | Byte-level BPE | 50,257 | å­—èŠ‚çº§ç¼–ç  |
| **GPT-4** | BPE (æ”¹è¿›ç‰ˆ) | ~100K | å¤šè¯­è¨€ä¼˜åŒ– |
| **BERT** | WordPiece | 30,522 | ä¸­æ–‡å­—ç¬¦çº§ |
| **T5** | SentencePiece | 32,128 | å¤šè¯­è¨€ç»Ÿä¸€ |
| **LLaMA** | SentencePiece BPE | 32,000 | æ•ˆç‡ä¼˜åŒ– |
| **Claude** | æœªçŸ¥ (æ¨æµ‹BPE) | ~100K | ä¸“æœ‰ç®—æ³• |

### é€‰æ‹©ç­–ç•¥

```python
def choose_tokenization_strategy(use_case):
    """æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©åˆ†è¯ç­–ç•¥"""
    
    strategies = {
        'english_only': {
            'algorithm': 'BPE',
            'vocab_size': 50000,
            'features': ['ç®€å•é«˜æ•ˆ', 'æˆç†Ÿç¨³å®š'],
            'examples': ['GPT-2', 'RoBERTa']
        },
        'multilingual': {
            'algorithm': 'SentencePiece Unigram',
            'vocab_size': 32000,
            'features': ['å¤šè¯­è¨€å‹å¥½', 'ç»Ÿä¸€å¤„ç†'],
            'examples': ['T5', 'mT5', 'XLM-R']
        },
        'chinese_focused': {
            'algorithm': 'SentencePiece BPE',
            'vocab_size': 21128,
            'features': ['ä¸­æ–‡ä¼˜åŒ–', 'å­—è¯å¹³è¡¡'],
            'examples': ['BERT-Chinese', 'ERNIE']
        },
        'code_generation': {
            'algorithm': 'Byte-level BPE',
            'vocab_size': 50000,
            'features': ['ä»£ç å‹å¥½', 'ç¬¦å·å¤„ç†'],
            'examples': ['CodeGPT', 'GitHub Copilot']
        },
        'domain_specific': {
            'algorithm': 'è‡ªå®šä¹‰BPE',
            'vocab_size': 30000,
            'features': ['é¢†åŸŸè¯æ±‡', 'å®šåˆ¶ä¼˜åŒ–'],
            'examples': ['BioBERT', 'FinBERT']
        }
    }
    
    if use_case in strategies:
        strategy = strategies[use_case]
        print(f"æ¨èç­–ç•¥ ({use_case}):")
        print(f"  ç®—æ³•: {strategy['algorithm']}")
        print(f"  è¯è¡¨å¤§å°: {strategy['vocab_size']}")
        print(f"  ç‰¹ç‚¹: {', '.join(strategy['features'])}")
        print(f"  æ¡ˆä¾‹: {', '.join(strategy['examples'])}")
        return strategy
    else:
        print(f"æœªçŸ¥ç”¨ä¾‹: {use_case}")
        return None

# æµ‹è¯•ä¸åŒåœºæ™¯
for case in ['english_only', 'multilingual', 'chinese_focused', 'code_generation']:
    choose_tokenization_strategy(case)
    print()
```

## é¢è¯•å¸¸è§é—®é¢˜

### Q1ï¼šBPEã€WordPieceã€Unigram ä¸‰ç§ç®—æ³•çš„æ ¸å¿ƒåŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ

**ç­”æ¡ˆ**ï¼š

**BPE (Byte Pair Encoding)**ï¼š
- åŸç†ï¼šè´ªå¿ƒåˆå¹¶æœ€é¢‘ç¹çš„å­—ç¬¦å¯¹
- ä¼˜åŠ¿ï¼šç®€å•é«˜æ•ˆï¼Œå‹ç¼©æ•ˆæœå¥½
- åŠ£åŠ¿ï¼šçº¯ç»Ÿè®¡æ–¹æ³•ï¼Œç¼ºä¹è¯­è¨€å­¦è€ƒè™‘
- é€‚ç”¨ï¼šé€šç”¨åœºæ™¯ï¼Œç‰¹åˆ«æ˜¯è‹±æ–‡

**WordPiece**ï¼š
- åŸç†ï¼šåŸºäºè¯­è¨€æ¨¡å‹æ¦‚ç‡é€‰æ‹©åˆå¹¶
- ä¼˜åŠ¿ï¼šè€ƒè™‘ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œæ›´æœ‰è¯­è¨€å­¦æ„ä¹‰
- åŠ£åŠ¿ï¼šè®­ç»ƒå¤æ‚åº¦é«˜
- é€‚ç”¨ï¼šç†è§£ä»»åŠ¡ï¼ŒBERTç³»åˆ—

**Unigram**ï¼š
- åŸç†ï¼šæ¦‚ç‡è¯­è¨€æ¨¡å‹ï¼ŒEMç®—æ³•ä¼˜åŒ–
- ä¼˜åŠ¿ï¼šå…¨å±€æœ€ä¼˜è§£ï¼Œå¤šè¯­è¨€å‹å¥½
- åŠ£åŠ¿ï¼šè®¡ç®—å¤æ‚ï¼Œè®­ç»ƒæ…¢
- é€‚ç”¨ï¼šå¤šè¯­è¨€æ¨¡å‹ï¼ŒSentencePieceé»˜è®¤

### Q2ï¼šä¸ºä»€ä¹ˆç°ä»£ LLM æ™®éé€‰æ‹©è¾ƒå¤§çš„è¯è¡¨ï¼ˆ32K-100Kï¼‰ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
1. **è¡¨è¾¾èƒ½åŠ›**ï¼šå¤§è¯è¡¨å‡å°‘åºåˆ—é•¿åº¦ï¼Œæé«˜æ¨¡å‹æ•ˆç‡
2. **å¤šè¯­è¨€æ”¯æŒ**ï¼šè¦†ç›–æ›´å¤šè¯­è¨€çš„å¸¸ç”¨è¯æ±‡
3. **é¢†åŸŸé€‚åº”**ï¼šåŒ…å«ä¸“ä¸šæœ¯è¯­å’Œæ–°è¯
4. **è®¡ç®—å¹³è¡¡**ï¼šåµŒå…¥å±‚å¢å¤§ vs åºåˆ—é•¿åº¦å‡å°‘çš„æƒè¡¡
5. **ç¡¬ä»¶å‘å±•**ï¼šç°ä»£GPUå†…å­˜å……è¶³ï¼Œæ”¯æŒå¤§è¯è¡¨

**ç»éªŒè§„å¾‹**ï¼š
- è‹±æ–‡ï¼š30-50K è¶³å¤Ÿ
- å¤šè¯­è¨€ï¼š50-100K
- ä»£ç ç”Ÿæˆï¼š50-100K
- ä¸“ä¸šé¢†åŸŸï¼šæ ¹æ®è¯­æ–™è°ƒæ•´

### Q3ï¼šå­—èŠ‚çº§ BPE ç›¸æ¯”ä¼ ç»Ÿ BPE æœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**ä¼ ç»ŸBPEé—®é¢˜**ï¼š
1. è¯è¡¨ä¼šé—æ¼ä¸€äº›Unicodeå­—ç¬¦
2. å¤„ç†å¤šè¯­è¨€æ—¶éœ€è¦å¤§è¯è¡¨
3. æ–°è¯­è¨€/emoji å¤„ç†å›°éš¾

**å­—èŠ‚çº§BPEä¼˜åŠ¿**ï¼š
1. **å®Œå…¨è¦†ç›–**ï¼šä»»ä½•æ–‡æœ¬éƒ½èƒ½ç¼–ç ï¼Œæ— OOV
2. **å¤šè¯­è¨€å‹å¥½**ï¼šç»Ÿä¸€å¤„ç†æ‰€æœ‰è¯­è¨€
3. **æ–°å†…å®¹å…¼å®¹**ï¼šemojiã€ç‰¹æ®Šç¬¦å·ã€æ–°è¯­è¨€
4. **å‹ç¼©æ•ˆç‡**ï¼šå­—èŠ‚çº§ç¼–ç æ›´ç´§å‡‘

**ä»£ä»·**ï¼š
- åºåˆ—å˜é•¿ï¼ˆä¸­æ–‡ç­‰ï¼‰
- ç¼–ç æ›´å¤æ‚
- è§£ç éœ€è¦é¢å¤–å¤„ç†

### Q4ï¼šå¦‚ä½•è¯„ä¼°åˆ†è¯è´¨é‡ï¼Ÿåˆ†è¯å¯¹ä¸‹æ¸¸ä»»åŠ¡æœ‰ä»€ä¹ˆå½±å“ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**è¯„ä¼°æŒ‡æ ‡**ï¼š
1. **å‹ç¼©ç‡**ï¼šåŸå§‹å­—ç¬¦æ•° / tokenæ•°
2. **è¯è¾¹ç•Œå‡†ç¡®ç‡**ï¼šä¸äººå·¥åˆ†è¯å¯¹æ¯”
3. **OOVç‡**ï¼šæœªè§è¯æ±‡æ¯”ä¾‹
4. **ä¸‹æ¸¸ä»»åŠ¡æ€§èƒ½**ï¼šæœ€ç»ˆè¯„ä»·æ ‡å‡†

**å½±å“åˆ†æ**ï¼š
- **è¿‡åº¦åˆ†å‰²**ï¼šä¿¡æ¯å¯†åº¦ä½ï¼Œåºåˆ—å˜é•¿ï¼Œè®­ç»ƒæ…¢
- **åˆ†å‰²ä¸è¶³**ï¼šOOVé—®é¢˜ï¼Œæ³›åŒ–èƒ½åŠ›å·®
- **ä¸ä¸€è‡´åˆ†å‰²**ï¼šåŒä¸€æ¦‚å¿µå¤šç§è¡¨ç¤ºï¼Œå­¦ä¹ å›°éš¾

**æœ€ä½³å®è·µ**ï¼š
```python
def evaluate_tokenization_quality(tokenizer, test_data):
    metrics = {
        'compression_ratio': [],
        'avg_token_length': [],
        'oov_rate': []
    }
    
    for text in test_data:
        tokens = tokenizer.tokenize(text)
        char_count = len(text.replace(' ', ''))
        token_count = len(tokens)
        
        metrics['compression_ratio'].append(char_count / token_count)
        metrics['avg_token_length'].append(
            sum(len(t.replace('##', '').replace('â–', '')) for t in tokens) / len(tokens)
        )
        # è®¡ç®—OOVç‡ç­‰
    
    return metrics
```

### Q5ï¼šåœ¨å®é™…é¡¹ç›®ä¸­å¦‚ä½•é€‰æ‹©å’Œä¼˜åŒ–åˆ†è¯ç­–ç•¥ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**é€‰æ‹©æµç¨‹**ï¼š

1. **éœ€æ±‚åˆ†æ**ï¼š
   - è¯­è¨€ç±»å‹ï¼ˆå•è¯­è¨€/å¤šè¯­è¨€ï¼‰
   - é¢†åŸŸç‰¹ç‚¹ï¼ˆé€šç”¨/ä¸“ä¸šï¼‰
   - è®¡ç®—èµ„æºçº¦æŸ

2. **baselineå»ºç«‹**ï¼š
   - ä½¿ç”¨ç°æœ‰åˆ†è¯å™¨ï¼ˆå¦‚GPT-2çš„BPEï¼‰
   - åœ¨éªŒè¯é›†ä¸Šæµ‹è¯•è¡¨ç°

3. **è‡ªå®šä¹‰ä¼˜åŒ–**ï¼š
```python
# é¢†åŸŸé€‚åº”ç¤ºä¾‹
def adapt_tokenizer_for_domain(base_tokenizer, domain_corpus):
    # 1. åˆ†æé¢†åŸŸç‰¹æœ‰è¯æ±‡
    domain_vocab = extract_domain_terms(domain_corpus)
    
    # 2. è°ƒæ•´è¯è¡¨
    extended_vocab = base_tokenizer.vocab.copy()
    extended_vocab.update(domain_vocab)
    
    # 3. é‡æ–°è®­ç»ƒï¼ˆæˆ–å¾®è°ƒï¼‰
    adapted_tokenizer = train_tokenizer(
        corpus=domain_corpus,
        base_vocab=extended_vocab,
        vocab_size=target_size
    )
    
    return adapted_tokenizer
```

4. **A/Bæµ‹è¯•**ï¼š
   - å¯¹æ¯”ä¸åŒåˆ†è¯ç­–ç•¥
   - åœ¨ä¸‹æ¸¸ä»»åŠ¡ä¸ŠéªŒè¯æ•ˆæœ

5. **æŒç»­ä¼˜åŒ–**ï¼š
   - ç›‘æ§æ–°è¯å‡ºç°
   - å®šæœŸæ›´æ–°è¯è¡¨

**å·¥ç¨‹å»ºè®®**ï¼š
- ä¼˜å…ˆä½¿ç”¨æˆç†Ÿæ–¹æ¡ˆï¼ˆSentencePieceï¼‰
- ä¿ç•™æ‰©å±•æ€§ï¼ˆæ˜“äºæ›´æ–°ï¼‰
- ç‰ˆæœ¬ç®¡ç†ï¼ˆåˆ†è¯å™¨ç‰ˆæœ¬ä¸æ¨¡å‹ç»‘å®šï¼‰
- å‘åå…¼å®¹ï¼ˆæ–°ç‰ˆæœ¬å…¼å®¹æ—§æ•°æ®ï¼‰

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Neural Machine Translation of Rare Words with Subword Units (BPE)](https://arxiv.org/abs/1508.07909) â€” BPE åŸæ–‡ï¼Œå­è¯åˆ†è¯çš„å¼€å±±ä¹‹ä½œ
- [SentencePiece: A simple and language independent subword tokenizer](https://arxiv.org/abs/1808.06226) â€” ç»Ÿä¸€åˆ†è¯æ¡†æ¶ï¼Œæ”¯æŒ BPE/Unigram
- [Subword Regularization (Unigram LM)](https://arxiv.org/abs/1804.10959) â€” Unigram åˆ†è¯ç®—æ³•ï¼Œæ¦‚ç‡æ¨¡å‹æ–¹æ³•

### æ·±åº¦è§£è¯»
- [HuggingFace Tokenizers æ•™ç¨‹](https://huggingface.co/docs/tokenizers/) â€” åˆ†è¯å™¨è®­ç»ƒå’Œä½¿ç”¨çš„æœ€ä½³å®è·µ â­â­â­â­â­
- [Let's build the GPT Tokenizer (Karpathy)](https://www.youtube.com/watch?v=zduSFxRajkE) â€” Karpathy ä»é›¶å®ç° BPE çš„è§†é¢‘æ•™ç¨‹ â­â­â­â­â­

### å®è·µèµ„æº
- [tiktoken](https://github.com/openai/tiktoken) â€” OpenAI çš„é«˜æ€§èƒ½ BPE å®ç°ï¼ŒGPT-4 ä½¿ç”¨
- [sentencepiece](https://github.com/google/sentencepiece) â€” Google å®˜æ–¹å®ç°ï¼ŒLLaMA/T5/Qwen ä½¿ç”¨
- [tokenizers](https://github.com/huggingface/tokenizers) â€” HuggingFace çš„ Rust é«˜æ€§èƒ½åˆ†è¯åº“

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **æ¨¡å‹é€‰å‹æ—¶çš„ Tokenizer è¯„ä¼°**ï¼šä¸­æ–‡åœºæ™¯ä¸‹ï¼Œtiktokenï¼ˆGPT-4ï¼‰å¹³å‡æ¯ä¸ªæ±‰å­— ~1.5 tokenï¼Œè€Œ LLaMA çš„ SentencePiece æ¯ä¸ªæ±‰å­— ~2.5 tokenâ€”â€”ç›´æ¥å½±å“æœ‰æ•ˆä¸Šä¸‹æ–‡é•¿åº¦
- **é¢†åŸŸè‡ªå®šä¹‰ Tokenizer**ï¼šåŒ»å­¦/æ³•å¾‹ç­‰ä¸“ä¸šé¢†åŸŸçš„æœ¯è¯­å¦‚æœè¢«è¿‡åº¦åˆ†å‰²ï¼Œä¼šæµªè´¹ä¸Šä¸‹æ–‡çª—å£ã€‚ç”¨é¢†åŸŸè¯­æ–™è®­ç»ƒè‡ªå®šä¹‰ BPE å¯æå‡ 15-30% çš„å‹ç¼©ç‡
- **å¤šè¯­è¨€éƒ¨ç½²**ï¼šé€‰æ‹© Byte-level BPE æˆ– SentencePiece Unigramï¼Œç¡®ä¿é›¶ OOV

### å·¥ç¨‹å®ç°è¦ç‚¹
- **è¯è¡¨å¤§å°ç»éªŒå€¼**ï¼šè‹±æ–‡ 30-50Kï¼Œå¤šè¯­è¨€ 50-100Kï¼Œä»£ç åœºæ™¯ 50-100K
- **Byte-level BPE çš„ä»£ä»·**ï¼šä¸­æ–‡æ¯å­—ç¬¦éœ€è¦ 3 ä¸ª UTF-8 å­—èŠ‚ï¼Œå‹ç¼©å‰åºåˆ—æ›´é•¿ï¼Œéœ€è¦æ›´å¤§è¯è¡¨è¡¥å¿
- **ç‰ˆæœ¬ç»‘å®š**ï¼šTokenizer ç‰ˆæœ¬å¿…é¡»ä¸æ¨¡å‹ç‰ˆæœ¬ä¸¥æ ¼ç»‘å®šï¼Œæ›´æ¢åˆ†è¯å™¨ç­‰äºæ¢äº†æ¨¡å‹

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: ä¸ºä»€ä¹ˆç°ä»£ LLM ä¸ç”¨å­—ç¬¦çº§æˆ–è¯çº§åˆ†è¯ï¼Ÿ
  A: å­—ç¬¦çº§åºåˆ—å¤ªé•¿ï¼ˆ$O(5\times)$ï¼‰ï¼Œè¯çº§æœ‰ OOV é—®é¢˜ä¸”æ— æ³•æ³›åŒ–åˆ°æ–°è¯ã€‚å­è¯åˆ†è¯æ˜¯æœ€ä¼˜å¹³è¡¡â€”â€”è¯è¡¨å¯æ§ã€æ—  OOVã€ä¿ç•™å½¢æ€å­¦ä¿¡æ¯ã€‚BPE/Unigram çš„å‹ç¼©ç‡åœ¨ 3-5 characters/token ä¹‹é—´ã€‚

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- **Tokenizer å†³å®šäº†æ¨¡å‹çš„"è§†åŠ›"**ï¼šåˆ†è¯ä¸å¥½ï¼Œæ¨¡å‹çœ‹åˆ°çš„å°±æ˜¯ç¢ç‰‡åŒ–çš„å­—ç¬¦è€Œéæœ‰æ„ä¹‰çš„è¯­ä¹‰å•å…ƒã€‚é€‰æ¨¡å‹æ—¶ä¸åªçœ‹å‚æ•°é‡ï¼Œè¿˜è¦çœ‹å®ƒçš„ tokenizer å¯¹ç›®æ ‡è¯­è¨€çš„æ•ˆç‡
- **ä¸­æ–‡åœºæ™¯çš„éšè—æˆæœ¬**ï¼šå¾ˆå¤šè‹±æ–‡ä¼˜å…ˆçš„æ¨¡å‹ï¼ˆå¦‚æ—©æœŸ LLaMAï¼‰å¯¹ä¸­æ–‡åˆ†è¯æ•ˆç‡ä½ï¼ŒåŒæ · 4K ä¸Šä¸‹æ–‡çª—å£çš„"æœ‰æ•ˆä¸­æ–‡å®¹é‡"å¯èƒ½åªæœ‰ GPT-4 çš„ 60%

### æœªè§£é—®é¢˜ä¸å±€é™
- å­è¯åˆ†è¯æ˜¯å¦å·²ç»æ˜¯æœ€ä¼˜æ–¹æ¡ˆï¼Ÿæœ€è¿‘æœ‰ byte-level æ¨¡å‹ï¼ˆå¦‚ ByT5ï¼‰ç›´æ¥åœ¨å­—èŠ‚ä¸Šå»ºæ¨¡ï¼Œè·³è¿‡åˆ†è¯æ­¥éª¤ï¼Œä½†è®­ç»ƒæˆæœ¬æ›´é«˜
- åˆ†è¯å¯¹ä¸‹æ¸¸ä»»åŠ¡çš„å½±å“é‡åŒ–ç ”ç©¶ä»ä¸å……åˆ†â€”â€”åŒä¸€ä¸ªæ¨¡å‹ï¼Œæ¢åˆ†è¯å™¨ä¼šé€ æˆå¤šå¤§çš„æ€§èƒ½å·®å¼‚ï¼Ÿ
- ä»£ç åˆ†è¯çš„ç‰¹æ®ŠæŒ‘æˆ˜ï¼šç¼©è¿›ã€æ‹¬å·ã€è¿ç®—ç¬¦çš„å¤„ç†æ²¡æœ‰ç»Ÿä¸€æœ€ä¼˜æ–¹æ¡ˆ

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- å¦‚æœ [[Mamba-SSM|Mamba]] çš„çº¿æ€§å¤æ‚åº¦è®©è¶…é•¿åºåˆ—å˜å¾—å»‰ä»·ï¼Œå­—ç¬¦çº§/å­—èŠ‚çº§æ¨¡å‹æ˜¯å¦ä¼šå·åœŸé‡æ¥ï¼Ÿï¼ˆä¸å†éœ€è¦å‹ç¼©åºåˆ—é•¿åº¦ï¼‰
- [[Qwen|Qwen]] çš„å¤šè¯­è¨€åˆ†è¯ç­–ç•¥ vs [[GPT|GPT-4]] çš„ tiktokenï¼šå“ªç§å¯¹ä¸­æ–‡æ›´å‹å¥½ï¼Ÿé‡åŒ–å¯¹æ¯”æ˜¯ä¸€ä¸ªæœ‰ä»·å€¼çš„å®éªŒ

---

## See Also

- [[BERT|BERT]] â€” ä½¿ç”¨ WordPiece åˆ†è¯ï¼ˆå­è¯çº§ï¼‰
- [[GPT|GPT]] â€” ä½¿ç”¨ Byte-level BPE (tiktoken)ï¼Œå­—èŠ‚çº§ BPE
- [[LLaMA|LLaMA]] â€” ä½¿ç”¨ SentencePiece BPEï¼Œå¤šè¯­è¨€æ”¯æŒ
- [[Qwen|Qwen]] â€” ä½¿ç”¨ SentencePieceï¼Œå¤šè¯­è¨€ä¼˜åŒ–ï¼Œä¸­æ–‡ token æ•ˆç‡é«˜
- [[AI/LLM/Architecture/Tokenizeræ·±åº¦ç†è§£]] â€” åŒä¸»é¢˜æ·±åº¦ç‰ˆï¼ˆBPE/WordPiece/SentencePiece åŸç†å¯¹æ¯” + é¢è¯•é¢˜ï¼‰
- [[AI/LLM/Architecture/Tokenizer-Embedding-æ‰‹æ’•å®æ“]] â€” æ‰‹æ’•å®æ“ç‰ˆï¼ˆBPE ç®—æ³• + Embedding å®Œæ•´å®ç°ï¼‰