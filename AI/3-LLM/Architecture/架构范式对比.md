---
title: Transformer æ¶æ„èŒƒå¼å¯¹æ¯”ï¼šEncoder-Only vs Encoder-Decoder vs Decoder-Only
brief: å…¨é¢å¯¹æ¯” Transformer å››ç§æ¶æ„èŒƒå¼ï¼ˆEncoder-Only/Encoder-Decoder/Decoder-Only/Prefix LMï¼‰ï¼Œè§£é‡Š Decoder-Only èƒœå‡ºçš„æ ¸å¿ƒåŸå› ï¼šç»Ÿä¸€çš„è‡ªå›å½’ç›®æ ‡å‡½æ•°ã€æ›´å¥½çš„ Scaling Laws è¡¨ç°ã€In-Context Learning æ¶Œç°ã€‚åŒ…å«å®Œæ•´çš„ä»£ç å®ç°ã€æ³¨æ„åŠ›æ©ç å¯è§†åŒ–å’Œé¢è¯•å¸¸è§é—®é¢˜ã€‚
type: survey
domain: ai/llm/architecture
created: 2026-02-14
updated: 2026-02-22
tags:
  - ai/llm/architecture
  - type/survey
status: complete
sources:
  - Vaswani et al. *Attention Is All You Need* arXiv:1706.03762
  - Devlin et al. *BERT* arXiv:1810.04805
  - Raffel et al. *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)* arXiv:1910.10683
  - Brown et al. *Language Models are Few-Shot Learners (GPT-3)* arXiv:2005.14165
  - Kaplan et al. *Scaling Laws for Neural Language Models* arXiv:2001.08361
related:
  - "[[GPT|GPT]]"
  - "[[AI/3-LLM/Architecture/BERT|BERT]]"
  - "[[AI/3-LLM/Architecture/Mamba-SSM|Mamba-SSM]]"
  - "[[Transformer é€šè¯†|Transformer é€šè¯†]]"
---

# Decoder-Only vs Encoder-Decoder æ¶æ„èŒƒå¼å¯¹æ¯”

åœ¨ Transformer å‘å±•å²ä¸­ï¼Œå‡ºç°äº†å¤šç§æ¶æ„èŒƒå¼ã€‚ä»æ—©æœŸçš„ Encoder-Decoder åˆ°åæ¥ç»Ÿæ²» LLM é¢†åŸŸçš„ Decoder-Onlyï¼Œæ¯ç§èŒƒå¼éƒ½æœ‰å…¶ç‹¬ç‰¹çš„è®¾è®¡ç†å¿µå’Œé€‚ç”¨åœºæ™¯ã€‚æœ¬æ–‡å°†å…¨é¢å¯¹æ¯”è¿™äº›æ¶æ„èŒƒå¼ï¼Œè§£é‡Šä¸ºä»€ä¹ˆ GPT è·¯çº¿æœ€ç»ˆèƒœå‡ºã€‚

## å››å¤§æ¶æ„èŒƒå¼

### 1. Encoder-Onlyï¼ˆBERT ç³»åˆ—ï¼‰

ä¸“æ³¨äºç†è§£å’Œè¡¨å¾å­¦ä¹ ï¼ŒåŒå‘æ³¨æ„åŠ›ã€‚

```python
class EncoderOnlyTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers, max_seq_len):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(max_seq_len, d_model)
        
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=d_model, 
            nhead=nhead,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # åˆ†ç±»å¤´
        self.classifier = nn.Linear(d_model, 2)  # äºŒåˆ†ç±»ç¤ºä¾‹
    
    def forward(self, input_ids, attention_mask=None):
        seq_len = input_ids.size(1)
        pos_ids = torch.arange(seq_len, device=input_ids.device)
        
        # åŒå‘æ³¨æ„åŠ› - èƒ½çœ‹åˆ°æ•´ä¸ªåºåˆ—
        x = self.embedding(input_ids) + self.pos_embedding(pos_ids)
        encoded = self.encoder(x, src_key_padding_mask=~attention_mask)
        
        # [CLS] token æˆ–æ± åŒ–ç”¨äºä¸‹æ¸¸ä»»åŠ¡
        return self.classifier(encoded[:, 0])  # å–ç¬¬ä¸€ä¸ªä½ç½®
```

**ç‰¹ç‚¹**ï¼š
- **åŒå‘æ³¨æ„åŠ›**ï¼šæ¯ä¸ª token èƒ½çœ‹åˆ°å‰åå…¨éƒ¨ä¸Šä¸‹æ–‡
- **å¹¶è¡Œè®­ç»ƒ**ï¼šæ‰€æœ‰ä½ç½®åŒæ—¶è®¡ç®—
- **ç†è§£å¯¼å‘**ï¼šç‰¹åˆ«é€‚åˆåˆ†ç±»ã€é—®ç­”ç­‰ç†è§£ä»»åŠ¡

### 2. Encoder-Decoderï¼ˆT5ã€BARTï¼‰

ç»å…¸çš„åºåˆ—åˆ°åºåˆ—æ¶æ„ã€‚

```python
class EncoderDecoderTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        
        # Encoderï¼šåŒå‘æ³¨æ„åŠ›
        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)
        
        # Decoderï¼šå› æœæ³¨æ„åŠ› + äº¤å‰æ³¨æ„åŠ›
        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, batch_first=True)
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers)
        
        self.output_proj = nn.Linear(d_model, vocab_size)
    
    def forward(self, src_ids, tgt_ids):
        # Encoder å¤„ç†è¾“å…¥
        src_emb = self.embedding(src_ids)
        memory = self.encoder(src_emb)
        
        # Decoder ç”Ÿæˆè¾“å‡º
        tgt_emb = self.embedding(tgt_ids)
        # å› æœæ©ç  - decoder åªèƒ½çœ‹åˆ°ä¹‹å‰çš„token
        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_ids.size(1))
        
        decoded = self.decoder(
            tgt_emb, 
            memory, 
            tgt_mask=tgt_mask  # å› æœæ©ç 
        )
        
        return self.output_proj(decoded)
```

**æ³¨æ„åŠ›æ¨¡å¼**ï¼š
- **Encoder è‡ªæ³¨æ„åŠ›**ï¼šåŒå‘ï¼Œç†è§£è¾“å…¥è¯­ä¹‰
- **Decoder è‡ªæ³¨æ„åŠ›**ï¼šå•å‘å› æœï¼Œä¿è¯ç”Ÿæˆé¡ºåº
- **äº¤å‰æ³¨æ„åŠ›**ï¼šDecoder å…³æ³¨ Encoder è¾“å‡º

### 3. Decoder-Onlyï¼ˆGPT ç³»åˆ—ï¼‰

çº¯ç”Ÿæˆå¼æ¶æ„ï¼Œç»Ÿä¸€çš„è¯­è¨€å»ºæ¨¡ç›®æ ‡ã€‚

```python
class DecoderOnlyTransformer(nn.Module):
    def __init__(self, vocab_size, d_model, nhead, num_layers, max_seq_len):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(max_seq_len, d_model)
        
        # æ‰€æœ‰å±‚éƒ½æ˜¯ç›¸åŒçš„ decoder layer
        self.layers = nn.ModuleList([
            DecoderBlock(d_model, nhead) for _ in range(num_layers)
        ])
        
        self.ln_f = nn.LayerNorm(d_model)
        self.output_proj = nn.Linear(d_model, vocab_size)
    
    def forward(self, input_ids, past_key_values=None):
        seq_len = input_ids.size(1)
        pos_ids = torch.arange(seq_len, device=input_ids.device)
        
        # å› æœæ©ç  - åªèƒ½çœ‹åˆ°ä¹‹å‰çš„token
        causal_mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()
        
        x = self.embedding(input_ids) + self.pos_embedding(pos_ids)
        
        for layer in self.layers:
            x = layer(x, causal_mask, past_key_values)
        
        x = self.ln_f(x)
        logits = self.output_proj(x)
        
        return logits

class DecoderBlock(nn.Module):
    def __init__(self, d_model, nhead):
        super().__init__()
        self.ln1 = nn.LayerNorm(d_model)
        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)
        self.ln2 = nn.LayerNorm(d_model)
        self.mlp = nn.Sequential(
            nn.Linear(d_model, 4 * d_model),
            nn.GELU(),
            nn.Linear(4 * d_model, d_model)
        )
    
    def forward(self, x, causal_mask, past_key_values=None):
        # Pre-Norm + Causal Self-Attention
        normed = self.ln1(x)
        attn_out, _ = self.attn(
            normed, normed, normed,
            attn_mask=causal_mask,
            need_weights=False
        )
        x = x + attn_out
        
        # Pre-Norm + MLP
        x = x + self.mlp(self.ln2(x))
        return x
```

### 4. Prefix LMï¼ˆPaLMã€GLMï¼‰

æ··åˆåŒå‘å’Œå•å‘æ³¨æ„åŠ›çš„æ–¹æ¡ˆã€‚

```python
def create_prefix_mask(seq_len, prefix_len):
    """
    åˆ›å»º Prefix LM çš„æ³¨æ„åŠ›æ©ç 
    prefix éƒ¨åˆ†åŒå‘ï¼Œsuffix éƒ¨åˆ†å› æœ
    """
    mask = torch.zeros(seq_len, seq_len, dtype=torch.bool)
    
    # Prefix éƒ¨åˆ†ï¼šåŒå‘æ³¨æ„åŠ›
    mask[:prefix_len, :prefix_len] = False  # å¯ä»¥äº’ç›¸çœ‹åˆ°
    
    # Suffix éƒ¨åˆ†ï¼šå› æœæ³¨æ„åŠ›
    for i in range(prefix_len, seq_len):
        mask[i, :i+1] = False  # åªèƒ½çœ‹åˆ°å‰é¢çš„ï¼ˆåŒ…æ‹¬æ•´ä¸ªprefixï¼‰
        mask[i, i+1:] = True   # ä¸èƒ½çœ‹åˆ°åé¢çš„
    
    return mask

# ä½¿ç”¨ç¤ºä¾‹
seq_len, prefix_len = 10, 4
mask = create_prefix_mask(seq_len, prefix_len)
print("Prefix LM Mask:")
print(mask.int())
# è¾“å‡ºï¼š
# 0 0 0 0 1 1 1 1 1 1  <- position 0 can see prefix
# 0 0 0 0 1 1 1 1 1 1  <- position 1 can see prefix  
# 0 0 0 0 1 1 1 1 1 1  <- position 2 can see prefix
# 0 0 0 0 1 1 1 1 1 1  <- position 3 can see prefix
# 0 0 0 0 0 1 1 1 1 1  <- position 4 causal
# 0 0 0 0 0 0 1 1 1 1  <- position 5 causal
# ...
```

## æ³¨æ„åŠ›æ©ç å¯¹æ¯”

ä¸åŒæ¶æ„çš„æ³¨æ„åŠ›æ¨¡å¼å·®å¼‚å·¨å¤§ï¼š

| æ¶æ„ | æ³¨æ„åŠ›æ¨¡å¼ | è®­ç»ƒå¹¶è¡Œæ€§ | ç”Ÿæˆæ–¹å¼ |
|------|-----------|------------|----------|
| **Encoder-Only** | å…¨åŒå‘ | å®Œå…¨å¹¶è¡Œ | ä¸ç›´æ¥ç”Ÿæˆ |
| **Encoder-Decoder** | EncoderåŒå‘ + Decoderå› æœ | éƒ¨åˆ†å¹¶è¡Œ | è‡ªå›å½’ |
| **Decoder-Only** | å…¨å› æœ | å®Œå…¨å¹¶è¡Œ | è‡ªå›å½’ |
| **Prefix LM** | æ··åˆ | å®Œå…¨å¹¶è¡Œ | è‡ªå›å½’ |

### å¯è§†åŒ–æ³¨æ„åŠ›æ¨¡å¼

```python
import matplotlib.pyplot as plt
import numpy as np

def visualize_attention_patterns():
    seq_len = 8
    
    fig, axes = plt.subplots(2, 2, figsize=(12, 10))
    
    # 1. Encoder-Only (BERT)
    encoder_mask = np.zeros((seq_len, seq_len))  # å…¨éƒ¨å¯è§
    axes[0,0].imshow(encoder_mask, cmap='RdBu', vmin=-1, vmax=1)
    axes[0,0].set_title('Encoder-Only\n(BERT)')
    
    # 2. Decoder-Only (GPT) 
    decoder_mask = np.triu(np.ones((seq_len, seq_len)), k=1)  # ä¸Šä¸‰è§’
    axes[0,1].imshow(decoder_mask, cmap='RdBu', vmin=-1, vmax=1)
    axes[0,1].set_title('Decoder-Only\n(GPT)')
    
    # 3. Encoder-Decoder (T5) - åªæ˜¾ç¤ºdecoderéƒ¨åˆ†
    encdec_mask = np.triu(np.ones((seq_len, seq_len)), k=1)
    axes[1,0].imshow(encdec_mask, cmap='RdBu', vmin=-1, vmax=1)
    axes[1,0].set_title('Encoder-Decoder\n(Decoderéƒ¨åˆ†)')
    
    # 4. Prefix LM
    prefix_len = 3
    prefix_mask = np.triu(np.ones((seq_len, seq_len)), k=1)
    prefix_mask[:prefix_len, :prefix_len] = 0  # prefixåŒå‘
    axes[1,1].imshow(prefix_mask, cmap='RdBu', vmin=-1, vmax=1)
    axes[1,1].set_title('Prefix LM\n(prefix=3)')
    
    for ax in axes.flat:
        ax.set_xlabel('Key Position')
        ax.set_ylabel('Query Position')
    
    plt.tight_layout()
    plt.show()

# è¿è¡Œå¯è§†åŒ–
visualize_attention_patterns()
```

## ä¸ºä»€ä¹ˆ GPT è·¯çº¿èƒœå‡ºï¼Ÿ

### 1. Scaling Laws ä¼˜åŠ¿

**ç»Ÿä¸€ç›®æ ‡å‡½æ•°**ï¼ˆKaplan et al. *Scaling Laws for Neural Language Models* arXiv:2001.08361ï¼‰ï¼šDecoder-Only ç”¨å•ä¸€çš„è¯­è¨€å»ºæ¨¡æŸå¤±ï¼š

$$\mathcal{L} = -\sum_{i=1}^{T} \log P(x_i | x_{<i})$$

è¿™ç§ç»Ÿä¸€æ€§å¸¦æ¥äº†å·¨å¤§ä¼˜åŠ¿ï¼š

```python
# GPT çš„ç®€æ´è®­ç»ƒå¾ªç¯
def train_step(model, batch):
    input_ids = batch['input_ids']  # [batch, seq_len]
    
    # è¾“å…¥æ˜¯ x[:-1]ï¼Œç›®æ ‡æ˜¯ x[1:]
    inputs = input_ids[:, :-1]
    targets = input_ids[:, 1:]
    
    logits = model(inputs)  # [batch, seq_len-1, vocab]
    
    # ç®€å•çš„äº¤å‰ç†µæŸå¤±
    loss = F.cross_entropy(
        logits.reshape(-1, logits.size(-1)), 
        targets.reshape(-1),
        ignore_index=-1
    )
    
    return loss
```

**å¯¹æ¯” Encoder-Decoder çš„å¤æ‚æ€§**ï¼š
- éœ€è¦è®¾è®¡è¾“å…¥/è¾“å‡ºæ ¼å¼
- å¤šä¸ªæŸå¤±å‡½æ•°ï¼ˆé‡æ„ã€åˆ†ç±»ç­‰ï¼‰
- æ•°æ®é¢„å¤„ç†æ›´å¤æ‚

### 2. å‚æ•°æ•ˆç‡

Decoder-Only é¿å…äº†æ¶æ„å¤æ‚æ€§ï¼š

| ç»„ä»¶ | Encoder-Decoder | Decoder-Only |
|------|----------------|--------------|
| **Encoder å±‚** | N å±‚ | 0 å±‚ |
| **Decoder å±‚** | N å±‚ | N å±‚ |
| **äº¤å‰æ³¨æ„åŠ›** | N Ã— nhead | 0 |
| **æ€»å‚æ•°** | ~2N Ã— dÂ² | ~N Ã— dÂ² |

```python
# å‚æ•°é‡å¯¹æ¯”
def count_parameters():
    d_model, nhead, num_layers = 768, 12, 12
    vocab_size = 50000
    
    # Encoder-Decoder
    enc_dec_params = (
        2 * num_layers * (4 * d_model**2) +  # Self-attn + FFN Ã— 2
        num_layers * (d_model**2) +          # Cross-attn
        2 * vocab_size * d_model             # Embedding Ã— 2
    )
    
    # Decoder-Only  
    decoder_params = (
        num_layers * (4 * d_model**2) +      # Self-attn + FFN
        vocab_size * d_model                 # Embedding
    )
    
    print(f"Encoder-Decoder: {enc_dec_params:,} å‚æ•°")
    print(f"Decoder-Only: {decoder_params:,} å‚æ•°")
    print(f"èŠ‚çœ: {(1 - decoder_params/enc_dec_params)*100:.1f}%")

count_parameters()
# Encoder-Decoder: 122,112,000 å‚æ•°
# Decoder-Only: 76,416,000 å‚æ•°  
# èŠ‚çœ: 37.4%
```

### 3. æ¶Œç°èƒ½åŠ›çš„æ‰©å±•æ€§

**Few-shot Learning**ï¼šGPT-3ï¼ˆarXiv:2005.14165ï¼‰å±•ç¤ºäº† Decoder-Only çš„æ¶Œç°èƒ½åŠ›ï¼š

```python
# In-Context Learning ç¤ºä¾‹
prompt = """
# ç¿»è¯‘ä»»åŠ¡ç¤ºä¾‹
English: Hello, how are you?
Chinese: ä½ å¥½ï¼Œä½ æ€ä¹ˆæ ·ï¼Ÿ

English: The weather is nice today.
Chinese: ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚

English: I love machine learning.
Chinese:"""

# GPT èƒ½ç›´æ¥ç†è§£ä»»åŠ¡å¹¶æ‰§è¡Œï¼Œæ— éœ€å¾®è°ƒ
response = model.generate(prompt)
# è¾“å‡ºï¼šæˆ‘å–œæ¬¢æœºå™¨å­¦ä¹ ã€‚
```

**ä»»åŠ¡æ³›åŒ–**ï¼šå•ä¸€æ¨¡å‹å¤„ç†å¤šç§ä»»åŠ¡ï¼š
- æ–‡æœ¬ç”Ÿæˆ
- é—®ç­”
- ç¿»è¯‘  
- ä»£ç ç”Ÿæˆ
- æ¨ç†

### 4. å·¥ç¨‹ä¼˜åŠ¿

**ç®€åŒ–æ¶æ„**ï¼š
```python
# Decoder-Only çš„ç®€æ´æ€§
class SimpleGPT(nn.Module):
    def __init__(self, vocab_size, d_model, num_layers):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.layers = nn.ModuleList([
            DecoderBlock(d_model) for _ in range(num_layers)
        ])
        self.output = nn.Linear(d_model, vocab_size)
    
    def forward(self, x):
        x = self.embedding(x)
        for layer in self.layers:
            x = layer(x)
        return self.output(x)

# ç›¸åŒåŠŸèƒ½çš„ Encoder-Decoder éœ€è¦æ›´å¤šä»£ç 
```

**æ¨ç†æ•ˆç‡**ï¼š
- **KV Cache**ï¼šåªéœ€ç¼“å­˜ä¸€ä¸ªæ–¹å‘
- **å¹¶è¡Œåº¦**ï¼šæ— äº¤å‰æ³¨æ„åŠ›ç“¶é¢ˆ
- **å†…å­˜å ç”¨**ï¼šæ¶æ„æ›´ç®€å•

## ä»£è¡¨æ¨¡å‹å¯¹æ¯”

### Encoder-Only ä»£è¡¨

| æ¨¡å‹ | å‚æ•°é‡ | ç‰¹ç‚¹ | é€‚ç”¨ä»»åŠ¡ |
|------|--------|------|----------|
| **BERT-Base** | 110M | åŒå‘ç†è§£ | åˆ†ç±»ã€é—®ç­”ã€NER |
| **RoBERTa** | 125M | ä¼˜åŒ–è®­ç»ƒç­–ç•¥ | ç†è§£ä»»åŠ¡ |
| **DeBERTa** | 134M | è§£è€¦æ³¨æ„åŠ› | GLUE æ¦œé¦– |

### Encoder-Decoder ä»£è¡¨

| æ¨¡å‹ | å‚æ•°é‡ | ç‰¹ç‚¹ | é€‚ç”¨ä»»åŠ¡ |
|------|--------|------|----------|
| **T5-Base** | 220M | Text-to-Text | åºåˆ—è½¬æ¢ |
| **BART** | 140M | å»å™ªé¢„è®­ç»ƒ | æ‘˜è¦ã€å¯¹è¯ |
| **UL2** | 20B | ç»Ÿä¸€è¯­è¨€å­¦ä¹  | å¤šä»»åŠ¡ |

### Decoder-Only ä»£è¡¨

| æ¨¡å‹ | å‚æ•°é‡ | ç‰¹ç‚¹ | é€‚ç”¨ä»»åŠ¡ |
|------|--------|------|----------|
| **GPT-3** | 175B | Few-shot Learning | é€šç”¨æ–‡æœ¬ |
| **PaLM** | 540B | æ¨ç†èƒ½åŠ› | å¤æ‚æ¨ç† |
| **GPT-4**ï¼ˆarXiv:2303.08774ï¼‰ | ~1.7T | å¤šæ¨¡æ€ | AGI æ–¹å‘ |
| **LLaMA** | 7B-65B | å¼€æºé«˜æ•ˆ | ç ”ç©¶/åº”ç”¨ |

## ç°ä»£è¶‹åŠ¿ä¸é€‰æ‹©å»ºè®®

### é€‰æ‹©æŒ‡å—

| åœºæ™¯ | æ¨èæ¶æ„ | ç†ç”± |
|------|----------|------|
| **ç†è§£ä»»åŠ¡**ï¼ˆåˆ†ç±»ã€æŠ½å–ï¼‰ | Encoder-Only | åŒå‘ç†è§£ï¼Œæ•ˆç‡é«˜ |
| **åºåˆ—è½¬æ¢**ï¼ˆç¿»è¯‘ã€æ‘˜è¦ï¼‰ | Encoder-Decoder | æ˜ç¡®è¾“å…¥è¾“å‡ºåˆ†ç¦» |
| **é€šç”¨ AI**ï¼ˆå¯¹è¯ã€æ¨ç†ï¼‰ | Decoder-Only | ç»Ÿä¸€èŒƒå¼ï¼Œå¯æ‰©å±• |
| **ç ”ç©¶åŸå‹** | Decoder-Only | ç¤¾åŒºæ”¯æŒï¼Œèµ„æºä¸°å¯Œ |

### æœªæ¥æ–¹å‘

1. **Decoder-Only ç»§ç»­ä¸»å¯¼**ï¼šGPT-4ã€Claude ç­‰è¯æ˜äº†è·¯å¾„æ­£ç¡®æ€§
2. **æ¶æ„åˆ›æ–°**ï¼šMoEã€RoPEã€ALiBi ç­‰æ”¹è¿›
3. **å¤šæ¨¡æ€èåˆ**ï¼šè§†è§‰ã€éŸ³é¢‘ç»Ÿä¸€åˆ° Decoder-Only
4. **æ•ˆç‡ä¼˜åŒ–**ï¼šMambaã€RetNet ç­‰æ›¿ä»£æ–¹æ¡ˆ

## é¢è¯•å¸¸è§é—®é¢˜

### Q1ï¼šä¸ºä»€ä¹ˆ Decoder-Only èƒ½å¤Ÿç»Ÿæ²» LLM é¢†åŸŸï¼Ÿ

**ç­”æ¡ˆ**ï¼š
1. **ç»Ÿä¸€ç›®æ ‡**ï¼šå•ä¸€çš„è¯­è¨€å»ºæ¨¡æŸå¤±ï¼Œç®€åŒ–è®­ç»ƒ
2. **å‚æ•°æ•ˆç‡**ï¼šé¿å… Encoder å’Œäº¤å‰æ³¨æ„åŠ›çš„å†—ä½™
3. **æ‰©å±•æ€§**ï¼šScaling Laws è¡¨ç°æ›´å¥½ï¼Œæ¶Œç°èƒ½åŠ›æ›´å¼º
4. **å·¥ç¨‹ç®€æ´**ï¼šæ¶æ„ç®€å•ï¼Œæ˜“äºä¼˜åŒ–å’Œéƒ¨ç½²
5. **ä»»åŠ¡æ³›åŒ–**ï¼šIn-Context Learning èƒ½åŠ›ï¼Œæ— éœ€å¾®è°ƒé€‚åº”æ–°ä»»åŠ¡

### Q2ï¼šEncoder-Decoder åœ¨ä»€ä¹ˆåœºæ™¯ä¸‹ä»ç„¶æœ‰ä¼˜åŠ¿ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
1. **æ˜ç¡®çš„è¾“å…¥è¾“å‡ºåˆ†ç¦»**ï¼šæœºå™¨ç¿»è¯‘ã€æ–‡æ¡£æ‘˜è¦
2. **è®¡ç®—èµ„æºå—é™**ï¼šå°æ¨¡å‹åœºæ™¯ä¸‹æ›´é«˜æ•ˆ
3. **ç‰¹å®šé¢†åŸŸä¼˜åŒ–**ï¼šé’ˆå¯¹æ€§è®¾è®¡çš„ä»»åŠ¡ï¼ˆå¦‚ä»£ç ç”Ÿæˆçš„ CodeT5ï¼‰
4. **å¯è§£é‡Šæ€§è¦æ±‚**ï¼šEncoder è¡¨ç¤ºå¯ä»¥å•ç‹¬åˆ†æ
5. **æ··åˆä»»åŠ¡**ï¼šåŒæ—¶éœ€è¦ç†è§£å’Œç”Ÿæˆçš„å¤æ‚åœºæ™¯

### Q3ï¼šä¸åŒæ¶æ„çš„æ³¨æ„åŠ›æ©ç æ˜¯å¦‚ä½•å½±å“æ¨¡å‹èƒ½åŠ›çš„ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**åŒå‘æ³¨æ„åŠ›ï¼ˆBERTï¼‰**ï¼š
- ä¼˜åŠ¿ï¼šå®Œæ•´ä¸Šä¸‹æ–‡ç†è§£ï¼Œé€‚åˆç†è§£ä»»åŠ¡
- åŠ£åŠ¿ï¼šæ— æ³•ç›´æ¥ç”Ÿæˆï¼Œéœ€è¦é¢å¤–è§£ç å¤´

**å•å‘å› æœæ³¨æ„åŠ›ï¼ˆGPTï¼‰**ï¼š
- ä¼˜åŠ¿ï¼šè‡ªç„¶ç”Ÿæˆèƒ½åŠ›ï¼Œç»Ÿä¸€è®­ç»ƒç›®æ ‡
- åŠ£åŠ¿ï¼šç†è§£ä»»åŠ¡å¯èƒ½æ¬¡ä¼˜ï¼ˆä½†å¤§æ¨¡å‹ä¸‹å·®è·å¾ˆå°ï¼‰

**æ··åˆæ¨¡å¼ï¼ˆT5ï¼‰**ï¼š
- ä¼˜åŠ¿ï¼šç»“åˆä¸¤è€…ä¼˜ç‚¹
- åŠ£åŠ¿ï¼šæ¶æ„å¤æ‚ï¼Œå‚æ•°æ•ˆç‡ä½

### Q4ï¼šå¦‚ä½•é€‰æ‹©åˆé€‚çš„ Transformer æ¶æ„ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**è€ƒè™‘å› ç´ **ï¼š
1. **ä»»åŠ¡ç±»å‹**ï¼šç”Ÿæˆ vs ç†è§£ vs è½¬æ¢
2. **æ•°æ®è§„æ¨¡**ï¼šå¤§æ•°æ®å€¾å‘ Decoder-Only
3. **è®¡ç®—é¢„ç®—**ï¼šå°æ¨¡å‹å¯è€ƒè™‘ Encoder-Decoder
4. **éƒ¨ç½²è¦æ±‚**ï¼šæ¨ç†æ•ˆç‡ã€å†…å­˜é™åˆ¶
5. **å¼€å‘å‘¨æœŸ**ï¼šDecoder-Only ç”Ÿæ€æ›´æˆç†Ÿ

**å†³ç­–æ ‘**ï¼š
```
if ä»»åŠ¡ == "çº¯ç†è§£" and æ¨¡å‹è§„æ¨¡ < 1B:
    return "Encoder-Only"
elif ä»»åŠ¡ == "åºåˆ—è½¬æ¢" and è¾“å…¥è¾“å‡ºå·®å¼‚å¤§:
    return "Encoder-Decoder"  
else:
    return "Decoder-Only"  # é»˜è®¤é€‰æ‹©
```

### Q5ï¼šPre-training ç›®æ ‡å¦‚ä½•å½±å“æ¶æ„é€‰æ‹©ï¼Ÿ

**ç­”æ¡ˆ**ï¼š
**BERT å¼æ©ç è¯­è¨€æ¨¡å‹**ï¼š
- éœ€è¦åŒå‘ä¸Šä¸‹æ–‡ï¼Œå¿…é¡»ç”¨ Encoder
- é€‚åˆç†è§£ä»»åŠ¡ï¼Œä½†ç”Ÿæˆèƒ½åŠ›æœ‰é™

**GPT å¼è‡ªå›å½’ç”Ÿæˆ**ï¼š
- å› æœçº¦æŸï¼Œé€‚åˆ Decoder-Only
- ç»Ÿä¸€å„ç§ä»»åŠ¡ä¸ºç”Ÿæˆé—®é¢˜

**T5 å¼ Span é‡æ„**ï¼š
- éœ€è¦ Encoder-Decoder å¤„ç†æŸåå’Œé‡æ„
- åœ¨ç†è§£å’Œç”Ÿæˆé—´å–å¹³è¡¡

**ç°ä»£è¶‹åŠ¿**ï¼š
ç»Ÿä¸€åˆ°è‡ªå›å½’ç”Ÿæˆï¼Œå› ä¸ºï¼š
1. ç®€åŒ–äº†é¢„è®­ç»ƒæµç¨‹
2. ä¸‹æ¸¸ä»»åŠ¡é€‚é…æ›´çµæ´»
3. æ‰©å±•æ€§æ›´å¥½

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) â€” Transformer åŸå§‹è®ºæ–‡ï¼ŒEncoder-Decoder æ¶æ„çš„èµ·ç‚¹
- [BERT](https://arxiv.org/abs/1810.04805) â€” Encoder-Only è·¯çº¿çš„ä»£è¡¨
- [T5: Exploring the Limits of Transfer Learning](https://arxiv.org/abs/1910.10683) â€” Text-to-Text ç»Ÿä¸€æ¡†æ¶ï¼ŒEncoder-Decoder è·¯çº¿çš„å·…å³°
- [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165) â€” Decoder-Only è·¯çº¿èƒœå‡ºçš„å…³é”®è®ºæ–‡
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361) â€” Decoder-Only ä¼˜åŠ¿çš„ç†è®ºè§£é‡Š

### æ·±åº¦è§£è¯»
- [What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?](https://arxiv.org/abs/2204.05832) â€” ç³»ç»Ÿæ€§å¯¹æ¯”ä¸åŒæ¶æ„èŒƒå¼ â­â­â­â­

### å®è·µèµ„æº
- [nanoGPT](https://github.com/karpathy/nanoGPT) â€” Decoder-Only æœ€ç®€å®ç°
- [T5 HuggingFace](https://huggingface.co/docs/transformers/model_doc/t5) â€” Encoder-Decoder å®è·µå…¥å£

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **æ¶æ„é€‰å‹å†³ç­–**ï¼šé€šç”¨ AI / å¯¹è¯ / æ¨ç† â†’ Decoder-Onlyï¼›çº¯ç†è§£ä»»åŠ¡ï¼ˆåˆ†ç±»/NERï¼‰ä¸”æ¨¡å‹ <1B â†’ Encoder-Onlyï¼›åºåˆ—è½¬æ¢ï¼ˆç¿»è¯‘/æ‘˜è¦ï¼‰ä¸”è¾“å…¥è¾“å‡ºå·®å¼‚å¤§ â†’ Encoder-Decoder
- **é¢è¯•å‡†å¤‡**ï¼š"ä¸ºä»€ä¹ˆ Decoder-Only èƒœå‡º"æ˜¯ LLM æ–¹å‘æœ€é«˜é¢‘çš„æ¶æ„é¢è¯•é¢˜

### å·¥ç¨‹å®ç°è¦ç‚¹
- Decoder-Only çš„ **KV Cache** åªéœ€ç¼“å­˜ä¸€ä¸ªæ–¹å‘ï¼Œå·¥ç¨‹å®ç°æ›´ç®€å•
- Encoder-Decoder åœ¨å°æ¨¡å‹ï¼ˆ<1Bï¼‰ä¸‹å‚æ•°æ•ˆç‡æ›´é«˜ï¼š$\sim N \times d^2$ï¼ˆDecoder-Onlyï¼‰vs $\sim 2N \times d^2$ï¼ˆEncoder-Decoderï¼‰ï¼Œä½†äº¤å‰æ³¨æ„åŠ›å¢åŠ æ¨ç†å»¶è¿Ÿ

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: Prefix LM å’Œ Decoder-Only çš„æœ¬è´¨åŒºåˆ«ï¼Ÿ
  A: Prefix LM å¯¹è¾“å…¥éƒ¨åˆ†ï¼ˆprefixï¼‰ä½¿ç”¨åŒå‘æ³¨æ„åŠ›ï¼Œç”Ÿæˆéƒ¨åˆ†ä½¿ç”¨å› æœæ³¨æ„åŠ›ã€‚æœ¬è´¨æ˜¯ Encoder-Decoder çš„"è½¯"ç‰ˆæœ¬â€”â€”ä¸éœ€è¦æ˜¾å¼åˆ†ç¦» Encoder/Decoderï¼Œä½†ä¿ç•™äº†åŒå‘ç†è§£è¾“å…¥çš„èƒ½åŠ›ã€‚PaLMã€GLM é‡‡ç”¨æ­¤èŒƒå¼ã€‚

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- **"ä¸€åˆ‡çš†ç”Ÿæˆ"å·²æˆå®šå±€**ï¼šDecoder-Only çš„ç»Ÿæ²»æ„å‘³ç€æœªæ¥å‡ ä¹æ‰€æœ‰ NLP ä»»åŠ¡éƒ½ä¼šè¢«è½¬åŒ–ä¸ºç”Ÿæˆé—®é¢˜ã€‚ç†è§£è¿™ä¸ªè¶‹åŠ¿å¯¹é€‰å‹ã€é¢è¯•ã€æŠ€æœ¯åˆ¤æ–­éƒ½è‡³å…³é‡è¦
- **ä½† Encoder æ¶æ„æ²¡æ­»**ï¼š[[AI/3-LLM/Architecture/BERT|BERT]] åœ¨ Embedding/Reranker/NER ç­‰åœºæ™¯ä»ä¸å¯æ›¿ä»£ï¼Œä¸è¦å› ä¸º"GPT èµ¢äº†"å°±å¿½è§† Encoder æ¶æ„

### æœªè§£é—®é¢˜ä¸å±€é™
- Decoder-Only åœ¨çº¯ç†è§£ä»»åŠ¡ä¸ŠçœŸçš„è¿½ä¸Š Encoder äº†å—ï¼Ÿå¤§æ¨¡å‹ä¸‹å·®è·å¾ˆå°ï¼Œä½†å°æ¨¡å‹ä¸‹ BERT ä»æœ‰ä¼˜åŠ¿
- [[AI/3-LLM/Architecture/Mamba-SSM|Mamba]] ç­‰ SSM æ¶æ„æ˜¯å¦ç®—"ç¬¬äº”ç§èŒƒå¼"ï¼Ÿå®ƒæ—¢ä¸æ˜¯æ ‡å‡† Attentionï¼Œä¹Ÿä¸æ˜¯ RNN

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- MoE + Decoder-Onlyï¼ˆ[[AI/3-LLM/Architecture/DeepSeek-R1|DeepSeek-R1]]ï¼‰vs SSM + Attention æ··åˆï¼ˆJambaï¼‰ï¼šä¸‹ä¸€ä¸ªåå¹´çš„æ¶æ„ç«äº‰ä¼šæ˜¯ä»€ä¹ˆæ ¼å±€ï¼Ÿ
- å¦‚æœ Encoder-Decoder åœ¨å°æ¨¡å‹åœºæ™¯ä¸‹æ›´é«˜æ•ˆï¼Œé‚£ [[Qwen|Qwen]] ç­‰å¼€æºæ¨¡å‹æ˜¯å¦åº”è¯¥ä¸º Edge åœºæ™¯æä¾› Encoder-Decoder ç‰ˆæœ¬ï¼Ÿ

---

## See Also

- [[GPT|GPT]] â€” Decoder-Only è·¯çº¿çš„ä»£è¡¨ï¼ŒGPT-2/3 ç¡®ç«‹è‡ªå›å½’ç”ŸæˆèŒƒå¼
- [[AI/3-LLM/Architecture/BERT|BERT]] â€” Encoder-Only è·¯çº¿çš„ä»£è¡¨ï¼ŒMLM é¢„è®­ç»ƒå¥ åŸº
- [[AI/3-LLM/Architecture/T5|T5]] â€” Encoder-Decoder è·¯çº¿çš„ä»£è¡¨ï¼Œ"ä¸€åˆ‡ä»»åŠ¡çš† seq2seq"
- [[AI/3-LLM/Architecture/Mamba-SSM|Mamba-SSM]] â€” çº¿æ€§å¤æ‚åº¦ SSMï¼Œå¯èƒ½æ˜¯ç¬¬äº”ç§èŒƒå¼
- [[AI/3-LLM/Architecture/MoE æ·±åº¦è§£æ]] â€” MoE åœ¨ Decoder-Only æ¡†æ¶ä¸‹çš„æ‰©å±•ï¼ˆDeepSeek-V3/R1ï¼‰
- [[Transformeræ¶æ„æ·±åº¦è§£æ-2026æŠ€æœ¯å…¨æ™¯]] â€” Transformer å®Œæ•´æ¶æ„å…¨æ™¯å›¾

## æ¨èé˜…è¯»

- Vaswani et al. *Attention Is All You Need* (2017) â€” [arXiv:1706.03762](https://arxiv.org/abs/1706.03762)
- Kaplan et al. *Scaling Laws for Neural Language Models* (2020) â€” [arXiv:2001.08361](https://arxiv.org/abs/2001.08361)
- Brown et al. *GPT-3: Language Models are Few-Shot Learners* (2020) â€” [arXiv:2005.14165](https://arxiv.org/abs/2005.14165)