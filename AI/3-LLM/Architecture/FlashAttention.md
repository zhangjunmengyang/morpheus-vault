---
title: FlashAttention v1/v2/v3 æ·±åº¦è§£æ
brief: FlashAttention é€šè¿‡ IO-aware tiling å°† Attention ä» memory-bound å˜ä¸º compute-boundï¼šv1 å¼•å…¥ online softmax å®ç° O(N) å†…å­˜ï¼›v2 åè½¬å¾ªç¯+å‡å°‘é matmul FLOPsï¼ŒGPU åˆ©ç”¨ç‡ 25%â†’73%ï¼›v3 é’ˆå¯¹ Hopper æ¶æ„ç”¨ TMA/WGMMA/FP8 è¿›ä¸€æ­¥ 3.2x åŠ é€Ÿã€‚ä¸æ˜¯è¿‘ä¼¼ç®—æ³•ï¼Œç»“æœç²¾ç¡®ç­‰ä»·æ ‡å‡† Attentionã€‚
date: 2026-02-13
updated: 2026-02-22
tags:
  - ai/llm/architecture
  - ai/llm/inference
  - ai/attention
  - type/concept
  - interview/hot
status: complete
sources:
  - "Dao et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. arXiv:2205.14135"
  - "Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. arXiv:2307.08691"
  - "Shah et al. FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision. arXiv:2407.08608"
related:
  - "[[AI/3-LLM/Architecture/GQA-MQA|GQA/MQA]]"
  - "[[AI/3-LLM/Inference/KV Cache|KV Cache åŸç†ä¸ä¼˜åŒ–]]"
  - "[[AI/3-LLM/Architecture/Attention å˜ä½“ç»¼è¿°|Attention å˜ä½“ç»¼è¿°]]"
  - "[[AI/3-LLM/Inference/é‡åŒ–ç»¼è¿°|é‡åŒ–ç»¼è¿°]]"
  - "Transformer æ¶æ„å…¨æ™¯"
---

# FlashAttention v1/v2/v3 æ·±åº¦è§£æ

> IO-aware Attention ç®—æ³•â€”â€”é€šè¿‡ tiling ä¸ kernel fusion å°† Attention ä» memory-bound å˜ä¸º compute-bound

## 1. ä¸ºä»€ä¹ˆéœ€è¦ FlashAttentionï¼Ÿ

### æ ‡å‡† Attention çš„å†…å­˜ç“¶é¢ˆ

æ ‡å‡† Self-Attention è®¡ç®—ï¼š

```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) Â· V
```

é—®é¢˜åœ¨äºä¸­é—´çŸ©é˜µ `S = QK^T` çš„å¤§å°ä¸º `NÃ—N`ï¼ˆN ä¸ºåºåˆ—é•¿åº¦ï¼‰ï¼š

| åºåˆ—é•¿åº¦ N | S çŸ©é˜µå¤§å° (FP16) | è¯´æ˜ |
|-----------|-------------------|------|
| 2K | 8 MB | å¯æ¥å— |
| 8K | 128 MB | æ˜¾å­˜å‹åŠ›å¤§ |
| 32K | 2 GB | å•å±‚å°±åƒæ»¡ |
| 128K | 32 GB | æ— æ³•æ”¾å…¥ HBM |

æ›´å…³é”®çš„æ˜¯ **IO ç“¶é¢ˆ**ï¼šæ ‡å‡†å®ç°éœ€è¦å°† `S` çŸ©é˜µå†™å…¥ HBMï¼Œå†è¯»å›æ¥åš softmaxï¼Œå†å†™å›ï¼Œå†è¯»å›åš `SÂ·V`â€”â€”å¤§é‡æ—¶é—´æµªè´¹åœ¨ HBM è¯»å†™ä¸Šï¼š

```mermaid
sequenceDiagram
    participant SRAM as SRAM (19 TB/s)
    participant HBM as HBM (2 TB/s)
    Note over SRAM,HBM: æ ‡å‡† Attention çš„ HBM è®¿é—®æ¨¡å¼
    SRAM->>HBM: å†™å…¥ S = QK^T (NÃ—N)
    HBM->>SRAM: è¯»å› S åš softmax
    SRAM->>HBM: å†™å…¥ P = softmax(S)
    HBM->>SRAM: è¯»å› P åš PV
    SRAM->>HBM: å†™å…¥ O = PV
    Note over SRAM,HBM: æ€» HBM è®¿é—®: O(NÂ²) æ¬¡è¯»å†™ â†’ ç“¶é¢ˆ!
```

A100 GPU çš„ SRAMï¼ˆ192KB/SMï¼‰é€Ÿåº¦çº¦ 19 TB/sï¼Œè€Œ HBMï¼ˆ80GBï¼‰ä»… 2 TB/sï¼Œ**å·®è·è¿‘ 10 å€**ã€‚æ ‡å‡† Attention å®Œå…¨æ²¡æœ‰åˆ©ç”¨è¿™ä¸ªå±‚æ¬¡ç»“æ„ã€‚

## 2. FlashAttention v1ï¼šIO-Aware Tiling

> æ¥æºï¼šDao et al., "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness", arXiv:2205.14135, Sec. 3

### æ ¸å¿ƒæ€æƒ³

**ä¸æŠŠ NÃ—N çš„ä¸­é—´çŸ©é˜µå†™å…¥ HBM**ï¼Œè€Œæ˜¯åˆ†å—ï¼ˆtilingï¼‰åœ¨ SRAM ä¸­å®Œæˆæ‰€æœ‰è®¡ç®—ï¼Œä¸€æ¬¡æ€§è¾“å‡ºæœ€ç»ˆç»“æœã€‚

### Tiling ç­–ç•¥

å°† Qã€Kã€V åˆ‡æˆå—ï¼Œæ¯å—å¤§å°ä¸º `B_r Ã— d` æˆ– `B_c Ã— d`ï¼š

```python
# FlashAttention v1 ä¼ªä»£ç 
def flash_attention_v1(Q, K, V, B_r, B_c):
    N, d = Q.shape
    O = zeros(N, d)           # è¾“å‡º
    l = zeros(N)              # softmax åˆ†æ¯ï¼ˆlog-sum-expï¼‰
    m = full(N, -inf)         # è¡Œæœ€å¤§å€¼

    # å¤–å±‚å¾ªç¯éå† K, V çš„å—
    for j in range(0, N, B_c):
        K_j = K[j:j+B_c]     # ä» HBM åŠ è½½
        V_j = V[j:j+B_c]

        # å†…å±‚å¾ªç¯éå† Q çš„å—
        for i in range(0, N, B_r):
            Q_i = Q[i:i+B_r]  # ä» HBM åŠ è½½
            O_i = O[i:i+B_r]
            l_i = l[i:i+B_r]
            m_i = m[i:i+B_r]

            # --- ä»¥ä¸‹å…¨åœ¨ SRAM ä¸­å®Œæˆ ---
            S_ij = Q_i @ K_j.T / sqrt(d)   # B_r Ã— B_cï¼Œåœ¨ SRAM ä¸­
            m_new = max(m_i, S_ij.max(dim=-1))
            P_ij = exp(S_ij - m_new[:, None])
            l_new = exp(m_i - m_new) * l_i + P_ij.sum(dim=-1)
            O_i = (exp(m_i - m_new)[:, None] * O_i + P_ij @ V_j) / l_new[:, None]
            # --- å†™å› HBM ---
            O[i:i+B_r] = O_i
            l[i:i+B_r] = l_new
            m[i:i+B_r] = m_new

    return O
```

### Online Softmax æŠ€å·§

å…³é”®åˆ›æ–°ï¼š**ä¸éœ€è¦çœ‹åˆ°æ•´è¡Œå°±èƒ½å¢é‡è®¡ç®— softmax**ã€‚é€šè¿‡ç»´æŠ¤ running max `m` å’Œ running sum `l`ï¼Œæ¯å¤„ç†ä¸€ä¸ª K å—å°±æ›´æ–°ï¼š

```
m_new = max(m_old, max(S_block))
l_new = e^(m_old - m_new) * l_old + Î£ e^(S_block - m_new)
O_new = (e^(m_old - m_new) * l_old * O_old + P_block @ V_block) / l_new
```

### åå‘ä¼ æ’­ï¼šé‡è®¡ç®—ç­–ç•¥

å‰å‘åªä¿å­˜ Oã€lã€mï¼ˆä¸ä¿å­˜ NÃ—N çš„ S çŸ©é˜µï¼‰ï¼Œåå‘æ—¶ç”¨ Qã€Kã€V é‡æ–°ç®—å‡º S å—â€”â€”ç”¨è®¡ç®—æ¢å†…å­˜ï¼š

- å†…å­˜ï¼šO(N) vs æ ‡å‡†çš„ O(NÂ²)
- è®¡ç®—ï¼šå¤šä¸€æ¬¡å‰å‘è®¡ç®—ï¼Œä½†å› ä¸ºå‡å°‘äº† HBM è¯»å†™ï¼Œ**å®é™…æ›´å¿«**

### v1 æ€§èƒ½

- **è®­ç»ƒåŠ é€Ÿ**ï¼šæ¯” PyTorch æ ‡å‡†å®ç°å¿« **2-4x**
- **å†…å­˜èŠ‚çœ**ï¼šä» O(NÂ²) é™åˆ° O(N)ï¼Œæ”¯æŒ 16K+ åºåˆ—é•¿åº¦
- **ç²¾ç¡®è®¡ç®—**ï¼šä¸æ˜¯è¿‘ä¼¼ï¼Œç»“æœä¸æ ‡å‡† Attention æ•°å€¼ä¸€è‡´

## 3. FlashAttention v2ï¼šå·¥ç¨‹ä¼˜åŒ–

> æ¥æºï¼šDao, "FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning", arXiv:2307.08691

v2 (2023.07) ä¸æ”¹å˜ç®—æ³•æœ¬è´¨ï¼Œä¸“æ³¨äº GPU åˆ©ç”¨ç‡ä¼˜åŒ–ï¼š

### ä¸‰å¤§æ”¹è¿›

#### (1) å¾ªç¯é¡ºåºåè½¬

```
v1: å¤–å±‚éå† K/V å—ï¼Œå†…å±‚éå† Q å—
     â†’ æ¯ä¸ª Q å—éœ€è¦åå¤è¯»å†™ O ç´¯åŠ å™¨

v2: å¤–å±‚éå† Q å—ï¼Œå†…å±‚éå† K/V å—
     â†’ æ¯ä¸ª Q å—çš„ O åªéœ€ä¸€æ¬¡æœ€ç»ˆå†™å› HBM

â”Œâ”€â”€â”€ v1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€ v2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ for kv_block:             â”‚  â”‚ for q_block:             â”‚
â”‚   for q_block:            â”‚  â”‚   load O_i once          â”‚
â”‚     load/store O_i â†HBM  â”‚  â”‚   for kv_block:          â”‚
â”‚                           â”‚  â”‚     update O_i in SRAM   â”‚
â”‚ HBM writes: O(T_k * N)   â”‚  â”‚   store O_i once         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ HBM writes: O(N)         â”‚
                               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### (2) å‡å°‘éçŸ©é˜µä¹˜ FLOPs

v1 åœ¨ online softmax ä¸­æœ‰å¤§é‡æ ‡é‡è¿ç®—ï¼ˆrescalingï¼‰ã€‚v2 å°†å…¶æœ€å°åŒ–ï¼Œè®© Tensor Core åˆ©ç”¨ç‡ä» ~25% æå‡åˆ° ~70%ã€‚

#### (3) Warp é—´å¹¶è¡Œ

v1 ä¸­ 4 ä¸ª warp åˆ†åˆ«å¤„ç† Q çš„ä¸åŒéƒ¨åˆ†ï¼Œéœ€è¦é€šä¿¡åŒæ­¥ã€‚v2 è®© 4 ä¸ª warp åˆ†å¤„ç† K/V çš„ä¸åŒå—ï¼Œæœ€å reduceâ€”â€”å‡å°‘ shared memory è¯»å†™å’ŒåŒæ­¥ã€‚

### v2 æ€§èƒ½æ•°æ®

| æŒ‡æ ‡ | v1 | v2 | ç†è®ºå³°å€¼ |
|------|-----|-----|---------|
| A100 TFLOPS åˆ©ç”¨ç‡ | 25-40% | 50-73% | 312 TFLOPS |
| å‰å‘é€Ÿåº¦ (seq=2K) | 1.0x | 2.0x | â€” |
| ç«¯åˆ°ç«¯è®­ç»ƒ | 1.0x | 1.3x | â€” |

## 4. FlashAttention v3ï¼šHopper æ¶æ„æ·±åº¦ä¼˜åŒ–

> æ¥æºï¼šShah et al., "FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision", arXiv:2407.08608

v3 (2024.07) é’ˆå¯¹ NVIDIA Hopper (H100/H200) æ¶æ„çš„ä¸‰å¤§æ–°ç‰¹æ€§ï¼š

### (1) å¼‚æ­¥åŒ– (Warp Specialization + TMA)

H100 å¼•å…¥äº† **TMA (Tensor Memory Accelerator)**ï¼Œå¯ä»¥å¼‚æ­¥æ¬è¿æ•°æ®ã€‚v3 åˆ©ç”¨ warp specialization å°† warp åˆ†ä¸ºä¸¤ç»„ï¼š

```
                    â”Œâ”€â”€ Producer Warps â”€â”€â”
                    â”‚  TMA å¼‚æ­¥åŠ è½½      â”‚
                    â”‚  K, V â†’ SRAM       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ (æ•°æ®å°±ç»ª)
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Consumer Warps    â”‚
                    â”‚  WGMMA è®¡ç®—       â”‚
                    â”‚  Q @ K^T, P @ V   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    
  æ•°æ®æ¬è¿ä¸è®¡ç®—å®Œå…¨é‡å  â†’ éšè—å†…å­˜å»¶è¿Ÿ
```

### (2) WGMMA æŒ‡ä»¤

åˆ©ç”¨ H100 çš„ **Warpgroup MMA** æŒ‡ä»¤ï¼Œç›´æ¥ä» shared memory å‘èµ·çŸ©é˜µä¹˜ï¼Œé¿å… register ä¸­è½¬ï¼š

```
v2 (Ampere): æ•°æ®è·¯å¾„ä¸º SMEM â†’ Registers â†’ Tensor Core
v3 (Hopper): æ•°æ®è·¯å¾„ä¸º SMEM â†’ Tensor Core (WGMMA)
                                 â†‘ å‡å°‘ä¸€è·³
```

### (3) FP8 ä½ç²¾åº¦æ”¯æŒ

v3 åŸç”Ÿæ”¯æŒ FP8 (E4M3/E5M2) ç²¾åº¦ï¼Œthroughput ç¿»å€ï¼š

- **éè¿è´¯å¤„ç† (Incoherent Processing)**ï¼šå¯¹ Q å’Œ K æ–½åŠ éšæœºæ­£äº¤å˜æ¢ï¼Œå‡å°‘é‡åŒ–è¯¯å·®
- Block-wise quantizationï¼šæ¯ä¸ª tile ç‹¬ç«‹é‡åŒ–ç¼©æ”¾

### v3 æ€§èƒ½æ•°æ®

| æŒ‡æ ‡ | v2 (A100) | v3 (H100) | æå‡ |
|------|-----------|-----------|------|
| FP16 TFLOPS | 230 | 740 | 3.2x |
| FP8 TFLOPS | â€” | 1200+ | â€” |
| FP16 åˆ©ç”¨ç‡ | 62% | 75% | â€” |

## 5. v1 â†’ v2 â†’ v3 æ¼”è¿›æ€»ç»“

```mermaid
flowchart TD
    A["FlashAttention v1 (2022.06)\nIO-Aware Tiling + Online Softmax\nç®—æ³•åˆ›æ–°: O(NÂ²) â†’ O(N) å†…å­˜"] -->|"å·¥ç¨‹ä¼˜åŒ–"| B["FlashAttention v2 (2023.07)\nå¾ªç¯åè½¬ + å‡å°‘é matmul FLOPs + Warp å¹¶è¡Œ\nGPU åˆ©ç”¨ç‡: 25% â†’ 73%"]
    B -->|"ç¡¬ä»¶é€‚é…"| C["FlashAttention v3 (2024.07)\nWarp Specialization + TMA å¼‚æ­¥ + WGMMA + FP8\nå……åˆ†åˆ©ç”¨ Hopper æ¶æ„æ–°ç‰¹æ€§"]
```

## 6. FlashAttention vs PagedAttention

ä¸¤è€…è§£å†³çš„æ˜¯ **ä¸åŒå±‚é¢** çš„é—®é¢˜ï¼Œäº’ä¸ºè¡¥å……ï¼š

| ç»´åº¦ | FlashAttention | PagedAttention |
|------|---------------|----------------|
| **ç›®æ ‡** | åŠ é€Ÿ Attention è®¡ç®— | ä¼˜åŒ– KV Cache å†…å­˜ç®¡ç† |
| **æ ¸å¿ƒæ€æƒ³** | IO-aware tilingï¼Œå‡å°‘ HBM è¯»å†™ | è™šæ‹Ÿå†…å­˜åˆ†é¡µï¼Œå‡å°‘ç¢ç‰‡ |
| **è§£å†³é—®é¢˜** | è®¡ç®—æ…¢ã€å†…å­˜å ç”¨å¤§ | æ˜¾å­˜æµªè´¹ã€ç¢ç‰‡åŒ– |
| **ä½œç”¨é˜¶æ®µ** | è®­ç»ƒ + æ¨ç† | ä»…æ¨ç† |
| **ååŒ** | FlashAttention è®¡ç®— Attention kernelï¼ŒPagedAttention ç®¡ç† KV å­˜å‚¨ |

åœ¨ [[AI/3-LLM/Inference/vLLM|vLLM]] ä¸­ï¼Œä¸¤è€…ååŒå·¥ä½œï¼šPagedAttention ç®¡ç† KV Cache çš„ç‰©ç†å†…å­˜åˆ†é¡µï¼ŒFlashAttention è´Ÿè´£é«˜æ•ˆè®¡ç®— Attention å¾—åˆ†ã€‚

## 7. å®é™…ä½¿ç”¨

### PyTorch SDPA (æ¨è)

```python
import torch
import torch.nn.functional as F

# PyTorch 2.2+ è‡ªåŠ¨é€‰æ‹© FlashAttention v2
with torch.backends.cuda.sdp_kernel(
    enable_flash=True,        # FlashAttention
    enable_math=False,        # å…³é—­æ ‡å‡†å®ç°
    enable_mem_efficient=False # å…³é—­ xformers
):
    output = F.scaled_dot_product_attention(
        query, key, value,
        attn_mask=None,
        dropout_p=0.0,
        is_causal=True  # causal maskï¼Œé€‚ç”¨äº decoder
    )
```

### Hugging Face Transformers

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-3.1-8B-Instruct",
    attn_implementation="flash_attention_2",  # æ˜¾å¼æŒ‡å®š
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
```

### æ‰‹åŠ¨å®‰è£…

```bash
# éœ€è¦ CUDA 11.8+, PyTorch 2.0+
pip install flash-attn --no-build-isolation

# éªŒè¯
python -c "import flash_attn; print(flash_attn.__version__)"
```

## 8. ä¸å…¶ä»–ä¼˜åŒ–æŠ€æœ¯çš„å…³ç³»

- **[[AI/3-LLM/Inference/KV Cache|KV Cache ä¼˜åŒ–]]**ï¼šFlashAttention é™ä½è®¡ç®—å¼€é”€ï¼ŒKV Cache å‡å°‘é‡å¤è®¡ç®—
- **[[AI/3-LLM/Inference/é‡åŒ–ç»¼è¿°|é‡åŒ–]]**ï¼šv3 çš„ FP8 æ”¯æŒä¸é‡åŒ–äº’è¡¥ï¼Œè¿›ä¸€æ­¥é™ä½æ˜¾å­˜
- **[[AI/3-LLM/Architecture/GQA-MQA|GQA/MQA]]**ï¼šå‡å°‘ KV head æ•°é‡ â†’ KV Cache æ›´å° â†’ FlashAttention æ¯å—å¤„ç†æ›´é«˜æ•ˆ
- **[[AI/3-LLM/Inference/æ¨ç†ä¼˜åŒ–|æ¨ç†ä¼˜åŒ–]]**ï¼šFlashAttention æ˜¯æ¨ç†ä¼˜åŒ– stack ä¸­ Attention å±‚çš„æ ¸å¿ƒç»„ä»¶
- **[[AI/3-LLM/Inference/Speculative Decoding|Speculative Decoding]]**ï¼šæ­£äº¤ä¼˜åŒ–ï¼ŒFlashAttention åŠ é€Ÿå•æ¬¡ Attentionï¼ŒSD å‡å°‘è§£ç æ­¥æ•°

## é¢è¯•å¸¸è§é—®é¢˜

### Q1: FlashAttention ä¸ºä»€ä¹ˆèƒ½åŠ é€Ÿï¼Ÿå®ƒæ˜¯è¿‘ä¼¼è®¡ç®—å—ï¼Ÿ

**ä¸æ˜¯è¿‘ä¼¼**ï¼Œæ˜¯ç²¾ç¡®è®¡ç®—ã€‚åŠ é€Ÿçš„æ ¸å¿ƒåœ¨äº **IO-aware**ï¼šé€šè¿‡ tiling å°† Qã€Kã€V åˆ†å—åŠ è½½åˆ° SRAMï¼ˆå¿« 10xï¼‰ï¼Œåœ¨ SRAM ä¸­å®Œæˆæ‰€æœ‰ Attention è®¡ç®—ï¼ˆåŒ…æ‹¬ softmaxï¼‰ï¼Œé¿å…å°† O(NÂ²) çš„ä¸­é—´çŸ©é˜µå†™å…¥ HBMã€‚æœ¬è´¨æ˜¯å‡å°‘ HBM è¯»å†™é‡ï¼Œä» O(NÂ² + Nd) é™åˆ° O(NÂ²d / M)ï¼ˆM ä¸º SRAM å¤§å°ï¼‰ï¼Œåœ¨å…¸å‹é…ç½®ä¸‹æ¥è¿‘æœ€ä¼˜ã€‚

### Q2: Online Softmax æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ

æ ‡å‡† softmax éœ€è¦å…ˆçœ‹åˆ°æ•´è¡Œçš„æ‰€æœ‰å€¼æ‰èƒ½è®¡ç®— max å’Œ sumã€‚ä½† tiling ç­–ç•¥ä¸‹ï¼Œæ¯æ¬¡åªçœ‹åˆ°ä¸€éƒ¨åˆ† K å—ï¼Œæ‰€ä»¥éœ€è¦ **å¢é‡æ›´æ–°**ï¼šç»´æŠ¤ running max `m` å’Œ running sum `l`ï¼Œæ¯å¤„ç†ä¸€ä¸ªæ–° K å—æ—¶ rescale ä¹‹å‰çš„ç»“æœã€‚æ•°å­¦ä¸Šç­‰ä»·äºæ ‡å‡† softmaxï¼Œä½†å…è®¸åˆ†å—æµå¼è®¡ç®—ã€‚

### Q3: v2 ç›¸æ¯” v1 çš„æ ¸å¿ƒæ”¹è¿›æ˜¯ä»€ä¹ˆï¼Ÿ

ä¸‰ç‚¹ï¼š(1) **åè½¬å¾ªç¯é¡ºåº**ï¼Œå¤–å±‚éå† Qã€å†…å±‚éå† K/Vï¼Œä½¿æ¯ä¸ª Q å—çš„ Output åªéœ€ä¸€æ¬¡ HBM å†™å›ï¼›(2) **å‡å°‘é matmul FLOPs**ï¼Œæœ€å¤§åŒ– Tensor Core åˆ©ç”¨ç‡ï¼›(3) **æ”¹è¿› warp å¹¶è¡Œç­–ç•¥**ï¼Œå‡å°‘ shared memory åŒæ­¥ã€‚ç»¼åˆæ•ˆæœæ˜¯ GPU è®¡ç®—åˆ©ç”¨ç‡ä» 25% æå‡åˆ° 73%ã€‚

### Q4: FlashAttention å’Œ PagedAttention æ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿèƒ½ä¸€èµ·ç”¨å—ï¼Ÿ

**å®Œå…¨äº’è¡¥**ï¼šFlashAttention ä¼˜åŒ– Attention **è®¡ç®—**ï¼ˆå‡å°‘ HBM IOï¼‰ï¼ŒPagedAttention ä¼˜åŒ– KV Cache **å†…å­˜ç®¡ç†**ï¼ˆåˆ†é¡µå‡å°‘ç¢ç‰‡ï¼‰ã€‚åœ¨ vLLM ä¸­äºŒè€…ååŒï¼šPagedAttention è´Ÿè´£å°† KV Cache æŒ‰é¡µå­˜å‚¨åœ¨éè¿ç»­ç‰©ç†å†…å­˜ä¸­ï¼ŒFlashAttentionï¼ˆæˆ–å…¶å˜ä½“ FlashInferï¼‰è´Ÿè´£é«˜æ•ˆè®¡ç®—æ³¨æ„åŠ›ã€‚

### Q5: v3 å¦‚ä½•åˆ©ç”¨ H100 çš„æ–°ç‰¹æ€§ï¼Ÿ

ä¸‰å¤§ç¡¬ä»¶ç‰¹æ€§åˆ©ç”¨ï¼š(1) **TMA** (Tensor Memory Accelerator) å®ç°å¼‚æ­¥æ•°æ®æ¬è¿ï¼Œç”¨ warp specialization å°† producerï¼ˆæ¬æ•°æ®ï¼‰å’Œ consumerï¼ˆç®—çŸ©é˜µä¹˜ï¼‰åˆ†å¼€ï¼Œè®¡ç®—ä¸æ¬è¿é‡å ï¼›(2) **WGMMA** æŒ‡ä»¤è®©æ•°æ®ç›´æ¥ä» shared memory è¿›å…¥ Tensor Coreï¼Œè·³è¿‡ register ä¸­è½¬ï¼›(3) **FP8** åŸç”Ÿæ”¯æŒï¼Œé…åˆéè¿è´¯å¤„ç†å‡å°‘é‡åŒ–è¯¯å·®ï¼Œååå†ç¿»å€ã€‚

---

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **è®­ç»ƒåŠ é€Ÿ**ï¼šPyTorch 2.2+ çš„ `F.scaled_dot_product_attention` è‡ªåŠ¨è°ƒç”¨ FlashAttention v2ï¼Œé›¶ä»£ç æ”¹åŠ¨è·å¾— 2-4x è®­ç»ƒåŠ é€Ÿ
- **æ¨ç†éƒ¨ç½²**ï¼šHuggingFace Transformers æŒ‡å®š `attn_implementation="flash_attention_2"` å³å¯å¯ç”¨
- **é•¿ä¸Šä¸‹æ–‡è®­ç»ƒ**ï¼šFlashAttention å°†å†…å­˜ä» $O(N^2)$ é™åˆ° $O(N)$ï¼Œä½¿ 32K-128K åºåˆ—é•¿åº¦è®­ç»ƒæˆä¸ºå¯èƒ½

### å·¥ç¨‹å®ç°è¦ç‚¹
- å®‰è£…éœ€è¦ CUDA 11.8+ï¼š`pip install flash-attn --no-build-isolation`
- v3 éœ€è¦ H100/H200 ç¡¬ä»¶ï¼ˆHopper æ¶æ„ï¼‰ï¼ŒA100 æœ€é«˜åªèƒ½ç”¨ v2
- Causal mask ä½¿ç”¨ `is_causal=True` å‚æ•°ï¼Œæ€§èƒ½ä¼˜äºæ‰‹åŠ¨æ„å»º mask çŸ©é˜µ
- FlashAttention çš„åå‘ä¼ æ’­ç”¨é‡è®¡ç®—ç­–ç•¥ï¼ˆä¸ä¿å­˜ $N \times N$ çŸ©é˜µï¼‰ï¼Œé¢å¤–è®¡ç®—é‡ç”±å‡å°‘çš„ IO è¡¥å¿

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: FlashAttention æ˜¯è¿‘ä¼¼è®¡ç®—å—ï¼Ÿ
  A: ä¸æ˜¯ã€‚Online Softmax æ•°å­¦ä¸Šç²¾ç¡®ç­‰ä»·æ ‡å‡† softmaxï¼Œåªæ˜¯æ”¹å˜äº†è®¡ç®—é¡ºåº
- Q: ä¸ºä»€ä¹ˆé‡è®¡ç®—åè€Œæ›´å¿«ï¼Ÿ
  A: å› ä¸ºå‡å°‘äº† HBM è¯»å†™ï¼ˆIO-bound â†’ compute-boundï¼‰ï¼Œçœä¸‹çš„ IO æ—¶é—´è¿œè¶…å¤šä¸€æ¬¡å‰å‘è®¡ç®—çš„æ—¶é—´

---

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- **FlashAttention æ˜¯"å…è´¹"çš„æ€§èƒ½æå‡**â€”â€”ä¸æ”¹å˜æ¨¡å‹è´¨é‡ï¼Œçº¯å·¥ç¨‹ä¼˜åŒ–ã€‚ä»»ä½• Transformer è®­ç»ƒ/æ¨ç†éƒ½åº”è¯¥é»˜è®¤å¼€å¯
- **IO-aware çš„è®¾è®¡å“²å­¦å¯ä»¥è¿ç§»**ï¼šä¸åªæ˜¯ Attentionï¼Œä»»ä½•æ¶‰åŠå¤§çŸ©é˜µä¸­é—´ç»“æœçš„è®¡ç®—éƒ½å¯ä»¥ç”¨ tiling + é‡è®¡ç®—çš„æ€è·¯ä¼˜åŒ–

### æœªè§£é—®é¢˜ä¸å±€é™
- FlashAttention ç›®å‰ä¸»è¦ä¼˜åŒ– self-attentionï¼Œcross-attention å’Œç‰¹æ®Š attention patternï¼ˆå¦‚ sparse attentionï¼‰çš„æ”¯æŒä¸å®Œæ•´
- v3 çš„ FP8 æ”¯æŒä¾èµ– Hopper æ¶æ„ï¼ŒA100 ç”¨æˆ·æ— æ³•å—ç›Š
- Online Softmax çš„å¢é‡æ›´æ–°å¼•å…¥äº†æµ®ç‚¹ç´¯ç§¯è¯¯å·®ï¼Œæé•¿åºåˆ—ï¼ˆ>100Kï¼‰å¯èƒ½æœ‰æ•°å€¼ç²¾åº¦é—®é¢˜ï¼ˆå®è·µä¸­é€šå¸¸å¯å¿½ç•¥ï¼‰

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- å¦‚æœæŠŠ FlashAttention çš„ tiling ç­–ç•¥å’Œ [[AI/3-LLM/Architecture/GQA-MQA|GQA]] çš„ KV head å…±äº«ç»“åˆï¼Œå¯ä»¥åœ¨ kernel å±‚é¢åšæ›´æ·±åº¦çš„èåˆä¼˜åŒ–ï¼ˆå‡å°‘ KV broadcast çš„å¼€é”€ï¼‰
- v4 å¯èƒ½çš„æ–¹å‘ï¼šé’ˆå¯¹ Blackwell (B200) æ¶æ„çš„è¿›ä¸€æ­¥é€‚é…ï¼Œä»¥åŠåŸç”Ÿæ”¯æŒ MLA çš„è§£å‹ç¼©è®¡ç®—

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [FlashAttention v1](https://arxiv.org/abs/2205.14135) â€” IO-aware tiling çš„åŸåˆ›è®ºæ–‡ï¼ŒSec. 3 çš„ç®—æ³•æè¿°æä¸ºæ¸…æ™°
- [FlashAttention v2](https://arxiv.org/abs/2307.08691) â€” å·¥ç¨‹ä¼˜åŒ–ç»†èŠ‚ï¼Œå¾ªç¯åè½¬å’Œ warp å¹¶è¡Œç­–ç•¥
- [FlashAttention v3](https://arxiv.org/abs/2407.08608) â€” Hopper æ¶æ„æ·±åº¦é€‚é…ï¼ŒTMA/WGMMA/FP8

### æ·±åº¦è§£è¯»
- [Tri Dao's Blog: FlashAttention](https://tridao.me/blog/) â€” ä½œè€…äº²è‡ªè§£è¯» FlashAttention çš„è®¾è®¡åŠ¨æœº â­â­â­â­â­
- [ELI5: FlashAttention](https://gordicaleksa.medium.com/eli5-flash-attention-5c44017022ad) â€” é€šä¿—æ˜“æ‡‚çš„å›¾è§£ â­â­â­â­

### å®è·µèµ„æº
- [FlashAttention GitHub](https://github.com/Dao-AILab/flash-attention) â€” å®˜æ–¹å®ç°ï¼Œæ”¯æŒ v1/v2/v3
- [PyTorch SDPA æ–‡æ¡£](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) â€” PyTorch é›†æˆçš„ FlashAttention æ¥å£

---

## See Also

> ğŸ”— See also: [[AI/3-LLM/Architecture/Attention å˜ä½“ç»¼è¿°|Attention å˜ä½“ç»¼è¿°]] â€” FlashAttention åŠ é€Ÿçš„è®¡ç®—å±‚ä¸ Attention å˜ä½“çš„æ¶æ„å±‚äº’è¡¥
> ğŸ”— See also: [[AI/3-LLM/Architecture/GQA-MQA|GQA/MQA]] â€” GQA å‡å°‘ KV æ•°é‡ï¼ŒFlashAttention å‡å°‘ IOï¼ŒäºŒè€…ååŒ
> ğŸ”— See also: [[AI/3-LLM/Inference/KV Cache|KV Cache]] â€” FlashAttentionï¼ˆè®¡ç®—åŠ é€Ÿï¼‰ä¸ PagedAttentionï¼ˆå†…å­˜ç®¡ç†ï¼‰åœ¨ vLLM ä¸­ååŒå·¥ä½œ
> ğŸ”— See also: [[AI/3-LLM/Inference/é‡åŒ–ç»¼è¿°|é‡åŒ–ç»¼è¿°]] â€” v3 çš„ FP8 æ”¯æŒä¸é‡åŒ–æŠ€æœ¯çš„äº¤å‰ç‚¹
> ğŸ”— See also: Transformer å…¨æ™¯ â€” FlashAttention æ˜¯ Transformer æ¨ç†ä¼˜åŒ– stack çš„æ ¸å¿ƒç»„ä»¶
