---
title: 长上下文处理：从位置编码外推到百万 Token 序列
brief: 长上下文处理是 LLM 实用性的关键技术。核心挑战包括 RoPE 外推（YaRN/LongRoPE 通过非均匀插值突破训练长度限制）、分布式长序列训练（Ring Attention 将内存从 O(N²) 降到 O(N²/P)）、以及 Lost in the Middle 问题（模型对中间位置信息检索能力显著下降）。实践中需权衡上下文长度、推理速度和理解质量。
type: survey
domain: ai/llm/architecture
created: 2026-02-14
updated: 2026-02-22
tags:
  - ai/llm/architecture
  - ai/llm/long-context
  - type/survey
status: complete
sources:
  - "Peng et al. *YaRN: Efficient Context Window Extension of Large Language Models* arXiv:2309.00071"
  - "Ding et al. *LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens* arXiv:2402.13753"
  - Liu et al. *Ring Attention with Blockwise Transformers for Near-Infinite Context* arXiv:2310.01889
  - "Liu et al. *Lost in the Middle: How Language Models Use Long Contexts* arXiv:2307.03172"
  - "Su et al. *RoFormer: Enhanced Transformer with Rotary Position Embedding* arXiv:2104.09864"
related:
  - "[[Transformer 通识|Transformer 通识]]"
  - "[[Attention 变体综述|Attention 详解]]"
  - "[[Mamba-SSM|Mamba-SSM]]"
  - "[[Qwen|Qwen]]"
---

> [!info] 另有面试版
> Foundations 精简版：[[AI/1-Foundations/Architecture/长上下文处理]]

# 长上下文处理技术

大语言模型的上下文处理能力是决定其实用性的关键技术之一。随着应用场景对长文档理解、代码分析、多轮对话的需求增长，如何有效扩展和处理长上下文序列成为了LLM架构设计的核心挑战。

## 位置编码外推技术

### RoPE外推方法

旋转位置编码（RoPE, arXiv:2104.09864）通过旋转矩阵将位置信息融入注意力计算。对于位置 $m$ 的 query $q$ 和位置 $n$ 的 key $k$，RoPE 的注意力分数为：

$$\text{Attn}(q_m, k_n) = \text{Re}\left[\sum_{i=0}^{d/2-1} q_{[2i:2i+1]} k_{[2i:2i+1]}^* e^{i(m-n)\theta_i}\right], \quad \theta_i = 10000^{-2i/d}$$

但在训练长度之外的外推能力有限（高频维度的 $\theta_i$ 在长距离上震荡剧烈）。几种主要的 RoPE 外推方法：

**NTK-aware Scaling**
- 基于神经切线核理论，通过调整RoPE的base频率来适应更长序列
- 核心思想：`base = base * (α^(d/d-2))`，其中α为缩放因子
- 优势：计算简单，对短序列影响小
- 缺点：在极长序列上仍可能出现性能退化

**YaRN (Yet another RoPE extensioN)**（arXiv:2309.00071）
- 结合温度缩放和注意力缩放的混合方法
- 对不同频率成分使用不同的缩放策略：低频维度线性插值，高频维度保持不变
- 引入注意力温度调节机制 $t = 0.1 \ln(s) + 1$（$s$ 为缩放因子），缓解 attention entropy 的变化
- 在保持短序列性能的同时，显著提升长序列表现

**Dynamic NTK**
- 动态调整NTK缩放参数，根据序列长度自适应
- 避免固定缩放比例的局限性
- 实现更平滑的长度外推

**Code LLaMA方法**
- 采用分组旋转和频率插值
- 对不同维度的位置编码使用不同的处理策略
- 专门针对代码等结构化长文本优化

相关：[[Transformer 位置编码详解|Transformer 位置编码详解]], [[Transformer 位置编码详解|RoPE详解]]

## 分布式长序列训练

### Ring Attention

Ring Attention（arXiv:2310.01889）是专为超长序列设计的分布式注意力计算方法：

- **核心机制**：将序列分割到多个设备，通过 ring 通信模式计算注意力
- **内存优势**：$O(N/P)$ 内存复杂度，其中 $P$ 为设备数
- **通信效率**：每个设备只需要与相邻设备通信，降低网络开销
- **适用场景**：百万token级别的序列处理

### Star Attention

相比Ring Attention的环形拓扑，Star Attention采用星形通信：

- **中心节点**：负责聚合全局注意力信息
- **并行计算**：各worker并行计算局部attention
- **通信简化**：减少多跳通信的延迟
- **扩展性**：更适合异构集群环境

相关：[[分布式训练|分布式训练]], [[Attention 变体综述|Attention 详解]]

## 高效长文本注意力机制

### Landmark Attention

- **关键思想**：识别序列中的"地标"token，保持对关键信息的全局注意力
- **实现方式**：
  - 选择信息密度高的token作为landmark
  - 对landmark保持全attention，其余使用sliding window
  - 动态更新landmark集合
- **适用场景**：文档问答、长文本摘要

### LongRoPE

针对 RoPE 在超长序列上的局限性设计（arXiv:2402.13753）：

- **搜索优化**：通过非均匀插值找到最优的位置编码参数
- **渐进式扩展**：从短到长逐步适应训练
- **效率提升**：相比其他方法需要更少的调优数据

### LM-Infinite

- **流式处理**：支持理论上无限长的输入序列
- **注意力窗口**：维护固定大小的注意力窗口
- **状态压缩**：将历史信息压缩为紧凑表示
- **增量计算**：新输入只需增量更新，不重新计算全序列

## 上下文窗口扩展训练策略

### Continue Pre-Training

长上下文能力通常通过继续预训练获得：

1. **数据准备**：收集长文档数据（书籍、论文、代码库）
2. **长度递增**：从4K开始，逐步扩展到目标长度
3. **学习率调节**：使用较小的学习率避免遗忘
4. **位置编码调整**：配合RoPE扩展方法

### ABF方法 (Attention Bridge Function)

- **核心思想**：在训练过程中插入"注意力桥接函数"
- **实现细节**：
  - 在不同长度的训练阶段间插入桥接层
  - 帮助模型适应突变的序列长度
  - 保持训练稳定性
- **优势**：减少长度扩展时的性能损失

相关：预训练策略, 模型微调

## "Lost in the Middle"问题

### 问题描述

研究发现（Liu et al. *Lost in the Middle* arXiv:2307.03172），LLM 在处理长文档时存在"中间丢失"现象：
- 对文档开头和结尾的信息敏感度较高
- 对中间部分的信息检索能力显著下降
- 这种现象在问答、摘要等任务中尤为明显

### 影响因素

1. **位置偏好**：训练数据中重要信息多出现在开头结尾
2. **注意力衰减**：长序列中间位置的注意力权重趋向分散
3. **信息干扰**：无关信息对目标信息的遮蔽效应

### 缓解策略

- **检索增强**：将长文档分块，使用检索策略定位相关段落
- **位置增强**：在训练中平衡不同位置的信息重要性
- **注意力重分配**：设计专门的注意力机制强化中间位置

相关：[[RAG-2026-技术全景|检索增强生成]], [[Attention 变体综述|注意力机制优化]]

## 实际应用对比

### 128K-1M Token模型对比

**GPT-4 Turbo (128K)**
- 优势：在代码理解和复杂推理上表现优秀
- 局限：上下文长度相对较短
- 适用：代码分析、技术文档处理

**Claude 3 (200K)**
- 优势：在长文档理解和多轮对话中表现突出
- 特点：对"Lost in the Middle"问题处理较好
- 适用：文档分析、长篇创作

**Gemini 1.5 Pro (1M)**
- 优势：支持最长的上下文窗口
- 特点：多模态长序列处理能力
- 适用：视频分析、超长文档处理

### 性能权衡

- **内存消耗**：O(N²)的注意力复杂度使得长上下文成本高昂
- **推理速度**：长序列处理显著增加延迟
- **质量保证**：超长上下文不等于更好的理解能力

相关：[[模型评估|模型评估]], 性能优化

## 面试常见问题

### Q1: RoPE外推的核心原理是什么？如何选择合适的扩展方法？

**答案要点**：
- RoPE将位置信息编码为旋转矩阵，在训练长度外需要外推
- NTK方法简单但有限，YaRN在质量和效率间平衡较好
- 选择标准：目标长度、计算资源、对短序列性能的影响
- 实际应用中常结合多种方法

### Q2: Ring Attention相比传统attention有什么优势？适用场景是什么？

**答案要点**：
- 内存复杂度从O(N²)降到O(N²/P)，P为设备数
- 通信开销相对较低，适合超长序列分布式训练
- 主要用于百万token级别的序列，如长视频、大型代码库
- 需要考虑通信延迟和负载均衡

### Q3: 如何解决"Lost in the Middle"问题？

**答案要点**：
- 问题本质：训练数据偏差导致的位置偏好
- 技术方案：检索增强、注意力重分配、位置平衡训练
- 评估方法：针对不同位置设计测试用例
- 实际应用：结合检索和生成，降低对超长上下文的依赖

### Q4: 长上下文训练的主要挑战和解决方案？

**答案要点**：
- **内存挑战**：使用梯度检查点、模型并行
- **计算效率**：Flash Attention、稀疏attention
- **训练稳定性**：渐进式长度扩展、学习率调节
- **数据质量**：高质量长文档数据稀缺

### Q5: 在实际项目中如何选择合适的长上下文方案？

**答案要点**：
- 评估实际需求：是否真正需要超长上下文
- 成本考量：计算资源、延迟要求、精度需求
- 技术选择：基于现有模型扩展 vs 从头训练
- 替代方案：检索增强、文档分块、摘要等方法
- 性能监控：建立长文本任务的评估体系

## 📚 推荐阅读

### 原始论文
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864) — RoPE 原文，理解位置编码外推的起点
- [YaRN: Efficient Context Window Extension](https://arxiv.org/abs/2309.00071) — 最实用的 RoPE 外推方法，被 Qwen/LLaMA 广泛采用
- [LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens](https://arxiv.org/abs/2402.13753) — 非均匀插值，200 万 token 窗口
- [Ring Attention with Blockwise Transformers](https://arxiv.org/abs/2310.01889) — 超长序列分布式训练的基础
- [Lost in the Middle](https://arxiv.org/abs/2307.03172) — LLM 长上下文使用能力的经典分析

### 深度解读
- [Extending Context is Hard…but not Impossible](https://kaiokendev.github.io/context) — NTK-aware scaling 的原始博客，直觉清晰 ⭐⭐⭐⭐⭐
- [YaRN 技术解读 (知乎)](https://zhuanlan.zhihu.com/p/660073229) — 中文社区优质解读

### 实践资源
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) — 支持 YaRN/NTK 长上下文训练的微调框架
- [FlashAttention-2](https://github.com/Dao-AILab/flash-attention) — 长上下文高效注意力的工程基础

## 🔧 落地应用

### 直接可用场景
- **RAG 长文档处理**：128K 上下文窗口允许将整个文档一次性输入，减少分块带来的信息丢失
- **代码库分析**：大型代码库理解需要跨文件的长距离依赖，Ring Attention 使百万 token 级别的代码库分析成为可能
- **多轮对话**：长上下文使模型能保持更完整的对话历史，减少"遗忘"

### 工程实现要点
- **YaRN vs NTK-aware**：YaRN 在质量和效率间平衡最好，是生产环境首选；NTK-aware 简单但极长序列衰减明显
- **Continue Pre-Training 数据量**：从 4K 扩展到 128K 通常需要 1-5B tokens 的长文档数据，学习率 $\sim 2 \times 10^{-5}$
- **Lost in the Middle 缓解**：实践中将关键信息放在 prompt 开头或结尾，或用检索将相关段落前置

### 面试高频问法
- Q: 为什么不直接训练更长的上下文，而要用 RoPE 外推？
  A: 长序列训练的 $O(N^2)$ 注意力成本极高（128K 是 4K 的 $1024\times$），且长文档数据稀缺。外推方法允许在短序列上训练、长序列上推理，大幅降低训练成本。

## 💡 启发与思考

### So What？对老板意味着什么
- **长上下文不等于好理解**：Lost in the Middle 研究表明，即使支持 128K 上下文，模型对中间位置的利用率仍然很低。RAG 的"检索+生成"范式在很多场景下比纯长上下文更可靠
- **YaRN 是当前最务实的选择**：如果需要扩展上下文窗口，YaRN 的训练成本最低、效果最稳定，[[Qwen|Qwen]] 2.5 就采用了 YaRN

### 未解问题与局限
- 所有外推方法在极长序列（>256K）上都有不同程度的质量衰减，理论上的"无限"上下文尚未实现
- Ring Attention 的通信开销在异构集群中可能成为瓶颈
- Lost in the Middle 的根因是训练数据偏差还是注意力机制的固有限制？目前没有定论

### 脑暴：如果往下延伸
- [[Mamba-SSM|Mamba]] 的 $O(L)$ 复杂度天然适合长序列，结合 RoPE 外推思想，能否实现真正的百万 token 高效处理？
- 如果把 Lost in the Middle 的发现反向利用——刻意在训练时对中间位置做位置增强——是否能训练出对全位置同等敏感的模型？

> 🔗 See also: [[架构范式对比|架构范式对比]] — 不同架构处理长序列的能力对比