---
title: Attention å˜ä½“ç»¼è¿°ï¼šä» MHA åˆ° Linear Attention
brief: æ³¨æ„åŠ›æœºåˆ¶ä» MHA â†’ MQA â†’ GQA â†’ MLA â†’ Linear Attention çš„å®Œæ•´æ¼”è¿›ç»¼è¿°ã€‚æ ¸å¿ƒçŸ›ç›¾æ˜¯è¡¨è¾¾èƒ½åŠ›ä¸è®¡ç®—/æ˜¾å­˜æ•ˆç‡çš„å¹³è¡¡ã€‚2025 å¹´ GQA æˆä¸ºäº‹å®æ ‡å‡†ï¼ŒMLA ä»£è¡¨æ›´æ¿€è¿›å‹ç¼©æ–¹å‘ï¼Œæ··åˆæ¶æ„ï¼ˆsoftmax + linearï¼‰æ˜¯è¶…é•¿åºåˆ—çš„è¶‹åŠ¿ã€‚
date: 2026-02-13
updated: 2026-02-22
tags:
  - ai/llm/architecture
  - ai/attention
  - ai/llm/inference
  - type/survey
  - interview/hot
status: complete
sources:
  - Vaswani et al. Attention Is All You Need. arXiv:1706.03762
  - "Shazeer. Fast Transformer Decoding: One Write-Head is All You Need (MQA). arXiv:1911.02150"
  - "Ainslie et al. GQA: Training Generalized Multi-Query Attention from Multi-Head Checkpoints. arXiv:2305.13245"
  - "Dao et al. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. arXiv:2205.14135"
  - "Beltagy et al. Longformer: The Long-Document Transformer (Sliding Window). arXiv:2004.05150"
  - "Liu et al. DeepSeek-V2: A Strong, Economical, and Efficient MoE LM (MLA). arXiv:2405.04434"
related:
  - "[[AI/3-LLM/Architecture/GQA-MQA|GQA/MQA æ·±åº¦è§£æ]]"
  - "[[AI/3-LLM/Architecture/FlashAttention|FlashAttention æ·±åº¦è§£æ]]"
  - "[[AI/3-LLM/Architecture/Multi-Head Latent Attention|MLA è¯¦è§£]]"
  - "[[AI/3-LLM/Architecture/Mamba-SSM|Mamba/SSM]]"
  - "[[AI/3-LLM/Inference/KV Cache|KV Cache åŸç†ä¸ä¼˜åŒ–]]"
---

# Attention å˜ä½“ç»¼è¿°ï¼šä» MHA åˆ° Linear Attention

> æ³¨æ„åŠ›æœºåˆ¶çš„æ¼”è¿›æœ¬è´¨æ˜¯åœ¨**è¡¨è¾¾èƒ½åŠ›**ä¸**è®¡ç®—/æ˜¾å­˜æ•ˆç‡**ä¹‹é—´å¯»æ‰¾æœ€ä¼˜å¹³è¡¡ç‚¹

## 1. æ¼”è¿›è„‰ç»œæ€»è§ˆ

```mermaid
flowchart LR
    A["MHA (2017)\nh ä¸ª KV heads\nGPT-3"] -->|"æç«¯å‹ç¼©\n1ä¸ªKV head"| B["MQA (2019)\n1 ä¸ª KV head\nPaLM/Falcon"]
    A -->|"åˆ†ç»„æŠ˜ä¸­\nGç»„KV heads"| C["GQA (2023)\nG ç»„ KV heads\nLLaMA 2/3"]
    C -->|"ä½ç§©å‹ç¼©\nlatent KV"| D["MLA (2024)\nä½ç§©æ½œåœ¨ KV\nDeepSeek-V2/V3"]
    A -->|"çªç ´O(nÂ²)"| E["Linear Attention\nO(n) å¤æ‚åº¦\nMamba/RWKV"]
    D -.->|"2025æ··åˆè¶‹åŠ¿"| F["æ··åˆæ¶æ„\nSoftmax+Linear\nJamba/MiniMax"]
    E -.-> F
```

**æ ¸å¿ƒçŸ›ç›¾**ï¼šæ ‡å‡† Attention çš„ $O(n^2)$ å¤æ‚åº¦åœ¨åºåˆ—é•¿åº¦å¢é•¿æ—¶æˆä¸ºç“¶é¢ˆï¼›KV Cache åœ¨æ¨ç†æ—¶çº¿æ€§å¢é•¿ï¼Œé™åˆ¶ batch size å’Œååã€‚

## 2. Multi-Head Attention (MHA)

> æ¥æºï¼šVaswani et al., "Attention Is All You Need", arXiv:1706.03762, Sec. 3.2

### æ ‡å‡†å®šä¹‰

æ¯ä¸ª head æ‹¥æœ‰ç‹¬ç«‹çš„ Qã€Kã€V æŠ•å½±ï¼š

$$\text{Attention}_i = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i$$

```python
# MHA ä¼ªä»£ç 
Q = x @ W_q  # (B, N, h*d_k) â†’ æ‹†æˆ h ä¸ª (B, N, d_k)
K = x @ W_k  # åŒä¸Š
V = x @ W_v  # åŒä¸Š
# æ¯ä¸ª head ç‹¬ç«‹åš attentionï¼Œæœ€å concat + çº¿æ€§æŠ•å½±
```

### å¤æ‚åº¦

| æŒ‡æ ‡ | å¤æ‚åº¦ |
|------|--------|
| è®¡ç®— (FLOPs) | $O(n^2 \cdot d)$ |
| KV Cache (æ¯ token æ¯å±‚) | $2 \times h \times d_k$ |
| å‚æ•°é‡ | $4 \times d^2$ï¼ˆQ/K/V/O å„ä¸€ä¸ªï¼‰ |

**é—®é¢˜**ï¼šLLaMA-1 65B åœ¨ 4096 é•¿åº¦ä¸‹ KV Cache å°±éœ€è¦ ~10.5 GBã€‚

## 3. Multi-Query Attention (MQA)

> æ¥æºï¼šShazeer, "Fast Transformer Decoding: One Write-Head is All You Need", arXiv:1911.02150

### æ ¸å¿ƒæ”¹è¿›

**æ‰€æœ‰ Q head å…±äº«åŒä¸€ç»„ K å’Œ V**ï¼ŒKV Cache å‹ç¼©è‡³ $1/h$ï¼š

```python
class MultiQueryAttention(nn.Module):
    def __init__(self, d_model, n_heads):
        super().__init__()
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.W_q = nn.Linear(d_model, d_model)       # h ä¸ª Q head
        self.W_k = nn.Linear(d_model, self.d_k)       # 1 ä¸ª K head!
        self.W_v = nn.Linear(d_model, self.d_k)       # 1 ä¸ª V head!
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x):
        B, N, _ = x.shape
        Q = self.W_q(x).view(B, N, self.n_heads, self.d_k).transpose(1, 2)
        K = self.W_k(x).view(B, N, 1, self.d_k).transpose(1, 2)
        V = self.W_v(x).view(B, N, 1, self.d_k).transpose(1, 2)
        # å¹¿æ’­åˆ°æ‰€æœ‰ head
        K = K.expand(-1, self.n_heads, -1, -1)
        V = V.expand(-1, self.n_heads, -1, -1)
        scores = (Q @ K.transpose(-2, -1)) / (self.d_k ** 0.5)
        attn = torch.softmax(scores, dim=-1)
        out = (attn @ V).transpose(1, 2).contiguous().view(B, N, -1)
        return self.W_o(out)
```

**ä»£ä»·**ï¼šæ‰€æœ‰ head è¢«è¿«å…±äº«åŒä¸€ attention patternï¼Œè´¨é‡ä¸‹é™ 2-3%ã€‚

**ä»£è¡¨æ¨¡å‹**ï¼šPaLMã€Falconã€StarCoderã€‚

## 4. Grouped Query Attention (GQA)

> æ¥æºï¼šAinslie et al., "GQA: Training Generalized Multi-Query Attention from Multi-Head Checkpoints", arXiv:2305.13245

è¯¦è§ [[AI/3-LLM/Architecture/GQA-MQA|GQA/MQA æ·±åº¦è§£æ]]ã€‚

### æ ¸å¿ƒæ€æƒ³

å°† $h$ ä¸ª Q head åˆ†æˆ $G$ ç»„ï¼Œæ¯ç»„å…±äº«ä¸€å¥— KVã€‚GQA æ˜¯ MHA ($G=h$) å’Œ MQA ($G=1$) çš„æ³›åŒ–ï¼š

```
GQA (h=32, G=8):
  æ¯ 4 ä¸ª Q head å…±äº« 1 ä¸ª KV head
  KV Cache èŠ‚çœ = (32-8)/32 = 75%
  è´¨é‡ä¸‹é™ < 0.5%
```

**2025 å¹´äº‹å®æ ‡å‡†**ï¼šLLaMA 2/3ã€Mistralã€Qwen 2.5 ç³»åˆ—å…¨éƒ¨é‡‡ç”¨ GQA (G=8)ã€‚

## 5. Multi-head Latent Attention (MLA)

> æ¥æºï¼šLiu et al., "DeepSeek-V2: A Strong, Economical, and Efficient MoE Language Model", arXiv:2405.04434, Sec. 3.1

### DeepSeek-V2/V3 çš„æ ¸å¿ƒåˆ›æ–°

MLA ä¸æ˜¯ç®€å•å‡å°‘ KV head æ•°é‡ï¼Œè€Œæ˜¯é€šè¿‡**ä½ç§©å‹ç¼©**å°† KV æŠ•å°„åˆ°ä¸€ä¸ª latent spaceï¼š

```
ä¼ ç»Ÿ MHA:
  cache: [K_1, K_2, ..., K_h, V_1, V_2, ..., V_h]  â†’ 2*h*d_k per token

MLA:
  compress: c_KV = x @ W_DKV           # d_model â†’ d_c (d_c << h*d_k)
  cache:    c_KV                        # åªç¼“å­˜ d_c ç»´å‘é‡ï¼
  decompress: K = c_KV @ W_UK           # d_c â†’ h*d_k (on-the-fly)
              V = c_KV @ W_UV           # d_c â†’ h*d_k (on-the-fly)
```

### å…³é”®è®¾è®¡

```python
class MultiHeadLatentAttention(nn.Module):
    """MLA ç®€åŒ–ç‰ˆï¼ˆä¸å« Decoupled RoPEï¼‰"""
    def __init__(self, d_model, n_heads, d_compress):
        super().__init__()
        self.n_heads = n_heads
        self.d_k = d_model // n_heads
        self.d_compress = d_compress  # å‹ç¼©ç»´åº¦ï¼Œè¿œå°äº n_heads * d_k

        # ä¸‹å‹ç¼©çŸ©é˜µï¼ˆç”¨äºæ¨ç†æ—¶ç¼“å­˜ï¼‰
        self.W_DKV = nn.Linear(d_model, d_compress)
        # ä¸Šè§£å‹çŸ©é˜µï¼ˆæ¨ç†æ—¶æŒ‰éœ€è®¡ç®—ï¼‰
        self.W_UK = nn.Linear(d_compress, n_heads * self.d_k)
        self.W_UV = nn.Linear(d_compress, n_heads * self.d_k)
        # Q ä¹Ÿæœ‰ç±»ä¼¼å‹ç¼©
        self.W_DQ = nn.Linear(d_model, d_compress)
        self.W_UQ = nn.Linear(d_compress, n_heads * self.d_k)
        self.W_o = nn.Linear(d_model, d_model)

    def forward(self, x):
        B, N, _ = x.shape
        # å‹ç¼© KV â†’ latent vectorï¼ˆæ¨ç†æ—¶åªç¼“å­˜è¿™ä¸ªï¼ï¼‰
        c_kv = self.W_DKV(x)              # (B, N, d_c)
        K = self.W_UK(c_kv).view(B, N, self.n_heads, self.d_k).transpose(1, 2)
        V = self.W_UV(c_kv).view(B, N, self.n_heads, self.d_k).transpose(1, 2)
        # Q ä¹Ÿç»è¿‡ä½ç§©å‹ç¼©
        c_q = self.W_DQ(x)
        Q = self.W_UQ(c_q).view(B, N, self.n_heads, self.d_k).transpose(1, 2)
        # æ ‡å‡† Attention
        scores = (Q @ K.transpose(-2, -1)) / (self.d_k ** 0.5)
        attn = torch.softmax(scores, dim=-1)
        out = (attn @ V).transpose(1, 2).contiguous().view(B, N, -1)
        return self.W_o(out)
```

### Decoupled RoPE

MLA çš„ä½ç§©åˆ†è§£ä¸ [[AI/3-LLM/Architecture/Transformer ä½ç½®ç¼–ç |RoPE]] ä¸å…¼å®¹ï¼ˆRoPE æ”¹å˜äº†çŸ©é˜µçš„ç§©ï¼‰ï¼Œå› æ­¤ DeepSeek æå‡º **Decoupled RoPE**ï¼š

```
K = concat([K_nope, K_rope])
  K_nope: ä» c_KV è§£å‹ï¼Œä¸å«ä½ç½®ä¿¡æ¯
  K_rope: å•ç‹¬çš„å°çŸ©é˜µï¼Œåº”ç”¨ RoPE
  ä¸¤è€…æ‹¼æ¥æˆå®Œæ•´ K head

Q ç±»ä¼¼å¤„ç†
```

### KV Cache å¯¹æ¯”

```
                 KV Cache per token per layer
MHA (h=128)   :  2 Ã— 128 Ã— d_k = 256 d_k
GQA (G=8)     :  2 Ã— 8 Ã— d_k = 16 d_k
MLA (d_c=512) :  d_c + d_rope = 512 + 64 â‰ˆ 576  (ç­‰æ•ˆ ~4.5 d_k)

DeepSeek-V3: å‹ç¼©æ¯”è¾¾ ~93%ï¼Œä¸”è´¨é‡ä¸é™åå‡
```

## 6. Linear Attention

### åŠ¨æœº

æ ‡å‡† Attention çš„ $O(n^2)$ åœ¨è¶…é•¿åºåˆ—æ—¶æ˜¯æ ¹æœ¬ç“¶é¢ˆã€‚Linear Attention å°†å¤æ‚åº¦é™è‡³ $O(n)$ã€‚

### æ ¸å¿ƒæ€è·¯ï¼šKernel åˆ†è§£

æ ‡å‡† softmax attentionï¼š
$$A_{ij} = \frac{\exp(q_i^T k_j)}{\sum_l \exp(q_i^T k_l)}$$

Linear Attention ç”¨ kernel å‡½æ•° $\phi$ æ›¿ä»£ expï¼š
$$A_{ij} = \frac{\phi(q_i)^T \phi(k_j)}{\sum_l \phi(q_i)^T \phi(k_l)}$$

å…³é”®ï¼šæ”¹å˜è®¡ç®—é¡ºåºï¼Œå…ˆç®— $\phi(K)^T V$ï¼Œå†ä¹˜ $\phi(Q)$ï¼š

```python
def linear_attention(Q, K, V, feature_map):
    """O(n*d^2) ä»£æ›¿ O(n^2*d)"""
    Q_prime = feature_map(Q)  # (B, h, N, d') 
    K_prime = feature_map(K)  # (B, h, N, d')
    # å…ˆè®¡ç®— KV çš„å¤–ç§¯å’Œï¼ˆä¸ç”¨æ˜¾å¼ç®— NÃ—N çŸ©é˜µï¼ï¼‰
    KV = torch.einsum('bhnd,bhnv->bhdv', K_prime, V)  # (B, h, d', d_v)
    # å†ç”¨ Q æŸ¥è¯¢
    out = torch.einsum('bhnd,bhdv->bhnv', Q_prime, KV)  # (B, h, N, d_v)
    # å½’ä¸€åŒ–
    Z = torch.einsum('bhnd,bhd->bhn', Q_prime, K_prime.sum(dim=2))
    return out / Z.unsqueeze(-1)
```

### ä¸»æµå˜ä½“

| æ–¹æ³• | æ ¸å¿ƒåˆ›æ–° | å¤æ‚åº¦ | ä»£è¡¨æ¨¡å‹ |
|------|---------|--------|---------|
| Linear Transformer | ELU+1 kernel | O(ndÂ²) | â€” |
| RWKV | æ—¶é—´æ··åˆ + é€šé“æ··åˆ | O(nd) | RWKV-6/7 |
| Mamba/Mamba-2 | é€‰æ‹©æ€§ SSM | O(nd) | Jamba |
| RetNet | å¤šå°ºåº¦æŒ‡æ•°è¡°å‡ | O(nd) | â€” |
| GLA | é—¨æ§ + æ•°æ®ä¾èµ–è¡°å‡ | O(nd) | MiniMax-01 |
| Lightning Attention | ç¡¬ä»¶ä¼˜åŒ–çš„çº¿æ€§æ³¨æ„åŠ› | O(nd) | MiniMax-01 |

### Linear Attention çš„å±€é™

```
âœ… ä¼˜åŠ¿ï¼š
  - O(n) å¤æ‚åº¦ï¼Œåºåˆ—é•¿åº¦æ— ä¸Šé™
  - æ¨ç†æ—¶ RNN å½¢å¼ï¼Œæ— éœ€ KV Cache
  - è®­ç»ƒæ—¶å¯å¹¶è¡Œ

âŒ åŠ£åŠ¿ï¼š
  - in-context retrieval èƒ½åŠ›å¼±ï¼ˆ"å¤§æµ·æé’ˆ"æµ‹è¯•è¡¨ç°å·®ï¼‰
  - é•¿è·ç¦»ç²¾ç¡®åŒ¹é…ä¸å¦‚ softmax attention
  - 2025 å¹´å®é™…æ¨¡å‹è´¨é‡ä»ä¸åŠåŒè§„æ¨¡ Transformer
```

### 2025 è¶‹åŠ¿ï¼šæ··åˆæ¶æ„

æœ€æ–°è¶‹åŠ¿æ˜¯**æ··åˆä½¿ç”¨** softmax attention å’Œ linear attentionï¼š

```
Jamba (AI21):    Mamba å±‚ + Transformer å±‚äº¤æ›¿
MiniMax-01:      Lightning Attention + Softmax Attention æ··åˆ
DeepSeek-V3.1:   MLA + ç¨€ç– Attention æ¨¡å¼

ç†å¿µ: å…³é”®å±‚ç”¨ softmaxï¼ˆç²¾ç¡®æ£€ç´¢ï¼‰+ å…¶ä½™å±‚ç”¨ linearï¼ˆé«˜æ•ˆå¤„ç†ï¼‰
```

## 7. å¤æ‚åº¦ç»¼åˆå¯¹æ¯”

| æ–¹æ³• | è®­ç»ƒå¤æ‚åº¦ | æ¨ç† KV Cache | è´¨é‡ (åŒè§„æ¨¡) | ä»£è¡¨ |
|------|-----------|-------------|-------------|------|
| MHA | $O(n^2 d)$ | $2hd_k$ / token | â˜…â˜…â˜…â˜…â˜… | GPT-3 |
| MQA | $O(n^2 d)$ | $2d_k$ / token | â˜…â˜…â˜…â˜†â˜† | PaLM |
| GQA | $O(n^2 d)$ | $2Gd_k$ / token | â˜…â˜…â˜…â˜…â˜† | LLaMA 3 |
| MLA | $O(n^2 d)$ | $d_c$ / token | â˜…â˜…â˜…â˜…â˜… | DeepSeek-V3 |
| Linear | $O(nd^2)$ | $O(d^2)$ å›ºå®š | â˜…â˜…â˜…â˜†â˜† | RWKV |
| æ··åˆ | $O(n \cdot d)$ ~ $O(n^2 d)$ | å–å†³äºæ¯”ä¾‹ | â˜…â˜…â˜…â˜…â˜† | Jamba |

## 8. é€‰å‹æŒ‡å—

```
åœºæ™¯                â†’ æ¨èæ–¹æ¡ˆ
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ ‡å‡† LLM (7B-70B)   â†’ GQA (G=8)ï¼Œå·²æˆä¸ºé»˜è®¤
æè‡´ KV å‹ç¼©       â†’ MLAï¼ˆéœ€è¦æ”¹æ¨¡å‹æ¶æ„ï¼‰
è¶…é•¿åºåˆ— (>128K)    â†’ æ··åˆæ¶æ„ æˆ– Ring Attention
è¾¹ç¼˜éƒ¨ç½²/ä½æ˜¾å­˜     â†’ MQA æˆ– MLA
å®æ—¶æµå¼å¤„ç†       â†’ Linear Attention / RWKV
```

## é¢è¯•å¸¸è§é—®é¢˜

### Q1: ä» MHA â†’ MQA â†’ GQA â†’ MLAï¼Œæ¯ä¸€æ­¥è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ

**MHA** æ˜¯æ ‡å‡† Transformer æ³¨æ„åŠ›ï¼Œè¡¨è¾¾åŠ›æœ€å¼ºä½† KV Cache å¤§ï¼›**MQA** å°† KV å‹ç¼©åˆ° 1 ä¸ª headï¼ŒèŠ‚çœ ~98% KV Cache ä½†è´¨é‡ä¸‹é™æ˜æ˜¾ï¼›**GQA** æŠ˜ä¸­å– G ç»„ KVï¼Œæˆä¸º 2024 å¹´äº‹å®æ ‡å‡†ï¼ˆèŠ‚çœ 75-87.5%ï¼Œè´¨é‡æŸå¤± < 0.5%ï¼‰ï¼›**MLA** é€šè¿‡ä½ç§©å‹ç¼©è¿›ä¸€æ­¥å°† KV Cache å‹ç¼©åˆ° ~7% åŸå§‹å¤§å°ï¼Œä¸”å› ä¸ºæ¯ä¸ª head ä»èƒ½è·å¾—ç‹¬ç‰¹ KVï¼ˆä» latent è§£å‹ï¼‰ï¼Œè´¨é‡ä¸é™åå‡ã€‚æ ¸å¿ƒè„‰ç»œæ˜¯**ç”¨æ›´èªæ˜çš„æ–¹å¼å…±äº«/å‹ç¼© KV ä¿¡æ¯**ã€‚

### Q2: MLA çš„ä½ç§©å‹ç¼©ä¸ºä»€ä¹ˆä¸æŸå¤±è´¨é‡ï¼Ÿ

ä¸¤ä¸ªåŸå› ï¼š(1) æ³¨æ„åŠ›æƒé‡çŸ©é˜µæœ¬èº«å°±å…·æœ‰ä½ç§©ç‰¹æ€§ï¼ˆå¤šé¡¹ç ”ç©¶è¡¨æ˜æœ‰æ•ˆç§©è¿œå°äºç»´åº¦ï¼‰ï¼Œä½ç§©å‹ç¼©åªæ˜¯åˆ©ç”¨äº†è¿™ä¸€å†…åœ¨ç»“æ„ï¼›(2) MLA è§£å‹åæ¯ä¸ª head å¾—åˆ°**ç‹¬ç‰¹çš„** K/Vï¼ˆè€Œé MQA çš„å…±äº«ï¼‰ï¼Œè¡¨è¾¾åŠ›ç±»ä¼¼ MHAã€‚æœ¬è´¨ä¸Š MLA æ˜¯åœ¨**å‚æ•°ç©ºé—´**åšå‹ç¼©ï¼ˆç±»ä¼¼ LoRA æ€æƒ³ï¼‰ï¼Œè€Œéåœ¨**ä¿¡æ¯ç©ºé—´**åšè£å‰ªï¼ˆå¦‚ GQA/MQAï¼‰ã€‚

### Q3: Linear Attention ä¸ºä»€ä¹ˆæ²¡æœ‰å–ä»£ Transformerï¼Ÿ

Linear Attention é€šè¿‡ kernel åˆ†è§£å°† $O(n^2)$ é™è‡³ $O(n)$ï¼Œä½†ä¸¢å¤±äº† softmax çš„**é”åˆ©èšç„¦**èƒ½åŠ›â€”â€”softmax èƒ½äº§ç”Ÿæ¥è¿‘ one-hot çš„ attention åˆ†å¸ƒï¼ˆç²¾ç¡®æ£€ç´¢ï¼‰ï¼Œè€Œ linear kernel çš„ attention åˆ†å¸ƒæ›´å‡åŒ€ï¼Œå¯¼è‡´ in-context recall èƒ½åŠ›å¼±ã€‚å®é™…è¡¨ç°ä¸º"å¤§æµ·æé’ˆ"æµ‹è¯•å¤±è´¥ã€‚2025 å¹´çš„è¶‹åŠ¿æ˜¯æ··åˆæ¶æ„ï¼šå…³é”®å±‚ä¿ç•™ softmax attentionï¼Œå…¶ä½™å±‚ç”¨ linear attentionã€‚

### Q4: Decoupled RoPE è§£å†³äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ

RoPE å¯¹ Q å’Œ K æ–½åŠ æ—‹è½¬å˜æ¢ï¼ˆä½ç½®ç›¸å…³ï¼‰ï¼Œè¿™ä¼š**ç ´åä½ç§©ç»“æ„**â€”â€”æ—‹è½¬åçš„çŸ©é˜µç§©å¯èƒ½å‡é«˜ï¼Œå¯¼è‡´å‹ç¼©æŸå¤±å¢å¤§ã€‚MLA å°†æ¯ä¸ª head æ‹†æˆä¸¤éƒ¨åˆ†ï¼šä¸å«ä½ç½®ä¿¡æ¯çš„ä¸»ä½“ï¼ˆä» latent è§£å‹ï¼‰å’Œæºå¸¦ RoPE çš„å°é¢å¤–å‘é‡ï¼ˆå•ç‹¬ç¼“å­˜ï¼‰ï¼Œä¸¤è€…æ‹¼æ¥å½¢æˆå®Œæ•´ headã€‚è¿™æ ·æ—¢ä¿è¯äº†ä½ç½®ç¼–ç çš„æ•ˆæœï¼Œåˆç»´æŒäº† KV å‹ç¼©çš„é«˜æ•ˆæ€§ã€‚

### Q5: å®é™…éƒ¨ç½²ä¸­ GQA çš„ G å€¼å¦‚ä½•é€‰æ‹©ï¼Ÿæœ‰ä»€ä¹ˆçº¦æŸï¼Ÿ

ç»éªŒæœ€ä¼˜åŒºé—´æ˜¯ **G = h/8 ~ h/4**ã€‚ä¸»æµé€‰æ‹© G=8ï¼ˆLLaMA 2/3 70Bã€Qwen 2.5 72Bï¼‰ã€‚é€‰æ‹© G æ—¶éœ€è¦æ»¡è¶³ï¼š(1) G èƒ½è¢« Tensor Parallel degree æ•´é™¤ï¼ˆå¦åˆ™ KV heads æ— æ³•å‡åŒ€åˆ‡åˆ†ï¼‰ï¼›(2) h èƒ½è¢« G æ•´é™¤ï¼ˆæ¯ç»„ Q head æ•°ç›¸ç­‰ï¼‰ï¼›(3) æ›´å¤§æ¨¡å‹å¯ç”¨æ›´å¤§ G/h æ¯”ï¼ˆå¦‚ 70B ç”¨ 8:1 è€Œ 7B ç”¨ 4:1ï¼‰ï¼Œå› ä¸ºå¤§æ¨¡å‹æœ‰æ›´å¤šå†—ä½™ã€‚è¿‡å°çš„ G è´¨é‡æŸå¤±æ˜æ˜¾ï¼Œè¿‡å¤§åˆ™èŠ‚çœä¸å¤Ÿã€‚

---

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **æ–°æ¨¡å‹è®¾è®¡**ï¼š2025 å¹´æ–°è®­ LLM é»˜è®¤é€‰ GQA (G=8)ï¼Œæ— éœ€è®¨è®ºã€‚MLA ä»…åœ¨æœ‰èƒ½åŠ›ä¿®æ”¹ attention kernel çš„å›¢é˜Ÿï¼ˆå¦‚ DeepSeekï¼‰é€‚ç”¨
- **æ¨ç†æœåŠ¡é€‰å‹**ï¼šæ ¹æ® KV Cache é¢„ç®—å€’æ¨å¯æœåŠ¡çš„æœ€å¤§ batch sizeï¼Œå…¬å¼ $\text{max\_batch} = \frac{\text{GPU\_mem} - \text{model\_size}}{2 \times G \times d_k \times L \times \text{seq\_len} \times \text{bytes}}$
- **é•¿ä¸Šä¸‹æ–‡éƒ¨ç½²**ï¼š128K+ åœºæ™¯ä¼˜å…ˆè€ƒè™‘æ··åˆæ¶æ„ï¼ˆJamba é£æ ¼ï¼‰æˆ– MLA + Ring Attention

### å·¥ç¨‹å®ç°è¦ç‚¹
- GQA çš„ G å¿…é¡»èƒ½è¢« TP degree æ•´é™¤ï¼Œå¦åˆ™ KV heads æ— æ³•å‡åŒ€åˆ‡åˆ†
- MLA çš„ Decoupled RoPE å®ç°éœ€è¦é¢å¤–ç¼“å­˜ $d_{\text{rope}}$ ç»´å‘é‡ï¼Œå®é™…å‹ç¼©æ¯”éœ€è®¡å…¥æ­¤é¡¹
- Linear Attention æ¨ç†æ—¶å¯é€€åŒ–ä¸º RNN å½¢å¼ï¼ˆæ—  KV Cacheï¼‰ï¼Œé€‚åˆæµå¼åœºæ™¯

---

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- **MLA çš„ä½ç§©å‹ç¼©æ€æƒ³ä¸ LoRA åŒæº**ï¼šéƒ½æ˜¯åˆ©ç”¨å‚æ•°çŸ©é˜µçš„å†…åœ¨ä½ç§©æ€§ã€‚è¿™ä¸ª insight å¯ä»¥è¿ç§»åˆ°å…¶ä»–éœ€è¦å‹ç¼©çš„åœºæ™¯ï¼ˆå¦‚ [[AI/3-LLM/SFT/LoRA|LoRA å¾®è°ƒ]]ï¼‰
- **æ··åˆæ¶æ„æ˜¯å·¥ç¨‹å¦¥åçš„å…¸èŒƒ**ï¼šçº¯ linear attention è´¨é‡ä¸å¤Ÿï¼Œçº¯ softmax attention åºåˆ—é•¿åº¦å—é™ã€‚æœ€ç»ˆè½åœ°çš„æ€»æ˜¯æŠ˜ä¸­æ–¹æ¡ˆ

### æœªè§£é—®é¢˜ä¸å±€é™
- Linear Attention çš„ in-context retrieval èƒ½åŠ›å¼±æ˜¯æ ¹æœ¬æ€§çš„â€”â€”kernel åˆ†è§£ä¸¢å¤±äº† softmax çš„é”åˆ©èšç„¦èƒ½åŠ›ï¼Œç›®å‰æ— ä¼˜é›…è§£æ³•
- MLA ç›®å‰ä»… DeepSeek ä½¿ç”¨ï¼Œç”Ÿæ€æ”¯æŒï¼ˆæ¨ç†æ¡†æ¶ã€é‡åŒ–å·¥å…·ï¼‰ä¸å¦‚ GQA æˆç†Ÿ

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- å¦‚æœæŠŠ [[AI/3-LLM/Architecture/Mamba-SSM|Mamba]] çš„é€‰æ‹©æ€§ SSM å’Œ Transformer çš„ MLA åœ¨åŒä¸€æ¨¡å‹ä¸­æ··åˆï¼Œå¯èƒ½åœ¨è¶…é•¿ä¸Šä¸‹æ–‡ï¼ˆ>1M tokensï¼‰åœºæ™¯å–å¾—çªç ´
- 6 ä¸ªæœˆåé¢„æµ‹ï¼šä¼šå‡ºç°è‡ªåŠ¨åŒ–çš„ attention æ¶æ„æœç´¢â€”â€”å“ªäº›å±‚ç”¨ softmaxã€å“ªäº›ç”¨ linearï¼Œç”± NAS è‡ªåŠ¨å†³å®š

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) â€” Transformer å’Œ MHA çš„å¥ åŸºè®ºæ–‡
- [Fast Transformer Decoding (MQA)](https://arxiv.org/abs/1911.02150) â€” ç¬¬ä¸€ä¸ªæå‡º KV head å…±äº«çš„å·¥ä½œ
- [GQA: Training Generalized Multi-Query Attention](https://arxiv.org/abs/2305.13245) â€” GQA åŸè®ºæ–‡ï¼Œå« MHAâ†’GQA çš„ uptraining æ–¹æ³•
- [FlashAttention](https://arxiv.org/abs/2205.14135) â€” IO-aware attention è®¡ç®—ï¼Œä¸æœ¬æ–‡ Sec. 6 çš„ linear attention å½¢æˆå¯¹æ¯”
- [Longformer (Sliding Window Attention)](https://arxiv.org/abs/2004.05150) â€” é•¿æ–‡æ¡£ Attention çš„ç»å…¸æ–¹æ¡ˆ
- [DeepSeek-V2 (MLA)](https://arxiv.org/abs/2405.04434) â€” MLA å’Œ Decoupled RoPE çš„åŸå§‹è®¾è®¡

### æ·±åº¦è§£è¯»
- [Jay Alammar: The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) â€” MHA å¯è§†åŒ–æœ€ä½³å…¥é—¨ â­â­â­â­â­

### å®è·µèµ„æº
- [FlashAttention GitHub](https://github.com/Dao-AILab/flash-attention) â€” FlashAttention çš„é«˜æ•ˆå®ç°ï¼Œæ”¯æŒ GQA/MQA
- [vLLM](https://github.com/vllm-project/vllm) â€” GQA + PagedAttention çš„å·¥ä¸šçº§æ¨ç†æ¡†æ¶

### ä»£ç æ‰‹æ’•ï¼ˆç†è®º â†’ ä»£ç ï¼‰
- [[Projects/MA-RLHF/lc10/lc10-00-FlashAttention-æ‰‹æ’•å®æ“|FlashAttention-æ‰‹æ’•å®æ“]] â€” **å¿…çœ‹**ï¼šä» Tiling/SRAM ç®¡ç†åˆ° CUDA kernel çš„ IO-aware å®Œæ•´å®ç°ï¼ŒMA-RLHF é¡¹ç›®é…å¥— â­â­â­â­â­
- [[Projects/MA-RLHF/lc2/lc2-01-Transformer-æ‰‹æ’•å®æ“|Transformer-æ‰‹æ’•å®æ“]] â€” åŒ…å« MHA çš„å®Œæ•´ Transformer ä»é›¶å®ç°ï¼ˆå« Self-Attention/Cross-Attentionï¼‰

---

## See Also

> ğŸ”— See also: [[AI/3-LLM/Architecture/GQA-MQA|GQA/MQA æ·±åº¦è§£æ]] â€” KV head å…±äº«æœºåˆ¶çš„è¯¦ç»†å®ç°å’Œæ€§èƒ½å¯¹æ¯”
> ğŸ”— See also: [[AI/3-LLM/Architecture/FlashAttention|FlashAttention]] â€” Attention è®¡ç®—åŠ é€Ÿï¼Œä¸æœ¬æ–‡æ¶æ„ä¼˜åŒ–äº’è¡¥
> ğŸ”— See also: [[AI/3-LLM/Inference/KV Cache|KV Cache]] â€” Attention å˜ä½“ç›´æ¥å½±å“ KV Cache å¤§å°ï¼Œæ¨ç†ä¼˜åŒ–çš„æ ¸å¿ƒå…³è”
> ğŸ”— See also: [[AI/3-LLM/Architecture/Multi-Head Latent Attention|MLA è¯¦è§£]] â€” DeepSeek MLA çš„å®Œæ•´æŠ€æœ¯ç»†èŠ‚
> ğŸ”— See also: [[AI/3-LLM/Architecture/Mamba-SSM|Mamba/SSM]] â€” Linear Attention çš„æ›¿ä»£è·¯çº¿ï¼šé€‰æ‹©æ€§çŠ¶æ€ç©ºé—´æ¨¡å‹
