---
title: "大语言模型 LLM"
type: moc
domain: ai/llm
tags:
  - ai/llm
  - type/moc
updated: 2026-02-22
---

# 🧠 大语言模型 LLM — 学习路线图

> 从基础概念到前沿研究的 LLM 全栈知识体系，按学习路径编排。

---

## 第一章 基础概念（Foundations）

> 前置知识：[[AI/1-Foundations/目录|数学 + ML + DL 基础]]

- [[Tokenizer|Tokenizer]] — 分词基础
- [[Tokenizer 深度理解|Tokenizer 深度理解]]
- [[AI/3-LLM/Inference/采样策略|采样策略]] — Temperature / Top-p / Top-k
- [[幻觉问题|幻觉问题]]
- [[幻觉问题与缓解|幻觉问题与缓解]]
- [[小规模训练手册|小规模训练手册]] — 构建世界级 LLM 的秘密

---

## 第二章 模型架构（Architecture）

> 从 Vanilla Transformer 到 MoE/SSM，理解 LLM 的骨架

### 核心架构

- [[Transformer架构深度解析-2026技术全景|🔥 Transformer 架构深度解析 2026]] ⭐ — 面试终极武器，1617行，从数学第一性原理到 MoE/SSM/2026前沿全覆盖 ★★★★★
- [[架构范式对比|架构范式对比]] — Encoder / Decoder / Encoder-Decoder

### 经典模型系列

| 模型 | 类型 | 说明 |
|------|------|------|
| [[BERT\|BERT]] | Encoder | 双向编码器 |
| [[GPT\|GPT]] | Decoder | 自回归生成 |
| [[T5\|T5]] | Enc-Dec | Encoder-Decoder |
| [[LLaMA\|LLaMA]] | Decoder | Meta 开源系列 |
| [[Qwen\|Qwen]] | Decoder | 阿里通义系列 |
| [[DeepSeek-R1\|DeepSeek-R1]] | Decoder | 推理能力突破 |
| [[Qwen3.5-Plus\|Qwen3.5-Plus]] | MoE | 397B-A17B + Linear Attention |

### Attention 机制

- [[Attention 变体综述|Attention 变体综述]]
- [[FlashAttention|FlashAttention]] — IO-aware 高效注意力
- [[GQA-MQA|GQA / MQA]] — Grouped / Multi-Query Attention
- [[Multi-Head Latent Attention|Multi-Head Latent Attention]]
- [[Transformer 位置编码|位置编码]] — RoPE / ALiBi 等

### 高级架构

- [[MoE 深度解析|MoE 深度解析]] — 混合专家架构
- [[Mamba-SSM|Mamba-SSM]] — 状态空间模型
- [[MiniCPM-SALA|MiniCPM-SALA]] — Sparse + Linear Attention 混合架构
- [[SLA2-Learnable-Router|SLA2]] — 可学习路由器动态选 sparse/linear 分支
- [[AI/3-LLM/Architecture/长上下文处理|长上下文处理]]
- [[长上下文技术|长上下文技术]]

### 前沿架构研究

- [[Engram-Conditional-Memory-DeepSeek-V4|Engram（DeepSeek V4 架构）]] — 记忆稀疏第二轴 ★★★★★
- [[mHC-Manifold-Constrained-Hyper-Connections-DeepSeek|mHC（DeepSeek V4 架构）]] — 流形约束超连接 ★★★★☆
- [[Manifold-Constrained Hyper-Connections|Manifold-Constrained Hyper-Connections（早期版）]]
- [[ReFINE-Fast-Weight-RL-Next-Sequence-Prediction|ReFINE]] — Fast Weight + GRPO ★★★★☆
- [[Growing-to-Looping-Iterative-Computation-Unification|Growing to Looping]] — 迭代计算统一理论 ★★★★☆
- [[LaViDa-R1-Diffusion-LLM-Reasoning|LaViDa-R1]] — 扩散语言模型推理 ★★★★☆
- [[GLM-5 Agentic Engineering|GLM-5]] — Agentic Engineering
- [[AI Models Collapse 论文|AI Models Collapse]] — 递归训练坍塌

---

## 第三章 预训练（Pretraining）

> 从零开始训练一个 LLM：数据、并行、Scaling Law

- [[预训练原理|预训练原理]] — 自回归预训练基础
- [[LLM-预训练与分布式训练-2026-全景|🔥 预训练与分布式训练 2026 全景]] ⭐ — 2183行全覆盖 ★★★★★
- [[LLM-数据工程-2026-技术全景|🔥 数据工程 2026 全景]] ⭐ — 3793行深度专项 ★★★★★
- [[自监督学习与对比学习-2026技术全景|🔥 自监督学习与对比学习 2026 全景]] ⭐ — SimCLR/MoCo/CLIP/MAE/DINO 四大范式全覆盖，含公式推导+面试高频题 ★★★★★
- [[Karpathy-nanochat|Karpathy nanochat]] — $72 训练 GPT-2

### 训练基础设施 → [[#附录 A 训练基础设施（Infra）]]

---

## 第四章 微调训练（SFT → RL）

> 预训练后的能力对齐：从 SFT 到 RLHF/DPO/GRPO

### 4.1 监督微调 SFT

- [[SFT 原理|SFT 原理]] — 监督微调基础
- [[LLM微调实战-2026技术全景|🔥 LLM 微调实战 2026 全景]] ⭐ — 1860行全链路 ★★★★★
- [[SFT-TRL实践|SFT-TRL 实践]]
- [[SFT-实战指南|SFT 实战指南]]
- [[AI/3-LLM/SFT/训练数据构建|训练数据构建]]
- [[Post-Training Unified View 论文|Post-Training 统一视角]]
- [[AI/LLM/SFT/SFT-手撕实操|SFT-手撕实操]] — Chat Template + Loss Mask + LoRA 完整代码（MA-RLHF）

### 4.2 参数高效微调 PEFT

- [[LoRA|LoRA]] — 低秩适应
- [[PEFT 方法对比|PEFT 方法对比]]（530行，正式版）
- [[EWC-LoRA-Continual-Learning-Low-Rank|EWC-LoRA]] ⭐ — 持续学习 + 低秩正则，ICLR 2026 ★★★★☆

### 4.3 强化学习 RL → [[AI/3-LLM/RL/目录|RL 详细 MOC]]

- PPO / GRPO / DPO / DAPO / KTO / RLOO 及更多算法
- TRL / verl / Unsloth / OpenRLHF 框架实践

---

## 第五章 推理部署（Inference & Deployment）

> 把训练好的模型高效上线

### 5.1 推理优化总览

- [[LLM-推理优化-2026-全景|🔥 推理优化 2026 全景]] — 941行全覆盖
- [[推理优化|推理优化综述]]
- [[AI/3-LLM/Inference/推理服务架构|推理服务架构]]
- [[模型部署实践|模型部署实践]]

### 5.2 推理引擎

| 引擎 | 说明 |
|------|------|
| [[vLLM\|vLLM]] | PagedAttention 高性能推理 |
| [[TensorRT-LLM\|TensorRT-LLM]] | NVIDIA 推理优化 |
| [[Ollama\|Ollama]] | 本地部署 |

### 5.3 KV Cache

- [[AI/3-LLM/Inference/KV Cache|KV Cache]]（830行，正式版）
- [[DMS KV Cache压缩|DMS KV Cache 压缩]]
- [[Continuous Batching|Continuous Batching]]

### 5.4 解码加速

- [[AI/3-LLM/Inference/Speculative Decoding|Speculative Decoding]] — 推测解码
- [[Sparrow-Video-LLM-Speculative-Decoding|Sparrow]] — Video LLM 推测解码 ★★★★☆
- [[MAGE-Block-Diffusion-LLM-Sparse-Attention|MAGE]] — Block Diffusion 稀疏注意力 ★★★★☆
- [[Sink-Aware-Pruning-Diffusion-LLM|Sink-Aware Pruning]] — Diffusion LLM 剪枝 ★★★★☆

### 5.5 量化

- [[量化综述|量化综述]]（正式版） — GPTQ / AWQ / GGUF
- [[剪枝与蒸馏|剪枝与蒸馏]]
- [[端侧推理量化精度陷阱-跨骁龙芯片精度失真|端侧量化精度陷阱]] ★★★★☆

### 5.6 Test-Time Compute (TTC) — 推理时扩展

- [[Test-Time-Compute|TTC 综述]] — CoT / PRM / Best-of-N / Budget Forcing
- [[TTC-Test-Time-Compute-Efficiency-2026-综合分析|🔥 TTC 效率 2026 综合分析]] ⭐ ★★★★★
- [[Gemini-3-Deep-Think|Gemini 3 Deep Think]] — ARC-AGI-2 84.6%
- [[Deep-Thinking-Ratio-DTR|DTR]] — 推翻"CoT 越长越好" ★★★★☆
- [[Deep-Thinking-Ratio-DTR-v2-Think-At-N|DTR v2 + Think@N]] ⭐ — "推理深度在开头50 token已决定" ★★★★★
- [[Progressive-Thought-Encoding-Cache-Efficient-RL|PTE]] ⭐ — KV cache 满时先学习再 evict，ICLR 2026 ★★★★★
- [[Accordion-Thinking-Self-Regulated-Step-Summaries|Accordion-Thinking]] — RL 学会主动压缩 ★★★★☆
- [[SIA-Sparse-Inference-time-Alignment|SIA（ICML 2026，NTU）]] — **稀疏推理时对齐**：只在高熵 Junction 节点干预（20% token = 100% 效果），6x compute 节省；证明 alignment 是 sparse control problem ★★★★☆
- [[ConformalThinking-Risk-Control-Test-Time-Compute|ConformalThinking]] ⭐ — 统计风险控制停止策略，ICML 2026 ★★★★★

---

## 第六章 应用层（Application: RAG / Prompt / Code）

> 用 LLM 构建实际产品

### 6.1 Prompt Engineering

- [[Prompt-Engineering-2026实战全景|🔥 Prompt Engineering 2026 实战全景]] ⭐ — 2784行 ★★★★★
- [[Prompt-Engineering-基础|Prompt Engineering 基础]]
- [[Prompt-Engineering-概述|Prompt 概述]]
- [[Prompt Engineering 高级|Prompt Engineering 高级]]
- [[高级-Prompt-技巧|高级 Prompt 技巧]]
- [[Prompt-攻击|Prompt 攻击]]
- [[Prompt-Tools|Prompt 工具]]
- [[AI/3-LLM/Application/数据合成|数据合成]]

### 6.2 RAG → 另见 [[AI/6-应用/RAG/_MOC|RAG 详细 MOC]]

- [[RAG-2026-技术全景|🔥 RAG 2026 技术全景]] ⭐ — 面试武器版，Naive→Advanced→Modular→Agentic RAG 全谱系 ★★★★★
- [[RAG 原理与架构|RAG 原理与架构]]
- [[AI/6-应用/RAG/Advanced RAG|Advanced RAG]]
- [[AI/RAG 工程实践|RAG 工程实践]]
- [[RAG vs Fine-tuning|RAG vs Fine-tuning]]
- [[RAG 评测|RAG 评测]]
- [[Reranker|Reranker]]
- [[向量数据库选型|向量数据库选型]]
- [[文本分块策略|文本分块策略]]
- [[文档解析|文档解析]]
- [[检索策略|检索策略]]

### 6.3 Embedding & 向量检索

- [[Embedding|Embedding]]
- [[Embedding 选型|Embedding 选型]]
- [[Embedding 与向量检索|Embedding 与向量检索]]
- [[大模型线上排查 SOP|线上排查 SOP]]

### 6.4 代码生成

- [[LLM代码生成-2026技术全景|🔥 LLM 代码生成 2026 全景]] ⭐ — 1083行 ★★★★★

### 6.5 合成数据

- [[合成数据与数据飞轮-2026技术全景|🔥 合成数据与数据飞轮 2026 全景]] ⭐ — 1738行 ★★★★★
- [[Synthetic Data|合成数据]]
- [[DataFlow|DataFlow]]

### 6.6 其他应用

- [[LLMOps|LLMOps]]
- [[OpenCharacter-Large-Scale-Synthetic-Persona-Training|OpenCharacter]] — 合成 Persona 角色扮演训练 ★★★

---

## 第七章 前沿进展（Latest Research）

### 效率与压缩

- [[知识蒸馏与模型压缩-2026技术全景|🔥 知识蒸馏与模型压缩 2026 全景]] ⭐ — 2061行 ★★★★★
- [[模型蒸馏|模型蒸馏]]

### 评估与趋势

- [[LLM评估与Benchmark-2026技术全景|🔥 LLM 评估与 Benchmark 2026 全景]] ⭐ — 1854行 ★★★★★
- [[AI/3-LLM/Evaluation/LLM 评测体系|LLM 评测体系]]
- [[ICLR-2026-趋势分析|ICLR 2026 趋势分析]] — 5357 篇论文趋势
- [[PERSIST-LLM-Personality-Stability-Benchmark|PERSIST]] ⭐ — LLM 人格稳定性基准，AAAI 2026 ★★★★★

### 前沿模型 → [[AI/Frontiers/目录|前沿详细 MOC]]

---

## 附录 A 训练基础设施（Infra）

- [[DeepSpeed|DeepSpeed]]
- [[FSDP|FSDP]] — PyTorch 原生分布式
- [[Megatron-LM|Megatron-LM]]
- [[Ray|Ray]]
- [[分布式训练|分布式训练综述]]
- [[GPU 显存计算指南|GPU 显存计算指南]]
- [[AI/3-LLM/Infra/混合精度训练|混合精度训练]]
- [[模型并行策略|模型并行策略]]

## 附录 B 工具框架（Frameworks）

### TRL
- [[TRL 概述|TRL]] — HuggingFace 训练框架

### OpenRLHF
- [[OpenRLHF|OpenRLHF]]

### Slime-RL
- [[Slime-RL-Framework|Slime-RL]] — THUDM 异步 RL 框架

### Unsloth
- [[Unsloth 概述|Unsloth 概述]] — 低资源微调
- [[训练示例概述|训练示例]] / [[CPT|CPT]] / [[Chat Templates|Templates]] / [[Checkpoint|Checkpoint]]
- [[运行 & 保存模型|运行保存]] / [[量化|量化]] / [[量化 & 显存预估|显存预估]] / [[多卡并行|多卡并行]]
- [[AI/3-LLM/Frameworks/Unsloth/数据合成|数据合成]] / [[notebook 合集|notebook 合集]]
- [[Gemma 3 训练|Gemma 3]] / [[Qwen3 训练|Qwen3]] / [[gpt-oss 训练|gpt-oss]] / [[TTS 训练|TTS]]

### verl
- [[verl 概述|verl 概述]] — 字节 RL 框架
- [[算法概述|算法]] / [[HybridFlow|HybridFlow]] / [[verl 训练参数|参数]] / [[配置文件|配置]]
- [[训练后端|后端]] / [[Reward Function|Reward]] / [[Post-Training 数据准备|数据准备]]
- [[RL with Lora|RL+LoRA]] / [[Off Policy 异步训练器|Off-Policy]] / [[多轮 RL 训练交互|多轮交互]] / [[实现其他 RL 方法|扩展算法]]
- [[性能调优|性能调优]] / [[硬件资源预估|硬件预估]] / [[Sandbox Fusion 沙箱|沙箱]] / [[grafana 看板|Grafana]]

---

## 附录 C 手撕实操系列（Code Practice — MA-RLHF）

> 来源：[MA-RLHF](https://github.com/dhcode-cpp/MA-RLHF) — 25 篇代码实操，约 10,000 行 PyTorch，面试代码题核心武器库

- [[MA-RLHF-手撕实操-系列索引|🔥 手撕实操总索引]] ⭐ — 全部 25 篇导航 + 三条面试速通路径

### 架构手撕
- [[AI/LLM/Architecture/基础数学组件手撕|基础数学组件]] — MHA/GQA/LayerNorm/RMSNorm/SwiGLU/LoRA
- [[AI/LLM/Architecture/Transformer-手撕实操|Transformer-手撕]] — Encoder-Decoder 完整实现
- [[AI/LLM/Architecture/GPT2-手撕实操|GPT2-手撕]] — Decoder-only + GPT-1→2→3 演进
- [[AI/LLM/Architecture/Llama-手撕实操|Llama-手撕]] — RoPE/GQA/KV Cache
- [[AI/LLM/Architecture/DeepSeek-V3-手撕实操|DeepSeek-V3-手撕]] — MLA（KV cache降16x）+ MoE
- [[AI/LLM/Architecture/Tokenizer-Embedding-手撕实操|Tokenizer-Embedding-手撕]] — BPE/位置编码谱系

### 推理手撕
- [[AI/LLM/Inference/FlashAttention-手撕实操|FlashAttention-手撕]] — Online Softmax + IO 复杂度 O(N²)→O(N)
- [[AI/LLM/Inference/vLLM-手撕实操|vLLM-手撕]] — PagedAttention + Continuous Batching
- [[AI/LLM/Inference/Continue-Batching-手撕实操|Continue-Batching-手撕]] — 动态 Batch 调度，GPU 利用率 30% → 90%+
- [[AI/LLM/Inference/vLLM-PageKVCache-手撕实操|vLLM-PageKVCache-手撕]] — 分页 KV Cache 管理（BlockTable + 逻辑→物理映射）
- [[AI/LLM/Inference/vLLM-PageAttention-手撕实操|vLLM-PageAttention-手撕]] — PagedAttention Kernel（非连续块 Attention 计算）
- [[AI/LLM/Inference/Chunked-Prefill-手撕实操|Chunked-Prefill-手撕]] — SARATHI：Prefill 分块 + Decode piggybacking，解决 TTFT/TPOT 矛盾
- [[AI/LLM/Inference/Speculative-Decoding-手撕实操|Speculative-Decoding-手撕]] — Draft-Target 对 + 拒绝采样，完整推测解码实现
- [[AI/LLM/Inference/PD-Disaggregation-手撕实操|PD-Disaggregation-手撕]] — Prefill-Decode 物理分离，Ray Actor + KV 异步传输
- [[AI/LLM/Inference/vLLM-V0-V1-完整系统实操|vLLM V0/V1 完整系统实操]] — V0 = PageKV+PageAttn 集成；V1 = SchedulerInfo + merge_prompt + KV.split 统一调度（调度粒度 request→token）

### 分布式训练手撕
- [[AI/LLM/Infra/分布式训练通信原语-手撕实操|通信原语-手撕]] — NCCL 8 原语 + Ring-AllReduce
- [[AI/LLM/Infra/ZeRO-手撕实操|ZeRO-手撕]] — ZeRO-1/2/3 显存分片
- [[AI/LLM/Infra/Tensor-Parallel-手撕实操|Tensor-Parallel-手撕]] — 列/行并行 + 序列并行
- [[AI/LLM/Infra/Pipeline-Parallel-手撕实操|Pipeline-Parallel-手撕]] — 1F1B + 交错式（Bubble率公式）
- [[AI/LLM/Infra/MoE-Context-Parallel-手撕实操|MoE-Context-Parallel-手撕]] — Expert并行 + Ring Attention
- [[AI/LLM/Infra/Ray-分布式RL训练实操|Ray-分布式RL训练-手撕]] — Generator-Coordinator-Trainer 三角架构
- [[AI/LLM/Infra/Ray-推理系统实操|Ray-推理系统-手撕]] — Rollout 生成侧设计

#### xtrain 从零手写（深度强化版）
- [[AI/LLM/Infra/xtrain-lc1-分布式通信原语从零手写|xtrain-lc1-通信原语]] — Ring-AllReduce + NCCL 8原语从零实现（不依赖框架）
- [[AI/LLM/Infra/xtrain-lc2-数据并行从零手写|xtrain-lc2-数据并行]] — DP/DDP 从零实现，理解 gradient sync 机制
- [[AI/LLM/Infra/xtrain-lc3-ZeRO优化器从零手写|xtrain-lc3-ZeRO]] — ZeRO-1/2/3 三阶段分片从零实现
- [[AI/LLM/Infra/xtrain-lc4-张量并行从零手写|xtrain-lc4-张量并行]] — 列/行并行 Linear + 序列并行从零实现
- [[AI/LLM/Infra/xtrain-lc5-流水线并行从零手写|xtrain-lc5-流水线并行]] — 1F1B + DualPipe 从零实现（Bubble率推导）
- [[AI/LLM/Infra/xtrain-lc6-Context并行RingAttention手写|xtrain-lc6-Context并行]] — Ring Attention 从零实现，超长上下文训练
- [[AI/LLM/Infra/xtrain-lc7-MoE专家并行从零手写|xtrain-lc7-MoE专家并行]] — Expert Parallelism + AllToAll 路由从零实现

### RL 对齐手撕
- [[AI/LLM/RL/Fundamentals/RL基础算法手撕实操|RL基础算法-手撕]] — REINFORCE/A2C/PPO/DQN
- [[AI/LLM/SFT/SFT-手撕实操|SFT-手撕]] — Chat Template + Loss Mask + LoRA
- [[AI/LLM/RL/PPO/PPO-手撕实操-MA-RLHF|PPO-手撕]] — 4模型 + GAE + MA-PPO 多适配器
- [[AI/LLM/RL/PPO/MA-RLHF-核心代码注解|MA-RLHF-代码注解]] — 完整项目逐行解析
- [[AI/LLM/RL/PPO/RLHF-PPO-完整Pytorch实现|RLHF-PPO-Notebook]] — 56-cell 完整实现（四模型架构 + GAE + KL约束全链路）
- [[AI/LLM/RL/PPO/LLaMA2-Reward-Model实现|LLaMA2-RM-实现]] — Bradley-Terry 偏好 RM 完整实现
- [[AI/LLM/RL/PPO/O1-PRM搜索完整实现|O1-PRM-搜索-Notebook]] — PRM + MCTS-O1 端到端实现（UCT选择+展开+回溯）
- [[AI/LLM/RL/GRPO/GRPO-手撕实操|GRPO-手撕]] — Clip版/简化版 + 与PPO差异
- [[AI/LLM/RL/GRPO/GRPO-完整Notebook实现|GRPO-Notebook]] — 完整端到端实现（组采样+advantage归一化+KL项）
- [[AI/LLM/RL/GRPO/GRPO-KL散度三种近似|GRPO-KL三种近似]] — k1/k2/k3 Schulman近似实现与对比
- [[AI/LLM/RL/DPO/DPO-手撕实操|DPO-手撕]] — Bradley-Terry + 隐式 reward 推导
- [[AI/LLM/RL/DPO/DPO-完整Notebook实现|DPO-Notebook]] — 偏好对处理 + log-ratio loss 完整实现
- [[AI/LLM/RL/DPO/Bradley-Terry模型实现|BT模型-实现]] — DPO 数学基础，偏好建模理论入口
- [[AI/LLM/RL/KTO/KTO-手撕实操|KTO-手撕]] — 前景理论 + 单样本偏好
- [[AI/LLM/RL/KTO/KTO-完整Notebook实现|KTO-Notebook]] — 前景理论偏好建模，无需成对数据
- [[AI/LLM/RL/PPO/PRM-O1-Search-手撕实操|PRM-O1-手撕]] — Process Reward + MCTS

**Batch B — 从零手写（工程级完整实现，2026-02-26）：**
- [[AI/LLM/MA-RLHF课程/lc8-LLaMA2-Reward-Model-从零手写|lc8 LLaMA2-RM-Batch-B]] — 生产级 RM，scalar reward head + BT Loss
- [[AI/LLM/MA-RLHF课程/lc8-RLHF-PPO-Pytorch从零手写|lc8 RLHF-PPO-Batch-B]] — 四模型完整 PPO，GPU 调度 + 显存管理
- [[AI/LLM/MA-RLHF课程/lc8-DPO-IPO-BT-偏好优化从零手写|lc8 DPO-IPO-BT-Batch-B]] — DPO/IPO/BT + GRPO-KL 三种近似
- [[AI/LLM/MA-RLHF课程/lc8-KTO-PRM-Search-从零手写|lc8 KTO-PRM-Batch-B]] — KTO 前景理论 + PRM-o1 搜索
- [[AI/LLM/MA-RLHF课程/lc8-GRPO-notebook-Pytorch从零手写|lc8 GRPO-Batch-B]] — GRPO group rollout 完整实现

### 多模态手撕
- [[CLIP-ViT-LLaVA-手撕实操|CLIP-ViT-LLaVA-手撕]] — ViT + InfoNCE + LLaVA 投影

---

## 附录 D MA-RLHF 课程体系（Course MOC）

> 完整课程体系 MOC，每个专题有独立导航地图。工程实现视角，配合附录 C 手撕实操使用。

- [[00-课程总览-MOC|🔥 MA-RLHF 课程总览]] — 10 专题 + 4 条面试速通路径 + 课程 vs Vault 关系说明
- [[AI/LLM/MA-RLHF课程/lc1-基础组件-MOC|lc1 基础组件]] — BPE Tokenizer / Embedding / 位置编码全家族
- [[AI/LLM/MA-RLHF课程/lc2-Transformer-MOC|lc2 Transformer]] — 完整 Encoder-Decoder 实现
- [[AI/LLM/MA-RLHF课程/lc3-GPT系列-MOC|lc3 GPT 系列]] — GELU / PreNorm / BPE / KV Cache / ICL
- [[AI/LLM/MA-RLHF课程/lc4-Llama系列-MOC|lc4 Llama 系列]] — RoPE / NTK / GQA / RMSNorm / SwiGLU
- [[AI/LLM/MA-RLHF课程/lc5-DeepSeek-V3-MOC|lc5 DeepSeek V3]] — MoE / MLA / MTP / YaRN / mHC / Load Balance
- [[AI/LLM/MA-RLHF课程/lc6-SFT全链路-MOC|lc6 SFT 全链路]] — 数据处理 / LoRA / 完整训练 / RAG / ReAct / LLM-Judge
- [[AI/LLM/MA-RLHF课程/lc7-RL基础-MOC|lc7 RL 基础]] — MC/TD/Q-Learning/DQN/PolicyGradient/GAE
- [[AI/LLM/MA-RLHF课程/lc8-RL×LLM-MOC|lc8 RL×LLM]] — RLHF-PPO / DPO / KTO / GRPO / PRM / MCTS-O1
- [[AI/LLM/MA-RLHF课程/lc9-分布式RL训练-MOC|lc9 分布式 RL 训练]] — Ray 三角架构 / 异步 GRPO / verl 实战 / R1 复现
- [[AI/LLM/MA-RLHF课程/lc10-推理系统-MOC|lc10 推理系统]] — Continue Batching / PageKV / PageAttention / vLLM V0→V1 / Chunked Prefill / PD 分离 / SD
- [[AI/LLM/MA-RLHF课程/xtrain-分布式并行手写-MOC|xtrain 分布式并行手写]] — DP/ZeRO/TP/PP(DualPipe)/CP(RingAttn)/EP(MoE) 从零手写

---

## 导航

- ↑ 上级：[[AI/目录]]
- ← 前置：[[AI/1-Foundations/目录]]
## 多模态 RL（MultiModal）
- [[Multimodal-Perception-RL-综合分析|🔥 多模态感知 RL 综合分析]] ⭐ — VPPO/Perception-R1/AT-RL/VisRL 四路线全景，统一设计原则 ★★★★★
- [[Perception-R1-Visual-Perception-Reward-RLVR|Perception-R1]] — 显式视觉感知 Reward，1,442样本超越200K ★★★★☆

## 相关 MOC
- → 相关：[[AI/3-LLM/MLLM/目录]] · [[AI/2-Agent/目录]] · [[AI/6-应用/RAG/_MOC]]
