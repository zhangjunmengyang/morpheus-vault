---
brief: "verl 支持算法概述——框架内置算法全景：PPO/GRPO/DAPO/DPO/KTO/RLOO 等的算法原理简介和适用场景对比；选型指南：什么场景用什么算法，verl 的算法接口设计哲学。"
title: "算法"
type: reference
domain: ai/llm/frameworks/verl
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm/frameworks/verl
  - type/reference
---
# 算法

## 概述

verl 支持多种 RL 对齐算法，从经典的 PPO 到最新的 GRPO/DAPO。这篇梳理 verl 中各算法的实现要点和选择建议。

## GRPO（Group Relative Policy Optimization）

verl 中最主推的算法，也是 DeepSeek-R1 的训练算法。

### 核心流程

```python
# verl 中 GRPO 的一个训练步骤（简化）

# 1. Generation: 每个 prompt 采样 G 个 response
prompts = batch["input_ids"]
responses = vllm_engine.generate(prompts, num_return_sequences=G)

# 2. Reward: 计算每个 response 的 reward
rewards = reward_function(prompts, responses)  # shape: [batch, G]

# 3. Advantage: 组内标准化
mean_r = rewards.mean(dim=1, keepdim=True)
std_r = rewards.std(dim=1, keepdim=True)
advantages = (rewards - mean_r) / (std_r + eps)

# 4. Policy Update: PPO-style clipped objective
for epoch in range(num_ppo_epochs):
    log_probs = policy_model.log_prob(responses)
    old_log_probs = old_policy.log_prob(responses)
    ratio = (log_probs - old_log_probs).exp()
    
    loss1 = ratio * advantages
    loss2 = ratio.clamp(1-clip, 1+clip) * advantages
    policy_loss = -torch.min(loss1, loss2).mean()
    
    kl_loss = kl_divergence(policy_model, ref_model)
    total_loss = policy_loss + beta * kl_loss
    
    total_loss.backward()
    optimizer.step()
```

### verl 中的 GRPO 配置

```yaml
algorithm:
  name: grpo
  grpo:
    num_generations: 16        # 每个 prompt 的采样数
    beta: 0.01                 # KL 惩罚系数
    clip_ratio: 0.2            # PPO clip 范围
    num_ppo_epochs: 1          # 每个 batch 更新几次
    temperature: 1.0           # 采样温度
    
    # reward 配置
    reward_normalize: true     # reward 标准化
    reward_clip: 10.0          # reward 裁剪范围
```

## PPO（Proximal Policy Optimization）

经典的 RLHF 算法，verl 完整支持。

### 与 GRPO 的关键区别

```
PPO:  需要 Critic (Value) Model → 估计 state value → 计算 GAE advantage
GRPO: 不需要 Critic             → 组内采样        → 计算相对 advantage
```

PPO 的 advantage 计算用 GAE（Generalized Advantage Estimation）：

$$\hat{A}_t^{GAE(\gamma,\lambda)} = \sum_{l=0}^{T-t} (\gamma\lambda)^l \delta_{t+l}$$

其中 $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ 是 TD error。

### 什么时候用 PPO vs GRPO

- **GRPO 优势**：省显存（不需要 Critic），训练更简单，在 outcome-based reward 场景下效果好
- **PPO 优势**：dense reward（过程奖励）场景更合适，因为 GAE 能利用逐步的 reward 信号
- **实践经验**：大多数场景（数学、代码、通用对齐）GRPO 就够了

## DAPO（Decoupled Alignment from PPO）

GRPO 的改进版，主要改动：

### 1. 去掉 KL 惩罚

DAPO 认为 KL 惩罚限制了模型的探索能力，直接去掉 $\beta \cdot D_{KL}$。为了防止模型偏移过远，使用更激进的 clip：

```python
# DAPO 的 clip 是非对称的
clip_high = 0.28  # 允许正向更大的更新
clip_low = 0.2    # 限制负向更新
```

### 2. 动态采样（Token-level filtering）

DAPO 过滤掉全对或全错的 prompt（因为这些提供不了有效的 gradient signal）：

```python
# 如果一个 prompt 下所有 response 都对（或都错），跳过
group_rewards = rewards[prompt_idx]
if group_rewards.std() < eps:  # 方差为 0，说明全对或全错
    continue  # 这个 prompt 不参与训练
```

### 3. 长度惩罚

避免模型生成过长的无用内容：

```python
length_penalty = max(0, len(response) - target_length) * penalty_coef
reward = task_reward - length_penalty
```

### verl 中的 DAPO 配置

```yaml
algorithm:
  name: dapo
  dapo:
    num_generations: 16
    clip_ratio_high: 0.28
    clip_ratio_low: 0.2
    no_kl_penalty: true
    filter_groups: true          # 过滤全对/全错的 group
    length_penalty_coef: 0.001
```

## DPO（Direct Preference Optimization）

Offline 算法，不需要在线生成 response：

```
DPO 流程:
1. 准备偏好数据: (prompt, chosen_response, rejected_response)
2. 直接优化 policy，使其偏好 chosen 而非 rejected
3. 不需要 reward model，不需要 online generation
```

DPO 的损失函数：

$$\mathcal{L}_{DPO} = -\mathbb{E}\left[\log\sigma\left(\beta \log\frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log\frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right]$$

verl 中 DPO 的使用场景主要是作为 GRPO 的补充——先做 DPO 对齐基本偏好，再用 GRPO 优化推理能力。

## RLOO（REINFORCE Leave-One-Out）

一种简单但有效的 baseline 估计方法：

```python
# 对于每个 response i，用其他 G-1 个 response 的平均 reward 作为 baseline
for i in range(G):
    baseline_i = (sum(rewards) - rewards[i]) / (G - 1)
    advantage_i = rewards[i] - baseline_i
```

相比 GRPO 的组内均值 baseline，RLOO 的 baseline 不包含当前 response 自身，理论上方差更小。但实际上效果差异不大。

## 算法选择指南

```
任务类型:
├── 数学/代码（可验证答案） → GRPO 或 DAPO
├── 通用对齐（偏好数据） → DPO → GRPO
├── 需要 dense reward → PPO
└── 简单快速实验 → RLOO

模型规模:
├── < 7B  → 什么都行，TRL 就够
├── 7B-32B → verl GRPO
└── > 32B → verl GRPO/DAPO (需要多节点)

数据类型:
├── 有偏好对 → DPO
├── 有正确答案 → GRPO (rule-based reward)
└── 有 reward model → PPO or GRPO
```

## 相关

- [[AI/3-LLM/Frameworks/verl/verl 概述|verl 概述]] — verl 框架总览
- [[AI/3-LLM/RL/GRPO/GRPO 深度理解|GRPO 深度理解]] — GRPO 算法深度解析
- [[AI/3-LLM/RL/PPO/PPO 原理|PPO 原理]] — PPO 算法详解
- [[AI/3-LLM/RL/DAPO/DAPO-verl实践|DAPO-verl实践]] — DAPO 实践
- [[AI/3-LLM/RL/GRPO/GRPO-verl实践|GRPO-verl实践]] — GRPO 实践
- [[AI/3-LLM/RL/PPO/PPO-verl实践|PPO-verl实践]] — PPO 实践
- [[AI/3-LLM/RL/DPO/DPO-TRL实践|DPO-TRL实践]] — DPO 实践
- [[AI/3-LLM/Architecture/DeepSeek-R1|DeepSeek-R1]] — GRPO 的大规模应用
- [[AI/3-LLM/RL/GRPO/DeepSeek-Math|DeepSeek-Math]] — GRPO 的起源
