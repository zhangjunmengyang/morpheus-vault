---
brief: "verl 配置文件详解——YAML 配置的完整字段说明；trainer/data/actor_rollout/critic/reward_model 各段配置解析；从零开始的训练配置模板，常见配置错误排查指南。"
title: "配置文件"
type: concept
domain: ai/llm/frameworks/verl
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm/frameworks/verl
  - type/concept
---
# 配置文件

> verl 使用 Hydra/OmegaConf 管理配置，所有训练参数通过 YAML 文件控制。

文档：https://verl.readthedocs.io/en/latest/examples/config.html

## 配置结构

verl 的配置分为几大块：

```yaml
# 完整的 PPO 训练配置示例
data:
  train_files: "data/train.parquet"
  val_files: "data/val.parquet"
  max_prompt_length: 512
  max_response_length: 1024
  train_batch_size: 256

actor_rollout_ref:
  model:
    path: "Qwen/Qwen2.5-7B-Instruct"
  actor:
    # 训练相关
    lr: 1e-6
    weight_decay: 0.01
    ppo_epochs: 1
    ppo_mini_batch_size: 64
    grad_clip: 1.0
    bf16: true
    gradient_checkpointing: true
  rollout:
    # 推理相关
    engine: sglang
    tensor_parallel_size: 4
    temperature: 0.7
    top_p: 0.9
    max_new_tokens: 1024
  ref:
    # Reference model
    # 通常和 actor 共享权重，frozen
    offload: false

critic:
  model:
    path: "Qwen/Qwen2.5-7B-Instruct"  # 可以和 actor 不同
  lr: 1e-5
  bf16: true

reward:
  type: "function"
  path: "my_rewards.math_reward"
  # 或者用 reward model:
  # type: "model"
  # model_path: "reward-model-path"

algorithm:
  name: "ppo"  # ppo, grpo, dapo, etc.
  gamma: 1.0
  lam: 0.95
  kl_coef: 0.05
  clip_range: 0.2
  # GRPO 特有参数
  # group_size: 8

trainer:
  total_steps: 1000
  save_freq: 100
  eval_freq: 50
  log_freq: 1
  project_name: "my-ppo-training"
```

## 关键配置解读

### data 部分

```yaml
data:
  # 数据格式：parquet，包含 prompt 和可选的 ground_truth
  train_files: "data/train.parquet"
  
  # 长度配置 — 直接影响显存和速度
  max_prompt_length: 512     # prompt 超过这个长度会被截断
  max_response_length: 1024  # 生成的最大 token 数
  # 总长度 = prompt + response，这个决定了 KV cache 大小
  
  # Batch size — RL 的核心参数
  train_batch_size: 256  # 每个 RL step 用多少条数据
  # 经验: batch_size 越大，advantage 估计越准，但需要更多 GPU
```

### actor_rollout_ref 部分

这是 verl 最独特的设计：actor、rollout、reference 共享同一个模型配置。

```yaml
actor_rollout_ref:
  model:
    path: "Qwen/Qwen2.5-7B-Instruct"
    # 这一个路径同时用于：
    # 1. Actor (可训练)
    # 2. Rollout engine (推理)  
    # 3. Reference model (frozen)
    
  actor:
    lr: 1e-6            # RL 训练用很小的学习率
    ppo_epochs: 1       # 每批数据训几个 epoch
    ppo_mini_batch_size: 64  # 一个 micro-batch 多大
    # 注意: ppo_mini_batch_size 受显存限制
    # train_batch_size / ppo_mini_batch_size = gradient accumulation steps
    
  rollout:
    engine: sglang       # 推理引擎
    tensor_parallel_size: 4  # 推理时的 TP
    # 和 training 的 TP 可以不同！
```

### algorithm 部分

```yaml
# PPO 参数
algorithm:
  name: "ppo"
  gamma: 1.0     # discount factor，LLM 通常用 1.0（不 discount）
  lam: 0.95      # GAE 的 λ
  kl_coef: 0.05  # KL penalty 系数
  clip_range: 0.2  # PPO clipping ε
  
# GRPO 参数
algorithm:
  name: "grpo"
  group_size: 8     # 每个 prompt 采样几条
  kl_coef: 0.01     # GRPO 通常用更小的 KL 系数
  # 不需要 gamma, lam（没有 value function）
```

## Hydra Override

命令行覆盖配置，不需要改文件：

```bash
# 基础用法
python train.py \
  actor_rollout_ref.model.path=Qwen/Qwen2.5-14B-Instruct \
  actor_rollout_ref.actor.lr=5e-7 \
  data.train_batch_size=128

# 多 run 实验
python train.py -m \
  actor_rollout_ref.actor.lr=1e-6,5e-7,1e-7 \
  algorithm.kl_coef=0.01,0.05,0.1
# 会自动排列组合跑 9 个实验
```

## 常见配置错误

```yaml
# ❌ TP size 大于 GPU 数量
rollout:
  tensor_parallel_size: 8  # 但只有 4 张卡

# ❌ batch_size 不能被 mini_batch_size 整除
data:
  train_batch_size: 256
actor:
  ppo_mini_batch_size: 100  # 256 / 100 不是整数

# ❌ max_response_length 太大导致 OOM
data:
  max_response_length: 8192  # KV cache 吃爆显存

# ✅ 安全的起步配置（7B, 8xA100-80GB）
data:
  max_prompt_length: 512
  max_response_length: 1024
  train_batch_size: 128
actor_rollout_ref:
  actor:
    lr: 1e-6
    ppo_mini_batch_size: 16
  rollout:
    tensor_parallel_size: 4
```

## 相关

- [[verl 训练参数|verl 训练参数]] — 参数含义深入解读
- [[性能调优|性能调优]] — 配置和性能的关系
- [[硬件资源预估|硬件资源预估]] — 根据配置估算资源
- [[Post-Training 数据准备|Post-Training 数据准备]] — 数据格式要求
