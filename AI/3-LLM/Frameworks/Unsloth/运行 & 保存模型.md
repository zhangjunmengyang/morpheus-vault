---
brief: "Unsloth 运行与保存模型——模型加载（BnB 4bit/8bit）、推理运行、LoRA adapter 合并导出的完整流程；保存为 GGUF/safetensors/HuggingFace Hub 的选项对比。"
title: "运行 & 保存模型"
type: concept
domain: ai/llm/frameworks/unsloth
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm/frameworks/unsloth
  - type/concept
---
# 运行 & 保存模型

> 文档：https://docs.unsloth.ai/basics/running-and-saving-models

训练完模型后，怎么跑推理、怎么保存、保存成什么格式——这些看似简单的事情其实有很多细节。

## 推理

### 基础推理

```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="./my_finetuned_model",
    max_seq_length=2048,
)

# 切换到推理模式（关闭 dropout 等）
FastLanguageModel.for_inference(model)

messages = [
    {"role": "user", "content": "解释一下 Spark 的 Shuffle 机制"}
]

inputs = tokenizer.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,  # 添加 assistant 头
    return_tensors="pt",
).to("cuda")

outputs = model.generate(
    input_ids=inputs,
    max_new_tokens=512,
    temperature=0.7,
    top_p=0.9,
    repetition_penalty=1.1,
)

# 只解码新生成的部分
response = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True)
print(response)
```

### 流式推理

```python
from transformers import TextStreamer

streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

model.generate(
    input_ids=inputs,
    max_new_tokens=512,
    streamer=streamer,
    temperature=0.7,
)
```

## 保存格式

### 1. LoRA Adapter（推荐开发阶段）

只保存训练过的 LoRA 权重，体积小（通常 10-100MB）：

```python
model.save_pretrained("./lora_model")
tokenizer.save_pretrained("./lora_model")

# 加载时需要基座模型 + adapter
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="./lora_model",
)
```

### 2. 合并后的完整模型（推荐部署）

将 LoRA 权重合并回基座模型：

```python
# 16bit 合并（精度最高）
model.save_pretrained_merged(
    "merged_16bit",
    tokenizer,
    save_method="merged_16bit",
)

# 4bit 量化（体积最小）
model.save_pretrained_merged(
    "merged_4bit",
    tokenizer,
    save_method="merged_4bit_forced",
)
```

### 3. GGUF 格式（Ollama/llama.cpp）

```python
# 导出为 GGUF，可直接在 Ollama 中使用
model.save_pretrained_gguf(
    "model_gguf",
    tokenizer,
    quantization_method="q4_k_m",  # 常用量化级别
)
```

GGUF 量化级别选择：

| 量化 | 大小(7B) | 质量 | 适合 |
|------|----------|------|------|
| q8_0 | ~7GB | 接近原始 | 显存充足 |
| q5_k_m | ~5GB | 很好 | 推荐默认 |
| q4_k_m | ~4GB | 好 | 性价比最高 |
| q3_k_m | ~3GB | 可用 | 资源受限 |
| q2_k | ~2.5GB | 有损 | 不推荐 |

### 4. 推送到 HuggingFace Hub

```python
# 推送 LoRA adapter
model.push_to_hub("your-username/model-name")
tokenizer.push_to_hub("your-username/model-name")

# 推送合并后的模型
model.push_to_hub_merged(
    "your-username/model-name-merged",
    tokenizer,
    save_method="merged_16bit",
)

# 推送 GGUF
model.push_to_hub_gguf(
    "your-username/model-name-gguf",
    tokenizer,
    quantization_method="q4_k_m",
)
```

## 在 Ollama 中使用

```bash
# 从 GGUF 文件创建 Ollama 模型
ollama create my-model -f Modelfile

# Modelfile 内容
FROM ./model_gguf/unsloth.Q4_K_M.gguf
TEMPLATE """{{ .System }}
{{ .Prompt }}"""
PARAMETER temperature 0.7
PARAMETER stop "<|im_end|>"
```

## 常见问题

- **推理速度慢**：确认调用了 `FastLanguageModel.for_inference(model)`
- **输出乱码**：检查 chat template 是否匹配
- **合并 OOM**：用 `save_method="merged_4bit_forced"` 或分步合并

## 相关

- [[Checkpoint|Checkpoint 管理]]
- [[Unsloth 概述|Unsloth 概述]]
- [[量化|量化]]
- [[Ollama|Ollama]]
- [[vLLM|vLLM]]
