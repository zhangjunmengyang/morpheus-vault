---
brief: "Unsloth 训练示例概述——官方示例任务全景：SFT/DPO/GRPO/CPT 等场景的配置模板对比；选型指南：不同训练目标用哪个 trainer、数据格式有何差异。"
title: "训练示例"
type: project
domain: ai/llm/frameworks/unsloth
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm/frameworks/unsloth
  - type/project
---
# 训练示例概述

> 调参指南：https://docs.unsloth.ai/get-started/fine-tuning-llms-guide
> 实践案例：https://docs.unsloth.ai/get-started/fine-tuning-llms-guide/tutorial-how-to-finetune-llama-3-and-use-in-ollama

## Unsloth 训练模式

Unsloth 支持三种主要训练模式：

| 模式 | 场景 | 数据需求 | 难度 |
|------|------|---------|------|
| SFT | 指令跟随、格式对齐 | (input, output) 对 | ⭐ |
| CPT | 领域知识注入 | 纯文本语料 | ⭐⭐ |
| RL (GRPO) | 推理能力提升 | prompt + reward function | ⭐⭐⭐ |

## SFT 最小示例

```python
from unsloth import FastLanguageModel
from trl import SFTTrainer, SFTConfig
from datasets import load_dataset

# 1. 加载模型
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2.5-7B-Instruct",
    max_seq_length=2048,
    load_in_4bit=True,
)

# 2. LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                     "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
)

# 3. 数据
dataset = load_dataset("yahma/alpaca-cleaned", split="train")

def format_instruction(sample):
    return tokenizer.apply_chat_template(
        [
            {"role": "user", "content": sample["instruction"]},
            {"role": "assistant", "content": sample["output"]},
        ],
        tokenize=False,
    )

# 4. 训练
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    args=SFTConfig(
        output_dir="sft_output",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        num_train_epochs=1,
        bf16=True,
        max_seq_length=2048,
    ),
    formatting_func=format_instruction,
    tokenizer=tokenizer,
)

trainer.train()
```

## 调参经验总结

### 学习率

```python
# LoRA 微调的 LR 比全参数大很多！
# 全参数微调: 1e-5 ~ 5e-5
# LoRA 微调:  1e-4 ~ 3e-4 (10x 以上)
# RL (GRPO):  1e-6 ~ 1e-5

# 原因: LoRA 的参数量远少于全参数，
# 需要更大的 LR 来让有限的参数产生足够大的变化
```

### LoRA rank (r)

```python
# r 的选择不是越大越好
# 
# r=8:   最轻量，适合风格迁移、格式调整
#        可训练参数 ≈ 50M (7B 模型)
# 
# r=16:  标准选择，大部分任务
#        可训练参数 ≈ 100M
# 
# r=32:  重任务，如新语言学习、领域适配
#        可训练参数 ≈ 200M
# 
# r=64:  接近全参数效果
#        可训练参数 ≈ 400M
# 
# 经验: 先用 r=16 跑个 baseline，效果不够再加
```

### Batch Size 与 Gradient Accumulation

```python
# effective_batch_size = per_device_batch_size × gradient_accumulation × n_gpus
#
# SFT 推荐: effective_batch_size = 16~64
# RL 推荐: effective_batch_size = 8~32
#
# 单卡 24G 的典型配置:
config = SFTConfig(
    per_device_train_batch_size=2,    # 2 才不 OOM
    gradient_accumulation_steps=8,    # 累积到 effective=16
)

# 单卡 80G 的配置:
config = SFTConfig(
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,    # effective=16
)
```

### Epoch 数

```python
# SFT 的过拟合比你想象的快得多:
# - 小数据集 (<10k): 1-3 epochs
# - 中数据集 (10k-100k): 1-2 epochs
# - 大数据集 (>100k): 通常 1 epoch 就够
#
# 观察 train loss: 如果 loss 接近 0，八成过拟合了
# 观察 eval loss: 上升就该停了
```

## 常见问题排查

### 1. 训练后模型变傻了

```python
# 原因: 过拟合或灾难性遗忘
# 解决:
# a) 减少 epoch
# b) 加入通用对话数据 (10-20%)
# c) 降低 LR
# d) 减小 LoRA rank
```

### 2. loss 不降

```python
# 检查清单:
# a) 数据格式是否正确? (检查 tokenizer 输出)
# b) chat template 是否匹配? (Qwen vs Llama 格式不同)
# c) LR 是否太小? (LoRA 需要 1e-4 级别)
# d) 数据是否太少? (<100 条可能不够)
```

### 3. 生成重复

```python
# 训练后模型疯狂重复
# 原因: 通常是过拟合 + temperature 太低
# 解决:
# a) 推理时 temperature=0.7, top_p=0.9
# b) 加入 repetition_penalty=1.1
# c) 检查训练数据是否有大量重复样本
```

## 从训练到部署

```python
# 1. 训练完成后保存
model.save_pretrained("my_model_lora")

# 2. 合并 LoRA
model.save_pretrained_merged(
    "my_model_merged",
    tokenizer,
    save_method="merged_16bit",
)

# 3. 导出 GGUF 给 Ollama
model.save_pretrained_gguf(
    "my_model_gguf",
    tokenizer,
    quantization_method="q4_k_m",
)

# 4. 创建 Ollama Modelfile
"""
FROM ./my_model_gguf/unsloth.Q4_K_M.gguf
TEMPLATE "{{ .System }}\n{{ .Prompt }}"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
"""

# 5. ollama create my-model -f Modelfile
# 6. ollama run my-model
```

## 相关

- [[AI/3-LLM/Frameworks/Unsloth/Unsloth 概述|Unsloth 概述]]
- [[AI/3-LLM/Frameworks/Unsloth/gpt-oss 训练|gpt-oss 训练]]
- [[AI/3-LLM/Frameworks/Unsloth/Qwen3 训练|Qwen3 训练]]
- [[AI/3-LLM/Frameworks/Unsloth/运行 & 保存模型|运行 & 保存模型]]
- [[AI/3-LLM/Frameworks/Unsloth/量化 & 显存预估|量化 & 显存预估]]
- [[AI/3-LLM/Frameworks/Unsloth/Chat Templates|Chat Templates]]
- [[AI/3-LLM/Frameworks/Unsloth/多卡并行|多卡并行]]
