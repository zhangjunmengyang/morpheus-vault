---
brief: "Unsloth 量化详解——BitsAndBytes 4bit/8bit 量化在 Unsloth 中的实现机制；NF4/FP4 格式对比、double quantization、量化感知训练（QAT）的 Unsloth 支持现状。"
title: "量化"
type: concept
domain: ai/llm/frameworks/unsloth
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm/frameworks/unsloth
  - type/concept
---
# 量化

量化（Quantization）是将模型权重从高精度（如 FP32/BF16）转换为低精度（如 INT8/INT4）的技术，目的是减少显存占用和加速推理。Unsloth 在量化方面做了深度集成。

## 量化基础

### 为什么要量化

一个 7B 参数的模型：
- FP32：7B × 4 bytes = **28 GB**
- BF16：7B × 2 bytes = **14 GB**
- INT8：7B × 1 byte = **7 GB**
- INT4：7B × 0.5 byte = **3.5 GB**

量化让消费级 GPU（如 RTX 4090 24GB）也能跑大模型。

### 常见量化方法

| 方法 | 位宽 | 特点 |
|------|------|------|
| GPTQ | 4-bit | Post-training，需要校准数据集 |
| AWQ | 4-bit | 保护重要权重，推理速度快 |
| GGUF | 2-8 bit | llama.cpp 格式，CPU/GPU 混合推理 |
| bitsandbytes | 4/8-bit | 训练时量化（QLoRA），HF 集成好 |

## Unsloth 的量化方案

### 4-bit QLoRA 训练

Unsloth 的核心卖点之一是高效的 4-bit QLoRA 训练：

```python
from unsloth import FastLanguageModel

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen2.5-7B-Instruct",
    max_seq_length=4096,
    load_in_4bit=True,           # 4-bit 量化加载
    dtype=None,                   # 自动检测最佳 dtype
)

# 添加 LoRA adapter（只训练 adapter，base model 保持 4-bit）
model = FastLanguageModel.get_peft_model(
    model,
    r=16,                         # LoRA rank
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                     "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing="unsloth",  # Unsloth 优化版 gradient checkpointing
)
```

Unsloth 在 4-bit 训练中做了几个关键优化：

1. **Custom CUDA kernels** —— 手写 Triton kernel 做 4-bit dequant + matmul 融合
2. **Memory-efficient gradient checkpointing** —— 比 HF 默认实现省 ~30% 显存
3. **Fused operations** —— RoPE、RMSNorm、CrossEntropy 融合

### 导出量化模型

训练完成后，Unsloth 支持导出多种量化格式：

```python
# 保存为 GGUF 格式（用于 llama.cpp / Ollama）
model.save_pretrained_gguf(
    "output_dir",
    tokenizer,
    quantization_method="q4_k_m"  # 常用的 4-bit 量化方案
)

# 可选的量化方法：
# q4_k_m  — 4-bit，推荐的平衡方案
# q5_k_m  — 5-bit，质量更好
# q8_0    — 8-bit，接近无损
# q2_k    — 2-bit，极致压缩（质量损失较大）

# 保存为 16-bit（用于 vLLM 部署）
model.save_pretrained_merged(
    "output_dir_16bit",
    tokenizer,
    save_method="merged_16bit"
)
```

### 量化质量评估

不同量化方法的 perplexity 损失（以 Qwen2.5-7B 为例）：

- BF16 baseline: ~6.5 PPL
- Q8_0: ~6.5 PPL（几乎无损）
- Q5_K_M: ~6.7 PPL（轻微损失）
- Q4_K_M: ~6.9 PPL（可接受）
- Q2_K: ~8.5 PPL（明显下降）

经验法则：**Q4_K_M 是性价比最高的选择**，大多数任务感知不到与 BF16 的差异。

## 量化的坑

1. **不是所有层都适合量化** —— embedding 层和 lm_head 量化会导致较大的性能损失，Unsloth 默认不量化这些层
2. **校准数据很重要** —— GPTQ/AWQ 的校准数据集应该和目标任务领域相关
3. **量化 + LoRA merge 的顺序** —— 先 merge LoRA 再量化，不要在 4-bit 基础上 merge
4. **推理框架兼容性** —— 不同框架支持的量化格式不同，提前规划部署方案

## 我的看法

Unsloth 把 4-bit QLoRA 训练做到了极致 —— 显存省一半，速度快一倍，这对个人开发者和小团队意义重大。但量化不是免费午餐，在需要精细控制的场景（如数学推理、代码生成），建议先用高精度训练，确认效果后再量化部署。

## 相关

- [[AI/3-LLM/Frameworks/Unsloth/Unsloth 概述]]
- [[AI/3-LLM/Frameworks/Unsloth/Checkpoint]]
- [[AI/3-LLM/Frameworks/Unsloth/运行 & 保存模型]]
- [[AI/3-LLM/Infra/分布式训练|分布式训练]]
