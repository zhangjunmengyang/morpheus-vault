---
brief: "小规模训练手册——在有限算力（单卡/多卡小集群）下构建高质量 LLM 的工程指南；数据效率最大化/LoRA 配置/评估策略的实战经验；适合个人研究者和小团队的 LLM 训练实践参考。"
title: "小规模训练手册：构建世界级LLM的秘密"
type: project
domain: ai/llm
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm
  - type/project
---
# 小规模训练手册：构建世界级LLM的秘密

- 《The Smol Training Playbook: The Secrets to Building World-Class LLMs》
- 链接：https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook#introduction
# 前置 learn by AI 

有较多错误，尚未校验，需要学习时候谨慎验证

```
# Introduction：为什么小模型训练是LLM工程师的“必修课”？  
在大模型（如GPT-4、Claude 3）占据 headlines 的时代，《Smol Training Playbook》却凭借“小模型训练”主题收获2000+社区点赞——这背后藏着一个被行业验证的核心趋势：**大多数企业和开发者的LLM落地需求，不需要“千亿参数的全能模型”，而是“低成本、高适配、可部署”的小模型（通常指7B~34B参数，如Llama 2 7B、Mistral 7B）**。  

本手册的定位并非“理论科普”，而是**实战导向的小模型训练指南**：它基于Hugging Face生态（Transformers、PEFT、Accelerate等工具），手把手教你用“单/双卡GPU”训练出能满足垂直场景（如企业知识库问答、代码补全、行业客服）的LLM，同时规避“训练崩显存、效果不达标、部署卡瓶颈”等经典坑点。  


## 01 我们为什么需要“小模型”？—— 从行业痛点看价值  
大模型的“全能性”背后是极高的成本门槛：训练一次千亿参数模型需数千万美元，推理需多卡GPU集群支持，这对90%以上的企业和开发者而言是“不可承受之重”。而小模型恰好解决了这些痛点，其核心价值可概括为三点：  

### 1. 资源门槛极低，个人/中小企业也能上手  
- 训练层面：7B参数模型用LoRA（低秩适应）微调，单张RTX 3090（24GB显存）即可运行，训练时长通常在几小时到1天内（对比大模型的“数周+百卡集群”）；  
- 推理层面：小模型量化后（如INT4）可在CPU（如Intel i7）或边缘设备（如手机端）运行，无需依赖昂贵的GPU服务器。  

### 2. 场景适配性更强，避免“大材小用”  
大模型的“全能性”往往伴随“垂直场景精度不足”（比如用GPT-4做“电商客服问答”，可能不如专门训练的7B模型精准）。小模型可通过“垂直领域数据微调”快速适配需求，例如：  
- 给7B模型喂入“医疗病历+问答数据”，就能得到适配基层医院的问诊助手；  
- 用企业内部文档（如产品手册、规章制度）微调，可生成“专属知识库问答模型”。  

### 3. 部署灵活，满足实时性需求  
大模型推理时的“高延迟”（通常几百毫秒）无法满足实时场景（如直播弹幕互动、工业设备实时诊断），而小模型的推理速度可达到“单token生成<10毫秒”，完美适配低延迟需求。  


## 02 本手册的核心目标：让你具备“小模型全流程能力”  
无论你是刚接触LLM的工程师，还是想优化现有模型的开发者，学完本手册后，你将掌握以下实战能力：  
1. **选型能力**：根据场景需求（如“是否需要多轮对话”“是否需量化”）选择合适的小模型基座（如Llama 2 7B vs Mistral 7B）；  
2. **训练能力**：熟练使用LoRA、量化训练等技术，在有限GPU资源下完成模型微调，且能解决“显存溢出”“训练震荡”等问题；  
3. **评估能力**：用通用指标（如MMLU）和自定义测试集（如企业内部数据）验证模型效果，避免“主观判断偏差”；  
4. **部署能力**：将训练好的小模型通过量化、推理框架优化（如vLLM），部署到CPU/GPU/边缘设备，满足实际业务的性能需求。  


## 03 学习前置：你需要具备这些基础  
为了让后续学习更高效，建议先掌握以下基础知识（若有缺失，可参考文末“补充材料”快速补全）：  
- **工具基础**：熟悉Python（基础语法、Pandas数据处理）、PyTorch（张量操作、模型训练流程）；  
- **LLM基础**：了解Transformer架构核心概念（注意力机制、编码器/解码器）、LLM微调的基本逻辑（指令微调、监督微调SFT）；  
- **环境基础**：会使用Docker（基础命令如`docker pull` `docker run`）、了解Hugging Face Hub的基本操作（下载模型/数据集）。  


## 补充材料：前置知识快速补全  
- PyTorch基础：[PyTorch官方教程（60分钟入门）](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)  
- Transformer架构：[The Illustrated Transformer（可视化讲解）](https://jalammar.github.io/illustrated-transformer/)  
- Hugging Face Hub入门：[Hugging Face Hub官方指南](https://huggingface.co/docs/hub/quick-start)  

接下来，我们将进入第一个核心章节：**小模型训练的“技术栈选型”——从基座模型到训练框架，一步选对不踩坑**。
```

```
# 第一章 小模型训练的技术栈选型：从基座到工具，一步选对不踩坑  
选型是小模型训练的“第一关”——选对基座模型能减少50%的后续优化成本，用对训练框架能避免“显存溢出”“训练停滞”等低级问题，选准数据工具能保证输入数据的质量。本章会聚焦**算法工程师最关心的3个核心选型维度**，每个维度都附带“场景匹配建议+实操代码片段+避坑指南”，确保你能直接落地。


## 1.1 基座模型选型：7B/13B参数，该怎么挑？  
基座模型是小模型训练的“地基”，直接决定了最终模型的上限。目前开源小模型的主流参数规模是**7B（70亿）和13B（130亿）**，核心选型逻辑是“**场景需求→资源匹配→许可合规**”，而非盲目追求大参数。

### 1.1.1 主流开源小模型对比（算法工程师必看）  
我们筛选了社区活跃度高、落地案例多的5个基座模型，从“适用场景、显存需求、中文支持、许可”4个关键维度做对比，帮你快速定位：

| 基座模型       | 参数规模 | 核心优势                                  | 适用场景                          | 训练显存需求（LoRA微调） | 中文支持 | 开源许可（商用友好度）       |
|----------------|----------|-------------------------------------------|-----------------------------------|--------------------------|----------|------------------------------|
| **Llama 2**    | 7B/13B   | 通用能力强，社区工具链最完善（如LoRA插件） | 通用对话、文本生成、代码补全      | 7B≈8GB，13B≈12GB         | 一般     | Meta许可（需申请，商用可行） |
| **Mistral 7B** | 7B       | 推理速度快（比Llama 2 7B快30%），支持多轮 | 实时对话、边缘设备部署            | ≈6GB                     | 一般     | Apache 2.0（商用完全自由）   |
| **Qwen 7B**    | 7B/13B   | 中文能力顶尖，支持多语言                  | 中文问答、中文文案生成            | 7B≈7GB，13B≈11GB         | 优秀     | Apache 2.0（商用完全自由）   |
| **Falcon 7B**  | 7B       | 上下文窗口大（支持8k tokens）             | 长文本理解（如文档总结）          | ≈7GB                     | 一般     | Apache 2.0（商用完全自由）   |
| **Phi-2**      | 2.7B     | 参数极小（2.7B），CPU可推理               | 轻量场景（如嵌入式设备、小程序）  | ≈4GB                     | 一般     | MIT许可（商用完全自由）      |

### 1.1.2 选型决策流程（实战方法论）  
算法工程师不用纠结“哪个模型最好”，按以下3步就能快速定：  
1. **先定场景核心需求**：  
   - 若需**中文能力**：优先选Qwen 7B/13B（字节跳动开源，中文语料训练多）；  
   - 若需**实时低延迟**：选Mistral 7B（推理速度最快，边缘设备友好）；  
   - 若需**长文本处理**：选Falcon 7B（8k上下文，比多数模型的4k窗口更实用）；  
   - 若需**最小部署成本**：选Phi-2（2.7B参数，CPU推理无压力）。  

2. **再匹配硬件资源**：  
   - 单卡GPU（显存≤12GB）：优先7B模型（如Mistral 7B，LoRA微调仅需6GB）；  
   - 单卡GPU（显存≥16GB）：可尝试13B模型（如Qwen 13B，LoRA微调需11GB）；  
   - 无GPU（仅CPU）：只能选Phi-2（2.7B参数，CPU推理耗时可接受）。  

3. **最后查许可合规**：  
   - 商用场景避坑：Llama 2需要在Meta官网申请商用许可（免费但需审核），而Mistral、Qwen、Falcon均为Apache 2.0/MIT许可，下载后可直接商用，无需申请。  


### 1.1.3 避坑指南：这3个错误别犯  
- ❌ 盲目选13B模型：13B的训练/推理显存需求比7B高50%，若硬件不够，会频繁出现“Out of Memory”，反而不如7B模型训练稳定；  
- ❌ 忽略模型版本：同一模型的不同版本差异大（如Mistral 7B有“base”和“instruct”版），“base”版是基础模型（需微调），“instruct”版已做指令微调（可直接用），别下错版本；  
- ❌ 不验证模型兼容性：部分模型（如Llama 2）需要特定库支持（如`transformers>=4.31.0`），下载前先查Hugging Face模型页的“Requirements”，避免版本不兼容。  


### 1.1.4 实操：用Hugging Face下载基座模型  
以“下载Mistral 7B”为例，一行代码即可（需先安装`transformers`和`accelerate`）：  
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# 模型名称（Hugging Face Hub地址）
model_name = "mistralai/Mistral-7B-v0.1"
# 加载模型（device_map="auto"自动分配GPU/CPU资源）
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto"  # 自动匹配模型精度（如FP16/FP32），节省显存
)
# 加载Tokenizer（处理文本输入）
tokenizer = AutoTokenizer.from_pretrained(model_name)
```


## 1.2 训练框架选型：LoRA微调vs全参数训练，该用啥？  
小模型训练分两种场景：**轻量微调（仅训练部分参数，如LoRA）** 和**全参数训练（训练所有模型参数）**。前者是算法工程师的主流选择（省显存、快），后者仅在“需大幅修改模型能力”时用。核心框架围绕Hugging Face生态展开，搭配加速工具解决“显存不足”问题。

### 1.2.1 轻量微调框架：首选PEFT（参数高效微调工具）  
PEFT（Parameter-Efficient Fine-Tuning）是Hugging Face官方库，专门用于“少参数微调”，支持LoRA、Prefix Tuning等技术，其中**LoRA（Low-Rank Adaptation）是小模型的黄金选择**——仅训练原模型0.1%~1%的参数，显存需求降低60%以上，训练速度提升3倍。

#### 核心优势（对算法工程师的价值）：  
- 显存友好：7B模型用LoRA微调，单卡8GB显存即可（全参数训练需24GB+）；  
- 训练稳定：仅更新低秩矩阵，避免全参数训练时的“过拟合”“训练震荡”；  
- 可复用性强：训练好的LoRA权重可挂载到不同基座模型（如把“医疗LoRA”挂到Llama 2或Mistral上）。  

#### 实操：用PEFT加载LoRA  
```python
from peft import LoraConfig, get_peft_model

# 1. 配置LoRA参数（核心调参项）
lora_config = LoraConfig(
    r=16,  # 秩（Rank）：越大效果越好，但显存需求越高（常用16/32）
    lora_alpha=32,  # 缩放因子（通常是r的2倍）
    target_modules=["q_proj", "v_proj"],  # 目标层（注意力层的Q/V矩阵，效果最明显）
    lora_dropout=0.05,  # Dropout比例（防止过拟合）
    bias="none",  # 不训练偏置参数（省显存）
    task_type="CAUSAL_LM"  # 任务类型（因果语言模型，适用于文本生成）
)

# 2. 将LoRA挂载到基座模型上
peft_model = get_peft_model(model, lora_config)
# 查看训练参数占比（通常仅0.5%左右）
peft_model.print_trainable_parameters()
# 输出示例：trainable params: 4,194,304 || all params: 7,249,403,904 || trainable%: 0.0579
```


### 1.2.2 全参数训练框架：Accelerate+DeepSpeed  
若需全参数训练（如“用自定义语料从头微调Qwen 7B”），需用**Accelerate（分布式训练工具）** 或**DeepSpeed（显存优化工具）**，核心解决“多卡协同”和“显存溢出”问题。  

| 框架工具       | 核心作用                                  | 适用场景                          | 显存优化效果                  |
|----------------|-------------------------------------------|-----------------------------------|-------------------------------|
| **Accelerate**  | 简化分布式训练配置（单卡→多卡一键切换）  | 单卡/双卡全参数训练              | 无额外优化，依赖PyTorch原生   |
| **DeepSpeed**   | 支持ZeRO优化（将模型参数/梯度分片存储）  | 多卡（4卡+）全参数训练            | 7B全参数训练：从24GB→10GB    |

#### 避坑点：  
- 全参数训练仅在“有大量高质量专属数据”时用（如100万+条行业数据），否则效果不如LoRA微调，还浪费资源；  
- DeepSpeed配置复杂（需写`ds_config.json`），新手建议先从Accelerate入手，熟悉后再用DeepSpeed。  


### 1.2.3 训练加速：必须掌握的2个工具  
无论轻量还是全参数训练，这两个工具能帮你“省时间”：  
1. **bitsandbytes**：4位/8位量化训练，将模型参数从FP16（2字节）压缩到INT4（0.5字节），显存需求再降50%，且不损失太多精度。  
   - 实操：加载模型时加`load_in_4bit=True`：  
     ```python
     model = AutoModelForCausalLM.from_pretrained(
         model_name,
         device_map="auto",
         load_in_4bit=True,  # 4位量化
         bnb_4bit_quant_type="nf4"  # 量化类型（NF4对LLM更友好）
     )
     ```  
2. **FlashAttention**：优化注意力计算效率，训练速度提升20%~30%，且支持更长的上下文窗口（如8k→16k）。  
   - 实操：需用支持FlashAttention的模型版本（如Mistral 7B-v0.2、Qwen 7B），加载时加`use_flash_attention_2=True`。  


## 1.3 数据工具选型：小模型“喂什么”比“怎么训”更重要  
小模型对数据质量的敏感度远高于大模型（大模型能靠海量数据“容错”，小模型不行）。数据工具的核心作用是“**高效加载→精准清洗→规范格式**”，确保输入模型的数据是“高质量、无噪声、适配任务”的。

### 1.3.1 数据加载工具：Hugging Face Datasets  
Datasets是LLM训练的“数据管道”，支持加载社区公开数据集（如Alpaca、ShareGPT）和自定义数据集（如企业内部Excel/JSON数据），且能自动处理“数据分片、多进程加载”，避免训练时“IO瓶颈”。  

#### 实操1：加载社区公开数据集（以Alpaca为例）  
```python
from datasets import load_dataset

# 加载Alpaca数据集（52k条指令-回复数据）
dataset = load_dataset("tatsu-lab/alpaca")
# 查看数据结构（包含instruction、input、output三列）
print(dataset["train"][0])
# 输出示例：
# {
#   "instruction": "Explain quantum computing in simple terms",
#   "input": "",
#   "output": "Quantum computing uses quantum bits (qubits)..."
# }
```

#### 实操2：加载自定义JSON数据集  
若你的数据是本地JSON文件（如`my_data.json`），格式如下：  
```json
[
  {"instruction": "介绍人工智能", "input": "", "output": "人工智能是..."},
  {"instruction": "计算1+1", "input": "", "output": "1+1=2"}
]
```  
加载代码：  
```python
# 加载自定义JSON数据集
dataset = load_dataset("json", data_files="my_data.json")
# 划分训练集/验证集（8:2）
dataset = dataset["train"].train_test_split(test_size=0.2)
```


### 1.3.2 数据清洗工具：3步筛掉“坏数据”  
低质量数据（如重复文本、乱码、无意义短句）会直接导致模型“学坏”（如生成重复内容、逻辑混乱），必须用以下工具清洗：  
1. **去重（Deduplication）**：用`datasets.Dataset.deduplicate()`删除重复样本；  
2. **过滤（Filtering）**：用`filter()`删除短文本（如output长度<20字符）、乱码（如含特殊符号过多）；  
3. **规范格式**：确保所有样本的“instruction-input-output”字段完整，无缺失值。  

#### 实操：数据清洗代码  
```python
def clean_dataset(example):
    # 1. 过滤output长度<20的样本
    if len(example["output"].strip()) < 20:
        return False
    # 2. 过滤含乱码的样本（如含连续特殊符号）
    if len([c for c in example["output"] if not c.isalnum() and not c.isspace()]) > 10:
        return False
    return True

# 应用清洗函数
cleaned_dataset = dataset.filter(clean_dataset)
# 去重
cleaned_dataset = cleaned_dataset.deduplicate(subset=["instruction", "output"])
# 查看清洗前后数据量（示例：从1000条→850条）
print(f"清洗前：{len(dataset['train'])}条，清洗后：{len(cleaned_dataset['train'])}条")
```


### 1.3.3 数据格式转换：适配LLM训练的“prompt模板”  
小模型训练需要将“instruction-input-output”转换为模型能理解的“prompt格式”（即告诉模型“这是指令，这是输入，你要输出什么”）。不同模型的模板略有差异，核心原则是“清晰、统一”。  

#### 示例：Mistral 7B的训练模板  
```python
def format_prompt(example):
    # 模板：<s>[INST] 指令 + 输入 [/INST] 输出 </s>
    prompt = f"<s>[INST] {example['instruction']}"
    if example["input"]:  # 若有输入，拼接上
        prompt += f"\n{example['input']}"
    prompt += f" [/INST] {example['output']} </s>"
    return {"text": prompt}  # 模型最终接收的输入是"text"字段

# 应用模板到数据集
formatted_dataset = cleaned_dataset.map(format_prompt)
```

#### 避坑点：  
- 模板必须和模型预训练时的格式一致（如Llama 2用`<<SYS>>...</SYS>\n\n`，Mistral用`[INST]...[/INST]`），否则会严重影响效果；  
- 不要在模板中加过多冗余内容（如“请你回答以下问题：”），会增加模型学习负担。  


## 第一章总结：选型决策清单  
学完本章后，你可以按以下清单快速确认选型是否正确：  
1. 基座模型：场景→中文选Qwen，实时选Mistral；硬件→12GB显存选7B，16GB选13B；商用→优先Apache 2.0许可。  
2. 训练框架：轻量微调→PEFT+LoRA+bitsandbytes；全参数训练→Accelerate（单卡）/DeepSpeed（多卡）；加速→FlashAttention。  
3. 数据工具：加载→Datasets（社区/自定义数据）；清洗→去重+过滤短文本；格式→按模型模板统一prompt。  

接下来，我们将进入**第二章：小模型训练的核心实操——从参数配置到训练监控，手把手避坑**，重点讲解LoRA微调的参数调优、训练过程中的显存管理、以及如何用TensorBoard监控训练效果。
```

```
# 第二章 小模型训练核心实操：从参数调优到监控，避坑指南全收录  
选型完成后，就进入最关键的“训练执行阶段”——这一步最容易出现“调参凭感觉、显存突然溢出、训练完发现过拟合”等问题。本章会以**“LoRA轻量微调”为核心场景**（90%算法工程师的首选方案），从“参数调优→显存管理→训练监控→异常处理”四个维度，提供“可复制的实操代码+量化指标标准+踩坑日志”，确保你能稳定训练出达标模型。


## 2.1 LoRA参数调优：3个核心参数，决定训练效果上限  
LoRA的调优逻辑是“**在显存限制内，最大化模型对任务的适配性**”，核心只需关注3个参数：`r`（秩）、`lora_alpha`（缩放因子）、`target_modules`（目标层）。其他参数（如`lora_dropout`）保持默认即可，无需过度纠结。


### 2.1.1 关键参数深度解析（原理+实操范围）  
每个参数的调整都会直接影响“训练效果”和“显存占用”，我们用表格拆解其核心逻辑：

| 参数名          | 核心作用                                                                 | 原理简化                                                                 | 推荐范围（7B模型） | 调优技巧                                                                 |
|-----------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|--------------------|--------------------------------------------------------------------------|
| **r（秩）**     | 控制LoRA低秩矩阵的维度，决定模型适配任务的能力                           | 秩越大，低秩矩阵能捕捉的“任务特征”越复杂，但显存占用越高                   | 8~64               | 1. 简单任务（如关键词提取）：r=8~16；<br>2. 复杂任务（如多轮对话）：r=32~64；<br>3. 显存不足时，优先降低r（如从32→16，显存可降20%） |
| **lora_alpha**  | 缩放LoRA更新的权重，平衡基座模型与LoRA的影响                             | alpha越大，LoRA学到的特征对最终输出的影响越强（避免被基座模型“覆盖”）       | r的2~4倍           | 固定为`r×2`（如r=16时alpha=32），无需额外调优（社区验证的最优比例）       |
| **target_modules** | 指定对模型的哪些层应用LoRA，决定“模型哪里能学”                           | 优先选**注意力层**（Q/V矩阵）——这是LLM生成逻辑的核心，调优性价比最高       | 不同模型有固定值   | 1. Llama 2/Mistral：["q_proj", "v_proj"]；<br>2. Qwen：["c_attn"]（Qwen将Q/V/K合并为c_attn）；<br>3. 不要选全连接层（如mlp），显存会增50%，效果提升有限 |


### 2.1.2 避坑指南：这3个错误90%新手会犯  
- ❌ 盲目调大`r`：比如给简单的“文本摘要”任务设`r=64`，结果显存溢出，且训练出的模型“过拟合”（只记住训练数据，泛化差）；  
- ❌ 错写`target_modules`：比如给Qwen模型设`["q_proj", "v_proj"]`，导致LoRA无法挂载到正确层，训练后模型效果无变化；  
- ❌ 忽略`bias`参数：默认`bias="none"`（不训练偏置），若设为`"all"`，显存会增30%，但效果提升微乎其微（除非数据量>100万条）。


### 2.1.3 实操：7B模型LoRA参数配置模板（直接复用）  
以“Mistral 7B做中文客服问答”为例（中等复杂度任务），配置如下：  
```python
from peft import LoraConfig

# 最优配置（显存占用≈6GB，效果适配客服问答）
lora_config = LoraConfig(
    r=32,  # 中等复杂度任务选32
    lora_alpha=64,  # r×2
    target_modules=["q_proj", "v_proj"],  # Mistral的注意力层
    lora_dropout=0.05,  # 默认值，防止过拟合
    bias="none",  # 不训练偏置，省显存
    task_type="CAUSAL_LM"  # 因果语言模型（文本生成类任务通用）
)

# 若显存不足（如只有6GB显存），降为以下配置：
# lora_config = LoraConfig(
#     r=16,
#     lora_alpha=32,
#     target_modules=["q_proj", "v_proj"],
#     lora_dropout=0.05,
#     bias="none",
#     task_type="CAUSAL_LM"
# )
```


## 2.2 训练超参数配置：学习率、Batch Size，不是越大越好  
训练超参数决定“模型如何学习”，小模型对超参数的敏感度比大模型高（大模型可通过海量数据“容错”），核心需关注**学习率（learning rate）、Batch Size、训练轮次（epoch）** 三个指标，其他参数（如优化器）保持社区默认即可。


### 2.2.1 超参数配置表（7B模型通用）  
基于Hugging Face社区100+小模型训练案例，整理出“兼顾效果与效率”的配置范围：

| 超参数          | 核心作用                                                                 | 推荐范围（7B模型） | 配置逻辑                                                                 |
|-----------------|--------------------------------------------------------------------------|--------------------|--------------------------------------------------------------------------|
| **学习率**      | 控制每步参数更新的幅度，决定“模型学多快/会不会学偏”                       | 1e-4 ~ 5e-4        | 1. LoRA微调：1e-4 ~ 3e-4（比全参数训练高10倍，因训练参数少）；<br>2. 4位量化训练：需略高（2e-4 ~ 5e-4），抵消量化精度损失；<br>3. 避坑：不要超过5e-4，否则训练震荡（loss忽高忽低） |
| **Batch Size**  | 每次训练用多少样本更新参数，决定“训练效率/显存占用”                       | 2 ~ 8（per device）| 1. 单卡24GB显存：per device=8；<br>2. 单卡12GB显存：per device=4；<br>3. 单卡8GB显存：per device=2；<br>4. 用`gradient_accumulation_steps`弥补小Batch（如Batch=2，accumulation=4，等效Batch=8） |
| **训练轮次**    | 模型遍历训练集的次数，决定“模型学没学够/会不会过拟合”                     | 2 ~ 5 epochs       | 1. 数据量<1万条：3 ~ 5 epochs；<br>2. 数据量>10万条：2 ~ 3 epochs；<br>3. 用“验证集loss”判断：若验证loss上升，立即停止（early stopping） |
| **优化器**      | 控制参数更新的策略，影响训练速度和稳定性                                 | AdamW              | 社区默认选择，无需替换；参数固定为`betas=(0.9, 0.999)`，`weight_decay=0.01`（防止过拟合） |


### 2.2.2 实操：用Transformers Trainer配置训练参数  
Hugging Face的`Trainer`类能简化训练流程，无需手写训练循环，直接传入超参数即可：  
```python
from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

# 1. 配置训练参数（核心超参数）
training_args = TrainingArguments(
    output_dir="./smol-llm-customer-service",  # 模型保存路径
    per_device_train_batch_size=4,  # 单卡Batch Size（根据显存调整）
    per_device_eval_batch_size=4,  # 验证集Batch Size
    gradient_accumulation_steps=2,  # 梯度累积（等效Batch=4×2=8）
    learning_rate=2e-4,  # LoRA+4位量化的最优学习率
    num_train_epochs=3,  # 训练轮次
    logging_steps=10,  # 每10步打印一次loss
    evaluation_strategy="epoch",  # 每轮结束后验证
    save_strategy="epoch",  # 每轮结束后保存模型
    load_best_model_at_end=True,  # 训练结束后加载最优模型（按验证loss）
    fp16=True,  # 混合精度训练（加速且省显存，需GPU支持）
    remove_unused_columns=False,  # 保留自定义字段（如text）
)

# 2. 数据拼接器（将多个样本拼接成模型输入的token序列）
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,  # 因果语言模型（生成任务）无需掩码（MLM）
)

# 3. 初始化Trainer
trainer = Trainer(
    model=peft_model,  # 挂载LoRA的模型
    args=training_args,
    train_dataset=formatted_dataset["train"],  # 训练集
    eval_dataset=formatted_dataset["test"],  # 验证集
    data_collator=data_collator,
)
```


## 2.3 显存管理：单卡8GB也能训7B模型的3个技巧  
显存不足（Out of Memory, OOM）是小模型训练的“第一杀手”，尤其是单卡用户。以下3个技巧能帮你“压榨”显存，8GB显存也能稳定训练7B模型，且几乎不损失效果。


### 2.3.1 技巧1：4位量化训练（必用）  
用`bitsandbytes`库将模型参数从FP16（2字节）压缩到INT4（0.5字节），显存占用直接降75%，且通过“NF4量化类型”保证精度损失<5%。  
- 实操：加载模型时开启4位量化（需先安装`bitsandbytes`：`pip install bitsandbytes`）：  
  ```python
  from transformers import AutoModelForCausalLM

  model = AutoModelForCausalLM.from_pretrained(
      "mistralai/Mistral-7B-v0.1",
      device_map="auto",  # 自动分配GPU/CPU资源
      load_in_4bit=True,  # 开启4位量化
      bnb_4bit_quant_type="nf4",  # 对LLM最优的量化类型
      bnb_4bit_use_double_quant=True,  # 二级量化，进一步降显存
      bnb_4bit_compute_dtype=torch.bfloat16,  # 计算时用bfloat16，平衡精度和速度
  )
  ```  
- 效果：7B模型从“无量化12GB显存”→“4位量化6GB显存”，直接节省一半。


### 2.3.2 技巧2：梯度检查点（Gradient Checkpointing）  
默认情况下，模型训练会保存所有层的激活值（用于反向传播），占用大量显存。梯度检查点会“牺牲少量速度（≈20%），换显存节省（≈30%）”——只保存关键层的激活值，其他层在反向传播时重新计算。  
- 实操：加载模型后开启梯度检查点：  
  ```python
  model.gradient_checkpointing_enable()
  # 配合LoRA使用时，需额外设置（避免训练报错）
  model.config.use_cache = False  # 关闭缓存，减少显存占用
  ```  
- 避坑：不要和“FlashAttention”同时开启（部分模型不兼容，会导致训练中断），二选一即可。


### 2.3.3 技巧3：混合精度训练（FP16/FP8）  
GPU支持的混合精度训练（如FP16）能让“计算时的显存占用”降50%——用FP16存储激活值和梯度，仅在关键步骤用FP32保证精度。  
- 实操：在`TrainingArguments`中开启`fp16=True`（需GPU支持CUDA Compute Capability ≥ 7.0，如RTX 20系列及以上）：  
  ```python
  training_args = TrainingArguments(
      # 其他参数...
      fp16=True,  # 开启FP16混合精度
      # 若GPU支持FP8（如RTX 40系列），可设为fp8=True（需安装transformers>=4.36.0）
  )
  ```  


### 2.3.4 显存不足时的排查步骤  
若仍出现OOM，按以下步骤排查：  
1. 用`nvidia-smi`查看显存占用：确认是“模型参数”还是“数据Batch”导致溢出；  
2. 优先降低`per_device_train_batch_size`（如从4→2），再开启`gradient_accumulation_steps`（如从2→4）；  
3. 若仍溢出，降低LoRA的`r`（如从32→16），或关闭梯度检查点（速度换显存）。


## 2.4 训练监控：用TensorBoard看3个关键指标，避免白跑  
训练时不能“闭着眼跑”，需实时监控指标，及时发现“过拟合”“训练停滞”等问题。核心监控工具是**TensorBoard**，重点关注3个指标：训练loss、验证loss、Perplexity（困惑度）。


### 2.4.1 关键指标解读（判断训练是否正常）  
| 指标名称         | 核心含义                                                                 | 正常趋势                                                                 | 异常情况及解决办法                                                                 |
|------------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|--------------------------------------------------------------------------|
| **训练loss**     | 模型在训练集上的预测误差，越低说明模型越“学懂”训练数据                   | 逐步下降，最后趋于平稳（如从10→3）                                       | 1. 不下降/上升：学习率太高（降为原来的1/2）；<br>2. 下降过快：可能过拟合（增加数据量或开启weight_decay） |
| **验证loss**     | 模型在未见过的验证集上的误差，反映泛化能力                               | 先下降，后趋于平稳，且与训练loss差距<1.5                                 | 1. 验证loss上升：过拟合（停止训练，加载前一轮模型）；<br>2. 验证loss远高于训练loss：数据分布不均（重新划分训练/验证集） |
| **Perplexity（PPL）** | 模型对文本的“困惑程度”，越低说明模型生成文本的“确定性”越高（越小越好）    | 与loss同步下降（PPL=e^loss，如loss=3→PPL≈20）                            | PPL上升：模型学习方向错误（检查数据格式或LoRA配置）                                                                 |


### 2.4.2 实操：开启TensorBoard监控  
1. 安装TensorBoard：`pip install tensorboard`；  
2. 在`TrainingArguments`中配置日志路径：  
   ```python
   training_args = TrainingArguments(
       # 其他参数...
       logging_dir="./logs",  # 日志保存路径
       logging_steps=10,  # 每10步记录一次指标
   )
   ```  
3. 启动TensorBoard：在终端执行`tensorboard --logdir=./logs`，浏览器访问`http://localhost:6006`查看指标。


## 第二章总结：训练前的检查清单  
训练前花5分钟确认以下配置，能避免80%的问题：  
1. LoRA配置：`target_modules`匹配模型（如Mistral用["q_proj", "v_proj"]），`r`根据任务复杂度选8~64；  
2. 超参数：学习率2e-4（LoRA+4位量化），Batch Size按显存设2~8，epoch 2~5；  
3. 显存优化：4位量化（必开）+ 混合精度（fp16）+ 梯度检查点（按需开）；  
4. 监控：开启TensorBoard，关注训练/验证loss和PPL。  

接下来，我们将进入**第三章：小模型的评估与调优——从通用指标到场景测试，确保模型能用**，重点讲解如何用MMLU/C-Eval评估通用能力，如何设计垂直场景的测试集，以及针对“生成重复、逻辑混乱”等问题的调优技巧。
```

```
# 第三章 小模型的评估与调优：从通用指标到场景落地，确保模型“能用且好用”  
训练完成不代表模型可用——很多时候会出现“通用指标高但场景效果差”“生成文本重复混乱”等问题。本章聚焦**算法工程师最关心的评估逻辑和调优手段**，先通过“通用指标+场景测试”验证模型能力，再针对常见问题（如重复生成、知识偏差）给出可落地的调优方案，最终让模型适配实际业务需求。


## 3.1 评估体系：先看通用能力，再验场景适配  
小模型的评估不能“一刀切”：通用指标（如MMLU）帮你判断模型“基础是否扎实”，场景化测试帮你确认模型“是否能解决具体问题”。两者结合才能避免“看似达标，实则无用”的坑。


### 3.1.1 通用能力评估：用3个核心指标快速“摸底”  
通用指标针对LLM的基础能力（如理解、推理、知识储备），无需自定义数据，直接用开源数据集和工具就能跑。适合快速对比不同模型（如“我的Mistral 7B微调后，比 baseline 强多少”）。

#### 主流通用评估工具对比（附实操）  
| 评估工具       | 核心能力覆盖                | 适用场景                          | 数据量  | 实操工具推荐                  | 指标解读（越低/越高越好）       |
|----------------|-----------------------------|-----------------------------------|---------|-------------------------------|----------------------------------|
| **MMLU**       | 多任务语言理解（如数学、法律） | 通用知识、逻辑推理能力评估        | 14k+    | `lm_eval`（Hugging Face库）   | 准确率越高越好（7B模型微调后目标：50%+） |
| **C-Eval**     | 中文通用能力（如语文、历史）  | 中文场景模型评估                  | 13k+    | `ceval`（官方开源工具）       | 准确率越高越好（Qwen 7B微调后目标：60%+） |
| **Perplexity** | 文本生成流畅度              | 所有生成类场景（如对话、文案）    | 自定义  | `transformers`内置函数        | 越低越好（7B模型在中文语料上目标：<50） |


#### 实操1：用`lm_eval`评估MMLU准确率  
`lm_eval`是社区最常用的LLM评估库，支持MMLU、C-Eval等200+指标，步骤如下：  
1. 安装依赖：`pip install lm_eval`  
2. 编写评估脚本（以Mistral 7B微调模型为例）：  
   ```python
   from lm_eval import evaluator, tasks
   from transformers import AutoModelForCausalLM, AutoTokenizer

   # 加载微调后的模型和Tokenizer（LoRA模型需加载PEFT权重）
   model = AutoModelForCausalLM.from_pretrained(
       "./smol-llm-customer-service",  # 模型保存路径
       device_map="auto",
       load_in_4bit=True  # 4位量化评估，省显存
   )
   tokenizer = AutoTokenizer.from_pretrained("./smol-llm-customer-service")

   # 配置评估任务（MMLU，指定5-shot评估）
   task_list = ["mmlu"]
   task_dict = tasks.get_task_dict(task_list)

   # 执行评估
   results = evaluator.simple_evaluate(
       model=model,
       tokenizer=tokenizer,
       tasks=task_dict,
       batch_size=4,  # 按显存调整
       no_cache=True
   )

   # 输出结果（重点看accuracy）
   print(f"MMLU准确率：{results['results']['mmlu']['acc']:.2%}")
   # 示例输出：MMLU准确率：52.35%（达到7B模型微调后合格线）
   ```


#### 实操2：计算中文语料的Perplexity  
Perplexity（困惑度）衡量模型对文本的“预测能力”，越低说明模型生成的文本越流畅。以中文客服语料为例：  
```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    "./smol-llm-customer-service",
    device_map="auto",
    load_in_4bit=True
)
tokenizer = AutoTokenizer.from_pretrained("./smol-llm-customer-service")

# 自定义中文测试文本（客服对话示例）
test_texts = [
    "用户：我的订单还没发货怎么办？客服：您好，麻烦提供一下订单号，我帮您查询物流状态。",
    "用户：商品收到有质量问题，能退换吗？客服：可以的，您需要在7天内申请退换货，并保持商品原包装完好。"
]

def calculate_ppl(text):
    # 编码文本
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(model.device)
    with torch.no_grad():  # 关闭梯度计算，省显存
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss  # 计算交叉熵损失
        ppl = torch.exp(loss).item()  # PPL = e^loss
    return ppl

# 计算所有文本的平均PPL
total_ppl = 0
for text in test_texts:
    ppl = calculate_ppl(text)
    total_ppl += ppl
    print(f"文本：{text[:50]}... | PPL：{ppl:.2f}")

avg_ppl = total_ppl / len(test_texts)
print(f"平均PPL：{avg_ppl:.2f}")  # 目标：<50，越低越好
```


### 3.1.2 场景化评估：自定义测试集，验证“能否解决业务问题”  
通用指标达标不代表模型能落地——比如“客服问答模型”需要能准确回答“订单查询”“退换货政策”等问题，这就需要**自定义场景测试集**。核心逻辑是“覆盖业务核心场景+量化效果+人工抽样验证”。


#### 步骤1：设计场景测试集（以客服问答为例）  
测试集需满足“3个覆盖”：  
- 覆盖核心问题：如订单查询、退换货、售后维修（占业务量80%的问题）；  
- 覆盖边缘案例：如“超过7天能否退换”“无订单号如何查询”（容易出错的场景）；  
- 覆盖负例：如“询问竞品价格”（模型应礼貌拒绝，而非乱回答）。  

测试集格式建议用JSON（便于自动化评估）：  
```json
[
  {
    "question": "我的订单下单3天了还没发货，怎么回事？",
    "reference_answer": "您好，正常订单会在48小时内发货，若超过3天未发货，可能是商品缺货或物流延迟。麻烦提供您的订单号，我帮您核实具体原因并催促发货。",
    "scene": "订单发货"
  },
  {
    "question": "我买的衣服洗了一次就掉色，能退换吗？",
    "reference_answer": "您好，根据售后政策，已洗涤的商品不支持退换货。若您认为是质量问题，可提供洗涤前后的对比照片，我们会提交给质检部门核实，若确认质量问题，会为您提供补偿方案。",
    "scene": "售后退换货（边缘案例）"
  },
  {
    "question": "你们和XX品牌的产品哪个更好？",
    "reference_answer": "您好，每个品牌的产品都有其特色，我们专注于提供[本品牌优势，如高性价比、优质售后]，建议您根据自身需求选择。若您想了解我们产品的具体功能，我可以为您详细介绍。",
    "scene": "负例（竞品对比）"
  }
]
```


#### 步骤2：自动化+人工结合评估  
- 自动化评估：用“语义相似度”衡量模型回答与参考答案的匹配度（工具：Sentence-BERT）；  
- 人工评估：抽样30%~50%的案例，按“准确性、流畅度、话术合规性”打分（1~5分）。  

**自动化评估实操（用Sentence-BERT）**：  
1. 安装依赖：`pip install sentence-transformers`  
2. 计算语义相似度：  
   ```python
   from sentence_transformers import SentenceTransformer, util
   from transformers import pipeline
   import json

   # 加载语义相似度模型（中文支持好）
   sim_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
   # 加载小模型生成回答（用pipeline简化调用）
   generator = pipeline(
       "text-generation",
       model="./smol-llm-customer-service",
       tokenizer="./smol-llm-customer-service",
       device_map="auto"
   )

   # 加载自定义测试集
   with open("customer_service_testset.json", "r", encoding="utf-8") as f:
       testset = json.load(f)

   # 评估每个案例
   total_similarity = 0
   for case in testset:
       question = case["question"]
       reference = case["reference_answer"]
       # 生成模型回答（控制长度）
       output = generator(
           question,
           max_new_tokens=100,  # 最大生成长度
           temperature=0.7,  # 控制随机性
           do_sample=True
       )
       model_answer = output[0]["generated_text"].replace(question, "").strip()
       
       # 计算语义相似度（0~1，越高越匹配）
       emb1 = sim_model.encode(model_answer, convert_to_tensor=True)
       emb2 = sim_model.encode(reference, convert_to_tensor=True)
       similarity = util.cos_sim(emb1, emb2).item()
       total_similarity += similarity
       
       print(f"问题：{question}")
       print(f"模型回答：{model_answer}")
       print(f"参考回答：{reference}")
       print(f"语义相似度：{similarity:.2f}\n")

   # 计算平均相似度（目标：>0.7，说明模型回答与业务标准匹配）
   avg_similarity = total_similarity / len(testset)
   print(f"平均语义相似度：{avg_similarity:.2f}")
   ```


## 3.2 常见问题调优：从“能用”到“好用”的4个关键技巧  
评估后常遇到“生成重复”“逻辑混乱”“领域知识不足”“回答过长”等问题，针对每个问题，我们给出“原因分析+解决方案+代码示例”，避免盲目调参。


### 3.2.1 问题1：生成文本重复（如“您好，您的订单您的订单您的订单...”）  
- **原因**：1. 训练数据有重复文本；2. 解码参数（如temperature）设置过低；3. 模型对“重复模式”过拟合。  
- **解决方案**：  
  1. 重新清洗训练数据，用`datasets`库的`deduplicate`去重，同时过滤“重复短语占比>30%”的样本；  
  2. 调整解码参数（提高temperature、增加top_p），增加生成随机性；  
  3. 训练时加入“重复惩罚”（`repetition_penalty`）。  

**实操：调整解码参数+重复惩罚**  
```python
# 生成时避免重复的参数配置
output = generator(
    "我的订单还没发货怎么办？",
    max_new_tokens=100,
    temperature=0.9,  # 从0.7提高到0.9，增加随机性
    top_p=0.9,  # 控制词汇多样性（0.9~0.95最佳）
    repetition_penalty=1.2,  # 重复惩罚（1.1~1.3，越高惩罚越强）
    do_sample=True
)
print(output[0]["generated_text"])
```


### 3.2.2 问题2：回答逻辑混乱（如“订单发货需要3天，建议您等待1周”）  
- **原因**：1. 训练数据中存在矛盾信息；2. 模型对长文本的理解能力不足（如上下文窗口不够）；3. LoRA微调时`target_modules`未覆盖关键注意力层。  
- **解决方案**：  
  1. 清洗训练数据，删除矛盾样本（如“发货时间3天”和“发货时间1周”不能同时存在）；  
  2. 若用7B模型，优先选支持8k上下文的（如Falcon 7B），或用`RoPE`技术扩展上下文窗口；  
  3. 调整LoRA的`target_modules`（如对Mistral，除了`q_proj`/`v_proj`，可增加`k_proj`/`o_proj`）。  

**实操：扩展模型上下文窗口（以Mistral 7B为例）**  
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# 加载模型时扩展上下文窗口（从4k→8k）
tokenizer = AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    device_map="auto",
    load_in_4bit=True
)

# 扩展RoPE（旋转位置编码）的上下文窗口
model.config.max_position_embeddings = 8192  # 设为8k
tokenizer.model_max_length = 8192
tokenizer.padding_side = "right"  # 避免生成时警告

# 验证长文本处理能力（输入500字长问题）
long_question = "用户：我10月1日下单买了一件外套，订单号是123456，当时客服说3天内发货，但到10月5日还没发，我联系客服后，客服让我等2天，结果到10月8日还是没发，现在我想知道到底什么时候能发货，要是不能尽快发货我就想退款了，另外退款需要多久能到账？"
output = generator(
    long_question,
    max_new_tokens=150,
    temperature=0.8,
    repetition_penalty=1.1
)
print(output[0]["generated_text"])
```


### 3.2.3 问题3：领域知识不足（如客服模型答不出“会员积分规则”）  
- **原因**：1. 训练数据中缺乏领域专属知识（如积分规则、产品参数）；2. 微调时“领域数据占比过低”（如1万条训练数据中仅100条是积分相关）。  
- **解决方案**：  
  1. 补充领域专属数据（如爬取官网积分规则、整理历史客服对话中的领域问题）；  
  2. 提高领域数据在训练集中的占比（建议≥30%），或用“领域数据优先微调”（先训通用数据，再训领域数据）；  
  3. 微调时适当增大LoRA的`r`（如从16→32），让模型更关注领域特征。  


### 3.2.4 问题4：回答过长（如用户问“怎么退款”，模型答500字）  
- **原因**：1. 训练数据中“回答长度差异大”；2. 生成时未限制`max_new_tokens`，或`temperature`过高导致冗余；3. 模型未学到“简洁回答”的指令。  
- **解决方案**：  
  1. 清洗训练数据，统一回答长度（如“简单问题≤100字，复杂问题≤200字”）；  
  2. 生成时严格限制`max_new_tokens`（如简单问题设50，复杂问题设100）；  
  3. 在prompt中加入“简洁回答”指令（如“请用不超过50字简洁回答用户问题：”）。  

**实操：通过prompt控制回答长度**  
```python
# 在prompt中加入长度约束
prompt = "请用不超过50字简洁回答用户问题：用户问“怎么申请退款？”"
output = generator(
    prompt,
    max_new_tokens=50,  # 双重限制
    temperature=0.7
)
print(output[0]["generated_text"].replace(prompt, "").strip())
# 示例输出：您好，在订单详情页点击“申请退款”，选择退款原因，提交后等待审核即可。
```


## 第三章总结：评估-调优闭环清单  
完成本章学习后，可按以下步骤确保模型达标：  
1. 通用评估：MMLU准确率≥50%（7B模型）、中文PPL<50；  
2. 场景评估：自定义测试集平均语义相似度≥0.7，人工打分≥4分（5分制）；  
3. 问题调优：生成重复→调`repetition_penalty`；逻辑混乱→扩上下文+洗数据；知识不足→补领域数据；回答过长→限长度+加指令。  

接下来，我们将进入**第四章：小模型的部署与优化——从权重导出到边缘设备，低成本落地**，重点讲解如何导出LoRA权重、用vLLM/GPTQ优化推理速度，以及在CPU/边缘设备上的部署方案，呼应网页中“HF Docker repository”的部署背景。
```

```
# 第四章 小模型的部署与优化：从权重处理到边缘落地，低成本实现高可用  
训练好的小模型只有落地到业务中才有价值——而部署的核心挑战是“**在有限资源下（如单CPU、边缘设备），实现低延迟、高并发的推理**”。本章会紧扣网页中“HF Docker repository”的部署背景，从“权重处理→推理框架选型→Docker封装→边缘/CPU部署”四个关键环节，提供**算法工程师可直接复用的部署方案**，同时解决“LoRA权重无法直接推理”“推理速度慢”“环境不兼容”等经典问题。


## 4.1 第一步：权重处理——LoRA权重如何变成“可部署模型”？  
训练阶段得到的通常是“基座模型+LoRA权重”（分开存储），无法直接用于推理，必须先做“权重合并”或“轻量加载”。两种方式各有适用场景，需根据部署资源选择。


### 4.1.1 场景1：资源充足（如GPU服务器）→ 权重合并  
将LoRA权重与基座模型合并，得到完整的“单文件模型”，推理时无需加载PEFT库，兼容性更强（支持所有推理框架）。  
- **适用场景**：GPU推理、高并发场景（如企业客服API）；  
- **避坑点**：合并需占用双倍显存（如7B模型合并需12GB显存），若显存不足，需用“分阶段合并”或在CPU上合并。  

#### 实操：LoRA权重合并（用PEFT库）  
```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# 1. 加载基座模型和LoRA权重
base_model_name = "mistralai/Mistral-7B-v0.1"
lora_model_path = "./smol-llm-customer-service"  # 训练好的LoRA权重路径
output_dir = "./merged-smol-llm"  # 合并后模型保存路径

# 加载基座模型（CPU合并可避免GPU显存不足，加torch_dtype=torch.float16减少CPU内存占用）
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    device_map="cpu",  # 设为"cpu"在CPU上合并
    torch_dtype=torch.float16,
    trust_remote_code=True
)
# 加载LoRA权重
lora_model = PeftModel.from_pretrained(base_model, lora_model_path)

# 2. 合并权重（关键步骤）
merged_model = lora_model.merge_and_unload()

# 3. 保存合并后的模型和Tokenizer
merged_model.save_pretrained(output_dir, safe_serialization=True)  # safe_serialization支持大模型分片
tokenizer = AutoTokenizer.from_pretrained(base_model_name)
tokenizer.save_pretrained(output_dir)

print(f"合并完成！模型保存在：{output_dir}")
```


### 4.1.2 场景2：资源有限（如CPU、边缘设备）→ 轻量加载（不合并）  
不合并LoRA权重，推理时通过PEFT库“动态挂载”LoRA到基座模型，显存/内存占用比合并低30%（如7B模型仅需8GB显存）。  
- **适用场景**：CPU推理、边缘设备（如树莓派、工业平板）；  
- **限制**：需依赖PEFT库，部分轻量推理框架（如ONNX Runtime）不支持。  

#### 实操：轻量加载LoRA权重推理  
```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

# 1. 加载基座模型（4位量化，进一步降内存）
base_model = AutoModelForCausalLM.from_pretrained(
    "mistralai/Mistral-7B-v0.1",
    device_map="auto",  # 自动分配CPU/GPU资源
    load_in_4bit=True,  # 4位量化
    torch_dtype=torch.float16
)
# 2. 挂载LoRA权重（不合并）
lora_model = PeftModel.from_pretrained(base_model, "./smol-llm-customer-service")

# 3. 构建推理pipeline
generator = pipeline(
    "text-generation",
    model=lora_model,
    tokenizer=AutoTokenizer.from_pretrained("mistralai/Mistral-7B-v0.1")
)

# 4. 推理
result = generator(
    "我的会员积分怎么用？",
    max_new_tokens=80,
    temperature=0.7,
    repetition_penalty=1.1
)
print(result[0]["generated_text"])
```


## 4.2 推理框架选型：3大主流框架，速度差10倍！  
推理框架直接决定小模型的“响应速度”和“资源占用”——选对框架，7B模型在单CPU上也能实现“1 token/秒”的可用速度，在GPU上能支持“100+并发”。以下是3个工程师必知的框架对比，附实操代码。


### 4.2.1 主流推理框架对比（按“速度-显存”优先级排序）  
| 框架名称       | 核心优势                                  | 适用硬件                          | 7B模型推理速度（单卡RTX 3090） | 显存占用（7B模型） | 部署复杂度 |
|----------------|-------------------------------------------|-----------------------------------|--------------------------------|--------------------|------------|
| **vLLM**       | 高并发（支持动态批处理），速度最快        | NVIDIA GPU                        | 300~500 token/s                | 10GB（FP16）       | 低         |
| **GPTQ-for-LLaMa** | 极致显存优化（INT4量化），支持低显存GPU  | NVIDIA GPU                        | 150~200 token/s                | 4GB（INT4）        | 中         |
| **ONNX Runtime** | 跨平台（支持CPU/GPU/边缘设备），兼容性强  | CPU、NVIDIA GPU、AMD GPU          | CPU：5~10 token/s；GPU：80~120 token/s | 8GB（FP16）/ 2GB（INT4） | 高         |


### 4.2.2 实操1：vLLM（GPU高并发首选）  
vLLM是当前小模型GPU推理的“天花板”，支持动态批处理（Dynamic Batching），能在单卡上处理10倍于原生PyTorch的并发请求，且部署极简单。  
1. 安装vLLM：`pip install vllm`（需CUDA ≥ 11.7）；  
2. 部署代码（支持API服务，可直接对接业务）：  
   ```python
   from vllm import LLM, SamplingParams
   from fastapi import FastAPI, Request

   # 1. 初始化vLLM（加载合并后的模型）
   sampling_params = SamplingParams(
       temperature=0.7,
       max_tokens=100,
       repetition_penalty=1.1
   )
   llm = LLM(
       model="./merged-smol-llm",  # 合并后的模型路径
       tensor_parallel_size=1,  # 单卡推理
       gpu_memory_utilization=0.9  # 利用90%的GPU显存（避免浪费）
   )

   # 2. 用FastAPI封装成API服务（便于业务调用）
   app = FastAPI()
   @app.post("/generate")
   async def generate(request: Request):
       data = await request.json()
       prompts = [data["prompt"]]  # vLLM支持批量输入，可提高并发
       outputs = llm.generate(prompts, sampling_params)
       # 解析结果
       result = outputs[0].outputs[0].text.strip()
       return {"prompt": data["prompt"], "result": result}

   # 启动服务：uvicorn main:app --host 0.0.0.0 --port 8000
   # 测试：curl -X POST http://localhost:8000/generate -H "Content-Type: application/json" -d '{"prompt":"我的订单还没发货怎么办？"}'
   ```


### 4.2.3 实操2：GPTQ（INT4量化，低显存GPU首选）  
GPTQ是专为LLM设计的量化算法，能将7B模型压缩到4GB显存，且精度损失<5%，适合“只有8GB以下显存的GPU”（如RTX 3060、Tesla T4）。  
1. 安装GPTQ：`pip install auto-gptq`；  
2. 量化+推理代码（将合并后的模型量化为INT4）：  
   ```python
   from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig
   from transformers import AutoTokenizer, pipeline

   # 1. 配置量化参数（INT4）
   quantize_config = BaseQuantizeConfig(
       bits=4,  # 4位量化
       group_size=128,  # 分组大小（128是平衡精度和速度的默认值）
       desc_act=False  # 关闭激活值描述（降低显存占用）
   )

   # 2. 加载并量化模型（仅需执行一次，量化后可重复使用）
   model = AutoGPTQForCausalLM.from_pretrained(
       "./merged-smol-llm",  # 合并后的模型
       quantize_config=quantize_config,
       device_map="auto"
   )
   # 保存量化后的模型（下次直接加载，无需重新量化）
   model.save_quantized("./smol-llm-gptq-4bit")

   # 3. 推理（显存仅需4GB）
   tokenizer = AutoTokenizer.from_pretrained("./merged-smol-llm")
   generator = pipeline(
       "text-generation",
       model=model,
       tokenizer=tokenizer
   )
   result = generator(
       "怎么查询我的会员等级？",
       max_new_tokens=80,
       temperature=0.7
   )
   print(result[0]["generated_text"])
   ```


### 4.2.4 实操3：ONNX Runtime（CPU/边缘设备首选）  
ONNX Runtime是跨平台推理框架，支持CPU、GPU、ARM架构（如树莓派），适合“无GPU的边缘场景”。需先将模型转换为ONNX格式，再用Runtime推理。  
1. 安装依赖：`pip install transformers onnx onnxruntime`；  
2. 模型转ONNX+CPU推理代码：  
   ```python
   import torch
   from transformers import AutoModelForCausalLM, AutoTokenizer
   import onnxruntime as ort
   import onnx

   # 步骤1：将合并后的模型转换为ONNX格式（仅执行一次）
   model_path = "./merged-smol-llm"
   onnx_path = "./smol-llm.onnx"

   # 加载模型和Tokenizer
   model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)
   tokenizer = AutoTokenizer.from_pretrained(model_path)
   tokenizer.pad_token = tokenizer.eos_token  # 设置PAD token

   # 构造虚拟输入（用于确定ONNX的输入形状）
   dummy_input = tokenizer("虚拟输入", return_tensors="pt", padding=True).to(model.device)
   input_names = ["input_ids", "attention_mask"]
   output_names = ["logits"]

   # 导出ONNX模型
   torch.onnx.export(
       model,
       (dummy_input["input_ids"], dummy_input["attention_mask"]),
       onnx_path,
       input_names=input_names,
       output_names=output_names,
       dynamic_axes={  # 支持动态输入长度（适配不同长度的prompt）
           "input_ids": {0: "batch_size", 1: "seq_len"},
           "attention_mask": {0: "batch_size", 1: "seq_len"},
           "logits": {0: "batch_size", 1: "seq_len"}
       },
       opset_version=14  # 兼容ONNX Runtime的版本
   )

   # 步骤2：用ONNX Runtime在CPU上推理
   # 配置CPU优化（开启多线程）
   sess_options = ort.SessionOptions()
   sess_options.intra_op_num_threads = 4  # 用4个CPU核心（根据设备调整）
   sess = ort.InferenceSession(onnx_path, sess_options)

   # 构造实际输入
   prompt = "我的退款还没到账，怎么办？"
   inputs = tokenizer(prompt, return_tensors="np", padding=True)
   onnx_inputs = {
       "input_ids": inputs["input_ids"],
       "attention_mask": inputs["attention_mask"]
   }

   # 推理（CPU上约5~10 token/s，满足轻量场景）
   outputs = sess.run(None, onnx_inputs)
   logits = torch.tensor(outputs[0])
   # 解码生成结果（用greedy搜索，简单高效）
   generated_ids = torch.argmax(logits, dim=-1)
   result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
   print(f"结果：{result.replace(prompt, '').strip()}")
   ```


## 4.3 Docker封装：用HF官方镜像，确保“一次构建，到处运行”  
网页中“Fetching metadata from the HF Docker repository”提示我们——Hugging Face提供了预配置的LLM推理镜像，用这些镜像封装模型，能避免“本地能跑、线上崩”的环境兼容问题，且部署效率提升10倍。


### 4.3.1 选择HF官方推理镜像  
根据推理框架选择对应的镜像，避免自己从零构建（节省数小时配置时间）：  
| 推理框架       | HF官方镜像地址                                  | 适用场景                          | 启动命令核心参数                  |
|----------------|-------------------------------------------|-----------------------------------|-----------------------------------|
| vLLM           | `huggingface/text-generation-inference:latest` | GPU高并发API服务                  | `--model-id 模型路径 --tensor-parallel-size 1` |
| GPTQ           | `huggingface/peft:latest-gpu`              | GPU量化推理（需额外安装auto-gptq） | `--gpus all`                      |
| ONNX Runtime   | `huggingface/transformers-pytorch-cpu:latest` | CPU/边缘设备推理                  | `--cpus 4`                        |


### 4.3.2 实操：用Docker封装vLLM推理服务（对接业务）  
以“vLLM高并发服务”为例，用HF的`text-generation-inference`镜像（内置vLLM优化），3步完成封装：  

#### 步骤1：编写Dockerfile（简化版）  
```dockerfile
# 基础镜像：HF官方vLLM推理镜像（已预装vLLM、FastAPI）
FROM huggingface/text-generation-inference:latest

# 复制合并后的模型到镜像中（也可挂载本地目录，避免镜像过大）
COPY ./merged-smol-llm /app/model

# 暴露API端口（默认8080）
EXPOSE 8080

# 启动命令：加载模型并开启API服务
CMD ["text-generation-launcher", "--model-id", "/app/model", "--tensor-parallel-size", "1", "--port", "8080"]
```

#### 步骤2：构建Docker镜像  
```bash
# 构建镜像（命名为smol-llm-vllm，版本v1）
docker build -t smol-llm-vllm:v1 .
```

#### 步骤3：启动Docker容器  
```bash
# 启动容器（挂载GPU，映射端口8080到主机）
docker run -d --gpus all -p 8080:8080 --name smol-llm-service smol-llm-vllm:v1

# 测试服务（用curl调用API）
curl http://localhost:8080/generate \
  -X POST \
  -H "Content-Type: application/json" \
  -d '{"inputs":"我的订单物流怎么查？","parameters":{"max_new_tokens":100,"temperature":0.7}}'
```

#### 避坑指南：  
- 若模型文件过大（如7B模型约13GB），不建议用`COPY`指令放入镜像，而是用`-v`挂载本地目录：  
  ```bash
  docker run -d --gpus all -p 8080:8080 -v /本地模型路径:/app/model --name smol-llm-service smol-llm-vllm:v1
  ```  
- 确保主机CUDA版本与镜像兼容（HF镜像默认支持CUDA 12.1，若主机是11.8，需指定镜像版本：`huggingface/text-generation-inference:1.4-cuda11.8`）。


## 4.4 边缘/CPU部署优化：让7B模型在树莓派上“跑起来”  
边缘设备（如树莓派4B、工业CPU服务器）的资源极其有限（通常4GB内存、无GPU），需通过“**极致量化+轻量推理引擎**”实现可用推理速度（目标：1~2 token/秒）。


### 4.4.1 核心优化手段  
1. **INT4量化（必做）**：用GPTQ或AWQ将模型量化到INT4，7B模型内存占用从13GB→2GB；  
2. **裁剪模型（可选）**：删除模型中的冗余层（如部分全连接层），但需重新微调（风险较高，新手不推荐）；  
3. **推理引擎优化**：用`llama.cpp`（专为LLM CPU推理设计）或`ONNX Runtime`开启CPU多线程、缓存优化。


### 4.4.2 实操：llama.cpp在树莓派上部署INT4模型  
`llama.cpp`是轻量LLM推理引擎，支持ARM架构（树莓派），INT4量化的7B模型在树莓派4B（4GB内存）上可实现“1 token/秒”。  
1. 在树莓派上安装llama.cpp：  
   ```bash
   git clone https://github.com/ggerganov/llama.cpp.git
   cd llama.cpp
   make  # 编译（需树莓派安装gcc）
   ```  
2. 将GPTQ量化后的模型转换为llama.cpp支持的格式（.gguf）：  
   ```bash
   # 先安装转换工具：pip install llama-cpp-python
   python -m llama_cpp.convert.py ./smol-llm-gptq-4bit --outtype q4_0 --outfile smol-llm-4bit.gguf
   ```  
3. 在树莓派上推理：  
   ```bash
   # 启动推理（--threads 4：用4个CPU核心；--n-predict 100：最大生成长度）
   ./main -m smol-llm-4bit.gguf -p "我的会员积分能兑换什么？" --threads 4 --n-predict 100
   ```  


## 第四章总结：部署决策清单  
部署前按以下步骤确认方案，确保低成本、高可用：  
1. 权重处理：GPU部署→合并权重；CPU/边缘→轻量加载；  
2. 框架选择：GPU高并发→vLLM；低显存GPU→GPTQ；CPU/边缘→ONNX Runtime/llama.cpp；  
3. 容器封装：用HF官方镜像，挂载模型目录（避免镜像过大）；  
4. 边缘优化：INT4量化+llama.cpp，树莓派4B可实现1 token/秒。  

至此，我们已完成《Smol Training Playbook》从“训练到部署”的全流程学习——从基座模型选型、LoRA微调，到评估调优、Docker部署，每个环节都提供了可落地的代码和避坑指南。最终，你可以用单卡GPU训练出适配业务的7B模型，并用Docker快速部署到企业服务或边缘设备，实现“低成本LLM落地”。
```

# 精读

## 一、Introduction

## 二、Pretraining

## 三、Post-training

### 3.1 前言

#### why

为什么要进行后训练？我们在预训练指南中概述的三个训练动机——研究、生产和战略开源——同样适用于后训练。例如，你可能正在探索 RL 是否能为现有模型解锁新的推理能力（研究），或者你可能需要将大型模型压缩成更小的模型以解决延迟问题（生产），或者你可能已经发现了一个特定用例领域目前没有强大的开源模型（战略开源）。区别在于后训练是建立在现有能力之上，而不是从零开始创造。然而，在动用你的 GPU 之前，先问问自己：

- **你真的需要进行后训练吗？**现在许多开源权重模型在广泛任务上已经可以媲美专有模型。有些甚至可以通过量化和适度的计算在本地运行。如果你需要一个通才助手，Hugging Face Hub 上的现成模型可能已经能满足你的需求。
- **你是否有访问高质量、特定领域的数据？**在针对特定任务或领域时，训练后微调最为合理，因为通用模型在这些领域表现不佳。有了合适的数据，你可以调整模型，使其为你最关心的应用生成更准确的输出。
- **你能衡量成功吗？**没有明确的评估标准，你将不知道训练后是否真的有帮助。
#### what

训练后应该达到什么目标？这取决于你的优先事项：

- 你想得到一个执行指令清晰且很少跑题的助手吗？
- 一个能根据需求切换语气和角色的多功能助手？
- 一个能处理数学、代码或代理问题的推理引擎？
- 一个能多语言对话的模型？
#### How

- SFT
- PO
- RL
- 数据管理
- 评估
### 3.2 评测

#### baseline

就像预训练一样，我们从基础开始：**评估和基线**，**因为每个大型模型都始于一个小型消融实验**。但我们在消融方式上有一个关键区别。在预训练中，“小”通常意味着更小的模型和更小的数据集。在训练后，“小”意味着更小的数据集和更简单的算法。我们几乎从不使用不同的基础模型进行消融实验，因为行为过于依赖模型，而且运行时间足够短，可以直接在目标模型上进行迭代。

#### evals

在训练后的第一个步骤——就像在预训练中一样——是要确定合适的评估集。由于目前大多数 LLMs 被用作助手，我们发现追求一个“工作良好”的模型比追逐抽象的“智能”基准（如 ARC-AGI）的目标更好。那么一个好的助手需要做什么？至少，它应该能够：

- 处理模糊指令
- 逐步规划
- 编写代码
- 在适当的时候调用工具
这些行为依赖于推理、长上下文处理以及数学、代码和工具使用技能的结合。即使只有 3B 参数甚至更小的模型也能很好地作为助手，但性能通常在低于 1B 时急剧下降。

在 Hugging Face，我们使用分层的评估套件，这反映了我们在预训练部分详细阐述的预训练原则（单调性、低噪声、高于随机信号、排名一致性）。

评估指南参见：

For SmolLM3 specifically, we wanted a hybrid reasoning model that could reliably follow instructions and **reason well in popular domains like mathematics and code.** We also wanted to ensure we preserved the base model’s capabilities of multilinguality and long-context retrieval.

This led us to the following set of evals:

Benchmark

Category

Number of prompts

Metric

AIME25

Competitive mathematics

30

avg@64

LiveCodeBench (v4 for validation, v5 for final release)

Competitive programming

100 (268)

avg@16

GPQA Diamond

Graduate-level reasoning

198

avg@8

IFEval

Instruction following

541

accuracy

MixEval Hard

Alignment

1000

accuracy

BFCL v3

Tool use

4441

mixed

Global MMLU (lite for validation)

Multilingual Q&A

590,000 (6,400)

accuracy

GSMPlus (mini for validation)

Robustness

10,000 (2,400)

accuracy

RULER

Long context

6,500

accuracy

Browse through the examples above to see the types of questions in each benchmark. Notice how the diversity of domains ensures we’re testing different aspects of model capability throughout our ablations.

For the 3B model scale we were working with, we felt these evals would give us actionable signal, run faster than training itself, **and give us confidence that improvements are real and not just noise from sampling**. We also tracked our pre-training evals (see the [ablation section](https%3A%2F%2Fhuggingfacetb-smol-training-playbook.hf.space%2F%23every-big-model-starts-with-a-small-ablation) for a full list) to make sure we weren’t regressing too much on the base model performance.

### 3.3 框架

Framework  框架

SFT

PO

RL

Multi-modal

FullFT

LoRA

Distributed  

[**TRL**](https%3A%2F%2Fgithub.com%2Fhuggingface%2Ftrl)

✅

✅

✅

✅

✅

✅

✅

[**Axolotl**](https%3A%2F%2Fgithub.com%2Faxolotl-ai-cloud%2Faxolotl)

✅

✅

✅

✅

✅

✅

✅

[**OpenInstruct**](https%3A%2F%2Fgithub.com%2Fallenai%2Fopen-instruct)

✅

✅

✅

❌

✅

✅

✅

[**Unsloth**](https%3A%2F%2Fgithub.com%2Funslothai%2Funsloth)

✅

✅

✅

✅

✅

✅

✅

[**vERL**](https%3A%2F%2Fgithub.com%2Fvolcengine%2Fverl)

✅

❌

✅

✅

✅

✅

✅

[**Prime RL**](https%3A%2F%2Fgithub.com%2FPrimeIntellect-ai%2Fprime-rl)

✅

❌

✅

❌

✅

✅

✅

[**PipelineRL**](https%3A%2F%2Fgithub.com%2FServiceNow%2FPipelineRL)

❌

❌

✅

❌

✅

✅

✅

[**ART**](https%3A%2F%2Fgithub.com%2FOpenPipe%2FART%2Ftree%2Fmain)

❌

❌

✅

❌

❌

✅

❌

[**TorchForge**](https%3A%2F%2Fgithub.com%2Fmeta-pytorch%2Ftorchforge)

✅

❌

✅

❌

✅

❌

✅

[**NemoRL**](https%3A%2F%2Fgithub.com%2FNVIDIA-NeMo%2FRL)

✅

✅

✅

❌

✅

❌

✅

[**OpenRLHF**](https%3A%2F%2Fgithub.com%2FOpenRLHF%2FOpenRLHF)

✅

✅

✅

❌

✅

✅

✅

### 3.4 SFT

#### Why (almost) every post-training pipeline starts with SFT？

- **It’s cheap:** SFT requires modest compute compared to RL. You can usually get meaningful gains without needing to burn a bonfire of silicon, and in fraction of the time required for RL.
- **It’s stable:** unlike RL, which is notoriously sensitive to reward design and hyperparameters, SFT “just works.”
它很稳定：与强化学习不同，强化学习以对奖励设计和超参数非常敏感而闻名，监督微调“总能奏效。”
- **It’s the right baseline:** a good SFT checkpoint usually gives most of the gains you’re after, and it makes later methods like DPO or RLHF far more effective.
它是正确的基线：一个好的监督微调检查点通常能带来您期望的大部分收益，并使后续方法（如 DPO 或 RLHF）更加有效。
先尝试 SFT 低成本进行实现，通常能获得大部分收益，RL 锦上添花，难度较大。

**R1 的 特例**：为什么 DeepSeek 跳过了 SFT，直接使用 R1-Zero 进行 RL？没有更强的模型可以用来蒸馏，而且人类标注对于长链思维等复杂行为来说过于嘈杂，希望通过 RL 发现无法通过标准监督进行教学的推理行为。

#### 基础模型选择

在为后续训练选择基础模型时，几个实际维度最为重要：

- **模型大小**：尽管小模型随着时间的推移取得了显著进步，但如今仍然存在一个事实，**即更大的模型泛化能力更强，并且通常需要更少的样本**。
- **架构（MoE 与密集型）**：MoE 模型每个 token 激活参数的子集，并提供每单位计算的更高容量。它们非常适合大规模服务，但在我们经验中，微调起来更具挑战性。相比之下，密集型模型训练起来更简单，并且在小规模上通常优于 MoE。
- **训练后的表现记录**：基准测试是有用的，但如果基础模型已经衍生出一系列与社区产生共鸣的强大预训练模型，那就更好了。这为模型训练效果提供了参考依据。
经验中，**Qwen、Mistral 和 DeepSeek** 的基础模型最易于进行后续训练，其中 Qwen 是一个明显的首选，因为每个模型系列通常覆盖较大的参数范围（例如 Qwen3 模型的规模从 0.6B 到 235B！）

#### 训练基线

这里的 case 的难点在于，这是个混合推理模型，构造数据会有些特殊：

Dataset

Reasoning mode

# examples

% of examples

# tokens (M)

% of tokens

Avg. # tokens per example

Avg. # tokens in context

Avg. # tokens in response

Avg. # turns

Everyday Conversations

/no_think

2,260

2.3

0.6

0.8

260.2

222.3

94.0

7.8

SystemChats 30k

/no_think

33,997

35.2

21.5

28.2

631.9

422.8

267.7

6.3

Tulu 3 SFT Personas IF

/no_think

29,970

31.0

13.3

17.5

444.5

119.8

380.7

2

Everyday Conversations (Qwen3-32B)

/think

2,057

2.1

3.1

4.1

1,522.4

376.8

1,385.6

4

SystemChats 30k (Qwen3-32B)

/think

27,436

28.4

29.4

38.6

1070.8

84.6

1,042.7

2

s1k-1.1

/think

835

0.9

8.2

10.8

8,859.3

370.9

9,728.5

2

Total

-

96,555

100.0

76.1

100.0

2,131.5

266.2

## 相关

- [[AI/LLM/SFT/SFT 原理|SFT 原理]]
- [[AI/LLM/SFT/LoRA|LoRA]]
- [[AI/LLM/Frameworks/TRL/TRL 概述|TRL 概述]]
- [[AI/LLM/Frameworks/Unsloth/Unsloth 概述|Unsloth 概述]]
- [[AI/LLM/RL/GRPO/GRPO 深度理解|GRPO 深度理解]]
