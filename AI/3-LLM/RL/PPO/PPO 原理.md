---
title: PPO åŸç†
brief: Proximal Policy Optimizationâ€”â€”é€šè¿‡ clip ratio é™åˆ¶ç­–ç•¥æ›´æ–°å¹…åº¦ï¼Œå®ç°ç¨³å®šçš„ç­–ç•¥æ¢¯åº¦ä¼˜åŒ–ï¼›æ˜¯ RLHF çš„ç»å…¸ç®—æ³•ï¼ˆInstructGPT/ChatGPTï¼‰ï¼Œéœ€è¦ Actor+Critic+RM+Ref å››ä¸ªæ¨¡å‹ã€‚
type: concept
domain: ai/llm/rl/ppo
created: 2026-02-13
updated: 2026-02-22
tags:
  - ai/llm/rl/ppo
  - type/concept
  - interview/hot
status: complete
sources:
  - Proximal Policy Optimization Algorithms â€” arXiv:1707.06347
  - Trust Region Policy Optimization (TRPO) â€” arXiv:1502.05477
  - Training language models to follow instructions with human feedback (InstructGPT) â€” arXiv:2203.02155
related:
  - "[[GRPO æ·±åº¦ç†è§£|GRPO æ·±åº¦ç†è§£]]"
  - "[[KLæ•£åº¦|KLæ•£åº¦]]"
  - "[[ç­–ç•¥æ¢¯åº¦æ–¹æ³•|ç­–ç•¥æ¢¯åº¦æ–¹æ³•]]"
---

# PPO åŸç†

> PPOï¼ˆProximal Policy Optimizationï¼‰æ˜¯ OpenAI æå‡ºçš„ç­–ç•¥æ¢¯åº¦ç®—æ³•ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯**é€šè¿‡ clip ratio é™åˆ¶æ–°æ—§ç­–ç•¥çš„å·®å¼‚**ï¼Œåœ¨ä¿è¯è®­ç»ƒç¨³å®šæ€§çš„åŒæ—¶æœ€å¤§åŒ–æœŸæœ›å¥–åŠ±ã€‚

> æ¥æºï¼šSchulman et al., "Proximal Policy Optimization Algorithms" arXiv:1707.06347

## ä» TRPO åˆ° PPO

TRPOï¼ˆTrust Region Policy Optimizationï¼‰ç”¨ KL æ•£åº¦ç¡¬çº¦æŸç­–ç•¥æ›´æ–°ï¼š

$$\max_\theta \mathbb{E}\left[\frac{\pi_\theta(a|s)}{\pi_{\theta_{\text{old}}}(a|s)} \hat{A}(s,a)\right] \quad \text{s.t.} \quad D_{\text{KL}}(\pi_{\theta_{\text{old}}} \| \pi_\theta) \leq \delta$$

> æ¥æºï¼šSchulman et al., "Trust Region Policy Optimization" arXiv:1502.05477

TRPO çš„é—®é¢˜ï¼šéœ€è¦è®¡ç®— Fisher ä¿¡æ¯çŸ©é˜µçš„é€†ï¼ŒäºŒé˜¶ä¼˜åŒ–è®¡ç®—é‡å¤§ã€å®ç°å¤æ‚ã€‚

PPO çš„è§£æ³•ï¼š**ç”¨ clip å‡½æ•°æ›¿ä»£ KL çº¦æŸ**ï¼ŒæŠŠçº¦æŸä¼˜åŒ–å˜æˆæ— çº¦æŸä¼˜åŒ–ã€‚

## PPO-Clip ç›®æ ‡å‡½æ•°

ä»¤ $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{\text{old}}}(a_t|s_t)}$ ä¸ºæ¦‚ç‡æ¯”ï¼ˆprobability ratioï¼‰ï¼ŒPPO çš„ç›®æ ‡å‡½æ•°ä¸ºï¼š

$$L^{\text{CLIP}}(\theta) = \mathbb{E}_t\left[\min\left(r_t(\theta) \hat{A}_t, \;\text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t\right)\right]$$

> æ¥æºï¼šarXiv:1707.06347, Sec. 3 â€” è¿™æ˜¯ PPO çš„æ ¸å¿ƒå…¬å¼

å…¶ä¸­ï¼š
- $\hat{A}_t$ï¼šä¼˜åŠ¿å‡½æ•°ä¼°è®¡ï¼ˆé€šå¸¸ç”¨ GAE-Î» è®¡ç®—ï¼‰
- $\epsilon$ï¼šclip èŒƒå›´ï¼Œé€šå¸¸å– **0.1-0.2**
- `min` å–è¾ƒæ‚²è§‚çš„ä¼°è®¡ï¼Œé˜²æ­¢è¿‡åº¦æ›´æ–°

**ç›´è§‰**ï¼šå½“ $\hat{A}_t > 0$ï¼ˆå¥½åŠ¨ä½œï¼‰ï¼Œå…è®¸ $r_t$ å¢å¤§ä½†ä¸è¶…è¿‡ $1+\epsilon$ï¼›å½“ $\hat{A}_t < 0$ï¼ˆååŠ¨ä½œï¼‰ï¼Œå…è®¸ $r_t$ å‡å°ä½†ä¸ä½äº $1-\epsilon$ã€‚

## GAEï¼ˆGeneralized Advantage Estimationï¼‰

PPO ä½¿ç”¨ GAE-Î» ä¼°è®¡ä¼˜åŠ¿å‡½æ•°ï¼š

$$\hat{A}_t^{\text{GAE}(\gamma, \lambda)} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}$$

å…¶ä¸­ $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ æ˜¯ TD è¯¯å·®ã€‚

- $\lambda = 0$ï¼šçº¯ TDï¼ˆä½æ–¹å·®é«˜åå·®ï¼‰
- $\lambda = 1$ï¼šè’™ç‰¹å¡æ´›ï¼ˆé«˜æ–¹å·®ä½åå·®ï¼‰
- é€šå¸¸å– $\lambda = 0.95$ï¼Œ$\gamma = 0.99$

## PPO åœ¨ RLHF ä¸­çš„åº”ç”¨

> æ¥æºï¼šOuyang et al., "InstructGPT" arXiv:2203.02155, Sec. 3

åœ¨ LLM å¯¹é½ä¸­ï¼ŒPPO çš„ RLHF æµç¨‹éœ€è¦ **4 ä¸ªæ¨¡å‹**ï¼š

```mermaid
flowchart LR
    P["Prompt"] --> Actor["Actor Model<br/>Ï€_Î¸ (ç”Ÿæˆ)"]
    Actor --> R["Response"]
    R --> RM["Reward Model<br/>(æ‰“åˆ†)"]
    R --> Ref["Reference Model<br/>Ï€_ref (KL çº¦æŸ)"]
    R --> Critic["Critic Model<br/>V(s) (ä¼°å€¼)"]
    RM --> Reward["Reward"]
    Ref --> KL["KL Penalty"]
    Critic --> Adv["Advantage"]
    Reward --> Update["PPO Update"]
    KL --> Update
    Adv --> Update
    Update --> Actor
```
> å›¾ï¼šPPO-RLHF çš„å››æ¨¡å‹æ¶æ„

### è®­ç»ƒç›®æ ‡

$$R_{\text{RLHF}}(s, a) = R_{\text{RM}}(s, a) - \beta \cdot D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})$$

- $R_{\text{RM}}$ï¼šReward Model ç»™å‡ºçš„å¥–åŠ±
- $\beta$ï¼šKL æƒ©ç½šç³»æ•°ï¼ˆé€šå¸¸ 0.01-0.2ï¼‰
- [[KLæ•£åº¦|KL æ•£åº¦]]æƒ©ç½šé˜²æ­¢æ¨¡å‹åç¦» SFT åŸºçº¿å¤ªè¿œï¼ˆreward hackingï¼‰

### PPO vs GRPO çš„æ ¸å¿ƒåŒºåˆ«

| ç»´åº¦ | PPO | [[GRPO æ·±åº¦ç†è§£\|GRPO]] |
|------|-----|------|
| Critic æ¨¡å‹ | éœ€è¦ï¼ˆV(s) ä¼°å€¼ï¼‰ | **ä¸éœ€è¦**ï¼ˆç»„å†…ç›¸å¯¹æ¯”è¾ƒï¼‰ |
| æ¨¡å‹æ•°é‡ | 4 ä¸ªï¼ˆActor+Critic+RM+Refï¼‰ | 2 ä¸ªï¼ˆActor+Refï¼‰ |
| ä¼˜åŠ¿ä¼°è®¡ | GAE-Î»ï¼ˆä¾èµ– Criticï¼‰ | ç»„å†… reward z-score å½’ä¸€åŒ– |
| æ˜¾å­˜éœ€æ±‚ | ~4x æ¨¡å‹å¤§å° | ~2x æ¨¡å‹å¤§å° |
| è®­ç»ƒç¨³å®šæ€§ | ä¾èµ– Critic è´¨é‡ | æ›´ç¨³å®šï¼ˆæ—  Critic å¼•å…¥çš„è¯¯å·®ï¼‰ |

## è¸©å‘è®°å½•

1. **Reward hacking**ï¼šæ¨¡å‹å­¦åˆ° RM çš„æ¼æ´ï¼ˆå¦‚ç”Ÿæˆé•¿æ–‡æœ¬å¾—é«˜åˆ†ï¼‰ï¼Œéœ€è¦ KL æƒ©ç½š + reward clipping
2. **Critic è®­ç»ƒåŒæ­¥**ï¼šCritic æ›´æ–°å¤ªå¿«/å¤ªæ…¢éƒ½ä¼šå¯¼è‡´ä¼˜åŠ¿ä¼°è®¡ä¸å‡†ï¼Œé€šå¸¸ Critic å­¦ä¹ ç‡è®¾ä¸º Actor çš„ 2-5 å€
3. **Generation å’Œ Training åˆ‡æ¢**ï¼šPPO éœ€è¦åœ¨ inferenceï¼ˆç”Ÿæˆ rolloutï¼‰å’Œ trainingï¼ˆæ¢¯åº¦æ›´æ–°ï¼‰é—´åˆ‡æ¢ï¼Œbatch size å’Œ micro-batch çš„é…ç½®ç›´æ¥å½±å“åå
4. **KL ç³»æ•° Î² åŠ¨æ€è°ƒèŠ‚**ï¼šå›ºå®š Î² ä¸ä¸€å®šæœ€ä¼˜ï¼ŒOpenAI åœ¨ InstructGPT ä¸­ä½¿ç”¨è‡ªé€‚åº” Î²

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) â€” PPO åŸå§‹è®ºæ–‡ï¼ŒSec. 3 çš„ clip ç›®æ ‡å‡½æ•°
- [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477) â€” TRPOï¼ŒPPO çš„ç†è®ºå‰é©±
- [InstructGPT](https://arxiv.org/abs/2203.02155) â€” PPO åœ¨ LLM å¯¹é½ä¸­çš„å¥ åŸºåº”ç”¨

### æ·±åº¦è§£è¯»
- [PPO ç®—æ³•è¯¦è§£ï¼ˆçŸ¥ä¹ï¼‰](https://zhuanlan.zhihu.com/p/512327050) â€” ä¸­æ–‡ç¤¾åŒºæœ€æ¸…æ™°çš„ PPO æ¨å¯¼ â­â­â­â­
- [The 37 Implementation Details of PPO (ICLR Blog)](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/) â€” PPO å·¥ç¨‹å®ç°çš„ 37 ä¸ªç»†èŠ‚ â­â­â­â­â­

### å®è·µèµ„æº
- [HuggingFace TRL PPOTrainer](https://huggingface.co/docs/trl/ppo_trainer) â€” æœ€å¸¸ç”¨çš„ PPO-RLHF å®ç°
- [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) â€” æ”¯æŒ PPO çš„é«˜æ€§èƒ½ RLHF æ¡†æ¶

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **LLM å¯¹é½è®­ç»ƒ**ï¼šInstructGPT/ChatGPT çš„ç»å…¸æ–¹æ¡ˆï¼ˆSFT â†’ RM â†’ PPOï¼‰
- **éœ€è¦ç²¾ç»† reward æ§åˆ¶çš„åœºæ™¯**ï¼šPPO çš„ Critic èƒ½æä¾› per-token çº§åˆ«çš„ä¼˜åŠ¿ä¼°è®¡

### å·¥ç¨‹å®ç°è¦ç‚¹
- **clip èŒƒå›´ Îµ**ï¼šé€šå¸¸ 0.1-0.2ï¼Œå¤ªå¤§è®­ç»ƒä¸ç¨³ï¼Œå¤ªå°æ”¶æ•›æ…¢
- **mini-batch æ•°é‡**ï¼šPPO å¯¹ä¸€æ‰¹ rollout åšå¤šä¸ª epoch æ›´æ–°ï¼ˆé€šå¸¸ 4 epochsï¼‰ï¼Œéœ€è¦è¶³å¤Ÿçš„ mini-batch é˜²æ­¢è¿‡æ‹Ÿåˆ
- **Generation + Training èµ„æºåˆ†é…**ï¼šverl çš„ [[HybridFlow|HybridFlow]] ç”¨ SPMD+MPMD æ··åˆè§£å†³æ­¤é—®é¢˜

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: PPO çš„ clip ç›®æ ‡å‡½æ•°æ€ä¹ˆç†è§£ï¼Ÿä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨ TRPOï¼Ÿ
  A: TRPO ç”¨ KL æ•£åº¦ç¡¬çº¦æŸï¼ˆéœ€è¦ Fisher çŸ©é˜µé€†ï¼‰ï¼Œè®¡ç®—æ˜‚è´µã€‚PPO ç”¨ $\text{clip}(r_t, 1-\epsilon, 1+\epsilon)$ æŠŠç­–ç•¥æ¯” clamp åœ¨ $[1-\epsilon, 1+\epsilon]$ å†…ï¼Œå– `min` å¾—åˆ°æ‚²è§‚ä¼°è®¡â€”â€”æ•ˆæœæ¥è¿‘ TRPO ä½†åªéœ€ä¸€é˜¶ä¼˜åŒ–ï¼Œå®ç°ç®€å• 10 å€ã€‚

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- PPO æ˜¯ç†è§£ RLHF çš„"å¿…ä¿®è¯¾"â€”â€”å³ä½¿å®é™…é¡¹ç›®ä¸­ GRPO æ›´å¸¸ç”¨ï¼ŒPPO çš„æ¦‚å¿µæ¡†æ¶ï¼ˆpolicy ratioã€clipã€advantageï¼‰æ˜¯æ‰€æœ‰å˜ä½“çš„åŸºç¡€
- PPO çš„ 4 æ¨¡å‹æ¶æ„æ˜¯ RLHF å·¥ç¨‹å¤æ‚æ€§çš„æ ¹æºâ€”â€”ç†è§£å®ƒæ‰èƒ½ç†è§£ GRPO ä¸ºä»€ä¹ˆæ˜¯ç®€åŒ–

### æœªè§£é—®é¢˜ä¸å±€é™
- PPO çš„ Critic æ¨¡å‹å¼•å…¥äº†é¢å¤–çš„è¿‘ä¼¼è¯¯å·®â€”â€”å½“ Critic ä¸å‡†æ—¶ï¼Œä¼˜åŠ¿ä¼°è®¡åå·®ä¼šä¼ å¯¼åˆ°ç­–ç•¥æ›´æ–°
- PPO åœ¨ LLM åœºæ™¯ä¸‹çš„ reward hacking é—®é¢˜æ˜¯å¦å¯ä»¥ä»æ ¹æœ¬ä¸Šè§£å†³ï¼Ÿ

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- [[GRPO æ·±åº¦ç†è§£|GRPO]] å»æ‰ Critic åç”¨ç»„å†…ç›¸å¯¹æ’åæ›¿ä»£ä¼˜åŠ¿ä¼°è®¡â€”â€”è¿™æ˜¯å¦æ„å‘³ç€ Critic åœ¨ LLM åœºæ™¯ä¸‹æœ¬èº«å°±ä¸å¿…è¦ï¼Ÿ
- å¦‚æœæŠŠ PPO çš„ per-token credit assignment å’Œ GRPO çš„ group comparison ç»“åˆï¼Œèƒ½å¦å¾—åˆ°æ›´å¥½çš„æ–¹æ³•ï¼Ÿ

## ç›¸å…³

> ğŸ”— See also: [[GRPO æ·±åº¦ç†è§£|GRPO æ·±åº¦ç†è§£]] â€” æ—  Critic çš„ PPO æ›¿ä»£æ–¹æ¡ˆ
> ğŸ”— See also: [[KLæ•£åº¦|KLæ•£åº¦]] â€” PPO ä¸­ KL æƒ©ç½šçš„æ•°å­¦åŸºç¡€
> ğŸ”— See also: [[ç­–ç•¥æ¢¯åº¦æ–¹æ³•|ç­–ç•¥æ¢¯åº¦æ–¹æ³•]] â€” PPO çš„ç†è®ºæ ¹åŸº

- [[DPO-TRLå®è·µ|DPO]] â€” ä¸éœ€è¦ RL çš„åå¥½ä¼˜åŒ–æ–¹æ³•
- [[TRL æ¦‚è¿°|TRL æ¦‚è¿°]] â€” PPO/GRPO/DPO Trainer å®ç°
- [[verl æ¦‚è¿°|verl æ¦‚è¿°]] â€” é«˜æ€§èƒ½ RLHF æ¡†æ¶
- [[OpenRLHF|OpenRLHF]] â€” æ”¯æŒ PPO çš„å¼€æº RLHF æ¡†æ¶
- [[DeepSeek-R1|DeepSeek-R1]] â€” ä½¿ç”¨ GRPO æ›¿ä»£ PPO çš„å®ä¾‹
- [[AI/LLM/RL/PPO/PPO-æ‰‹æ’•å®æ“-MA-RLHF|PPO-æ‰‹æ’•å®æ“ï¼ˆMA-RLHFï¼‰]] â€” **ä»£ç è·¯å¾„**ï¼šä»é›¶å®ç° PPO actor-criticï¼Œå« GAE/clip/KL æƒ©ç½šå®Œæ•´ä»£ç æ³¨è§£ â­â­â­â­â­
- [[AI/LLM/RL/PPO/RLHF-PPO-å®Œæ•´Pytorchå®ç°|RLHF-PPO å®Œæ•´ Pytorch å®ç°]] â€” **ç«¯åˆ°ç«¯ 56-cell Notebook**ï¼šå››æ¨¡å‹æ¶æ„ + GAE + KL çº¦æŸå®Œæ•´é“¾è·¯ï¼Œé…åˆæ‰‹æ’•å®æ“æŸ¥å·¥ç¨‹ç»†èŠ‚ â­â­â­â­â­
- [[AI/LLM/RL/PPO/LLaMA2-Reward-Modelå®ç°|LLaMA2 Reward Model å®ç°]] â€” **RM å·¥ç¨‹**ï¼šBradley-Terry Loss + LLaMA2 RM Headï¼ŒRLHF ç¬¬ä¸€æ­¥çš„ä»£ç å‚è€ƒ
- [[AI/LLM/RL/PPO/MA-RLHF-æ ¸å¿ƒä»£ç æ³¨è§£|MA-RLHF æ ¸å¿ƒä»£ç æ³¨è§£]] â€” PPO åœ¨ LLM RLHF åœºæ™¯çš„å®Œæ•´è®­ç»ƒæ¡†æ¶æ³¨è§£
- [[AI/LLM/RL/PPO/PRM-O1-Search-æ‰‹æ’•å®æ“|PRM-O1-Search æ‰‹æ’•å®æ“]] â€” PPO ç»“åˆ Process Reward Model çš„ O1 é£æ ¼æœç´¢å®ç°
- [[AI/LLM/RL/PPO/O1-PRMæœç´¢å®Œæ•´å®ç°|O1-PRM æœç´¢å®Œæ•´å®ç°]] â€” **MCTS Notebook**ï¼šUCT + æ ‘å›æº¯ + PRM æ‰“åˆ†å®Œæ•´ MCTS å®ç°

> ğŸ¤– **PPO åœ¨ Agent åœºæ™¯çš„å˜ä½“**ï¼ˆCritic-free åŒ–æ–¹å‘ï¼‰ï¼š
> - [[LOOP-Leave-One-Out-PPO-Long-Horizon-Agent-RL|LOOPï¼ˆApple Researchï¼‰]] â€” Leave-One-Out PPOï¼šå»æ‰ Criticï¼ˆå•å€ LLM æ˜¾å­˜ï¼‰ï¼Œç”¨åŒç»„å…¶ä»– rollout çš„ outcome ä¼°è®¡ baselineï¼›é•¿ horizon IDA ä»»åŠ¡ï¼›32B è¶… o1 +9pp
> - [[SCoRe-Self-Correction-via-Reinforcement-Learning|SCoReï¼ˆDeepMindï¼‰]] â€” åŒé˜¶æ®µ PPO è®­ç»ƒ multi-turn è‡ªæˆ‘çº é”™ï¼šPhase 1 KL çº¦æŸåˆå§‹åŒ– + Phase 2 reward bonusï¼›PPO ç”¨äº behavior collapse é˜²æŠ¤
