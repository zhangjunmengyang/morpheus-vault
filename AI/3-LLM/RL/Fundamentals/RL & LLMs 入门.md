---
title: RL & LLMs 入门
brief: 强化学习与 LLM 对齐的入门指南--从 RL 基本循环（观察→动作→反馈→学习）到 RLHF 流程（人类偏好→Reward Model→RL 微调），以及 PPO/DPO/GRPO 三种主流算法的定位对比。
type: tutorial
domain: ai/llm/rl/fundamentals
created: 2026-02-13
updated: 2026-02-22
tags:
  - ai/llm/rl/fundamentals
  - type/tutorial
status: complete
sources:
  - "Sutton & Barto, Reinforcement Learning: An Introduction, 2nd ed., MIT Press, 2018"
  - Proximal Policy Optimization Algorithms - arXiv:1707.06347
  - Training language models to follow instructions with human feedback (InstructGPT) - arXiv:2203.02155
  - HuggingFace TRL documentation - https://huggingface.co/docs/trl
related:
  - "[[GRPO 深度理解|GRPO 深度理解]]"
  - "[[PPO 原理|PPO 原理]]"
  - "[[KL散度|KL散度]]"
---
# 1. RL & LLMs

## What is Reinforcement Learning (RL)?

强化学习是通过反复试验的过程进行的：

> 来源：Sutton & Barto, *Reinforcement Learning: An Introduction*, 2nd ed., MIT Press, 2018, Ch. 1

Step

Process

Description

1. Observation 1. 观察

The agent observes the environment
代理观察环境

The agent takes in information about its current state and surroundings
代理接收其当前状态和周围环境的信息

2. Action 2. 动作

The agent takes an action based on its current policy
代理根据当前策略采取行动

Using its learned strategy (policy), the agent decides what to do next
使用其学习到的策略（策略），智能体决定接下来该做什么

3. Feedback 反馈

The environment gives the agent a reward
环境会给代理一个奖励

The agent receives feedback on how good or bad its action was
代理会收到对其行为好坏的反馈

4. Learning 学习 -

The agent updates its policy based on the reward
代理根据奖励更新其策略

The agent adjusts its strategy - reinforcing actions that led to high rewards and avoiding those that led low rewards
代理调整其策略--强化导致高奖励的行为，避免导致低奖励的行为

5. Iteration 第 5 次迭代

Repeat the process 重复此过程

This cycle continues, allowing the agent to continuously improve its decision-making
这个循环会持续进行，使智能体能够不断改进其决策能力

那么，为什么强化学习对大型语言模型如此重要呢？

训练非常优秀的LLMs是很有挑战性的。我们可以用互联网上的大量文本对其进行训练，使它们在句子中预测下一个单词的能力变得非常出色。这就是它们如何学习生成流畅且语法正确的文本的原因，正如我们在第二章中所学的那样。

然而，仅仅流利是不够的。我们希望我们的LLMs不仅能很好地串联词语，还希望他们能够做到：

- **Helpful**: Provide useful and relevant information.
有益的：提供有用且相关的信息。
- **Harmless**: Avoid generating toxic, biased, or harmful content.
无害：避免生成有毒、有偏见或有害的内容。
- **Aligned with Human Preferences**: Respond in ways that humans find natural, helpful, and engaging.
与人类偏好一致：以人类觉得自然、有帮助且引人入胜的方式回应。
预训练 LLM 方法，这些方法大多依赖于从文本数据中预测下一个单词，有时在这些方面表现不佳。

SFT在生成结构化输出方面非常出色，但在生成有益、无害且对齐的响应方面可能效果较差。我们在第 11 章探讨监督训练。

微调后的模型可能会生成流畅且结构化的文本，但这些文本仍然可能是事实错误的、带有偏见的，或者根本不能以有益的方式回答用户的问题。

进入强化学习！RL 为我们提供了一种方法，可以对这些预训练的LLMs进行微调，以更好地实现这些期望的品质。

## Reinforcement Learning from Human Feedback (RLHF)

一种非常流行的使语言模型与人类价值观对齐的技术是人类反馈的强化学习（RLHF）。在 RLHF 中，我们使用人类反馈作为 RL 中的"奖励"信号的代理。

> 来源：Ouyang et al., "Training language models to follow instructions with human feedback" (InstructGPT) arXiv:2203.02155

这是如何运作的：

1. Get Human Preferences: We might ask humans to compare different responses generated by the LLM for the same input prompt and tell us which response they prefer. For example, we might show a human two different answers to the question "What is the capital of France?" and ask them "Which answer is better?".
获取人类偏好：我们可能会请人类比较同一个输入提示下由LLM生成的不同回答，并告诉我们他们更喜欢哪个回答。例如，我们可能会向人类展示两个不同的关于"法国的首都是什么？"问题的答案，并询问他们"哪个答案更好？"。
1. Train a Reward Model: We use this human preference data to train a separate model called a reward model. This reward model learns to predict what kind of responses humans will prefer. It learns to score responses based on helpfulness, harmlessness, and alignment with human preferences.
训练奖励模型：我们使用这些人类偏好数据来训练一个称为奖励模型的单独模型。这个奖励模型学会了预测人类会偏好哪种回答。它学会了根据有用性、无害性和与人类偏好的一致性来评分回答。
1. Fine-tune the LLM with RL: Now we use the reward model as the environment for our LLM agent. The LLM generates responses (actions), and the reward model scores these responses (provides rewards). In essence, we're training the LLM to produce text that our reward model (which learned from human preferences) thinks is good.
Fine-tune the LLM 通过 RL：现在我们使用奖励模型作为 LLM 代理的环境。LLM 生成响应（动作），奖励模型对这些响应进行评分（提供奖励）。本质上，我们是在训练 LLM 生成奖励模型（根据人类偏好学习）认为好的文本。
![image](W9jxdE1S2oC4eZxZM1lcWhIjnXb.png)

从总体上看，让我们来看看在LLMs中使用 RL 的好处：

Benefit

Description

Improved Control  提高控制能力

RL allows us to have more control over the kind of text LLMs generate. We can guide them to produce text that is more aligned with specific goals, like being helpful, creative, or concise.
RL 使我们能够对生成文本的类型有更多控制。我们可以引导它们生成更符合特定目标的文本，比如更有帮助性、更具创意性或更简洁。

Enhanced Alignment with Human Values
更好的与人类价值观对齐

RLHF, in particular, helps us align LLMs with complex and often subjective human preferences. It's hard to write down rules for "what makes a good answer," but humans can easily judge and compare responses. RLHF lets the model learn from these human judgments.
特别是 RLHF 帮助我们使模型与复杂的、往往具有主观性的人类偏好对齐。很难用规则来定义"什么是好的答案"，但人类很容易判断和比较不同的回答。RLHF 让模型从这些人类的判断中学习。

Mitigating Undesirable Behaviors
缓解不良行为

RL can be used to reduce negative behaviors in LLMs, such as generating toxic language, spreading misinformation, or exhibiting biases. By designing rewards that penalize these behaviors, we can nudge the model to avoid them.
强化学习可以用于减少LLMs中的负面行为，例如生成有毒语言、传播虚假信息或表现出偏见。通过设计惩罚这些行为的奖励，可以使模型避免这些行为。

强化学习从人类反馈中被用于训练当今许多最受欢迎的LLMs，例如 OpenAI 的 GPT-4、Google 的 Gemini 和 DeepSeek 的 R1。强化学习从人类反馈中存在多种技术，复杂性和精巧程度各不相同。在本章中，我们将重点介绍组相对策略优化（GRPO），这是一种已被证明对训练LLMs非常有效的强化学习从人类反馈技术，这些模型是有用的、无害的，并且与人类偏好保持一致。

## Why should we care about GRPO (Group Relative Policy Optimization)?

更多的 trainer 参考：https://huggingface.co/docs/trl/main/en/ppo_trainer

有许多 RLHF 的技术，但本课程专注于 GRPO，因为它在语言模型的强化学习方面代表了重要进步。

让我们简要考虑一下其他两种流行的 RLHF 技术：

- Proximal Policy Optimization (PPO)
代理策略优化（PPO）
- Direct Preference Optimization (DPO)
直接偏好优化（DPO）
代理策略优化（PPO）是最早的一种高效的强化学习人类反馈技术之一。它使用策略梯度方法根据来自单独的奖励模型的奖励来更新策略。

> 来源：Schulman et al., "Proximal Policy Optimization Algorithms" arXiv:1707.06347

直接偏好优化（DPO）后来被开发出来，作为一种更简单的技术，它不需要单独的奖励模型，而是直接使用偏好数据。本质上，将问题重新定义为选择响应和拒绝响应之间的分类任务。

与 DPO 和 PPO 不同，GRPO 将相似的样本分组在一起并作为一组进行比较。基于组的方法提供了与其他方法相比更稳定的梯度和更好的收敛特性。

GRPO 不使用像 DPO 那样的偏好数据，而是使用模型或函数提供的奖励信号来比较相似样本组。

GRPO 在获取奖励信号方面具有灵活性 - 它可以使用奖励模型（就像 PPO 做的那样），但并不严格需要一个。这是因为 GRPO 可以从任何可以评估响应质量的函数或模型中获取奖励信号。

例如，我们可以使用长度函数来奖励较短的响应，使用数学求解器来验证解的正确性，或者使用事实正确性函数来奖励更符合事实的响应。这种灵活性使 GRPO 在不同的对齐任务中特别具有通用性。

---

## 📚 推荐阅读

### 原始论文
- [Training language models to follow instructions with human feedback (InstructGPT)](https://arxiv.org/abs/2203.02155) — RLHF 应用于 LLM 的奠基工作
- [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347) — PPO 原始论文
- [DeepSeekMath: Pushing the Limits of Mathematical Reasoning](https://arxiv.org/abs/2402.03300) — GRPO 的实际应用

### 深度解读
- [Sutton & Barto, *Reinforcement Learning: An Introduction*](http://incompleteideas.net/book/the-book-2nd.html) — RL 经典教材，免费在线阅读 ⭐⭐⭐⭐⭐
- [HuggingFace TRL 文档](https://huggingface.co/docs/trl) — PPO/DPO/GRPO Trainer 的实战指南

### 实践资源
- [HuggingFace Open R1 项目](https://github.com/huggingface/open-r1) — 开源复现 DeepSeek-R1 的 GRPO 训练
- [TRL Examples](https://github.com/huggingface/trl/tree/main/examples) — PPO/DPO/GRPO 完整训练示例

## 🔧 落地应用

### 直接可用场景
- **对齐训练入门**：InstructGPT 的 SFT → RM → PPO 三阶段流程是理解所有 RLHF 变体的基础
- **GRPO 快速上手**：TRL 的 `GRPOTrainer` 不需要 Critic 模型，资源门槛远低于 PPO

### 工程实现要点
- **PPO vs GRPO 资源对比**：PPO 需要 4 个模型（Actor/Critic/RM/Ref），GRPO 只需 2 个（Actor/Ref），显存需求差 ~2x
- **Reward 信号设计**：GRPO 支持任意可调用函数作为 reward（数学正确性、代码编译通过、长度惩罚等），不限于 RM
- **KL 约束**：所有 RLHF 方法都需要 [[KL散度|KL 散度]]惩罚防止模型偏离参考策略太远

### 面试高频问法
- Q: RLHF 的三个阶段分别是什么？为什么需要 RL 而不是纯 SFT？
  A: ①SFT 学格式；②训练 Reward Model 学人类偏好排序；③PPO/GRPO 用 RM 信号微调。纯 SFT 只能学"像人类写的"，RL 能学"人类认为好的"——后者是对偏好排序的建模，SFT 做不到。

## 💡 启发与思考

### So What？对老板意味着什么
- 这篇是 RL for LLM 的"第一课"——PPO/DPO/GRPO 的定位差异搞清楚，后续深入才有方向
- GRPO 的"无 Critic"设计是 2024-2025 年最重要的实践简化，直接降低了 RL 训练的工程门槛

### 未解问题与局限
- RLHF 的 reward hacking 问题（模型学到 RM 的缺陷而非真正的人类偏好）仍是开放问题
- 人类标注的质量和一致性直接决定 RLHF 上限——如何 scale 高质量标注？

### 脑暴：如果往下延伸
- 如果 reward 信号完全来自可验证的函数（数学、代码），是否可以跳过 RM 阶段？→ 这正是 [[GRPO 深度理解|GRPO]] + verifiable rewards 的方向
- RLHF → RLAIF（AI Feedback）的转变是否意味着人类标注最终会被淘汰？

## 相关

> 🔗 See also: [[PPO 原理|PPO 原理]] — RLHF 经典算法的数学细节
> 🔗 See also: [[GRPO 深度理解|GRPO 深度理解]] — 无 Critic 的 RL 对齐方法
> 🔗 See also: [[KL散度|KL散度]] — RLHF 中防止策略偏移的核心约束

- [[RL 概览|RL 概览]] — 同方向伴侣笔记，概念互补
- [[AI/3-LLM/RL/目录|RL MOC]] — LLM 强化学习全图谱
- [[机器学习|机器学习]] — RL 的 ML 基础
