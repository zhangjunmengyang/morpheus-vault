---
title: KLæ•£åº¦ï¼šK1-K3 ä¼°è®¡å™¨ & Forward vs. Reverse
brief: KL æ•£åº¦çš„è’™ç‰¹å¡æ´›ä¼°è®¡ï¼ˆK1 æ— åé«˜æ–¹å·®ã€K2 æœ‰åä½æ–¹å·®ã€K3 æ— åä½æ–¹å·®ï¼‰åŠ Forward/Reverse KL åœ¨ RL å¯¹é½ä¸­çš„åº”ç”¨â€”â€”GRPO ä½¿ç”¨ Reverse KL + K3 ä¼°è®¡å™¨é˜²æ­¢ç­–ç•¥åç§»ã€‚
type: concept
domain: ai/llm/rl/fundamentals
created: 2026-02-13
updated: 2026-02-22
tags:
  - ai/llm/rl/fundamentals
  - type/concept
  - interview/hot
status: complete
sources:
  - Schulman, Approximating KL Divergence â€” http://joschu.net/blog/kl-approx.html
  - Trust Region Policy Optimization (TRPO) â€” arXiv:1502.05477
  - DeepSeekMath (GRPO) â€” arXiv:2402.03300
  - Cover & Thomas, Elements of Information Theory, 2nd ed., Wiley, 2006
related:
  - "[[GRPO æ·±åº¦ç†è§£|GRPO æ·±åº¦ç†è§£]]"
  - "[[PPO åŸç†|PPO åŸç†]]"
  - "[[ä¿¡æ¯è®º|ä¿¡æ¯è®º]]"
---
#  K1-K3 & Forward vs. Reverse KLæ•£åº¦

# ä¸€ã€KL æ•£åº¦

https://github.com/chunhuizhang/llm_rl/blob/main/tutorials/r1-k1.5/grpo_kl.ipynb

## åŸºæœ¬æ¦‚å¿µ

ç”¨äºè¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒ $P$ å’Œ $Q$ ä¹‹é—´çš„å·®å¼‚ï¼Œå¯¹äºç¦»æ•£éšæœºå˜é‡ï¼Œå…¬å¼å®šä¹‰ä¸ºï¼š

$$D_{\text{KL}}(P \| Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}$$

- $P(x)$ï¼šçœŸå®åˆ†å¸ƒçš„æ¦‚ç‡è´¨é‡å‡½æ•°
- $Q(x)$ï¼šè¿‘ä¼¼åˆ†å¸ƒçš„æ¦‚ç‡è´¨é‡å‡½æ•°

æ¯ä¸€é¡¹ $P(x) \log \frac{P(x)}{Q(x)}$ è¡¨ç¤ºåœ¨äº‹ä»¶ $x$ ä¸Šï¼ŒçœŸå®åˆ†å¸ƒ $P$ ä¸è¿‘ä¼¼åˆ†å¸ƒ $Q$ çš„å·®å¼‚åŠ æƒåçš„ä¿¡æ¯æŸå¤±ã€‚

> æ¥æºï¼šCover & Thomas, *Elements of Information Theory*, 2nd ed., Ch. 2

## å…³é”®æ€§è´¨

- **éå¯¹ç§°æ€§**ï¼šKL æ•£åº¦ä¸æ˜¯è·ç¦»åº¦é‡â€”â€”$D_{\text{KL}}(P \| Q) \neq D_{\text{KL}}(Q \| P)$ï¼Œä¹Ÿä¸æ»¡è¶³ä¸‰è§’ä¸ç­‰å¼
- **ç›´è§‰ç†è§£**ï¼šKL æ•£åº¦æœ¬è´¨æ˜¯ç”¨æœ€ä¼˜ç¼–ç çš„ä¿¡æ¯è®ºä»£ç†é‡ï¼Œè¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒæœ‰å¤šå¤§åŒºåˆ«ï¼ˆ[éœå¤«æ›¼ç¼–ç è§’åº¦](https://www.zhihu.com/question/345907033/answer/3072696582)ï¼‰
- **éè´Ÿæ€§**ï¼š$D_{\text{KL}}(P \| Q) \geq 0$ï¼Œç­‰å·å½“ä¸”ä»…å½“ $P = Q$
- **ä¸äº¤å‰ç†µå…³ç³»**ï¼š$D_{\text{KL}}(P \| Q) = H(P, Q) - H(P)$ï¼Œæœ€å°åŒ–äº¤å‰ç†µç­‰ä»·äºæœ€å°åŒ– KL æ•£åº¦ï¼ˆå½“ $P$ å›ºå®šæ—¶ï¼‰
åœ¨å®é™…åº”ç”¨ä¸­ï¼Œç²¾ç¡®è®¡ç®—KLæ•£åº¦é€šå¸¸ä¸å¯è¡Œï¼Œå› ä¸ºï¼š

1. **æˆæœ¬è¿‡é«˜**ï¼šè®¡ç®—æ‰€æœ‰ ( xx*x* ) çš„æ¦‚ç‡å’Œéœ€è¦è¿‡å¤šçš„è®¡ç®—æˆ–å†…å­˜ã€‚
1. **æ— æ³•è®¡ç®—**ï¼šåˆ†å¸ƒå¯èƒ½æ²¡æœ‰é—­åˆè¡¨è¾¾å¼ï¼Œæ— æ³•è§£ææ±‚è§£ã€‚
è’™ç‰¹å¡æ´›æ–¹æ³•æˆä¸ºä¼°è®¡ KL æ•£åº¦çš„å¸¸ç”¨ç­–ç•¥ã€‚å‡è®¾æˆ‘ä»¬ä»åˆ†å¸ƒ q ä¸­é‡‡æ ·ï¼Œ

å¦‚ä½•æ„é€ ä¸€ä¸ª**æ— åã€ä½æ–¹å·®çš„ä¼°è®¡å™¨**æ˜¯å…³é”®é—®é¢˜ã€‚

## K1 K2 K3

### K1 æ— åé«˜æ–¹å·®

> æ¥æºï¼šSchulman, [Approximating KL Divergence](http://joschu.net/blog/kl-approx.html)

æœ€ç›´æ¥çš„è’™ç‰¹å¡æ´›ä¼°è®¡å™¨ï¼Œä»¤ $r = \frac{p(x)}{q(x)}$ï¼š

$$K_1 = \mathbb{E}_{x \sim q}\left[-\log r\right] = \mathbb{E}_{x \sim q}\left[\log \frac{q(x)}{p(x)}\right]$$

- **ç‰¹ç‚¹**ï¼šè¿™ä¸ªä¼°è®¡å™¨æ˜¯**æ— åçš„**ï¼ˆ$\mathbb{E}[K_1] = D_{\text{KL}}(q \| p)$ï¼‰ã€‚ç„¶è€Œï¼Œç”±äº $\log r$ åœ¨æ­£è´Ÿä¹‹é—´å˜åŒ–ï¼ˆ$r > 1$ æ—¶ä¸ºæ­£ï¼Œ$r < 1$ æ—¶ä¸ºè´Ÿï¼‰ï¼Œ**æ–¹å·®å¾ˆé«˜**ï¼Œè€Œ KL æ•£åº¦æœ¬èº«å§‹ç»ˆä¸ºæ­£ã€‚è¿™ç§é«˜æ–¹å·®ä½¿å¾— K1 åœ¨å®é™…åº”ç”¨ä¸­è¡¨ç°ä¸ä½³ã€‚
### K2 æœ‰åä½æ–¹å·®

$$K_2 = \mathbb{E}_{x \sim q}\left[\frac{(\log r)^2}{2}\right]$$

- **ç‰¹ç‚¹ 1**ï¼šæœ‰åï¼Œä½†æ–¹å·®æ˜¾è‘—ä½äº K1â€”â€”æ¯ä¸ªæ ·æœ¬ $\frac{(\log r)^2}{2} \geq 0$ï¼Œä¸ KL çš„éè´Ÿæ€§ä¸€è‡´
- **ç‰¹ç‚¹ 2**ï¼š$K_2$ çš„æœŸæœ›æ˜¯ä¸€ä¸ª f-æ•£åº¦ï¼š$\mathbb{E}[K_2] = D_f(q \| p)$ï¼Œå…¶ä¸­ $f(r) = \frac{(\log r)^2}{2}$

å½“ $p$ å’Œ $q$ æ¥è¿‘æ—¶ï¼Œæ‰€æœ‰å¯å¾®çš„ f-æ•£åº¦åœ¨äºŒé˜¶è¿‘ä¼¼ä¸‹ä¸ KL æ•£åº¦ç­‰ä»·ã€‚

### K3 æ— åä½æ–¹å·®

ä¸ºäº†å…¼é¡¾æ— åå’Œä½æ–¹å·®ï¼ŒSchulman å¼•å…¥äº†æ§åˆ¶å˜é‡ï¼ˆcontrol variateï¼‰æ–¹æ³•ã€‚åˆ©ç”¨ $\mathbb{E}_{x \sim q}[r - 1] = 0$ï¼Œæ„é€ ï¼š

$$K_3 = \mathbb{E}_{x \sim q}\left[(r - 1) - \log r\right]$$

ç”±äº $\log$ çš„å‡¹æ€§ï¼š$r - 1 \geq \log r$ï¼ˆJensen ä¸ç­‰å¼ï¼‰ï¼Œæ‰€ä»¥ **$K_3$ æ’éè´Ÿã€æ— åä¸”ä½æ–¹å·®**â€”â€”è¿™æ˜¯ GRPO çš„é»˜è®¤é€‰æ‹©ã€‚

## ä»£ç éªŒè¯

```
*import* torch.distributions *as* dis
p = dis.Normal(*loc*=0, *scale*=1)
q = dis.Normal(*loc*=0.1, *scale*=1)
x = q.sample(*sample_shape*=(10_000_000,))
truekl = dis.kl_divergence(q, p)
print("true", truekl)
logr = p.log_prob(x) - q.log_prob(x)
k1 = -logr
k2 = logr ** 2 / 2
k3 = (logr.exp() - 1) - logr
*for* k *in* (k1, k2, k3):
    print(k.mean(), (k.mean() - truekl) / truekl, k.std() / truekl)
```

# **äºŒã€æ­£å‘KL vs. åå‘KL**

- **æ­£å‘KL**ï¼š$D_{\text{KL}}(P \| Q)$ â€” çœŸå®åˆ†å¸ƒ $P$ ç›¸å¯¹äºæ¨¡å‹åˆ†å¸ƒ $Q$
- **åå‘KL**ï¼š$D_{\text{KL}}(Q \| P)$ â€” æ¨¡å‹åˆ†å¸ƒ $Q$ ç›¸å¯¹äºçœŸå®åˆ†å¸ƒ $P$
## **æ­£å‘KLæ•£åº¦ï¼šMean-seeking**

$$D_{\text{KL}}(P \| Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)} = -\sum_x P(x) \log Q(x) + \text{const}$$

ç”±äº $H(P)$ ä¸ä¾èµ–äº $Q$ï¼Œæœ€å°åŒ– $D_{\text{KL}}(P \| Q)$ ç­‰ä»·äºæœ€å¤§åŒ– $\mathbb{E}_{x \sim P}[\log Q(x)]$ã€‚

**å…³é”®ç‰¹å¾**ï¼šæœŸæœ›åœ¨çœŸå®åˆ†å¸ƒ $P$ ä¸‹è®¡ç®—ã€‚

å½“ $P$ æ˜¯å¤šæ¨¡æ€åˆ†å¸ƒæ—¶ï¼š

- ä¼˜åŒ–ç›®æ ‡è¦æ±‚ $Q$ åœ¨ $P$ çš„**æ‰€æœ‰**é«˜æ¦‚ç‡åŒºåŸŸéƒ½è¦æœ‰åˆç†æ¦‚ç‡å¯†åº¦
- å¦‚æœ $Q$ å¿½ç•¥ $P$ çš„æŸä¸ªæ¨¡æ€ï¼Œè¯¥åŒºåŸŸçš„ $\log Q(x) \to -\infty$ï¼Œå¯¼è‡´ç›®æ ‡å‡½æ•°å˜å·®

**ç»“æœ**ï¼š$Q$ å€¾å‘äº"è¦†ç›–"$P$ çš„æ‰€æœ‰æ¨¡æ€ï¼ˆmean-seekingï¼‰ï¼Œå³ä½¿è¦åœ¨æ¨¡æ€ä¹‹é—´åˆ†é…æ¦‚ç‡è´¨é‡ï¼Œå¯¼è‡´æ¨¡ç³Šçš„å¹³å‡åŒ–æ•ˆæœã€‚

## **åå‘KLæ•£åº¦ï¼šMode-seeking**

$$D_{\text{KL}}(Q \| P) = \sum_x Q(x) \log \frac{Q(x)}{P(x)} = -\sum_x Q(x) \log P(x) + \sum_x Q(x) \log Q(x)$$

ç­‰ä»·äºæœ€å¤§åŒ– $\mathbb{E}_{x \sim Q}[\log P(x)] + H(Q)$ã€‚

**å…³é”®ç‰¹å¾**ï¼šæœŸæœ›åœ¨æ¨¡å‹åˆ†å¸ƒ $Q$ ä¸‹è®¡ç®—ã€‚

å½“ $P$ æ˜¯å¤šæ¨¡æ€åˆ†å¸ƒæ—¶ï¼š

- ä¼˜åŒ–ç›®æ ‡é¼“åŠ± $Q$ å°†æ¦‚ç‡è´¨é‡é›†ä¸­åœ¨ $P$ çš„**é«˜æ¦‚ç‡**åŒºåŸŸ
- å¦‚æœ $Q$ åœ¨ $P$ çš„ä½æ¦‚ç‡åŒºåŸŸåˆ†é…è´¨é‡ï¼Œ$\log P(x)$ å¾ˆå°ï¼Œæ‹–ä½ç›®æ ‡å‡½æ•°
- ç†µé¡¹ $H(Q)$ é¼“åŠ±åˆ†æ•£ï¼Œä½†ç¬¬ä¸€é¡¹å½±å“æ›´å¼º

**ç»“æœ**ï¼š$Q$ å€¾å‘äº"é€‰æ‹©"$P$ çš„ä¸€ä¸ªæˆ–å°‘æ•°å‡ ä¸ªä¸»è¦æ¨¡æ€ï¼ˆmode-seekingï¼‰ï¼Œå¿½ç•¥å…¶ä»–æ¨¡æ€ã€‚

> æ¥æºï¼šTRPO arXiv:1502.05477 ä¸­ä½¿ç”¨ Reverse KL çº¦æŸç­–ç•¥æ›´æ–°ï¼Œæ­£æ˜¯åˆ©ç”¨äº† mode-seeking ç‰¹æ€§
## å¯¹æ¯”

- å¯è§†åŒ–ï¼š[csc413-2020.github.io](https%3A%2F%2Fcsc413-2020.github.io%2Fassets%2Ftutorials%2Ftut09_infotheory.pdf)
![image](LSJNd9ocfoVOOpxOHXVcpxHfn3e.png)

### åº”ç”¨å¯¹æ¯”

| æ–¹å‘ | åº”ç”¨ | åŸå›  |
|------|------|------|
| Forward KL | GAN è®­ç»ƒ | å¸Œæœ›ç”Ÿæˆæ ·æœ¬è¦†ç›–æ•°æ®æ‰€æœ‰å˜åŒ– |
| Forward KL | VAE æ­£åˆ™åŒ– | çº¦æŸ $Q(z)$ æ¥è¿‘å…ˆéªŒ $P(z) = \mathcal{N}(0, I)$ |
| Reverse KL | æ¨¡å‹è’¸é¦ | å­¦ç”Ÿé›†ä¸­å­¦æ•™å¸ˆçš„ä¸»è¦æ¨¡å¼ |
| Reverse KL | RL å¯¹é½ï¼ˆ[[GRPO æ·±åº¦ç†è§£\|GRPO]]ï¼‰ | $D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}})$ é˜²æ­¢æ–°ç­–ç•¥åç¦»å‚è€ƒç­–ç•¥å¤ªè¿œ |

> å»¶ä¼¸é˜…è¯»ï¼š[Reverse vs Forward KL](https://www.tuananhle.co.uk/notes/reverse-forward-kl.html)
# ä¸‰ã€GRPO åº”ç”¨

GRPO ä¸­ä½¿ç”¨çš„æ˜¯**åå‘ KL + K3 ä¼°è®¡å™¨**ï¼š

> æ¥æºï¼šShao et al., "DeepSeekMath" arXiv:2402.03300, Sec. 3.2

- $\pi_\theta$ æ˜¯æ–°æ¨¡å‹ï¼Œ$\pi_{\text{ref}}$ æ˜¯å‚è€ƒæ¨¡å‹
- $r = \frac{\pi_\theta(a|s)}{\pi_{\text{ref}}(a|s)}$
- KL æƒ©ç½šé¡¹ï¼š$D_{\text{KL}}(\pi_\theta \| \pi_{\text{ref}}) \approx \mathbb{E}_{a \sim \pi_\theta}[(r - 1) - \log r]$ï¼ˆK3 ä¼°è®¡å™¨ï¼‰

**ä»£ç éªŒè¯ï¼š**

- æ— å
```
*import* torch.distributions *as* dis
*import* torch
p = dis.Normal(*loc*=0, *scale*=1)
q = dis.Normal(*loc*=0.1, *scale*=1)
x = q.sample(*sample_shape*=(10_000_000,))
torch.sum((q.log_prob(x) - p.log_prob(x)) < 0)
```

- éè´Ÿ
```
*import* matplotlib.pyplot *as* plt
*import* numpy *as* np
xs = np.arange(0.01, 5, 0.01)
plt.plot(np.log(xs), *label*=r'$\log x$')
plt.plot(xs-1, *label*='$x-1$')
plt.legend()
```

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Schulman, Approximating KL Divergence](http://joschu.net/blog/kl-approx.html) â€” K1/K2/K3 ä¼°è®¡å™¨çš„åŸå§‹æ¨å¯¼
- [Trust Region Policy Optimization (TRPO)](https://arxiv.org/abs/1502.05477) â€” KL çº¦æŸåœ¨ RL ç­–ç•¥ä¼˜åŒ–ä¸­çš„ç»å…¸åº”ç”¨
- [DeepSeekMath](https://arxiv.org/abs/2402.03300) â€” GRPO ä¸­ K3 ä¼°è®¡å™¨çš„å®é™…ä½¿ç”¨

### æ·±åº¦è§£è¯»
- [KL æ•£åº¦çš„éœå¤«æ›¼ç¼–ç ç›´è§‰ï¼ˆçŸ¥ä¹ï¼‰](https://www.zhihu.com/question/345907033/answer/3072696582) â€” ä¿¡æ¯è®ºè§’åº¦ç†è§£ KL æ•£åº¦ â­â­â­â­
- [Reverse vs Forward KL Visualization](https://www.tuananhle.co.uk/notes/reverse-forward-kl.html) â€” Mean-seeking vs Mode-seeking å¯è§†åŒ–

### å®è·µèµ„æº
- [chunhuizhang/llm_rl: GRPO KL notebook](https://github.com/chunhuizhang/llm_rl/blob/main/tutorials/r1-k1.5/grpo_kl.ipynb) â€” K1/K2/K3 ä»£ç éªŒè¯
- [CSC413 Info Theory Tutorial (PDF)](https://csc413-2020.github.io/assets/tutorials/tut09_infotheory.pdf) â€” Forward/Reverse KL å¯è§†åŒ–æ•™ç¨‹

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **GRPO/PPO çš„ KL æƒ©ç½šå®ç°**ï¼šK3 ä¼°è®¡å™¨æ˜¯ TRL `GRPOTrainer` çš„é»˜è®¤ KL penaltyï¼ˆ`kl_type="k3"`ï¼‰
- **æ¨¡å‹è’¸é¦ loss è®¾è®¡**ï¼šReverse KL è®©å­¦ç”Ÿæ¨¡å‹é›†ä¸­å­¦æ•™å¸ˆçš„é«˜ç½®ä¿¡è¾“å‡º

### å·¥ç¨‹å®ç°è¦ç‚¹
- **K3 å…¬å¼**ï¼š`kl = (ratio - 1) - torch.log(ratio)`ï¼Œå…¶ä¸­ `ratio = exp(logprob_new - logprob_ref)`
- **Î² ç³»æ•°è°ƒä¼˜**ï¼šGRPO ä¸­ KL æƒ©ç½šç³»æ•° $\beta$ é€šå¸¸å– 0.01-0.1ï¼Œå¤ªå¤§é™åˆ¶æ¢ç´¢ï¼Œå¤ªå°å¯¼è‡´ç­–ç•¥æ¼‚ç§»
- **æ•°å€¼ç¨³å®šæ€§**ï¼š`ratio` æ¥è¿‘ 0 æ—¶ `log(ratio)` ä¼šæº¢å‡ºï¼Œå®è·µä¸­åš clamp

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: Forward KL å’Œ Reverse KL çš„ç›´è§‰åŒºåˆ«ï¼Ÿä¸ºä»€ä¹ˆ RLHF ç”¨ Reverseï¼Ÿ
  A: Forward KLï¼ˆ$D_{\text{KL}}(P \| Q)$ï¼‰æ˜¯ mean-seekingâ€”â€”$Q$ å¿…é¡»è¦†ç›– $P$ çš„æ‰€æœ‰æ¨¡æ€ï¼›Reverse KLï¼ˆ$D_{\text{KL}}(Q \| P)$ï¼‰æ˜¯ mode-seekingâ€”â€”$Q$ å€¾å‘é›†ä¸­åœ¨ $P$ çš„ä¸»æ¨¡æ€ã€‚RLHF ç”¨ Reverse æ˜¯å› ä¸ºæˆ‘ä»¬å¸Œæœ›æ–°ç­–ç•¥ $\pi_\theta$ é›†ä¸­åœ¨å‚è€ƒç­–ç•¥ $\pi_{\text{ref}}$ çš„é«˜æ¦‚ç‡åŒºåŸŸï¼Œè€Œéå¼ºåˆ¶è¦†ç›–æ‰€æœ‰å¯èƒ½è¾“å‡ºã€‚

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- KL æ•£åº¦æ˜¯ RL å¯¹é½çš„"å®‰å…¨é˜€"â€”â€”ç†è§£ K1/K2/K3 çš„åå·®-æ–¹å·® trade-off æ˜¯è°ƒä¼˜ GRPO/PPO çš„åŸºç¡€
- Forward/Reverse çš„é€‰æ‹©ä¸æ˜¯éšæ„çš„â€”â€”å®ƒå†³å®šäº†æ¨¡å‹æ›´æ–°çš„"ä¿å®ˆç¨‹åº¦"

### æœªè§£é—®é¢˜ä¸å±€é™
- K3 è™½ç„¶æ— åä½æ–¹å·®ï¼Œä½†åœ¨ $\pi_\theta$ ä¸ $\pi_{\text{ref}}$ å·®å¼‚å¾ˆå¤§æ—¶ï¼Œå•ç‚¹ä¼°è®¡ä»å¯èƒ½ä¸å‡†
- æ˜¯å¦å­˜åœ¨æ¯” K3 æ›´ä¼˜çš„ KL ä¼°è®¡å™¨ï¼Ÿè‡ªé€‚åº”æ–¹å·®çš„ä¼°è®¡å™¨ï¼ˆå¦‚ RLOOï¼‰æ˜¯å¦æ›´å¥½ï¼Ÿ

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- å¦‚æœæŠŠ [[PPO åŸç†|PPO]] çš„ clip ratio å’Œ KL æƒ©ç½šç»Ÿä¸€åˆ°ä¸€ä¸ªæ¡†æ¶ï¼Œèƒ½å¦å¾—åˆ°æ›´ä¼˜çš„ç­–ç•¥çº¦æŸï¼Ÿ
- Jensen-Shannon æ•£åº¦ï¼ˆForward + Reverse çš„å¯¹ç§°å¹³å‡ï¼‰åœ¨ RLHF ä¸­æ˜¯å¦æœ‰åº”ç”¨æ½œåŠ›ï¼Ÿ

## ç›¸å…³

> ğŸ”— See also: [[GRPO æ·±åº¦ç†è§£|GRPO æ·±åº¦ç†è§£]] â€” ä½¿ç”¨ Reverse KL + K3 çš„ä¸»æµ RL å¯¹é½æ–¹æ³•
> ğŸ”— See also: [[PPO åŸç†|PPO åŸç†]] â€” KL çº¦æŸçš„å¦ä¸€ç§å®ç°ï¼ˆclip ratioï¼‰
> ğŸ”— See also: [[ä¿¡æ¯è®º|ä¿¡æ¯è®º]] â€” KL æ•£åº¦çš„æ•°å­¦åŸºç¡€

- [[æ¦‚ç‡ä¸åˆ†å¸ƒ|æ¦‚ç‡ä¸åˆ†å¸ƒ]]
- [[DPO-TRLå®è·µ|DPO]]
