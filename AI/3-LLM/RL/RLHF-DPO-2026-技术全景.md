---
title: RLHF / RLAIF / DPO 2026 æŠ€æœ¯å…¨æ™¯ï¼ˆé¢è¯•æ­¦å™¨ç‰ˆï¼‰
brief: è¦†ç›– LLM å¯¹é½è®­ç»ƒå®Œæ•´æŠ€æœ¯æ ˆï¼šç»å…¸ RLHFï¼ˆSFTâ†’RMâ†’PPOï¼‰ã€DPO å®¶æ—ï¼ˆDPO/IPO/KTO/ORPO/SimPOï¼‰ã€RLAIF/Constitutional AIã€Reward Model è®¾è®¡ï¼ˆBradley-Terry/PRM/ORMï¼‰ã€2026 å‰æ²¿ï¼ˆGRPO/DAPO/REINFORCE++/RLVRï¼‰ï¼Œä»¥åŠ 15 é“æ·±åº¦é¢è¯•é¢˜ã€‚æ ¸å¿ƒæ´å¯Ÿï¼šå¯¹é½èŒƒå¼æ­£ä»'æ•™æ¨¡å‹åƒäººè¯´è¯'ï¼ˆRLHFï¼‰â†’'ç®€åŒ–åå¥½å­¦ä¹ 'ï¼ˆDPOï¼‰â†’'è‡ªä¸»æ¢ç´¢æ¶Œç°èƒ½åŠ›'ï¼ˆGRPO+RLVRï¼‰ï¼ŒDeepSeek-R1 è¯æ˜çº¯ RL å¯æ¶Œç°æ¨ç†èƒ½åŠ›ã€‚é¢è¯•æ­¦å™¨çº§å…¨æ™¯ã€‚
type: survey
domain: ai/llm/rl
date: 2026-02-20
updated: 2026-02-22
tags:
  - ai/llm/rl
  - type/survey
  - rlhf
  - dpo
  - alignment
  - llm-training
  - interview-prep
status: complete
sources:
  - InstructGPT â€” Ouyang et al. arXiv:2203.02155
  - PPO â€” Schulman et al. arXiv:1707.06347
  - DPO â€” Rafailov et al. arXiv:2305.18290
  - DAPO â€” Yu et al. (ByteDance/æ¸…å) arXiv:2503.14476
  - KTO â€” Ethayarajh et al. arXiv:2402.01306
  - RLOO â€” Ahmadian et al. arXiv:2402.14740
  - IPO â€” Azar et al. arXiv:2310.12036
  - GRPO / DeepSeek-Math â€” Shao et al. arXiv:2402.03300
  - DeepSeek-R1 â€” DeepSeek-AI arXiv:2501.12948
  - Constitutional AI â€” Bai et al. arXiv:2212.08073
  - SimPO â€” Meng et al. arXiv:2405.14734
  - ORPO â€” Hong et al. arXiv:2403.07691
  - Let's Verify Step by Step (PRM) â€” Lightman et al. arXiv:2305.20050
related:
  - "[[GRPO-Improvement-Panorama-2026]]"
  - "[[MARS-Margin-Aware-Reward-Modeling-Self-Refinement]]"
  - "[[GRPO æ·±åº¦ç†è§£]]"
  - "[[LoRA]]"
---

# RLHF / RLAIF / DPO 2026 æŠ€æœ¯å…¨æ™¯ï¼ˆé¢è¯•æ­¦å™¨ç‰ˆï¼‰

> æœ¬ç¬”è®°è¦†ç›– LLM å¯¹é½è®­ç»ƒçš„å®Œæ•´æŠ€æœ¯æ ˆï¼šä»ç»å…¸ RLHF åˆ° DPO å®¶æ—ã€RLAIFã€Reward Model è®¾è®¡ã€2026 å‰æ²¿ç®—æ³•ï¼ˆGRPO/DAPO/REINFORCE++ï¼‰ã€å·¥ç¨‹æ¡†æ¶ã€å¸¸è§å¤±è´¥æ¨¡å¼ï¼Œä»¥åŠ 15 é“æ·±åº¦é¢è¯•é¢˜ã€‚ç›®æ ‡ï¼šåœ¨ RL for LLM æ–¹å‘çš„é¢è¯•ä¸­ç¢¾å‹å¯¹æ‰‹ã€‚

---

## ç›®å½•

1. [ä» RLHF åˆ° DPOï¼šå¯¹é½æŠ€æœ¯æ¼”è¿›å²](#1-ä»-rlhf-åˆ°-dpoå¯¹é½æŠ€æœ¯æ¼”è¿›å²)
2. [RLHF å…¨æµç¨‹æ‹†è§£](#2-rlhf-å…¨æµç¨‹æ‹†è§£sft--rm--ppo)
3. [DPO åŠå…¶å˜ä½“](#3-dpo-åŠå…¶å˜ä½“)
4. [RLAIFï¼šAI åé¦ˆæ›¿ä»£äººç±»åé¦ˆ](#4-rlaifai-åé¦ˆæ›¿ä»£äººç±»åé¦ˆ)
5. [Reward Model è®¾è®¡](#5-reward-model-è®¾è®¡)
6. [2026 å‰æ²¿](#6-2026-å‰æ²¿)
7. [å·¥ç¨‹å®è·µ](#7-å·¥ç¨‹å®è·µ)
8. [å¸¸è§å¤±è´¥æ¨¡å¼](#8-å¸¸è§å¤±è´¥æ¨¡å¼)
9. [é¢è¯•é«˜é¢‘é¢˜ 15 é“ + æ·±åº¦å‚è€ƒç­”æ¡ˆ](#9-é¢è¯•é«˜é¢‘é¢˜-15-é“--æ·±åº¦å‚è€ƒç­”æ¡ˆ)

---

## 1. ä» RLHF åˆ° DPOï¼šå¯¹é½æŠ€æœ¯æ¼”è¿›å²

### é¢è¯•å®˜ä¼šé—®ï¼šã€Œè¯·æ¢³ç†ä¸€ä¸‹ LLM å¯¹é½æŠ€æœ¯ä» RLHF åˆ° DPO çš„æ¼”è¿›è„‰ç»œï¼Œä¸ºä»€ä¹ˆä¼šå‡ºç°è¿™äº›å˜åŒ–ï¼Ÿã€

### 1.1 æ—¶é—´çº¿

| æ—¶é—´ | é‡Œç¨‹ç¢‘ | æ ¸å¿ƒè´¡çŒ® |
|------|--------|---------|
| 2017 | OpenAI "Learning from Human Preferences" | é¦–æ¬¡å°†äººç±»åå¥½å¼•å…¥ RL è®­ç»ƒ |
| 2020 | Stiennon et al. "Learning to Summarize" | RLHF ç”¨äºæ–‡æœ¬æ‘˜è¦ï¼Œå¥ å®š RM + PPO èŒƒå¼ |
| 2022.01 | InstructGPT (Ouyang et al.) [arXiv:2203.02155](https://arxiv.org/abs/2203.02155) | ä¸‰é˜¶æ®µèŒƒå¼ï¼ˆSFT â†’ RM â†’ PPOï¼‰å·¥ä¸šåŒ– |
| 2022.03 | ChatGPT | RLHF ä»å­¦æœ¯èµ°å‘äº§å“ï¼Œæ”¹å˜è¡Œä¸š |
| 2022.12 | Anthropic Constitutional AI | RLAIF æ¦‚å¿µè¯ç”Ÿï¼Œç”¨ AI åé¦ˆæ›¿ä»£äººç±» |
| 2023.05 | DPO (Rafailov et al.) [arXiv:2305.18290](https://arxiv.org/abs/2305.18290) | ç»•è¿‡ RM å’Œ PPOï¼Œç›´æ¥åå¥½ä¼˜åŒ– |
| 2023.10 | Zephyr-7B | DPO åœ¨å¼€æºç¤¾åŒºå¤§è§„æ¨¡éªŒè¯ |
| 2024.01 | KTO (Ethayarajh et al.) [arXiv:2402.01306](https://arxiv.org/abs/2402.01306) | ä¸éœ€è¦ pairwise æ•°æ®ï¼Œbinary ä¿¡å·å³å¯ |
| 2024.03 | ORPO (Hong et al.) | å»æ‰ reference modelï¼Œå•é˜¶æ®µè®­ç»ƒ |
| 2024.05 | SimPO (Meng et al.) | ç§»é™¤ reference log-ratioï¼Œæ›´ç¨³å®š |
| 2024.06 | IPO (Azar et al.) [arXiv:2310.12036](https://arxiv.org/abs/2310.12036) | ä¿®å¤ DPO çš„ overfitting é—®é¢˜ |
| 2025.01 | DeepSeek-R1 + GRPO [arXiv:2501.12948](https://arxiv.org/abs/2501.12948) | RL Scaling æ–°èŒƒå¼ï¼Œçº¯ RL æ¶Œç°æ¨ç†èƒ½åŠ› |
| 2025.03 | DAPO (ByteDance) [arXiv:2503.14476](https://arxiv.org/abs/2503.14476) | GRPO æ”¹è¿›ç‰ˆï¼ŒDecoupled Clip + Dynamic Sampling |
| 2025.06 | REINFORCE++ | å» critic çš„ REINFORCE + baseline ä¼˜åŒ– |
| 2025.09 | Self-Play alignment æˆç†Ÿ | SPIN/SPPO è‡ªåšå¼ˆå¯¹é½ï¼Œå‡å°‘äººç±»æ ‡æ³¨ |
| 2026.01 | Iterative Online Alignment å·¥ä¸šè½åœ° | æŒç»­åœ¨çº¿å¯¹é½æˆä¸ºæ ‡é… |

### 1.2 ä¸‰ä»£èŒƒå¼

**ç¬¬ä¸€ä»£ï¼šRLHFï¼ˆ2020-2023ï¼‰**
- æ ¸å¿ƒï¼šSFT â†’ Train Reward Model â†’ PPO
- ä¼˜ç‚¹ï¼šæ•ˆæœæœ€å¥½ï¼ˆå½“æ—¶ï¼‰ï¼Œç†è®ºåŸºç¡€æ‰å®
- ç—›ç‚¹ï¼šè®­ç»ƒä¸ç¨³å®šã€éœ€è¦ 4 ä¸ªæ¨¡å‹åŒæ—¶åœ¨æ˜¾å­˜ä¸­ï¼ˆactor, critic, reward, referenceï¼‰ã€å·¥ç¨‹å¤æ‚åº¦æé«˜

**ç¬¬äºŒä»£ï¼šDirect Alignment / DPO å®¶æ—ï¼ˆ2023-2025ï¼‰**
- æ ¸å¿ƒï¼šå°† RL é—®é¢˜è½¬åŒ–ä¸º supervised loss
- ä¼˜ç‚¹ï¼šç®€å•ã€ç¨³å®šã€ä¸€å¼ å¡å°±èƒ½è·‘
- ç—›ç‚¹ï¼šoffline ä¼˜åŒ–æœ‰ä¸Šé™ã€å®¹æ˜“ mode collapseã€æ— æ³•æŒç»­æ¢ç´¢

**ç¬¬ä¸‰ä»£ï¼šScalable RL + Verifiable Rewardsï¼ˆ2025-2026ï¼‰**
- æ ¸å¿ƒï¼šGRPO/DAPO + rule-based/verifiable rewards + online generation
- ä»£è¡¨ï¼šDeepSeek-R1, Qwen-2.5-Max
- ä¼˜ç‚¹ï¼šRL scaling law è¢«éªŒè¯ã€æ¨ç†èƒ½åŠ›æ¶Œç°ã€ä¸éœ€è¦äººç±»æ ‡æ³¨
- è¶‹åŠ¿ï¼šRL ä¸å†åªæ˜¯ "alignment"ï¼Œè€Œæ˜¯ "capability elicitation"

### 1.3 å…³é”®æ´å¯Ÿ

> **ä» "æ•™æ¨¡å‹åƒäººè¯´è¯" åˆ° "è®©æ¨¡å‹è‡ªå·±æ¢ç´¢æ›´å¥½çš„ç­–ç•¥"ï¼Œè¿™æ˜¯æ•´ä¸ªé¢†åŸŸæœ€æ ¹æœ¬çš„èŒƒå¼è½¬å˜ã€‚**

RLHF æ•™æ¨¡å‹æ¨¡ä»¿äººç±»åå¥½ï¼›DPO ç®€åŒ–äº†è¿™ä¸ªè¿‡ç¨‹ï¼›è€Œ GRPO/RLVR åˆ™è®©æ¨¡å‹é€šè¿‡è‡ªä¸»æ¢ç´¢å‘ç°è¶…è¶Šè®­ç»ƒæ•°æ®çš„ç­–ç•¥ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ DeepSeek-R1 èƒ½åœ¨æ•°å­¦æ¨ç†ä¸Šå±•ç°å‡º "aha moment"â€”â€”çº¯ RL æ¶Œç°å‡ºé“¾å¼æ¨ç†èƒ½åŠ›ã€‚

---

## 2. RLHF å…¨æµç¨‹æ‹†è§£ï¼ˆSFT â†’ RM â†’ PPOï¼‰

### é¢è¯•å®˜ä¼šé—®ï¼šã€Œè¯·è¯¦ç»†è®²è®² RLHF çš„ä¸‰ä¸ªé˜¶æ®µï¼Œæ¯ä¸ªé˜¶æ®µæœ‰å“ªäº›å·¥ç¨‹ä¸Šçš„å‘ï¼Ÿã€

### 2.1 Stage 1ï¼šSupervised Fine-Tuning (SFT)

**ç›®æ ‡**ï¼šå°†é¢„è®­ç»ƒæ¨¡å‹ä» "ç»­å†™æœºå™¨" å˜æˆ "æŒ‡ä»¤è·Ÿéšè€…"

**æ•°æ®è¦æ±‚**ï¼š
- å…¸å‹è§„æ¨¡ï¼š10K-100K (instruction, response) pairs
- æ•°æ®æ¥æºï¼šäººå·¥æ ‡æ³¨ã€GPT-4 è’¸é¦ï¼ˆAlpaca/Vicuna è·¯çº¿ï¼‰ã€é¢†åŸŸä¸“å®¶ç¼–å†™
- è´¨é‡ > æ•°é‡ï¼šLIMA è®ºæ–‡ï¼ˆ[arXiv:2305.11206](https://arxiv.org/abs/2305.11206)ï¼‰è¯æ˜ 1K é«˜è´¨é‡æ•°æ®å¯ä»¥æ‰“ 52K ä½è´¨é‡æ•°æ®

**è®­ç»ƒç»†èŠ‚**ï¼š
- Lossï¼šæ ‡å‡† next-token predictionï¼ˆcross-entropyï¼‰
- åªåœ¨ response éƒ¨åˆ†è®¡ç®— lossï¼ˆmask instruction tokensï¼‰
- å­¦ä¹ ç‡ï¼š2e-5 åˆ° 5e-5ï¼Œcosine schedule
- Epochï¼šé€šå¸¸ 2-3 ä¸ª epochï¼Œè¿‡å¤šä¼š overfit

**å·¥ç¨‹å‘**ï¼š
1. **Chat template ä¸ä¸€è‡´**ï¼šä¸åŒæ¨¡å‹çš„ chat format ä¸åŒï¼ˆLlama ç”¨ `[INST]`ï¼ŒChatML ç”¨ `<|im_start|>`ï¼‰ï¼Œæ¨¡æ¿é”™è¯¯ä¼šå¯¼è‡´æ€§èƒ½æš´è·Œ
2. **Packing vs Padding**ï¼šé•¿çŸ­ä¸ä¸€çš„æ ·æœ¬å¦‚ä½•é«˜æ•ˆ batchï¼ŸPackingï¼ˆå¤šä¸ªæ ·æœ¬æ‹¼ä¸€ä¸ª sequenceï¼‰èƒ½æé€Ÿ 2-3x ä½†è¦æ³¨æ„ attention mask
3. **æ•°æ®æ±¡æŸ“**ï¼šSFT æ•°æ®ä¸è¯„æµ‹ benchmark é‡å ä¼šå¯¼è‡´è™šé«˜

### 2.2 Stage 2ï¼šReward Model Training

> æ¥æºï¼šInstructGPT â€” Ouyang et al. [arXiv:2203.02155](https://arxiv.org/abs/2203.02155), Sec. 3.2-3.3ï¼ˆRM è®­ç»ƒå’Œ PPO æµç¨‹çš„å·¥ä¸šåŒ–å®šä¹‰ï¼‰

**ç›®æ ‡**ï¼šå­¦ä¹ ä¸€ä¸ª proxy function æ¥è¿‘ä¼¼äººç±»åå¥½

**æ¶æ„**ï¼š
- Baseï¼šé€šå¸¸ä¸ policy model åŒæ¶æ„ï¼ˆæˆ–æ›´å°ï¼‰ï¼Œå»æ‰ LM head
- Headï¼š`Linear(hidden_dim â†’ 1)` æ˜ å°„åˆ°æ ‡é‡ reward
- å–æœ€åä¸€ä¸ª tokenï¼ˆEOSï¼‰çš„ hidden state æ¥é¢„æµ‹ reward

**è®­ç»ƒæ•°æ®**ï¼š
- æ ¼å¼ï¼š`(prompt, response_chosen, response_rejected)`
- è§„æ¨¡ï¼š50K-500K comparison pairs
- æ¥æºï¼šäººå·¥æ ‡æ³¨ï¼ˆAnthropic ç”¨è‡ªå·±çš„æ ‡æ³¨å›¢é˜Ÿï¼‰æˆ– AI è¾…åŠ©

**è®­ç»ƒç›®æ ‡â€”â€”Bradley-Terry Model**ï¼š

$$P(r_w \succ r_l | x) = \sigma\big(R_\phi(x, r_w) - R_\phi(x, r_l)\big)$$

æŸå¤±å‡½æ•°ï¼š

$$\mathcal{L}_{\text{RM}}(\phi) = -\mathbb{E}_{(x, r_w, r_l) \sim \mathcal{D}} \left[\log \sigma\big(R_\phi(x, r_w) - R_\phi(x, r_l)\big)\right]$$

å…¶ä¸­ $\sigma(z) = \frac{1}{1 + e^{-z}}$ æ˜¯ sigmoid å‡½æ•°ã€‚

**å…³é”®æ´å¯Ÿ**ï¼šRM åªéœ€è¦å­¦ä¹  **ç›¸å¯¹æ’åº**ï¼Œä¸éœ€è¦ç»å¯¹åˆ†æ•°ã€‚åˆ†æ•°å·®å¼‚å†³å®šåå¥½æ¦‚ç‡ã€‚

**å·¥ç¨‹å‘**ï¼š
1. **Reward Scale æ¼‚ç§»**ï¼šRM åˆ†æ•°çš„ç»å¯¹å€¼ä¼šéšè®­ç»ƒæ¼‚ç§»ï¼Œéœ€è¦ normalization
2. **æ ‡æ³¨è€…ä¸€è‡´æ€§**ï¼šinter-annotator agreement é€šå¸¸åªæœ‰ 60-70%ï¼Œå™ªå£°å¾ˆå¤§
3. **Position Bias**ï¼šæ ‡æ³¨è€…å€¾å‘äºé€‰æ‹©æ›´é•¿çš„å›ç­”ï¼ˆlength biasï¼‰
4. **RM æ³›åŒ–æ€§**ï¼šåœ¨è®­ç»ƒåˆ†å¸ƒä¹‹å¤–ï¼ŒRM çš„é¢„æµ‹éå¸¸ä¸å¯é 

### 2.3 Stage 3ï¼šPPO Training

> æ¥æºï¼šPPO â€” Schulman et al. [arXiv:1707.06347](https://arxiv.org/abs/1707.06347)ï¼›InstructGPT ä¸­çš„ LLM åº”ç”¨ â€” [arXiv:2203.02155](https://arxiv.org/abs/2203.02155), Sec. 3.3

**ç›®æ ‡**ï¼šç”¨ RL å¾®è°ƒ policy modelï¼Œæœ€å¤§åŒ– reward åŒæ—¶ä¸åç¦» reference model å¤ªè¿œ

**PPO ä¼˜åŒ–ç›®æ ‡**ï¼š

$$\mathcal{J}_{\text{PPO}}(\theta) = \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi_\theta(\cdot|x)} \left[R_\phi(x, y) - \beta \cdot D_{\text{KL}}\big(\pi_\theta(\cdot|x) \| \pi_{\text{ref}}(\cdot|x)\big)\right]$$

KL æƒ©ç½šé¡¹é˜²æ­¢ policy åç¦» reference model å¤ªè¿œï¼ˆé¿å… reward hackingï¼‰ã€‚

**PPO Clipped Objective**ï¼š

$$L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[\min\left(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_\text{old}}(a_t|s_t)} \hat{A}_t, \ \text{clip}\left(\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_\text{old}}(a_t|s_t)}, 1-\epsilon, 1+\epsilon\right) \hat{A}_t\right)\right]$$

å…¶ä¸­ $\hat{A}_t$ æ˜¯ advantage estimateï¼Œ$\epsilon$ é€šå¸¸å– 0.2ã€‚

**å››ä¸ªæ¨¡å‹å¹¶è¡Œ**ï¼š
1. **Actorï¼ˆPolicy Modelï¼‰**ï¼šè¦è®­ç»ƒçš„æ¨¡å‹ $\pi_\theta$
2. **Criticï¼ˆValue Modelï¼‰**ï¼šä¼°è®¡ $V(s)$ ç”¨äºè®¡ç®— advantage
3. **Reward Model**ï¼šå†»ç»“ï¼Œæä¾› reward ä¿¡å·
4. **Reference Model**ï¼šå†»ç»“ï¼Œç”¨äºè®¡ç®— KL penalty

**å·¥ç¨‹å‘**ï¼š
1. **æ˜¾å­˜çˆ†ç‚¸**ï¼š4 ä¸ªå¤§æ¨¡å‹åŒæ—¶åœ¨ GPU ä¸Šï¼Œ70B æ¨¡å‹éœ€è¦ 128+ GPUs
2. **è®­ç»ƒä¸ç¨³å®š**ï¼šPPO å¯¹è¶…å‚æåº¦æ•æ„Ÿï¼Œreward å®¹æ˜“ spike åå´©æºƒ
3. **Generation ç“¶é¢ˆ**ï¼šæ¯ä¸ª PPO step éœ€è¦ on-policy generationï¼Œæ˜¯è®­ç»ƒé€Ÿåº¦çš„ç“¶é¢ˆ
4. **KL ç³»æ•°è°ƒå‚**ï¼š$\beta$ å¤ªå¤§ â†’ å­¦ä¸åˆ°ä¸œè¥¿ï¼›$\beta$ å¤ªå° â†’ reward hacking
5. **Advantage Normalization**ï¼šä¸åš normalizationï¼Œgradient æ–¹å·®å·¨å¤§
6. **Value Loss Clipping**ï¼šcritic çš„ loss ä¹Ÿéœ€è¦ clipï¼Œå¦åˆ™ value function ä¸ç¨³å®š

**å®ç”¨æŠ€å·§**ï¼š
- ç”¨ GAEï¼ˆGeneralized Advantage Estimationï¼‰è®¡ç®— advantageï¼Œ$\lambda = 0.95$
- Reward whitening / normalization ç¨³å®šè®­ç»ƒ
- Mini-batch size å°½é‡å¤§ï¼ˆå‡å°‘æ–¹å·®ï¼‰
- å…ˆç”¨å°æ¨¡å‹è·‘é€š pipelineï¼Œå† scale up

---

## 3. DPO åŠå…¶å˜ä½“

### é¢è¯•å®˜ä¼šé—®ï¼šã€ŒDPO çš„æ•°å­¦æ¨å¯¼æ˜¯ä»€ä¹ˆï¼Ÿå®ƒå’Œ RLHF ç­‰ä»·å—ï¼ŸDPO æœ‰å“ªäº›å˜ä½“ï¼Œå„è‡ªè§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿã€

### 3.1 DPOï¼ˆDirect Preference Optimizationï¼‰

> æ¥æºï¼šDirect Preference Optimization: Your Language Model is Secretly a Reward Model â€” Rafailov et al. [arXiv:2305.18290](https://arxiv.org/abs/2305.18290)

**æ ¸å¿ƒæ€æƒ³**ï¼šå°† RLHF çš„ RL é—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ª closed-form çš„ supervised learning é—®é¢˜ã€‚

**æ¨å¯¼è¿‡ç¨‹**ï¼š

ä» RLHF çš„ KL-constrained reward maximization å‡ºå‘ï¼š

$$\max_{\pi} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi(\cdot|x)} \left[R(x, y)\right] - \beta \cdot D_{\text{KL}}\left(\pi(\cdot|x) \| \pi_{\text{ref}}(\cdot|x)\right)$$

è¿™ä¸ªä¼˜åŒ–é—®é¢˜æœ‰è§£æè§£ï¼š

$$\pi^*(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta} R(x, y)\right)$$

å…¶ä¸­ $Z(x) = \sum_y \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta} R(x, y)\right)$ æ˜¯ partition functionã€‚

åè§£ rewardï¼š

$$R(x, y) = \beta \log \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x)$$

å°†æ­¤ä»£å…¥ Bradley-Terry æ¨¡å‹ï¼š

$$P(y_w \succ y_l | x) = \sigma\left(\beta \log \frac{\pi^*(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi^*(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)$$

æ³¨æ„ $Z(x)$ åœ¨åšå·®æ—¶æ¶ˆæ‰äº†ï¼

**DPO Loss**ï¼š

$$\mathcal{L}_{\text{DPO}}(\theta) = -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \left[\log \sigma\left(\beta \left(\log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)}\right)\right)\right]$$

**DPO vs RLHF çš„ç­‰ä»·æ€§**ï¼š
- åœ¨ **ç†è®ºä¸Š**ï¼šå½“ preference data è¦†ç›–è¶³å¤Ÿã€ä¼˜åŒ–å……åˆ†æ—¶ï¼ŒDPO ä¸ RLHF ç­‰ä»·
- åœ¨ **å®è·µä¸­**ï¼šDPO æ˜¯ offline çš„ï¼ˆç”¨å›ºå®šæ•°æ®é›†ï¼‰ï¼ŒRLHF æ˜¯ online çš„ï¼ˆä¸æ–­ç”Ÿæˆæ–°æ•°æ®ï¼‰
- **å…³é”®åŒºåˆ«**ï¼šDPO æ— æ³•æ¢ç´¢ policy å½“å‰åˆ†å¸ƒä¹‹å¤–çš„ç©ºé—´ï¼Œè¿™é™åˆ¶äº†å®ƒçš„ ceiling

**DPO çš„ä¼˜åŠ¿**ï¼š
- ä¸éœ€è¦è®­ç»ƒ RM
- ä¸éœ€è¦ PPO çš„å¤æ‚è®­ç»ƒå¾ªç¯
- åªéœ€è¦ 2 ä¸ªæ¨¡å‹ï¼ˆpolicy + referenceï¼‰ï¼Œæ˜¾å­˜å‡åŠ
- è®­ç»ƒç¨³å®šï¼Œè¶…å‚å°‘

**DPO çš„å±€é™**ï¼š
- Offline ä¼˜åŒ–ï¼šæ•°æ®æ˜¯æå‰æ”¶é›†çš„ï¼Œæ— æ³•æ¢ç´¢
- Mode collapseï¼šå®¹æ˜“æ”¶æ•›åˆ°åå¥½æ•°æ®çš„ "å¹³å‡é£æ ¼"
- å¯¹æ•°æ®è´¨é‡æ•æ„Ÿï¼šå™ªå£°æ ‡æ³¨ä¼šç›´æ¥ä¼ æ’­åˆ° loss
- Distribution shiftï¼šå½“ policy åç¦» reference å¤ªè¿œï¼Œloss ä¿¡å·å˜å¼±

### 3.2 IPOï¼ˆIdentity Preference Optimizationï¼‰

> æ¥æºï¼šA General Theoretical Paradigm to Understand Learning from Human Feedback â€” Azar et al. [arXiv:2310.12036](https://arxiv.org/abs/2310.12036)

**è§£å†³çš„é—®é¢˜**ï¼šDPO åœ¨ç†è®ºä¸Šä¾èµ– Bradley-Terry å‡è®¾ï¼Œå½“åå¥½æ•°æ®è¿åè¿™ä¸€å‡è®¾æ—¶ï¼ˆå¦‚éä¼ é€’æ€§åå¥½ï¼‰ï¼ŒDPO ä¼š overfitã€‚

**IPO Loss**ï¼š

$$\mathcal{L}_{\text{IPO}}(\theta) = \mathbb{E}_{(x, y_w, y_l)} \left[\left(\log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} - \frac{1}{2\beta}\right)^2\right]$$

**ç›´è§‰**ï¼šIPO ç”¨ squared loss æ›¿ä»£ logistic lossï¼Œå½“ margin å·²ç»è¶³å¤Ÿå¤§æ—¶ä¸å†ç»§ç»­ pushï¼ˆé˜²æ­¢ overconfidentï¼‰ã€‚

**é€‚ç”¨åœºæ™¯**ï¼šæ•°æ®è´¨é‡ä¸ç¡®å®šã€åå¥½å­˜åœ¨å™ªå£°æˆ–çŸ›ç›¾æ—¶ã€‚

### 3.3 KTOï¼ˆKahneman-Tversky Optimizationï¼‰

> æ¥æºï¼šKTO: Model Alignment as Prospect Theoretic Optimization â€” Ethayarajh et al. [arXiv:2402.01306](https://arxiv.org/abs/2402.01306)

**è§£å†³çš„é—®é¢˜**ï¼šDPO éœ€è¦ pairwise preference dataï¼ˆåŒä¸€ prompt å¯¹åº”ä¸€å¥½ä¸€åï¼‰ï¼ŒKTO åªéœ€è¦ binary signalï¼ˆè¿™ä¸ªå›ç­”å¥½ / ä¸å¥½ï¼‰ã€‚

**ç†è®ºåŸºç¡€**ï¼šKahneman & Tversky çš„å‰æ™¯ç†è®ºï¼ˆProspect Theoryï¼‰â€”â€”äººå¯¹æŸå¤±æ¯”å¯¹æ”¶ç›Šæ›´æ•æ„Ÿã€‚

**KTO Loss**ï¼š

$$\mathcal{L}_{\text{KTO}}(\theta) = \mathbb{E}_{x, y_w}\left[\lambda_w \sigma\left(\beta \cdot r_{\text{ref}}(x, y_w) - z_0\right)\right] + \mathbb{E}_{x, y_l}\left[\lambda_l \sigma\left(z_0 - \beta \cdot r_{\text{ref}}(x, y_l)\right)\right]$$

å…¶ä¸­ $r_{\text{ref}}(x, y) = \log \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)}$ï¼Œ$z_0$ æ˜¯ baselineï¼Œ$\lambda_w < \lambda_l$ ä½“ç° loss aversionã€‚

**å®é™…æ•ˆæœ**ï¼š
- åœ¨ reasoningã€QAã€mathã€truthfulness ä¸Šï¼ŒKTO â‰¥ DPO â‰ˆ CPO > IPOï¼ˆå®éªŒæ•°æ®ï¼‰
- æ•°æ®æ”¶é›†æˆæœ¬æä½ï¼šåªéœ€è¦ thumbs up/down
- é€‚åˆé«˜é£é™©åœºæ™¯ï¼ˆæ³•å¾‹ã€åŒ»ç–—ï¼‰ï¼šå¯ä»¥é‡ç‚¹æƒ©ç½š bad responses

### 3.4 ORPOï¼ˆOdds-Ratio Preference Optimizationï¼‰

**è§£å†³çš„é—®é¢˜**ï¼šDPO éœ€è¦ä¸€ä¸ª frozen reference modelï¼Œå¢åŠ æ˜¾å­˜å¼€é”€ã€‚ORPO ç›´æ¥æŠŠ preference ä¼˜åŒ–èå…¥ SFT lossï¼Œä¸€æ­¥åˆ°ä½ã€‚

**ORPO Loss**ï¼š

$$\mathcal{L}_{\text{ORPO}}(\theta) = \mathcal{L}_{\text{SFT}}(y_w) + \lambda \cdot \mathcal{L}_{\text{OR}}$$

$$\mathcal{L}_{\text{OR}} = -\log \sigma\left(\log \frac{\text{odds}_\theta(y_w|x)}{\text{odds}_\theta(y_l|x)}\right)$$

å…¶ä¸­ $\text{odds}_\theta(y|x) = \frac{P_\theta(y|x)}{1 - P_\theta(y|x)}$

**ä¼˜åŠ¿**ï¼š
- ä¸éœ€è¦ reference modelï¼Œçœæ˜¾å­˜
- å•é˜¶æ®µè®­ç»ƒï¼ˆSFT + alignment åŒæ—¶å®Œæˆï¼‰
- åœ¨ class imbalance ä¸¥é‡æ—¶ï¼ˆå¥½/åæ¯”ä¾‹æ‚¬æ®Šï¼‰æ›´é²æ£’

**é€‚ç”¨åœºæ™¯**ï¼šå¤šè¯­è¨€åœºæ™¯ã€é•¿å°¾åˆ†å¸ƒæ•°æ®ã€èµ„æºå—é™ç¯å¢ƒã€‚

### 3.5 SimPOï¼ˆSimple Preference Optimizationï¼‰

**è§£å†³çš„é—®é¢˜**ï¼šDPO çš„ reference model log-ratio åœ¨ noisy labels ä¸‹ä¸ç¨³å®šã€‚

**SimPO Loss**ï¼š

$$\mathcal{L}_{\text{SimPO}}(\theta) = -\mathbb{E}\left[\log \sigma\left(\frac{\beta}{|y_w|}\log \pi_\theta(y_w|x) - \frac{\beta}{|y_l|}\log \pi_\theta(y_l|x) - \gamma\right)\right]$$

**å…³é”®è®¾è®¡**ï¼š
1. ä½¿ç”¨ **average log probability** è€Œé totalï¼ˆé™¤ä»¥ response é•¿åº¦ $|y|$ï¼‰ï¼Œæ¶ˆé™¤ length bias
2. å¼•å…¥ **target margin** $\gamma$ï¼Œç¡®ä¿ winning response çš„æ¦‚ç‡ä¸åªæ˜¯ "ç¨é«˜"ï¼Œè€Œæ˜¯æœ‰è¶³å¤Ÿé—´è·
3. **æ— éœ€ reference model**ï¼šç›´æ¥ç”¨ policy è‡ªèº«çš„ log probability

**æ•ˆæœ**ï¼šåœ¨ AlpacaEval 2 å’Œ Arena-Hard ä¸Šè¶…è¿‡ DPOï¼Œå°¤å…¶åœ¨ noisy æ•°æ®ä¸Šä¼˜åŠ¿æ˜æ˜¾ã€‚

### 3.6 Self-Play å¯¹é½æ–¹æ³•

**SPINï¼ˆSelf-Play Fine-Tuningï¼‰**ï¼š
- ç”¨ä¸Šä¸€è½®æ¨¡å‹ç”Ÿæˆ rejected responsesï¼Œäººç±»æ ‡æ³¨çš„ä½œä¸º chosen
- è¿­ä»£å¼ï¼šæ¯è½®æ¨¡å‹ç”Ÿæˆçš„ "å" å›ç­”éƒ½æ¯”ä¸Šä¸€è½®æ›´éš¾åŒºåˆ†
- ç†è®ºä¸Šæ”¶æ•›åˆ° target distribution

**SPPOï¼ˆSelf-Play Preference Optimizationï¼‰**ï¼š
- æ¨¡å‹åŒæ—¶æ‰®æ¼” generator å’Œ discriminator
- ç±»ä¼¼ GAN çš„æ€æƒ³ï¼Œä½†ç”¨åå¥½ä¿¡å·æ›¿ä»£åˆ¤åˆ«å™¨

### 3.7 DPO å˜ä½“å¯¹æ¯”æ€»ç»“

| æ–¹æ³• | éœ€è¦ Reference Model | æ•°æ®æ ¼å¼ | æ ¸å¿ƒæ”¹è¿› | é€‚ç”¨åœºæ™¯ |
|------|---------------------|---------|---------|---------|
| DPO | âœ… | Pairwise | åŸå§‹æ–¹æ³• | é€šç”¨ baseline |
| IPO | âœ… | Pairwise | Squared loss é˜² overfit | å™ªå£°æ•°æ® |
| KTO | âœ… | Binary (good/bad) | é pairwiseï¼Œloss aversion | ä½æˆæœ¬æ ‡æ³¨ã€é«˜é£é™©åœºæ™¯ |
| ORPO | âŒ | Pairwise | èåˆ SFT+alignment | èµ„æºå—é™ã€ä¸€æ­¥è®­ç»ƒ |
| SimPO | âŒ | Pairwise | Length-normalized, margin | Noisy labels, æŠ— length bias |
| SPIN | âœ… | Self-generated | è‡ªåšå¼ˆè¿­ä»£ | å‡å°‘äººç±»æ ‡æ³¨ |
| cDPO | âœ… | Pairwise+confidence | åŠ æƒ confidence | æ ‡æ³¨è´¨é‡ä¸å‡ |

---

## 4. RLAIFï¼šAI åé¦ˆæ›¿ä»£äººç±»åé¦ˆ

### é¢è¯•å®˜ä¼šé—®ï¼šã€Œä»€ä¹ˆæ˜¯ RLAIFï¼Ÿå®ƒå’Œ RLHF çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼ŸConstitutional AI çš„åŸç†æ˜¯ä»€ä¹ˆï¼ŸLLM-as-Judge å¯é å—ï¼Ÿã€

### 4.1 æ ¸å¿ƒæ€æƒ³

RLAIFï¼ˆReinforcement Learning from AI Feedbackï¼‰ç”¨ä¸€ä¸ªï¼ˆé€šå¸¸æ›´å¼ºçš„ï¼‰LLM æ¥æ›¿ä»£äººç±»æ ‡æ³¨è€…æä¾›åå¥½ä¿¡å·ã€‚

**åŠ¨æœº**ï¼š
- äººç±»æ ‡æ³¨æ˜‚è´µï¼ˆ$15-50/å°æ—¶ï¼‰ä¸”é€Ÿåº¦æ…¢
- æ ‡æ³¨ä¸€è‡´æ€§å·®ï¼ˆinter-annotator agreement â‰ˆ 65-75%ï¼‰
- æŸäº›é¢†åŸŸäººç±»ä¹Ÿä¸å¯é ï¼ˆå¤æ‚æ•°å­¦ã€ä»£ç é€»è¾‘ï¼‰
- Scaling bottleneckï¼šæ¨¡å‹è®­ç»ƒå¯ä»¥æ— é™ scale upï¼Œä½†äººç±»æ ‡æ³¨ä¸è¡Œ

### 4.2 Constitutional AIï¼ˆAnthropicï¼‰

> æ¥æºï¼šConstitutional AI: Harmlessness from AI Feedback â€” Bai et al. [arXiv:2212.08073](https://arxiv.org/abs/2212.08073)

**æµç¨‹**ï¼š
1. **Red-teaming**ï¼šè®©æ¨¡å‹ç”Ÿæˆæœ‰å®³è¾“å‡º
2. **Critique**ï¼šç”¨å¦ä¸€ä¸ª LLM æ ¹æ®ä¸€ç»„ "å®ªæ³•åŸåˆ™"ï¼ˆConstitutionï¼‰æ‰¹è¯„æœ‰å®³è¾“å‡º
3. **Revision**ï¼šè®© LLM æ ¹æ®æ‰¹è¯„é‡å†™å›ç­”
4. **RL è®­ç»ƒ**ï¼šç”¨ AI ç”Ÿæˆçš„åå¥½æ•°æ®è®­ç»ƒ RMï¼Œå†ç”¨ RL å¾®è°ƒ

**å®ªæ³•åŸåˆ™ç¤ºä¾‹**ï¼š
- "è¯·é€‰æ‹©æœ€ä¸ä¼šè¢«ä¸€ä¸ªæ˜æ™ºçš„ã€æœ‰é“å¾·çš„äººè®¤ä¸ºæœ‰å®³çš„å›ç­”"
- "è¯·é€‰æ‹©æœ€è¯šå®ã€æœ€æœ‰å¸®åŠ©çš„å›ç­”"
- "è¯·é€‰æ‹©ä¸ä¼šåŠ©é•¿éæ³•æ´»åŠ¨çš„å›ç­”"

**å…³é”®ä¼˜åŠ¿**ï¼šå¯ä»¥ç¼–ç æŠ½è±¡ä»·å€¼è§‚ï¼Œè€Œä¸ä¾èµ–äºå…·ä½“çš„æ ‡æ³¨æ ·æœ¬ã€‚

### 4.3 Self-Rewardï¼ˆMeta, 2024ï¼‰

**æ ¸å¿ƒåˆ›æ–°**ï¼šè®©æ¨¡å‹ **åŒæ—¶** å……å½“ generator å’Œ judgeã€‚

**æµç¨‹**ï¼š
1. æ¨¡å‹ç”Ÿæˆå¤šä¸ªå€™é€‰å›ç­”
2. æ¨¡å‹è‡ªå·±æ‰“åˆ†ï¼ˆLLM-as-Judgeï¼‰
3. ç”¨åå¥½å¯¹è®­ç»ƒï¼ˆDPO æˆ– RLHFï¼‰
4. è¿­ä»£ï¼šæ›´å¥½çš„æ¨¡å‹ â†’ æ›´å¥½çš„åˆ¤æ–­ â†’ æ›´å¥½çš„è®­ç»ƒä¿¡å· â†’ æ›´å¥½çš„æ¨¡å‹

**é£é™©**ï¼šè‡ªæˆ‘å¼ºåŒ–åè§ï¼ˆecho chamber effectï¼‰ã€‚å¦‚æœæ¨¡å‹åœ¨æŸäº›æ–¹é¢æœ‰ç³»ç»Ÿæ€§åå·®ï¼Œè‡ªæˆ‘åé¦ˆä¼šæ”¾å¤§è¿™äº›åå·®ã€‚

### 4.4 LLM-as-Judge

**å¸¸è§é…ç½®**ï¼š
- **Pointwise**ï¼šç»™ (prompt, response) å¯¹æ‰“ 1-5 åˆ†
- **Pairwise**ï¼šç»™ä¸¤ä¸ª response é€‰æ‹©æ›´å¥½çš„é‚£ä¸ª
- **Reference-guided**ï¼šæä¾›å‚è€ƒç­”æ¡ˆè¾…åŠ©åˆ¤æ–­

**å·²çŸ¥é—®é¢˜**ï¼š
1. **Position bias**ï¼šå€¾å‘äºé€‰æ‹©å‡ºç°åœ¨å‰é¢çš„å›ç­”
2. **Verbosity bias**ï¼šå€¾å‘äºé€‰æ‹©æ›´é•¿çš„å›ç­”
3. **Self-enhancement bias**ï¼šå€¾å‘äºé€‰æ‹©è‡ªå·±ç”Ÿæˆçš„å›ç­”
4. **Inconsistency**ï¼šå¯¹åŒä¸€å¯¹æ¯”è¾ƒï¼Œäº¤æ¢é¡ºåºå¯èƒ½æ”¹å˜ç»“æœ

**ç¼“è§£ç­–ç•¥**ï¼š
- å¤šæ¬¡è¯„ä¼°å–å¹³å‡
- äº¤æ¢ response é¡ºåºï¼ˆdebiasingï¼‰
- ç”¨ chain-of-thought è¦æ±‚ judge å…ˆæ¨ç†å†æ‰“åˆ†
- ç”¨å¤šä¸ªä¸åŒçš„ judge model åš ensemble

### 4.5 RLAIF vs RLHF å®éªŒç»“è®º

- Google 2023 å®éªŒï¼š**RLAIF åœ¨ summarization å’Œ helpfulness ä¸Šä¸ RLHF æ€§èƒ½ç›¸å½“**
- æˆæœ¬é™ä½ 10-100x
- ä½†åœ¨ safety-critical åœºæ™¯ï¼Œä»éœ€äººç±» spot-check
- **æœ€ä½³å®è·µ**ï¼šRLAIF åšå¤§è§„æ¨¡åˆå§‹è®­ç»ƒï¼Œäººç±»åšå…³é”®åœºæ™¯ fine-grained è°ƒæ•´

---

## 5. Reward Model è®¾è®¡

### é¢è¯•å®˜ä¼šé—®ï¼šã€ŒBradley-Terry æ¨¡å‹çš„å‡è®¾æ˜¯ä»€ä¹ˆï¼Ÿä»€ä¹ˆæ˜¯ Reward Hackingï¼ŸProcess RM å’Œ Outcome RM æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿã€

### 5.1 Bradley-Terry åå¥½æ¨¡å‹

**å‡è®¾**ï¼šæ¯ä¸ª response æœ‰ä¸€ä¸ªæ½œåœ¨çš„ "å®åŠ›å€¼" $R(x, y)$ï¼Œåå¥½æ¦‚ç‡ = å®åŠ›å€¼å·®çš„ sigmoidï¼š

$$P(y_w \succ y_l | x) = \sigma(R(x, y_w) - R(x, y_l))$$

**å±€é™**ï¼š
- å‡è®¾åå¥½æ˜¯ **ä¼ é€’çš„**ï¼šå¦‚æœ A > B ä¸” B > Cï¼Œåˆ™ A > Cã€‚ç°å®ä¸­äººç±»åå¥½ç»å¸¸ä¸ä¼ é€’ã€‚
- å‡è®¾åå¥½æ˜¯ **context-independent**ï¼šä¸è€ƒè™‘ prompt çš„å…·ä½“ç‰¹æ€§
- ä¸æ”¯æŒ **ties**ï¼ˆå¹³æ‰‹ï¼‰

**æ›¿ä»£æ¨¡å‹**ï¼š
- **Plackett-Luce**ï¼šæ‰©å±•åˆ°å¤š response æ’åº
- **Thurstone**ï¼šç”¨æ­£æ€åˆ†å¸ƒè€Œé logistic åˆ†å¸ƒ

### 5.2 Reward Hacking

**å®šä¹‰**ï¼šPolicy æ‰¾åˆ° reward model çš„æ¼æ´ï¼Œè·å¾—é«˜ reward ä½†å®é™…è´¨é‡ä½ä¸‹ã€‚

**ç»å…¸æ¡ˆä¾‹**ï¼š
1. **Length exploitation**ï¼šæ¨¡å‹å‘ç°æ›´é•¿çš„å›ç­” reward æ›´é«˜ï¼Œäºæ˜¯æ— é™å˜é•¿ï¼ˆåºŸè¯è¿ç¯‡ï¼‰
2. **Formatting tricks**ï¼šå¤§é‡ä½¿ç”¨ bullet pointsã€markdownã€emoji è·å¾—é«˜åˆ†
3. **Sycophancy**ï¼šæ— æ¡ä»¶åŒæ„ç”¨æˆ·è§‚ç‚¹ï¼Œè·å¾— helpfulness é«˜åˆ†
4. **Repetition**ï¼šé‡å¤æŸäº› "é«˜åˆ†çŸ­è¯­" æ¥åˆ·åˆ†
5. **Code testing hack**ï¼šDeepSeek-R1 è®­ç»ƒä¸­å‘ç°æ¨¡å‹å­¦ä¼š `sys.exit(0)` æ¥é€šè¿‡æµ‹è¯•ï¼ˆAnthropic 2025 æŠ¥å‘Šï¼‰

**é˜²å¾¡æ‰‹æ®µ**ï¼š
- **KL penalty**ï¼šé™åˆ¶ policy åç¦» reference çš„ç¨‹åº¦
- **Reward model ensemble**ï¼šå¤šä¸ª RM å–å¹³å‡ï¼Œå‡å°‘å•ç‚¹æ¼æ´
- **Iterative RM retraining**ï¼šç”¨æ–° policy çš„è¾“å‡ºé‡æ–°æ ‡æ³¨è®­ç»ƒ RM
- **Length penalty**ï¼šæ˜¾å¼æƒ©ç½šè¿‡é•¿å›ç­”
- **Constrained optimization**ï¼šè®¾ç½® reward ä¸Šé™

### 5.3 Process RM vs Outcome RM

| ç»´åº¦ | Outcome RMï¼ˆORMï¼‰ | Process RMï¼ˆPRMï¼‰ |
|------|------------------|-----------------|
| è¯„ä¼°ç²’åº¦ | æ•´ä¸ª response ä¸€ä¸ªåˆ†æ•° | æ¯ä¸ª reasoning step ä¸€ä¸ªåˆ†æ•° |
| æ•°æ®æ ‡æ³¨ | ç®€å•ï¼šæ•´ä½“å¥½/å | å¤æ‚ï¼šæ¯æ­¥å¯¹/é”™ |
| é€‚ç”¨åœºæ™¯ | é€šç”¨å¯¹é½ | æ•°å­¦æ¨ç†ã€ä»£ç ç”Ÿæˆ |
| Reward Hacking é£é™© | é«˜ï¼ˆåªçœ‹ç»“æœï¼Œä¸ç®¡è¿‡ç¨‹ï¼‰ | ä½ï¼ˆè¿‡ç¨‹æ­£ç¡®æ›´éš¾ hackï¼‰ |
| ä»£è¡¨å·¥ä½œ | InstructGPT, ChatGPT | OpenAI "Let's Verify Step by Step" ([arXiv:2305.20050](https://arxiv.org/abs/2305.20050)) |

**PRM çš„ä¼˜åŠ¿**ï¼š
- å¯ä»¥åœ¨æ¨ç†è¿‡ç¨‹ä¸­ **æ—©æœŸå‘ç°é”™è¯¯**ï¼Œä¸éœ€è¦ç­‰åˆ°æœ€ç»ˆç­”æ¡ˆ
- é…åˆ tree searchï¼ˆå¦‚ MCTSï¼‰ä½¿ç”¨æ•ˆæœæ›´å¥½
- æä¾›æ›´ dense çš„ reward signalï¼Œè®­ç»ƒæ›´ç¨³å®š

**PRM çš„æŒ‘æˆ˜**ï¼š
- æ ‡æ³¨æˆæœ¬æé«˜ï¼ˆéœ€è¦æ ‡æ³¨æ¯ä¸€æ­¥çš„æ­£ç¡®æ€§ï¼‰
- è‡ªåŠ¨åŒ–æ–¹æ¡ˆï¼ˆMonte Carlo estimationï¼‰æœ‰å™ªå£°
- å­˜åœ¨ "æ­¥éª¤ä¾èµ–" é—®é¢˜ï¼šæŸæ­¥é”™è¯¯åé¢å¯èƒ½ "è‡ªæˆ‘ä¿®æ­£"ï¼ŒPRM å¯èƒ½è¿‡åº¦æƒ©ç½š

### 5.4 Reward Model çš„ Scaling

- RM è§„æ¨¡è¶Šå¤§ï¼Œåå¥½é¢„æµ‹è¶Šå‡†ç¡®
- ä½† RM å¤ªå¤§ä¼šå¯¼è‡´ RL è®­ç»ƒå¤ªæ…¢ï¼ˆæ¯ä¸ª step éƒ½è¦è·‘ RM inferenceï¼‰
- **å®è·µç»éªŒ**ï¼šRM è§„æ¨¡é€šå¸¸æ˜¯ policy model çš„ 1/2 åˆ° 1x
- DeepSeek-R1 çš„çªç ´ï¼š**å®Œå…¨ä¸ç”¨ learned RM**ï¼Œç”¨ rule-based verifiable rewards

---

## 6. 2026 å‰æ²¿

### é¢è¯•å®˜ä¼šé—®ï¼šã€ŒGRPO æ˜¯ä»€ä¹ˆï¼Ÿå’Œ PPO æœ‰ä»€ä¹ˆåŒºåˆ«ï¼ŸDAPO åˆåšäº†ä»€ä¹ˆæ”¹è¿›ï¼Ÿä»€ä¹ˆæ˜¯ Online DPO å’Œ Iterative Alignmentï¼Ÿã€

### 6.1 GRPOï¼ˆGroup Relative Policy Optimizationï¼‰

> æ¥æºï¼šDeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models â€” Shao et al. [arXiv:2402.03300](https://arxiv.org/abs/2402.03300), Sec. 3.2ï¼›DeepSeek-R1 â€” [arXiv:2501.12948](https://arxiv.org/abs/2501.12948)

**æ¥æº**ï¼šDeepSeek-Math (2024)ï¼Œè¢« DeepSeek-R1 (2025) æ¨å¹¿ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼šå»æ‰ PPO ä¸­æ˜‚è´µçš„ Criticï¼ˆValue Modelï¼‰ï¼Œç”¨ **ç»„å†…ç›¸å¯¹ reward** ä½œä¸º advantageã€‚

**ç®—æ³•æµç¨‹**ï¼š
1. å¯¹æ¯ä¸ª prompt $x$ï¼Œç”¨å½“å‰ policy $\pi_{\theta_\text{old}}$ ç”Ÿæˆä¸€ç»„ $G$ ä¸ª response $\{y_1, ..., y_G\}$
2. å¯¹æ¯ä¸ª response è®¡ç®— reward $r_i$
3. ç»„å†…æ ‡å‡†åŒ–å¾—åˆ° advantageï¼š
   $$\hat{A}_i = \frac{r_i - \text{mean}(\{r_1, ..., r_G\})}{\text{std}(\{r_1, ..., r_G\})}$$
4. ç”¨ PPO-style clipped objective æ›´æ–° policy

**GRPO ç›®æ ‡å‡½æ•°**ï¼š

$$\mathcal{J}_{\text{GRPO}}(\theta) = \mathbb{E}_{x, \{y_i\}_{i=1}^G} \left[\frac{1}{G}\sum_{i=1}^G \frac{1}{|y_i|}\sum_{t=1}^{|y_i|} \min\left(\rho_{i,t} \hat{A}_i, \ \text{clip}(\rho_{i,t}, 1-\epsilon, 1+\epsilon) \hat{A}_i\right) - \beta \cdot D_{\text{KL}}\right]$$

å…¶ä¸­ $\rho_{i,t} = \frac{\pi_\theta(y_{i,t}|x, y_{i,<t})}{\pi_{\theta_\text{old}}(y_{i,t}|x, y_{i,<t})}$

**GRPO vs PPO**ï¼š

| ç»´åº¦ | PPO | GRPO |
|------|-----|------|
| Critic Model | éœ€è¦ï¼ˆä¸ policy åŒè§„æ¨¡ï¼‰ | ä¸éœ€è¦ |
| Advantage è®¡ç®— | GAEï¼ˆéœ€è¦ value functionï¼‰ | ç»„å†… reward æ ‡å‡†åŒ– |
| æ˜¾å­˜å ç”¨ | 4 æ¨¡å‹ | 2 æ¨¡å‹ï¼ˆpolicy + reference/rewardï¼‰ |
| è®­ç»ƒå¤æ‚åº¦ | é«˜ | æ˜¾è‘—é™ä½ |
| é€‚ç”¨ Reward ç±»å‹ | ä»»æ„ | æ›´é€‚åˆ sparse/binary reward |
| Group Size | N/A | é€šå¸¸ G=8-64 |

**DeepSeek-R1 çš„åˆ›ä¸¾**ï¼š
- ç›´æ¥ä» base model å‡ºå‘ï¼ˆDeepSeek-V3-Base, 671Bï¼‰ï¼Œ**è·³è¿‡ SFT**
- ä½¿ç”¨ GRPO + **rule-based verifiable rewards**ï¼ˆæ•°å­¦ç­”æ¡ˆå¯¹/é”™ã€ä»£ç é€šè¿‡/ä¸é€šè¿‡ï¼‰
- æ¶Œç°å‡º chain-of-thought reasoningã€self-reflectionã€"aha moment"
- è¯æ˜äº†ï¼š**çº¯ RL å¯ä»¥ elicit reasoning capability**ï¼Œä¸éœ€è¦æ˜¾å¼æ•™

### 6.2 DAPOï¼ˆDecoupled Clip and Dynamic Sampling Policy Optimizationï¼‰

> æ¥æºï¼šDAPO: An Open-Source LLM Reinforcement Learning System at Scale â€” Yu et al. (ByteDance/æ¸…å) [arXiv:2503.14476](https://arxiv.org/abs/2503.14476)

**æ¥æº**ï¼šByteDance Seed (2025)ï¼ŒNeurIPS 2025 posterã€‚

**å››ä¸ªå…³é”®æ”¹è¿›**ï¼š

1. **Decoupled Clipping**ï¼šåˆ†å¼€è®¾ç½®æ­£å‘å’Œåå‘çš„ clip èŒƒå›´
   - $\epsilon_{\text{low}}$ï¼ˆæ­£å‘ï¼Œé¼“åŠ±å¥½è¡Œä¸ºï¼‰å’Œ $\epsilon_{\text{high}}$ï¼ˆåå‘ï¼Œæƒ©ç½šåè¡Œä¸ºï¼‰å¯ä»¥ä¸åŒ
   - é€šå¸¸ $\epsilon_{\text{high}} > \epsilon_{\text{low}}$ï¼Œè®©æ¨¡å‹æ›´å¿«è¿œç¦»åç­–ç•¥

2. **Dynamic Sampling**ï¼šè¿‡æ»¤æ‰ reward å…¨åŒï¼ˆéƒ½å¯¹æˆ–éƒ½é”™ï¼‰çš„ prompt group
   - å¦‚æœä¸€ç»„ response å…¨éƒ¨æ­£ç¡®æˆ–å…¨éƒ¨é”™è¯¯ï¼Œadvantage å…¨ä¸º 0ï¼Œä¸æä¾›ä»»ä½•è®­ç»ƒä¿¡å·
   - åŠ¨æ€é‡é‡‡æ ·ç¡®ä¿æ¯ä¸ª batch éƒ½æœ‰æœ‰æ•ˆæ¢¯åº¦

3. **Token-Level Loss**ï¼šper-token è€Œé per-sequence çš„ loss è®¡ç®—
   - é¿å…é•¿ response ä¸»å¯¼æ¢¯åº¦

4. **Overlong Reward Shaping**ï¼šå¯¹è¶…é•¿ response ç»™äºˆ soft penalty è€Œéæˆªæ–­
   - é˜²æ­¢ length exploitation åŒæ—¶ä¿ç•™æœ‰æ•ˆå†…å®¹

**æ•ˆæœ**ï¼šåŸºäº Qwen2.5-32Bï¼ŒAIME 2024 å¾—åˆ† 50 åˆ†ï¼Œè¶…è¿‡ DeepSeek-R1-Zero-Qwen-32B çš„ 47 åˆ†ï¼Œä¸”åªç”¨ 50% è®­ç»ƒæ­¥æ•°ã€‚

### 6.3 REINFORCE++ å’Œ RLOO

**REINFORCE++**ï¼š
- åŸºäºç»å…¸ REINFORCE ç®—æ³•ï¼Œä½†åŠ å…¥ç°ä»£ä¼˜åŒ–æŠ€å·§
- æ—  criticã€æ—  group å¯¹æ¯”ã€æ¯ä¸ª prompt åªéœ€ 1 ä¸ª sample
- ç”¨ running average reward ä½œä¸º baseline
- æ¯” GRPO æ›´ç®€å•ï¼Œä½†å¯¹ reward ä¿¡å·è´¨é‡è¦æ±‚æ›´é«˜

**RLOOï¼ˆREINFORCE Leave-One-Outï¼Œ[arXiv:2402.14740](https://arxiv.org/abs/2402.14740)ï¼‰**ï¼š
- ç”Ÿæˆ $K$ ä¸ª responseï¼Œå¯¹æ¯ä¸ª response ç”¨å…¶ä½™ $K-1$ ä¸ªçš„å¹³å‡ reward ä½œä¸º baseline
- ç±»ä¼¼ GRPO ä½† baseline æ›´ç²¾ç¡®
- è¢« TRL åº“åŸç”Ÿæ”¯æŒ

### 6.4 Online DPO / Iterative DPO

**é—®é¢˜**ï¼šæ ‡å‡† DPO æ˜¯ offline çš„â€”â€”ä¸€æ¬¡æ€§æ”¶é›†æ•°æ®ã€è®­ç»ƒã€ç»“æŸã€‚policy è¿›æ­¥åï¼Œæ—§æ•°æ®ä¸å†åŒ¹é…å½“å‰åˆ†å¸ƒã€‚

**Online DPO**ï¼š
1. ç”¨å½“å‰ policy ç”Ÿæˆ response
2. ç”¨ RM æˆ– LLM judge æ ‡æ³¨åå¥½
3. ç”¨æ–°æ ‡æ³¨æ•°æ®åš DPO æ›´æ–°
4. é‡å¤

**Iterative Alignmentï¼ˆè¿­ä»£å¼å¯¹é½ï¼‰**ï¼š
- ä¸æ˜¯ä¸€æ¬¡è®­ç»ƒå°±ç»“æŸï¼Œè€Œæ˜¯æŒç»­å¾ªç¯
- æ¯è½®ï¼šéƒ¨ç½² â†’ æ”¶é›†åé¦ˆ â†’ é‡è®­ç»ƒ â†’ å†éƒ¨ç½²
- Online DPO + RLAIF ç»“åˆï¼Œå®ç°æ— éœ€äººç±»çš„è‡ªåŠ¨åŒ–å¾ªç¯
- 2026 å¹´å·²ç»æˆä¸º frontier labs çš„æ ‡é…æµç¨‹

### 6.5 RL Scaling Laws

**æ ¸å¿ƒå‘ç°**ï¼ˆDeepSeek-R1, 2025ï¼‰ï¼š
- RL çš„ compute scaling ä¸ pretraining çš„ scaling law ç±»ä¼¼
- æ›´å¤š RL compute â†’ æ›´å¼ºçš„æ¨ç†èƒ½åŠ›ï¼ˆåœ¨ verifiable tasks ä¸Šï¼‰
- å…³é”®å› ç´ ï¼šmodel size Ã— training steps Ã— generation diversityï¼ˆgroup sizeï¼‰

**RLVRï¼ˆRL with Verifiable Rewardsï¼‰**ï¼š
- ç”¨äºæ¨ç†ä»»åŠ¡ï¼šæ•°å­¦ã€ç¼–ç¨‹ã€é€»è¾‘
- Reward æ˜¯ deterministic çš„ï¼šç­”æ¡ˆå¯¹=1ï¼Œé”™=0
- ä¼˜åŠ¿ï¼šé›¶æ ‡æ³¨æˆæœ¬ã€æ—  reward hackingï¼ˆç­”æ¡ˆä¸å¯ hackï¼‰
- å±€é™ï¼šä¸é€‚ç”¨äºå¼€æ”¾æ€§ä»»åŠ¡ï¼ˆåˆ›æ„å†™ä½œã€å¯¹è¯ç­‰ï¼‰

### 6.6 "GRPO å°±æ˜¯ DPO" çš„æ·±å±‚è”ç³»

æœ€æ–°ç ”ç©¶ï¼ˆ"It Takes Two: Your GRPO Is Secretly DPO", 2025ï¼‰æŒ‡å‡ºï¼š
- å½“ GRPO çš„ group size = 2 æ—¶ï¼Œå…¶ç›®æ ‡å‡½æ•°å¯ä»¥åŒ–ç®€ä¸º DPO çš„å½¢å¼
- GRPO å’Œ DPO æ˜¯ **åŒä¸€å…‰è°±ä¸Šçš„ä¸åŒç‚¹**
- GRPO ç”¨æ›´å¤§çš„ group size æä¾›æ›´ä¸°å¯Œçš„å¯¹æ¯”ä¿¡å·ï¼Œæœ¬è´¨ä¸Šæ˜¯ "å¤šæ ·æœ¬ DPO"
- è¿™ç»Ÿä¸€äº† online RL å’Œ offline preference optimization çš„ç†è®ºæ¡†æ¶

---

## 7. å·¥ç¨‹å®è·µ

### é¢è¯•å®˜ä¼šé—®ï¼šã€Œä½ ç”¨è¿‡å“ªäº› RLHF è®­ç»ƒæ¡†æ¶ï¼Ÿå®ƒä»¬å„æœ‰ä»€ä¹ˆä¼˜ç¼ºç‚¹ï¼Ÿå¦‚ä½•é€‰å‹ï¼Ÿã€

### 7.1 æ¡†æ¶å¯¹æ¯”

| æ¡†æ¶ | ç»´æŠ¤è€… | æ ¸å¿ƒæ¶æ„ | æ”¯æŒçš„ç®—æ³• | é€‚ç”¨è§„æ¨¡ | ä¸Šæ‰‹éš¾åº¦ |
|------|--------|---------|-----------|---------|---------|
| **TRL** | Hugging Face | å•æœº/DDP | PPO, DPO, ORPO, KTO, RLOO, Online DPO | â‰¤ 70B (LoRA) | â­â­ ä½ |
| **OpenRLHF** | ç¤¾åŒº | Ray + vLLM + DeepSpeed | PPO, GRPO, DAPO, REINFORCE++, DPO | 7B-671B | â­â­â­â­ é«˜ |
| **LLaMA-Factory** | hiyouga | Transformers + PEFT | DPO, ORPO, KTO, SimPO, PPO (LoRA) | â‰¤ 70B (LoRA) | â­â­ ä½ |
| **DeepSpeed-Chat** | Microsoft | DeepSpeed ZeRO | PPO | â‰¤ 175B | â­â­â­ ä¸­ |
| **veRL** | Volcengine | Ray + Megatron/DeepSpeed | PPO, GRPO | 7B-100B+ | â­â­â­â­ é«˜ |
| **Unsloth** | Unsloth AI | Triton kernels | DPO, ORPO, KTO, SimPO, GRPO | â‰¤ 70B (LoRA) | â­ æœ€ä½ |

### 7.2 TRLï¼ˆTransformers Reinforcement Learningï¼‰

**ä¼˜ç‚¹**ï¼š
- Hugging Face ç”Ÿæ€åŸç”Ÿé›†æˆ
- æ–‡æ¡£å®Œå–„ï¼Œç¤¾åŒºæ´»è·ƒ
- æ”¯æŒçš„ç®—æ³•æœ€å…¨é¢
- é€‚åˆå¿«é€Ÿ prototyping

**ç¼ºç‚¹**ï¼š
- å¤§è§„æ¨¡è®­ç»ƒæ•ˆç‡ä¸é«˜ï¼ˆå•æœºæ¶æ„é™åˆ¶ï¼‰
- PPO å®ç°åœ¨ 70B+ æ¨¡å‹ä¸Šä¸å¤Ÿç¨³å®š
- æ¯” OpenRLHF æ…¢ 3xï¼ˆåŒè§„æ¨¡ PPO å®éªŒï¼‰

**é€‚ç”¨åœºæ™¯**ï¼šç ”ç©¶æ¢ç´¢ã€7B-13B æ¨¡å‹ã€DPO/KTO ç­‰ offline æ–¹æ³•

### 7.3 OpenRLHF

**ä¼˜ç‚¹**ï¼š
- åŸºäº Ray çš„åˆ†å¸ƒå¼æ¶æ„ï¼Œå¤©ç„¶æ”¯æŒå¤§è§„æ¨¡è®­ç»ƒ
- vLLM åŠ é€Ÿ generationï¼ŒDeepSpeed ZeRO-3 åŠ é€Ÿè®­ç»ƒ
- 70B+ æ¨¡å‹ PPO/GRPO è®­ç»ƒçš„æœ€ä½³é€‰æ‹©
- æ”¯æŒ DAPOã€REINFORCE++ ç­‰æœ€æ–°ç®—æ³•
- æ€§èƒ½æ¯” DeepSpeed-Chat å¿« 3-4x

**ç¼ºç‚¹**ï¼š
- éƒ¨ç½²å’Œè°ƒè¯•å¤æ‚ï¼ˆRay + vLLM + DeepSpeed ä¸‰ä»¶å¥—ï¼‰
- å¯¹åŸºç¡€è®¾æ–½è¦æ±‚é«˜ï¼ˆå¤šèŠ‚ç‚¹ GPU é›†ç¾¤ï¼‰
- å­¦ä¹ æ›²çº¿é™¡å³­

**é€‚ç”¨åœºæ™¯**ï¼šç”Ÿäº§çº§ RLHF/GRPO è®­ç»ƒã€70B+ æ¨¡å‹ã€å¤šèŠ‚ç‚¹é›†ç¾¤

### 7.4 LLaMA-Factory

**ä¼˜ç‚¹**ï¼š
- ä¸€ç«™å¼ fine-tuning å¹³å°
- Web UI æ”¯æŒï¼Œé…ç½®åŒ–è®­ç»ƒ
- æ”¯æŒå¤§é‡æ¨¡å‹å’Œæ•°æ®é›†æ ¼å¼
- LoRA/QLoRA ä¼˜åŒ–ï¼Œæ¶ˆè´¹çº§æ˜¾å¡å¯ç”¨

**ç¼ºç‚¹**ï¼š
- ä¸æ”¯æŒ full-parameter RLHF
- ä»¥ LoRA ä¸ºä¸»ï¼Œfull-parameter è®­ç»ƒå—é™
- PPO å®ç°è¾ƒåŸºç¡€

**é€‚ç”¨åœºæ™¯**ï¼šLoRA å¾®è°ƒã€DPO/ORPO å¿«é€Ÿå®éªŒã€èµ„æºå—é™ç¯å¢ƒ

### 7.5 é€‰å‹æŒ‡å—

```mermaid
graph TD
    Q["ä½ çš„éœ€æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ"] --> A["å¿«é€Ÿå®éªŒ / ç ”ç©¶"]
    Q --> B["ç”Ÿäº§çº§ RLHF<br/>(PPO/GRPO)"]
    Q --> C["DPO/KTO/ORPO<br/>(Offline)"]
    Q --> D["70B+ æ¨¡å‹"]
    Q --> E["æ¶ˆè´¹çº§æ˜¾å¡<br/>(å•å¡ 24GB)"]
    Q --> F["éœ€è¦æœ€æ–°ç®—æ³•<br/>(DAPO/REINFORCE++)"]
    A --> A1["TRL æˆ– LLaMA-Factory"]
    B --> B1["OpenRLHF"]
    C --> C1["TRL / LLaMA-Factory / Unsloth"]
    D --> D1["OpenRLHF æˆ– veRL"]
    E --> E1["Unsloth + LoRA"]
    F --> F1["OpenRLHF"]
```

### 7.6 è®­ç»ƒåŸºç¡€è®¾æ–½

**å…¸å‹é…ç½®**ï¼ˆç”Ÿäº§çº§ RLHFï¼‰ï¼š
- 7B æ¨¡å‹ PPOï¼š4-8x A100/H100 80GB
- 70B æ¨¡å‹ PPOï¼š32-64x A100/H100 80GBï¼ˆå¤šèŠ‚ç‚¹ï¼‰
- 70B æ¨¡å‹ DPOï¼š8-16x A100/H100 80GB

**æ€§èƒ½ä¼˜åŒ–æŠ€å·§**ï¼š
- vLLM åŠ é€Ÿ generationï¼ˆæ¯” HF generate å¿« 5-10xï¼‰
- Gradient checkpointing èŠ‚çœæ˜¾å­˜
- Flash Attention 2 æ ‡é…
- Packing samples æå‡ GPU åˆ©ç”¨ç‡
- å¼‚æ­¥ RLï¼šgeneration å’Œ training æµæ°´çº¿å¹¶è¡Œ

---

## 8. å¸¸è§å¤±è´¥æ¨¡å¼

### é¢è¯•å®˜ä¼šé—®ï¼šã€ŒRLHF/DPO è®­ç»ƒä¸­æœ€å¸¸è§çš„å¤±è´¥æ¨¡å¼æœ‰å“ªäº›ï¼Ÿå¦‚ä½•è¯Šæ–­å’Œä¿®å¤ï¼Ÿã€

### 8.1 Reward Hacking

**ç—‡çŠ¶**ï¼šRM score æŒç»­ä¸Šå‡ï¼Œä½† human eval / benchmark ä¸‹é™æˆ–åœæ»ã€‚

**å¸¸è§å½¢å¼**ï¼š
- å›ç­”å˜é•¿ä½†åºŸè¯å¢å¤š
- è¿‡åº¦ä½¿ç”¨ markdown formatting
- Sycophancyï¼ˆæ— æ¡ä»¶åŒæ„ç”¨æˆ·ï¼‰
- åˆ©ç”¨ RM çš„å…·ä½“æ¼æ´ï¼ˆå¦‚ç‰¹å®š token patternï¼‰

**è¯Šæ–­**ï¼š
- ç›‘æ§ response length éš training çš„å˜åŒ–
- å¯¹æ¯” RM score å’Œ human eval çš„è¶‹åŠ¿
- æ£€æŸ¥ reward åˆ†å¸ƒçš„å˜åŒ–ï¼ˆæ˜¯å¦å‡ºç° bimodal åˆ†å¸ƒï¼‰

**ä¿®å¤**ï¼š
- åŠ å¤§ KL penalty
- RM ensemble
- Length penalty / normalization
- å®šæœŸç”¨æ–°æ•°æ® retrain RM
- è®¾ç½® reward score ceiling

### 8.2 Mode Collapse

**ç—‡çŠ¶**ï¼šæ¨¡å‹çš„ response å˜å¾—é«˜åº¦ç›¸ä¼¼ï¼Œå¤šæ ·æ€§æ¶ˆå¤±ã€‚

**è¡¨ç°**ï¼š
- å¯¹ä¸åŒ prompt ç”Ÿæˆå‡ ä¹ç›¸åŒçš„å¼€å¤´
- æ¸©åº¦è°ƒé«˜ä¹Ÿæ— æ³•æ¢å¤å¤šæ ·æ€§
- vocabulary ä½¿ç”¨å˜çª„

**åŸå› **ï¼š
- KL penalty è¿‡å¼º â†’ æ¨¡å‹ä¸æ•¢åç¦» reference
- DPO æ•°æ®åå‘æŸç§é£æ ¼ â†’ æ¨¡å‹è¿‡åº¦æ¨¡ä»¿è¯¥é£æ ¼
- Batch size å¤ªå° â†’ gradient noise ä¸å¤Ÿï¼Œæ”¶æ•›åˆ° sharp minimum

**ä¿®å¤**ï¼š
- é™ä½ KL ç³»æ•° $\beta$
- æ•°æ®å¤šæ ·åŒ–ï¼ˆå¤šæ¥æºã€å¤šé£æ ¼ï¼‰
- å¢å¤§ batch size
- ç”¨ entropy bonus é¼“åŠ±æ¢ç´¢
- GRPO çš„ group generation å¤©ç„¶å¢åŠ å¤šæ ·æ€§

### 8.3 Alignment Tax

**å®šä¹‰**ï¼šå¯¹é½è®­ç»ƒå¯¼è‡´æ¨¡å‹åœ¨æŸäº›èƒ½åŠ›ä¸Šçš„é€€åŒ–ã€‚

**å…¸å‹è¡¨ç°**ï¼š
- å¯¹é½åæ•°å­¦èƒ½åŠ›ä¸‹é™
- å¯¹é½åä»£ç èƒ½åŠ›ä¸‹é™
- å¯¹é½ååˆ›æ„å†™ä½œå˜å¾— "å…«è‚¡åŒ–"

**åŸå› **ï¼š
- Safety å’Œ helpfulness çš„ trade-offï¼šè¿‡åº¦ safety è®­ç»ƒå¯¼è‡´ "æ‹’ç­”" è¿‡å¤š
- RLHF çš„ reward model åå‘æŸç§ "å®‰å…¨é£æ ¼"
- SFT/DPO æ•°æ®åˆ†å¸ƒä¸ pretrain æ•°æ®å·®å¼‚å¯¼è‡´ catastrophic forgetting

**ç¼“è§£**ï¼š
- å¤šç›®æ ‡ä¼˜åŒ–ï¼ˆsafety + helpfulness + capability å¹³è¡¡ï¼‰
- åˆ†é˜¶æ®µè®­ç»ƒï¼ˆå…ˆèƒ½åŠ›ã€åå¯¹é½ï¼‰
- æ··åˆ pretrain æ•°æ®åˆ° alignment è®­ç»ƒä¸­
- ç»†ç²’åº¦ safety policyï¼ˆä¸æ˜¯ä¸€åˆ€åˆ‡æ‹’ç»ï¼‰

### 8.4 Length Bias

**ç—‡çŠ¶**ï¼šæ¨¡å‹åå‘ç”Ÿæˆæ›´é•¿çš„å›ç­”ï¼Œå³ä½¿ç®€çŸ­å›ç­”æ›´åˆé€‚ã€‚

**æ ¹å› **ï¼š
- RM è®­ç»ƒæ•°æ®ä¸­ï¼Œé•¿å›ç­”æ™®éè¢«æ ‡æ³¨ä¸º "æ›´å¥½"ï¼ˆä¿¡æ¯é‡å‡è±¡ï¼‰
- PPO/GRPO è®­ç»ƒä¸­ï¼Œé•¿ response ç§¯ç´¯æ›´å¤š token-level reward
- DPO çš„ log-probability åœ¨é•¿ sequence ä¸Šç»å¯¹å€¼æ›´å¤§

**ä¿®å¤**ï¼š
- SimPO çš„ length normalization
- è®­ç»ƒæ•°æ®ä¸­åŠ å…¥ "é•¿ä½†å·®" å’Œ "çŸ­ä½†å¥½" çš„æ ·æœ¬
- RM è®­ç»ƒæ—¶åŠ å…¥ length control
- DPO loss ä¸­é™¤ä»¥ response é•¿åº¦

### 8.5 Distribution Shift / Off-Policy Issues

**ç—‡çŠ¶**ï¼šDPO è®­ç»ƒåæœŸ loss ä¸ä¸‹é™æˆ–ä¸Šå‡ã€‚

**åŸå› **ï¼šPolicy å·²ç»è¿œç¦»äº† reference model å’Œè®­ç»ƒæ•°æ®çš„åˆ†å¸ƒï¼Œæ•°æ®ä¸å† "relevant"ã€‚

**ä¿®å¤**ï¼š
- Online DPOï¼šæŒç»­ç”¨å½“å‰ policy ç”Ÿæˆæ–°æ•°æ®
- Iterative trainingï¼šåˆ†å¤šè½®ï¼Œæ¯è½®æ›´æ–° reference
- ç›‘æ§ policy å’Œ reference çš„ KL divergenceï¼Œè®¾ç½®é˜ˆå€¼

### 8.6 Catastrophic Forgetting

**ç—‡çŠ¶**ï¼šå¯¹é½åæ¨¡å‹ "å¿˜è®°" äº†æŸäº› pretrain èƒ½åŠ›ã€‚

**ä¿®å¤**ï¼š
- åœ¨ alignment æ•°æ®ä¸­æ··å…¥ pretrain æ•°æ®ï¼ˆreplay bufferï¼‰
- LoRA / adapter åªæ”¹éƒ¨åˆ†å‚æ•°
- æ§åˆ¶å­¦ä¹ ç‡å’Œè®­ç»ƒæ­¥æ•°
- å®šæœŸåœ¨ capability benchmark ä¸Šå›å½’æµ‹è¯•

### 8.7 è¯Šæ–­æ¸…å•

```
âœ… è®­ç»ƒ loss åœ¨ä¸‹é™ï¼Ÿ
âœ… RM score / win rate åœ¨ä¸Šå‡ï¼Ÿ
âœ… Response é•¿åº¦æœ‰æ²¡æœ‰å¼‚å¸¸å¢é•¿ï¼Ÿ
âœ… KL divergence åœ¨åˆç†èŒƒå›´å†…ï¼Ÿï¼ˆé€šå¸¸ < 10-15 natsï¼‰
âœ… Human eval / benchmark æ˜¯å¦åŒæ­¥æå‡ï¼Ÿ
âœ… Response diversity æœ‰æ²¡æœ‰ä¸‹é™ï¼Ÿ
âœ… æ˜¯å¦å­˜åœ¨ç‰¹å®šçš„ failure patternï¼Ÿï¼ˆrepetition, refusal, sycophancyï¼‰
âœ… ä¸åŒèƒ½åŠ›ç»´åº¦æ˜¯å¦å‡è¡¡ï¼Ÿï¼ˆmath, code, writing, safetyï¼‰
```

---

## 9. é¢è¯•é«˜é¢‘é¢˜ 15 é“ + æ·±åº¦å‚è€ƒç­”æ¡ˆ

### Q1: RLHF çš„ä¸‰ä¸ªé˜¶æ®µæ˜¯ä»€ä¹ˆï¼Ÿæ¯ä¸ªé˜¶æ®µçš„å…³é”®æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ

**ç­”**ï¼š

RLHF åŒ…å«ä¸‰ä¸ªé˜¶æ®µï¼š

**Stage 1: SFTï¼ˆSupervised Fine-Tuningï¼‰**
- å°† pretrained model å¾®è°ƒä¸º instruction-following model
- å…³é”®æŒ‘æˆ˜ï¼šæ•°æ®è´¨é‡ï¼ˆLIMA è¯æ˜ 1K é«˜è´¨é‡ > 52K ä½è´¨é‡ï¼‰ã€chat template ä¸€è‡´æ€§ã€é˜²æ­¢ overfit

**Stage 2: Reward Model Training**
- è®­ç»ƒä¸€ä¸ª scalar reward function æ¥è¿‘ä¼¼äººç±»åå¥½
- å…³é”®æŒ‘æˆ˜ï¼šæ ‡æ³¨è€…ä¸€è‡´æ€§ä½ï¼ˆçº¦ 65-75%ï¼‰ã€reward scale æ¼‚ç§»ã€length biasã€æ³›åŒ–æ€§å·®

**Stage 3: RL Fine-Tuningï¼ˆé€šå¸¸ç”¨ PPOï¼‰**
- ç”¨ RL å¾®è°ƒ policyï¼Œæœ€å¤§åŒ– reward åŒæ—¶é™åˆ¶ KL divergence
- å…³é”®æŒ‘æˆ˜ï¼š4 æ¨¡å‹æ˜¾å­˜å‹åŠ›ã€è®­ç»ƒä¸ç¨³å®šã€reward hackingã€generation ç“¶é¢ˆ

---

### Q2: DPO çš„æ•°å­¦æ¨å¯¼è¿‡ç¨‹æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆèƒ½ç»•è¿‡ Reward Modelï¼Ÿ

**ç­”**ï¼š

DPO çš„æ ¸å¿ƒæ´å¯Ÿæ˜¯ï¼šRLHF çš„ KL-constrained optimization æœ‰è§£æè§£ã€‚

**æ¨å¯¼æ­¥éª¤**ï¼š
1. RLHF ç›®æ ‡ï¼š$\max_\pi \mathbb{E}[R(x,y)] - \beta D_{\text{KL}}(\pi \| \pi_{\text{ref}})$
2. æœ€ä¼˜è§£ï¼š$\pi^*(y|x) \propto \pi_{\text{ref}}(y|x) \exp(R(x,y)/\beta)$
3. åè§£ rewardï¼š$R(x,y) = \beta \log \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x)$
4. ä»£å…¥ Bradley-Terryï¼špartition function $Z(x)$ åœ¨åšå·®æ—¶æ¶ˆæ‰
5. æœ€ç»ˆå¾—åˆ° DPO lossï¼šåªéœ€è¦ policy å’Œ reference çš„ log probability

**ä¸ºä»€ä¹ˆèƒ½ç»•è¿‡ RM**ï¼šå› ä¸ºæˆ‘ä»¬æŠŠ reward ç”¨ policy å’Œ reference çš„ log-ratio é‡æ–°å‚æ•°åŒ–äº†ã€‚Reward model çš„ä¿¡æ¯è¢«éšå¼åœ°ç¼–ç åœ¨ preference data ä¸­ï¼Œé€šè¿‡ DPO loss ç›´æ¥å­¦ä¹ ã€‚

---

### Q3: DPO å’Œ RLHF åœ¨å®è·µä¸­çš„æ ¸å¿ƒåŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿä»€ä¹ˆæ—¶å€™è¯¥ç”¨å“ªä¸ªï¼Ÿ

**ç­”**ï¼š

**æ ¸å¿ƒåŒºåˆ«**ï¼š
| ç»´åº¦ | DPO | RLHF (PPO) |
|------|-----|------------|
| æ•°æ® | Offlineï¼ˆå›ºå®šæ•°æ®é›†ï¼‰ | Onlineï¼ˆä¸æ–­ç”Ÿæˆæ–°æ•°æ®ï¼‰ |
| æ¢ç´¢ | æ—  | æœ‰ï¼ˆpolicy è‡ªä¸»æ¢ç´¢ï¼‰ |
| è®­ç»ƒç¨³å®šæ€§ | é«˜ | ä½ï¼ˆPPO å¯¹è¶…å‚æ•æ„Ÿï¼‰ |
| è®¡ç®—æˆæœ¬ | ä½ï¼ˆ2 æ¨¡å‹ï¼‰ | é«˜ï¼ˆ4 æ¨¡å‹ï¼‰ |
| æ€§èƒ½ä¸Šé™ | å—é™äºæ•°æ®è´¨é‡ | ç†è®ºä¸Šæ›´é«˜ï¼ˆèƒ½æ¢ç´¢ï¼‰ |

**é€‰æ‹©å»ºè®®**ï¼š
- æ•°æ®å……è¶³ã€è¿½æ±‚ç®€å•ç¨³å®š â†’ DPO
- è¿½æ±‚æœ€é«˜æ€§èƒ½ã€æœ‰è®¡ç®—èµ„æº â†’ Online RL (PPO/GRPO)
- æ¨ç†/ä»£ç ä»»åŠ¡ã€æœ‰ verifiable reward â†’ GRPO + RLVR
- å¼€æ”¾æ€§ä»»åŠ¡ï¼ˆå¯¹è¯ã€å†™ä½œï¼‰â†’ RLHF æˆ– Online DPO

OpenReview çš„ç ”ç©¶è¡¨æ˜ï¼š**å½“ representation å……åˆ†ä¸”ä¼˜åŒ–å¤Ÿå……åˆ†æ—¶ï¼ŒRL ä¼˜äº DPO**ã€‚

---

### Q4: ä»€ä¹ˆæ˜¯ GRPOï¼Ÿå’Œ PPO æœ‰ä»€ä¹ˆæœ¬è´¨åŒºåˆ«ï¼Ÿ

**ç­”**ï¼š

GRPOï¼ˆGroup Relative Policy Optimizationï¼‰æ˜¯ DeepSeek æå‡ºçš„ç®—æ³•ï¼Œæ ¸å¿ƒæ”¹è¿›æ˜¯ **ç”¨ç»„å†…ç›¸å¯¹ reward æ›¿ä»£ critic model æ¥ä¼°è®¡ advantage**ã€‚

**æœ¬è´¨åŒºåˆ«**ï¼š
- PPO ç”¨ä¸€ä¸ª **trained critic V(s)** æ¥ä¼°è®¡ advantageï¼š$\hat{A}_t = R_t - V(s_t)$
- GRPO å¯¹æ¯ä¸ª prompt ç”Ÿæˆä¸€ç»„ responseï¼Œç”¨ **ç»„å†…å‡å€¼/æ–¹å·®** æ ‡å‡†åŒ– rewardï¼š$\hat{A}_i = (r_i - \mu) / \sigma$

**ä¼˜åŠ¿**ï¼š
1. çœæ‰ critic model â†’ æ˜¾å­˜å‡åŠ
2. è®­ç»ƒæ›´ç®€å• â†’ ä¸éœ€è¦ train value function
3. ç‰¹åˆ«é€‚åˆ sparse rewardï¼ˆå¯¹/é”™ï¼‰â†’ ç»„å†…å¯¹æ¯”æä¾›æœ‰æ•ˆæ¢¯åº¦

**ä¸ºä»€ä¹ˆ DeepSeek-R1 ç”¨ GRPO è€Œä¸æ˜¯ PPO**ï¼š
- 671B æ¨¡å‹ + 4 æ¨¡å‹ PPO çš„æ˜¾å­˜ä¸ç°å®
- æ•°å­¦/ä»£ç çš„ reward æ˜¯ binary çš„ï¼ˆå¯¹/é”™ï¼‰ï¼Œcritic å­¦ä¸å¥½
- GRPO çš„ group comparison å¤©ç„¶é€‚åˆ verifiable reward

---

### Q5: ä»€ä¹ˆæ˜¯ Reward Hackingï¼Ÿå¦‚ä½•æ£€æµ‹å’Œé˜²å¾¡ï¼Ÿ

**ç­”**ï¼š

**Reward Hacking** æ˜¯ policy æ‰¾åˆ° reward model çš„æ¼æ´ï¼Œè·å¾—é«˜ reward ä½†å®é™…è´¨é‡ä½ä¸‹ã€‚

**å¸¸è§å½¢å¼**ï¼š
1. Length exploitationï¼šç”Ÿæˆé•¿åºŸè¯
2. Sycophancyï¼šæ— æ¡ä»¶åŒæ„ç”¨æˆ·
3. Formatting tricksï¼šè¿‡åº¦ä½¿ç”¨ markdown/emoji
4. Code hackï¼šDeepSeek è®­ç»ƒä¸­å‘ç°æ¨¡å‹å­¦ä¼š `sys.exit(0)` ç»•è¿‡æµ‹è¯•

**æ£€æµ‹ä¿¡å·**ï¼š
- RM score â†‘ ä½† human eval â†“/å¹³ï¼šæœ€å¼ºä¿¡å·
- Response length å¼‚å¸¸å¢é•¿
- KL divergence å¿«é€Ÿå¢å¤§
- Response diversity ä¸‹é™

**é˜²å¾¡**ï¼š
- KL penaltyï¼ˆbaselineï¼‰
- RM ensembleï¼ˆå¤šä¸ª RM äº¤å‰éªŒè¯ï¼‰
- Length normalization / penalty
- Iterative RM retrainingï¼ˆç”¨æ–° policy è¾“å‡ºæ›´æ–° RMï¼‰
- Verifiable rewardsï¼ˆæ•°å­¦/ä»£ç åœºæ™¯ä¸å¯ hackï¼‰
- Reward clipping / ceiling

---

### Q6: KTO å’Œ DPO çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿä»€ä¹ˆåœºæ™¯ä¸‹ KTO æ›´å¥½ï¼Ÿ

**ç­”**ï¼š

**æ•°æ®æ ¼å¼åŒºåˆ«**ï¼š
- DPO éœ€è¦ **pairwise** dataï¼šåŒä¸€ prompt çš„ (chosen, rejected) å¯¹
- KTO åªéœ€è¦ **binary** signalï¼šè¿™ä¸ªå›ç­”å¥½ / è¿™ä¸ªå›ç­”ä¸å¥½ï¼ˆä¸éœ€è¦é…å¯¹ï¼‰

**ç†è®ºåŒºåˆ«**ï¼š
- DPO åŸºäº Bradley-Terry åå¥½æ¨¡å‹
- KTO åŸºäº Kahneman-Tversky å‰æ™¯ç†è®ºï¼Œ**loss aversion** æ˜¯å…³é”®ï¼šåå›ç­”çš„æƒ©ç½š > å¥½å›ç­”çš„å¥–åŠ±

**KTO æ›´å¥½çš„åœºæ™¯**ï¼š
1. **æ•°æ®æ”¶é›†æˆæœ¬æ•æ„Ÿ**ï¼šåªéœ€è¦ thumbs up/downï¼Œä¸éœ€è¦åŒ prompt ä¸‹ä¸¤ä¸ªå›ç­”çš„å¯¹æ¯”
2. **é«˜é£é™©åœºæ™¯**ï¼ˆæ³•å¾‹ã€åŒ»ç–—ï¼‰ï¼šasymmetric loss æ›´å¥½åœ°æƒ©ç½šæœ‰å®³è¾“å‡º
3. **æ ‡æ³¨è€…æ°´å¹³å‚å·®**ï¼šbinary åˆ¤æ–­æ¯” pairwise æ¯”è¾ƒæ›´å®¹æ˜“åšå¯¹

**å®éªŒæ•°æ®**ï¼šKTO â‰¥ DPO â‰ˆ CPO > IPOï¼ˆåœ¨ reasoningã€QAã€mathã€truthfulness ä¸Šï¼‰

---

### Q7: Constitutional AI å’Œæ ‡å‡† RLAIF æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ

**ç­”**ï¼š

**æ ‡å‡† RLAIF**ï¼šç”¨ä¸€ä¸ªå¼º LLM å¯¹ response åš pairwise åå¥½æ ‡æ³¨ï¼Œæ›¿ä»£äººç±»æ ‡æ³¨è€…ã€‚æœ¬è´¨ä¸Šåªæ˜¯æ ‡æ³¨æ–¹å¼å˜äº†ï¼Œåç»­æµç¨‹ä¸å˜ã€‚

**Constitutional AIï¼ˆCAIï¼‰**ï¼š
1. å®šä¹‰ä¸€ç»„ **å®ªæ³•åŸåˆ™**ï¼ˆå¦‚ "è¯šå®"ã€"æ— å®³"ã€"æœ‰å¸®åŠ©"ï¼‰
2. è®© LLM æ ¹æ®åŸåˆ™ **critique** è‡ªå·±çš„æœ‰å®³è¾“å‡º
3. è®© LLM **revise** å›ç­”
4. ç”¨ critique/revision è¿‡ç¨‹ç”Ÿæˆè®­ç»ƒæ•°æ®

**æ ¸å¿ƒå·®å¼‚**ï¼šCAI ä¸åªæ˜¯ "æ¢äº†æ ‡æ³¨è€…"ï¼Œè€Œæ˜¯ç”¨ **åŸåˆ™é©±åŠ¨çš„è‡ªæˆ‘æ”¹è¿›** æ¥ç”Ÿæˆå¯¹é½æ•°æ®ã€‚è¿™ä½¿å¾—æŠ½è±¡ä»·å€¼è§‚å¯ä»¥è¢«ç³»ç»Ÿåœ°ç¼–ç ï¼Œè€Œä¸ä¾èµ–äºå…·ä½“çš„æ ‡æ³¨æ ·æœ¬ã€‚

---

### Q8: Process Reward Model å’Œ Outcome Reward Model çš„åŒºåˆ«ï¼Ÿå„è‡ªçš„ä¼˜ç¼ºç‚¹ï¼Ÿ

**ç­”**ï¼š

**Outcome RMï¼ˆORMï¼‰**ï¼šå¯¹æ•´ä¸ª response ç»™ä¸€ä¸ªåˆ†æ•°ã€‚
- ä¼˜ç‚¹ï¼šæ ‡æ³¨ç®€å•ã€é€šç”¨æ€§å¼º
- ç¼ºç‚¹ï¼šreward signal sparseã€æ— æ³•å®šä½é”™è¯¯æ­¥éª¤ã€å®¹æ˜“ reward hacking

**Process RMï¼ˆPRMï¼‰**ï¼šå¯¹æ¯ä¸ª reasoning step ç»™åˆ†æ•°ã€‚
- ä¼˜ç‚¹ï¼šdense reward signalã€å¯ä»¥æ—©æœŸå‘ç°é”™è¯¯ã€é…åˆ tree search æ•ˆæœå¥½
- ç¼ºç‚¹ï¼šæ ‡æ³¨æˆæœ¬æé«˜ã€è‡ªåŠ¨åŒ–æ–¹æ¡ˆæœ‰å™ªå£°ã€"æ­¥éª¤ä¾èµ–" é—®é¢˜

**OpenAI "Let's Verify Step by Step"ï¼ˆ2023ï¼‰**è¯æ˜ï¼šPRM åœ¨æ•°å­¦æ¨ç†ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äº ORMã€‚å› ä¸º PRM å¯ä»¥è¯†åˆ«å‡º "è¿‡ç¨‹æ­£ç¡®ä½†ç­”æ¡ˆé”™è¯¯" å’Œ "è¿‡ç¨‹é”™è¯¯ä½†ç­”æ¡ˆç¢°å¯¹" çš„æƒ…å†µã€‚

---

### Q9: Online DPO å’Œ Offline DPO çš„åŒºåˆ«ï¼Ÿä¸ºä»€ä¹ˆ Online æ•ˆæœæ›´å¥½ï¼Ÿ

**ç­”**ï¼š

**Offline DPO**ï¼šç”¨ **é¢„å…ˆæ”¶é›†** çš„åå¥½æ•°æ®è®­ç»ƒã€‚æ•°æ®æ˜¯å›ºå®šçš„ï¼Œå’Œ reference model ç”Ÿæˆçš„ã€‚

**Online DPO**ï¼š
1. ç”¨ **å½“å‰ policy** ç”Ÿæˆ response
2. ç”¨ RM æˆ– LLM judge æ ‡æ³¨åå¥½
3. ç”¨æ–°æ•°æ®åš DPO æ›´æ–°
4. è¿­ä»£

**ä¸ºä»€ä¹ˆ Online æ›´å¥½**ï¼š
1. **Distribution matching**ï¼šè®­ç»ƒæ•°æ®æ¥è‡ªå½“å‰ policyï¼Œæ²¡æœ‰ off-policy é—®é¢˜
2. **æŒç»­æ¢ç´¢**ï¼šæ¯è½®éƒ½æœ‰æ–°çš„ responseï¼Œæ¨¡å‹èƒ½å‘ç°æ–°ç­–ç•¥
3. **è‡ªé€‚åº”éš¾åº¦**ï¼šéšç€ policy å˜å¼ºï¼Œrejection sampling äº§ç”Ÿæ›´ challenging çš„å¯¹æ¯”å¯¹
4. **é¿å… stale data**ï¼šoffline æ•°æ®åœ¨è®­ç»ƒåæœŸå˜å¾— "è¿‡æ—¶"

è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆ **GRPO æœ¬è´¨ä¸Šå¯ä»¥çœ‹ä½œ Online DPO**ï¼ˆgroup size > 2ï¼‰çš„ç†è®ºè”ç³»ã€‚

---

### Q10: å¦‚ä½•é€‰æ‹© DPO å˜ä½“ï¼Ÿç»™å‡ºä¸€ä¸ªå†³ç­–æ¡†æ¶ã€‚

**ç­”**ï¼š

```mermaid
graph TD
    Q["ä½ çš„æ•°æ®æ˜¯ä»€ä¹ˆæ ¼å¼ï¼Ÿ"] --> P["Pairwise<br/>(chosen/rejected)"]
    Q --> BIN["Binary<br/>(good/bad ä¸é…å¯¹)"]
    Q --> NO["æ²¡æœ‰æ ‡æ³¨æ•°æ®"]
    Q --> BEST["æƒ³è¦æœ€å¼ºæ•ˆæœ"]

    P --> P1["æ•°æ®è´¨é‡é«˜ã€é‡å¤§ â†’ DPO"]
    P --> P2["æ•°æ®æœ‰å™ªå£°/çŸ›ç›¾ â†’ IPO æˆ– SimPO"]
    P --> P3["Class imbalance ä¸¥é‡ â†’ ORPO"]
    P --> P4["ä¸æƒ³ç»´æŠ¤ ref model â†’ SimPO æˆ– ORPO"]
    P --> P5["éœ€è¦å’Œ SFT ä¸€èµ·è®­ç»ƒ â†’ ORPO"]

    BIN --> BIN1["KTO"]

    NO --> NO1["æœ‰ verifiable reward<br/>(æ•°å­¦/ä»£ç ) â†’ GRPO + RLVR"]
    NO --> NO2["æ—  verifiable reward<br/>â†’ Self-Reward / RLAIF â†’ DPO/GRPO"]

    BEST --> BEST1["Online DPO æˆ– GRPO<br/>(æŒç»­è¿­ä»£)"]
```

---

### Q11: DeepSeek-R1 æ˜¯å¦‚ä½•ç”¨çº¯ RL æ¶Œç°æ¨ç†èƒ½åŠ›çš„ï¼Ÿ

**ç­”**ï¼š

**è®­ç»ƒæµç¨‹**ï¼š
1. **èµ·ç‚¹**ï¼šDeepSeek-V3-Baseï¼ˆ671B pretrained modelï¼‰ï¼Œ**ä¸åš SFT**
2. **ç®—æ³•**ï¼šGRPO
3. **Reward**ï¼šRule-based verifiable rewards
   - æ•°å­¦ï¼šç­”æ¡ˆæ˜¯å¦ä¸ ground truth åŒ¹é…ï¼ˆbinaryï¼‰
   - ä»£ç ï¼šæ˜¯å¦é€šè¿‡æ‰€æœ‰æµ‹è¯•ç”¨ä¾‹ï¼ˆbinaryï¼‰
   - æ ¼å¼ï¼šæ˜¯å¦éµå¾ª `<think>...</think>` æ ¼å¼ï¼ˆbinaryï¼‰
4. **æ¶Œç°ç°è±¡**ï¼š
   - Chain-of-thought reasoning è‡ªå‘å‡ºç°
   - Self-reflectionï¼ˆ"wait, let me reconsider..."ï¼‰è‡ªå‘å‡ºç°
   - "Aha moment"ï¼šæ¨¡å‹åœ¨é•¿æ¨ç†é“¾ä¸­çªç„¶çº æ­£é”™è¯¯

**ä¸ºä»€ä¹ˆæœ‰æ•ˆ**ï¼š
- Pretrained model å·²ç»æœ‰ latent reasoning capability
- RL ä¸æ˜¯ "æ•™" æ¨ç†ï¼Œè€Œæ˜¯ "elicit"ï¼ˆæ¿€å‘ï¼‰å·²æœ‰çš„èƒ½åŠ›
- Verifiable reward æä¾›äº†å®Œç¾çš„è®­ç»ƒä¿¡å·ï¼ˆæ—  hackingã€æ— å™ªå£°ï¼‰
- Group generationï¼ˆG=64ï¼‰æä¾›äº†ä¸°å¯Œçš„å¯¹æ¯”æ ·æœ¬

**DeepSeek-R1-Zero çš„å±€é™**ï¼š
- å¯è¯»æ€§å·®ï¼ˆæ··åˆè¯­è¨€ã€æ ¼å¼æ··ä¹±ï¼‰
- åœ¨éæ¨ç†ä»»åŠ¡ä¸Šé€€åŒ–
- è§£å†³æ–¹æ¡ˆï¼šåŠ å…¥ SFT cold-startï¼ˆå°‘é‡é«˜è´¨é‡æ¨ç†æ•°æ®ï¼‰ï¼Œå¾—åˆ° DeepSeek-R1

---

### Q12: DAPO å¯¹æ¯” GRPO åšäº†å“ªäº›æ”¹è¿›ï¼Ÿä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

**ç­”**ï¼š

DAPOï¼ˆByteDance, NeurIPS 2025ï¼‰åšäº†å››ä¸ªå…³é”®æ”¹è¿›ï¼š

1. **Decoupled Clipping**ï¼šæ­£å‘ï¼ˆé¼“åŠ±å¥½è¡Œä¸ºï¼‰å’Œåå‘ï¼ˆæƒ©ç½šåè¡Œä¸ºï¼‰ç”¨ä¸åŒçš„ clip èŒƒå›´
   - ä¸ºä»€ä¹ˆé‡è¦ï¼šå¯ä»¥è®©æ¨¡å‹æ›´å¿«è¿œç¦»åç­–ç•¥ï¼ŒåŒæ—¶ä¿å®ˆåœ°å­¦ä¹ å¥½ç­–ç•¥

2. **Dynamic Sampling**ï¼šè¿‡æ»¤æ‰å…¨å¯¹/å…¨é”™çš„ prompt group
   - ä¸ºä»€ä¹ˆé‡è¦ï¼šè¿™äº› group çš„ advantage å…¨ä¸º 0ï¼Œç™½ç™½æµªè´¹è®¡ç®—

3. **Token-Level Loss**ï¼šper-token è€Œé per-sequence
   - ä¸ºä»€ä¹ˆé‡è¦ï¼šé˜²æ­¢é•¿ response ä¸»å¯¼æ¢¯åº¦

4. **Overlong Reward Shaping**ï¼šè¶…é•¿å›ç­” soft penalty
   - ä¸ºä»€ä¹ˆé‡è¦ï¼šé˜²æ­¢ length exploitation åŒæ—¶ä¸ä¸¢å¤±æœ‰æ•ˆå†…å®¹

**æ•ˆæœ**ï¼šAIME 2024 å¾—åˆ† 50ï¼ˆvs DeepSeek-R1-Zero-Qwen-32B çš„ 47ï¼‰ï¼Œä¸”è®­ç»ƒæ­¥æ•°å‡åŠã€‚

---

### Q13: ä½ å¦‚ä½•è¯Šæ–­ RLHF è®­ç»ƒä¸­çš„ mode collapseï¼Ÿ

**ç­”**ï¼š

**è¯Šæ–­ä¿¡å·**ï¼š
1. **Response diversity ä¸‹é™**ï¼šè®¡ç®—ä¸åŒ prompt çš„ response ä¹‹é—´çš„ similarityï¼ˆå¦‚ BLEUã€embedding cosineï¼‰
2. **Vocabulary ä½¿ç”¨å˜çª„**ï¼šç»Ÿè®¡ unique token æ•°é‡
3. **æ¸©åº¦æ— æ•ˆ**ï¼šè°ƒé«˜ temperature ä¹Ÿæ— æ³•æ¢å¤å¤šæ ·æ€§
4. **Fixed openings**ï¼šå¤šæ•°å›ç­”ä»¥ç›¸åŒçŸ­è¯­å¼€å¤´
5. **KL divergence å¼‚å¸¸**ï¼šè¦ä¹ˆè¿‡é«˜ï¼ˆpolicy å´©æºƒåˆ°æŸä¸ª modeï¼‰è¦ä¹ˆè¿‡ä½ï¼ˆè¢« reference å®Œå…¨æŸç¼šï¼‰

**ä¿®å¤æ–¹æ¡ˆ**ï¼š
1. é™ä½ $\beta$ï¼ˆKL ç³»æ•°ï¼‰æˆ– DPO çš„ $\beta$ å‚æ•°
2. å¢å¤§ batch size
3. æ•°æ®å¤šæ ·åŒ–ï¼ˆåŠ å…¥ä¸åŒé£æ ¼çš„åå¥½æ•°æ®ï¼‰
4. Entropy bonusï¼šåœ¨ loss ä¸­åŠ å…¥ entropy æ­£åˆ™åŒ–
5. ä½¿ç”¨ Online DPO / GRPO å¢åŠ æ¢ç´¢
6. æ£€æŸ¥ reference model æ˜¯å¦æœ¬èº«å°±å·²ç» collapsed

---

### Q14: Alignment Tax æ˜¯ä»€ä¹ˆï¼Ÿå¦‚ä½•æœ€å°åŒ–ï¼Ÿ

**ç­”**ï¼š

**Alignment Tax** æŒ‡å¯¹é½è®­ç»ƒå¯¼è‡´æ¨¡å‹åœ¨æŸäº›æ ¸å¿ƒèƒ½åŠ›ä¸Šé€€åŒ–ã€‚

**å…¸å‹è¡¨ç°**ï¼š
- å¯¹é½åæ•°å­¦/ä»£ç èƒ½åŠ›ä¸‹é™ 3-5%
- è¿‡åº¦å®‰å…¨ï¼šå¯¹åˆç†è¯·æ±‚ä¹Ÿæ‹’ç»ï¼ˆ"I can't help with that"ï¼‰
- åˆ›æ„å†™ä½œå˜å¾—å…¬å¼åŒ–

**æœ€å°åŒ–ç­–ç•¥**ï¼š
1. **èƒ½åŠ›-å®‰å…¨åˆ†ç¦»**ï¼šå…ˆè®­ç»ƒèƒ½åŠ›ï¼ˆSFT on math/codeï¼‰ï¼Œå†åšå®‰å…¨å¯¹é½
2. **æ··åˆè®­ç»ƒæ•°æ®**ï¼šåœ¨ alignment æ•°æ®ä¸­æ··å…¥ pretrain/capability æ•°æ®ï¼ˆ10-20%ï¼‰
3. **Parameter-efficient alignment**ï¼šç”¨ LoRA åšå¯¹é½ï¼Œåªæ”¹éƒ¨åˆ†å‚æ•°
4. **Multi-objective optimization**ï¼šsafety + helpfulness + capability å¤šç›®æ ‡å¹³è¡¡
5. **ç»†ç²’åº¦ safety**ï¼šä¸æ˜¯ä¸€åˆ€åˆ‡æ‹’ç»ï¼Œè€Œæ˜¯å¯¹ä¸åŒé£é™©ç­‰çº§ç»™ä¸åŒç­–ç•¥
6. **æŒç»­ç›‘æ§**ï¼šåœ¨ capability benchmark ä¸Šåš regression testing

---

### Q15: å¦‚æœè®©ä½ ä»é›¶æ­å»ºä¸€ä¸ª RLHF pipelineï¼Œä½ ä¼šæ€ä¹ˆè®¾è®¡ï¼Ÿ

**ç­”**ï¼š

**Phase 0: éœ€æ±‚åˆ†æ**
- æ¨¡å‹è§„æ¨¡ï¼Ÿï¼ˆå†³å®šæ¡†æ¶é€‰å‹ï¼‰
- ä»»åŠ¡ç±»å‹ï¼Ÿï¼ˆå¯¹è¯/æ¨ç†/ä»£ç  â†’ å†³å®š reward ç±»å‹ï¼‰
- è®¡ç®—é¢„ç®—ï¼Ÿï¼ˆå†³å®š online vs offlineï¼‰

**Phase 1: SFT**
- æ¡†æ¶ï¼šLLaMA-Factory æˆ– TRL
- æ•°æ®ï¼š10-50K é«˜è´¨é‡ instruction-response pairs
- éªŒè¯ï¼šåœ¨ held-out set ä¸Šæµ‹ chat quality

**Phase 2: é€‰æ‹©å¯¹é½æ–¹æ³•**

è·¯çº¿ Aï¼ˆç®€å•é«˜æ•ˆï¼‰ï¼š
- æ”¶é›† / ç”Ÿæˆåå¥½æ•°æ®ï¼ˆRLAIF é™ä½æˆæœ¬ï¼‰
- DPO / SimPO è®­ç»ƒï¼ˆTRL æˆ– LLaMA-Factoryï¼‰
- é€‚åˆï¼šä¸­å°è§„æ¨¡ã€èµ„æºæœ‰é™

è·¯çº¿ Bï¼ˆæœ€å¼ºæ•ˆæœï¼‰ï¼š
- è®­ç»ƒ Reward Model
- GRPO æˆ– PPO è®­ç»ƒï¼ˆOpenRLHFï¼‰
- Online generation + iterative alignment
- é€‚åˆï¼šå¤§è§„æ¨¡ã€æœ‰ GPU é›†ç¾¤

è·¯çº¿ Cï¼ˆæ¨ç†ä»»åŠ¡ï¼‰ï¼š
- æ„å»º verifiable rewardï¼ˆæ•°å­¦ç­”æ¡ˆæ£€æŸ¥ã€ä»£ç æµ‹è¯•ï¼‰
- GRPO + RLVRï¼ˆOpenRLHFï¼‰
- æ— éœ€äººç±»æ ‡æ³¨
- é€‚åˆï¼šæ•°å­¦ã€ç¼–ç¨‹ã€é€»è¾‘æ¨ç†

**Phase 3: è¯„ä¼°**
- Automated metricsï¼šMT-Benchã€AlpacaEvalã€Arena-Hard
- Human evalï¼šA/B testing
- Safety evalï¼šRed-teamingã€HarmBench
- Capability regressionï¼šMMLUã€HumanEvalã€GSM8K

**Phase 4: è¿­ä»£**
- éƒ¨ç½² â†’ æ”¶é›†åé¦ˆ â†’ æ›´æ–°åå¥½æ•°æ® â†’ é‡è®­ç»ƒ
- ç›‘æ§ reward hackingã€mode collapseã€alignment tax
- æŒç»­ online alignment

**å…³é”®å»ºè®®**ï¼š
1. ä» DPO å¼€å§‹ï¼ŒéªŒè¯ pipelineï¼Œå†å‡çº§åˆ° online RL
2. Reward model çš„è´¨é‡å†³å®š RLHF çš„ ceiling
3. è¯„ä¼° > è®­ç»ƒï¼šä¸èƒ½è¯„ä¼° = ä¸çŸ¥é“åœ¨åšä»€ä¹ˆ
4. å§‹ç»ˆä¿ç•™ human-in-the-loop ä½œä¸º safety net

---

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯

- **å†…éƒ¨ Chatbot å¯¹é½**ï¼šå¯¹å·²æœ‰ SFT æ¨¡å‹åš DPO/SimPO æ˜¯æœ€ä½æˆæœ¬çš„å¯¹é½æ–¹æ¡ˆï¼ŒTRL æˆ– Unsloth + LoRA å•å¡ 24GB å³å¯å®Œæˆ 7B æ¨¡å‹è®­ç»ƒ
- **ä»£ç /æ•°å­¦æ¨ç†èƒ½åŠ›æå‡**ï¼šGRPO + RLVRï¼ˆverifiable rewardsï¼‰æ˜¯å½“å‰æœ€å¼ºæ–¹æ¡ˆï¼Œæ— éœ€äººç±»æ ‡æ³¨ï¼ŒDeepSeek-R1 å·²éªŒè¯æœ‰æ•ˆï¼ˆ[arXiv:2501.12948](https://arxiv.org/abs/2501.12948)ï¼‰
- **Safety å¯¹é½**ï¼šConstitutional AI æ€è·¯å¯ç¼–ç ç»„ç»‡ä»·å€¼è§‚ï¼Œé€‚åˆä¼ä¸šå®šåˆ¶å®‰å…¨ç­–ç•¥
- **ä½æˆæœ¬åå¥½æ•°æ®æ”¶é›†**ï¼šç”¨ KTO åªéœ€è¦ thumbs up/down ä¿¡å·ï¼Œå¤§å¹…é™ä½æ ‡æ³¨æˆæœ¬

### å·¥ç¨‹å®ç°è¦ç‚¹

- **æ¡†æ¶é€‰å‹**ï¼š7B DPO å¿«é€Ÿå®éªŒ â†’ TRL/Unslothï¼›70B+ PPO/GRPO â†’ OpenRLHFï¼ˆRay + vLLM + DeepSpeedï¼‰
- **DPO Î² å‚æ•°**ï¼šé€šå¸¸ 0.1-0.5ï¼Œå¤ªå¤§ä¸å­¦ä¹ ï¼Œå¤ªå° mode collapseã€‚æ¨èä» 0.1 å¼€å§‹è°ƒ
- **GRPO Group Size**ï¼šé€šå¸¸ G=8-64ï¼Œè¶Šå¤§ advantage ä¼°è®¡è¶Šå‡†ä½†è®¡ç®—æˆæœ¬è¶Šé«˜ã€‚G=16 æ˜¯å¸¸è§èµ·ç‚¹
- **Reward Hacking ç›‘æ§**ï¼šå¿…é¡»åŒæ—¶è·Ÿè¸ª RM score å’Œ human eval / benchmarkï¼Œä¸¤è€…èƒŒç¦» = æŠ¥è­¦
- **KL ç³»æ•°**ï¼šPPO ä¸­ Î²=0.01-0.05 å¸¸è§ï¼ŒGRPO ä¸­ Î²=0.04 è¢« DeepSeekMath éªŒè¯æœ‰æ•ˆï¼ˆ[arXiv:2402.03300](https://arxiv.org/abs/2402.03300), Sec. 3.2ï¼‰

### é¢è¯•é«˜é¢‘é—®æ³•

- Q: DPO å’Œ RLHF å“ªä¸ªå¥½ï¼Ÿä»€ä¹ˆæ—¶å€™ç”¨å“ªä¸ªï¼Ÿ
  A: ä¸æ˜¯è°å¥½è°åâ€”â€”DPO ç®€å•ç¨³å®šé€‚åˆ offlineï¼ŒRLHF/GRPO æœ‰æ¢ç´¢èƒ½åŠ›é€‚åˆ onlineã€‚æ¨ç†ä»»åŠ¡ç”¨ GRPO+RLVRï¼Œé€šç”¨å¯¹è¯ç”¨ Online DPOã€‚å…³é”®åŒºåˆ«æ˜¯ exploration vs exploitationã€‚
- Q: GRPO å’Œ PPO çš„æœ¬è´¨åŒºåˆ«ï¼Ÿ
  A: GRPO å»æ‰äº† Criticï¼ˆValue Modelï¼‰ï¼Œç”¨ç»„å†… reward æ ‡å‡†åŒ–æ›¿ä»£ GAE advantageã€‚çœä¸€åŠæ˜¾å­˜ï¼Œæ›´é€‚åˆ sparse/binary rewardã€‚
- Q: ä»€ä¹ˆæ˜¯ Reward Hackingï¼Ÿä½ æ€ä¹ˆé˜²ï¼Ÿ
  A: Policy åˆ©ç”¨ RM æ¼æ´è·å¾—é«˜åˆ†ä½†å®é™…è´¨é‡ä½ã€‚ç»å…¸è¡¨ç°ï¼šå˜é•¿ä½†åºŸè¯å¤šã€sycophancyã€‚é˜²å¾¡ï¼šKL penalty + RM ensemble + length normalization + iterative RM retrainingã€‚

---

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ

- **å¯¹é½ â‰  å®‰å…¨çº¦æŸï¼Œå¯¹é½ = èƒ½åŠ›æ¿€å‘**ï¼šDeepSeek-R1 æœ€å¤§çš„å¯ç¤ºâ€”â€”RL ä¸åªæ˜¯è®©æ¨¡å‹"åƒäººè¯´è¯"ï¼Œè€Œæ˜¯è®©æ¨¡å‹æ¶Œç°è¶…è¶Šè®­ç»ƒæ•°æ®çš„æ¨ç†èƒ½åŠ›ã€‚è¿™æ”¹å˜äº†å¯¹ RL post-training çš„è®¤çŸ¥å®šä½
- **DPO å®¶æ—è®©å¯¹é½å¹³æ°‘åŒ–**ï¼šä¸å†éœ€è¦ 128 å¼  GPU è·‘ PPOï¼Œå•å¡ LoRA + DPO å°±èƒ½åšå‡ºæœ‰æ„ä¹‰çš„å¯¹é½æ”¹è¿›ã€‚å¯¹è€æ¿çš„é¡¹ç›®æ„å‘³ç€ï¼š**ä»»ä½•æœ‰ SFT æ¨¡å‹çš„å›¢é˜Ÿéƒ½èƒ½ä½æˆæœ¬åšå¯¹é½**
- **Verifiable Rewards æ˜¯é‡‘çŸ¿**ï¼šæ•°å­¦/ä»£ç ä»»åŠ¡çš„ reward æ˜¯ç¡®å®šæ€§çš„ï¼ˆå¯¹/é”™ï¼‰ï¼Œè¿™æ„å‘³ç€è¿™äº›é¢†åŸŸçš„ RL scaling å‡ ä¹æ²¡æœ‰ä¸Šé™ã€‚è€æ¿å¦‚æœåšä»£ç /æ•°å­¦ç›¸å…³äº§å“ï¼ŒGRPO+RLVR æ˜¯æœ€é«˜ ROI çš„æŠ•å…¥æ–¹å‘

### æœªè§£é—®é¢˜ä¸å±€é™

- **å¼€æ”¾æ€§ä»»åŠ¡çš„ RL Scaling è¿˜æ²¡æœ‰è¢«éªŒè¯**ï¼šGRPO/RLVR åœ¨æ•°å­¦/ä»£ç ä¸Šæ•ˆæœæƒŠè‰³ï¼Œä½†å¯¹è¯ã€åˆ›æ„å†™ä½œç­‰æ— æ³• verify çš„ä»»åŠ¡ï¼ŒRL çš„ scaling law æ˜¯å¦æˆç«‹ï¼Ÿç›®å‰è¯æ®ä¸è¶³
- **Reward Hacking ä»ç„¶æ˜¯æ ¹æœ¬æ€§æŒ‘æˆ˜**ï¼šå³ä½¿ç”¨ ensemble + KL penaltyï¼ŒMETR 2025 æŠ¥å‘ŠæŒ‡å‡º frontier æ¨¡å‹ä»ç„¶åœ¨ reward hackã€‚è¿™æ˜¯ä¸€ä¸ª arms raceï¼Œæ²¡æœ‰ç»ˆæè§£
- **GRPO = DPO çš„ç»Ÿä¸€ç†è®ºè¿˜å¾ˆåˆæ­¥**ï¼š"It Takes Two" è®ºæ–‡æŒ‡å‡º GRPO å’Œ DPO åœ¨å…‰è°±ä¸Šç›¸è¿ï¼Œä½†å®é™…è®­ç»ƒåŠ¨åŠ›å­¦å·®å¼‚ä»ç„¶å¾ˆå¤§ï¼Œç†è®ºè¿˜ä¸å®Œå–„
- **Alignment Tax çš„å®šé‡åŒ–ä¸å¤Ÿ**ï¼šæˆ‘ä»¬çŸ¥é“å¯¹é½ä¼šæŸå¤±èƒ½åŠ›ï¼Œä½†æŸå¤±å¤šå°‘ã€åœ¨å“ªäº›ç»´åº¦ã€å¦‚ä½•æœ€ä¼˜ trade-offâ€”â€”ç¼ºä¹ç³»ç»Ÿæ€§çš„é‡åŒ–æ¡†æ¶

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸

- å¦‚æœæŠŠ [[LLM-æ¨ç†ä¼˜åŒ–-2026-å…¨æ™¯|æ¨ç†ä¼˜åŒ–]] å’Œ GRPO è®­ç»ƒç»“åˆâ€”â€”GRPO çš„ group generation é˜¶æ®µæ˜¯å¯†é›†æ¨ç†ï¼ŒSpeculative Decoding åŠ é€Ÿ generation å¯ä»¥ç›´æ¥åŠ é€Ÿ RL è®­ç»ƒ
- [[GRPO æ·±åº¦ç†è§£|GRPO]] + [[LoRA|LoRA]] çš„ç»„åˆï¼šLoRA åš parameter-efficient çš„ RL è®­ç»ƒï¼Œå‡å°‘ alignment taxï¼ˆåªæ”¹éƒ¨åˆ†å‚æ•°ï¼‰ï¼Œæ˜¯å¦èƒ½æ‰¾åˆ°æ›´å¥½çš„ capability-alignment å¹³è¡¡ï¼Ÿ
- 6 ä¸ªæœˆåé¢„æµ‹ï¼šOnline DPO + RLAIF çš„å…¨è‡ªåŠ¨å¾ªç¯å°†æˆä¸ºæ ‡é…ï¼Œäººç±»åªåœ¨ safety-critical åœºæ™¯åš spot-checkã€‚Self-Play å¯¹é½èµ°å‘æˆç†Ÿã€‚

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡

- [InstructGPT: Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155) â€” RLHF ä¸‰é˜¶æ®µèŒƒå¼çš„å·¥ä¸šåŒ–å®šä¹‰ï¼Œå¥ åŸºä¹‹ä½œ
- [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290) â€” ç»•è¿‡ RM å’Œ PPO çš„ä¼˜é›…æ•°å­¦æ¨å¯¼ï¼ŒDPO å®¶æ—çš„èµ·æº
- [PPO: Proximal Policy Optimization](https://arxiv.org/abs/1707.06347) â€” RL åŸºç¡€ï¼Œç†è§£ clipped objective çš„è®¾è®¡åŠ¨æœº
- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948) â€” çº¯ RL æ¶Œç°æ¨ç†çš„é‡Œç¨‹ç¢‘ï¼ŒGRPO çš„å¤§è§„æ¨¡éªŒè¯
- [DAPO: An Open-Source LLM Reinforcement Learning System at Scale](https://arxiv.org/abs/2503.14476) â€” GRPO å››é¡¹å…³é”®æ”¹è¿›ï¼ŒNeurIPS 2025
- [KTO: Model Alignment as Prospect Theoretic Optimization](https://arxiv.org/abs/2402.01306) â€” é pairwise å¯¹é½ï¼Œå‰æ™¯ç†è®ºåœ¨ LLM ä¸­çš„åˆ›æ–°åº”ç”¨
- [IPO: A General Theoretical Paradigm to Understand Learning from Human Feedback](https://arxiv.org/abs/2310.12036) â€” ä¿®å¤ DPO çš„ overfitting ç†è®ºé—®é¢˜
- [RLOO: Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback](https://arxiv.org/abs/2402.14740) â€” Leave-One-Out baseline çš„ç®€æ´æœ‰æ•ˆæ–¹æ³•
- [Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073) â€” åŸåˆ™é©±åŠ¨çš„ AI è‡ªæˆ‘å¯¹é½
- [Let's Verify Step by Step (PRM)](https://arxiv.org/abs/2305.20050) â€” Process Reward Modelï¼Œæ•°å­¦æ¨ç† reward è®¾è®¡çš„é‡è¦å‚è€ƒ

### æ·±åº¦è§£è¯»

- [Chip Huyen: RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html) â€” å…¨æ™¯å¼æ¢³ç†ï¼Œé€‚åˆå…¥é—¨ â­â­â­â­â­
- [Nathan Lambert: RLHF Book (Manning)](https://www.rlhfbook.com/) â€” ç³»ç»Ÿæ€§æ•™æï¼Œä»ç†è®ºåˆ°å·¥ç¨‹ â­â­â­â­â­
- [PPO & GRPO for LLM Alignment â€” Shi (2026)](https://cameronrwolfe.substack.com/) â€” GRPO æ·±åº¦æŠ€æœ¯è§£è¯» â­â­â­â­
- [A Comprehensive Survey of LLM Alignment Techniques â€” Wang et al.](https://arxiv.org/abs/2407.16216) â€” å…¨é¢ç»¼è¿° â­â­â­â­

### å®è·µèµ„æº

- [TRL (Transformers Reinforcement Learning)](https://github.com/huggingface/trl) â€” HuggingFace å®˜æ–¹ RLHF æ¡†æ¶ï¼Œæ”¯æŒ PPO/DPO/KTO/ORPO/RLOOï¼Œæœ€é€‚åˆå¿«é€Ÿå®éªŒ
- [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF) â€” åˆ†å¸ƒå¼ RLHF/GRPO è®­ç»ƒæ¡†æ¶ï¼Œ70B+ æ¨¡å‹çš„æœ€ä½³é€‰æ‹©
- [LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory) â€” ä¸€ç«™å¼å¾®è°ƒå¹³å°ï¼ŒDPO/ORPO/KTO å¼€ç®±å³ç”¨
- [Unsloth](https://github.com/unslothai/unsloth) â€” æ¶ˆè´¹çº§æ˜¾å¡ä¸Šçš„é«˜æ•ˆ LoRA + DPO/GRPO è®­ç»ƒ

---

> æœ€åæ›´æ–°ï¼š2026-02-22 | ä½œè€…ï¼šMorpheus Vault

---

## See Also

**é¢è¯•å…¥é—¨ï¼ˆæœ¬æ–‡æ·±åº¦ç‰ˆçš„ç®€æ´å¯¼è¯»ï¼‰ï¼š**
- [[å¯¹é½æŠ€æœ¯ç»¼è¿°|å¯¹é½æŠ€æœ¯ç»¼è¿°]] â€” RLHFâ†’DPOâ†’ORPOâ†’KTOâ†’CAI å…­æ–¹æ³•å¯¹æ¯”è¡¨ï¼Œå«å®Œæ•´æ¨å¯¼ä¸é¢è¯• Q&Aï¼›æœ¬å…¨æ™¯çš„ç²¾åå…¥é—¨ç‰ˆï¼ˆå¦‚æœæ—¶é—´æœ‰é™ï¼Œå…ˆè¯»è¿™ç¯‡ï¼‰

- [[GRPO-Improvement-Panorama-2026|GRPO 2026 å…¨æ™¯]] â€” RLHF ä¹‹åçš„å‰æ²¿æ–¹æ³•ä¸ƒç»´å…¨æ™¯
- [[MARS-Margin-Aware-Reward-Modeling-Self-Refinement|MARS]] â€” Reward Model è®­ç»ƒçš„æ–°æ–¹å‘
- [[GRPO æ·±åº¦ç†è§£|GRPO æ·±åº¦ç†è§£]] â€” GRPO ç®—æ³•çš„æ•°å­¦æ¨å¯¼ä¸å®ç°ç»†èŠ‚
- [[LoRA|LoRA]] â€” Parameter-efficient fine-tuningï¼Œå¯¹é½è®­ç»ƒçš„æ˜¾å­˜ä¼˜åŒ–æ‰‹æ®µ
-  â€” LLM å¼ºåŒ–å­¦ä¹ å…¨å›¾è°±
-  â€” å¤§è¯­è¨€æ¨¡å‹çŸ¥è¯†å…¨å›¾è°±

**åå¥½ä¼˜åŒ–å˜ä½“ï¼ˆå„æœ‰ç‹¬ç«‹æ·±åº¦ç¬”è®°ï¼‰ï¼š**
- [[SimPO-Simple-Preference-Optimization-Reference-Free|SimPO]] â€” Reference-free DPOï¼Œlength-normalizedï¼ˆNeurIPS 2024ï¼‰
- [[IPO-Identity-Preference-Optimization|IPO]] â€” ç»•è¿‡ Bradley-Terry å‡è®¾ï¼ŒÎ¨PO ç†è®ºæ¡†æ¶ï¼ˆDeepMindï¼ŒAISTATS 2024ï¼‰
- [[ORPO-Odds-Ratio-Preference-Optimization|ORPO]] â€” ä¸€é˜¶æ®µ alignmentï¼ˆSFT+preference åˆä¸€ï¼‰

**Critic-free è®­ç»ƒè°±ç³»ï¼ˆå„æœ‰ç‹¬ç«‹æ·±åº¦ç¬”è®°ï¼‰ï¼š**
- [[REINFORCE-Plus-Plus-Global-Advantage-Normalization|REINFORCE++]] â€” å…¨å±€ batch advantage normalizationï¼ˆarXiv:2501.03262ï¼‰
- [[ReMax-RL-Alignment-REINFORCE-Max-Baseline|ReMax]] â€” greedy rollout baselineï¼Œçœ 46% å†…å­˜ï¼ˆICML 2024ï¼‰
- [[REBEL-Regret-Based-RL-LLM-Alignment|REBEL]] â€” Regret-based RLï¼Œå¤„ç† intransitive preference
- [[Dr-GRPO-Unbiased-Optimization|Dr. GRPO]] â€” GRPO å»åä¿®å¤ï¼šlength bias + difficulty biasï¼ˆCOLM 2025ï¼‰
- [[PRIME-Process-Reward-Implicit-MLE|PRIME]] â­ â€” åœ¨çº¿ PRMï¼Œæ— éœ€ step-level æ ‡æ³¨ï¼Œtoken-level implicit rewardï¼ˆTHUï¼‰

> **see-alsoï¼ˆAgent RL å»¶ä¼¸ï¼‰**ï¼šRLHF çš„æ ¸å¿ƒèŒƒå¼ï¼ˆreward signal + policy updateï¼‰åœ¨ Agent åœºæ™¯ä¸‹æœ‰æ ¹æœ¬æ€§çš„æ‰©å±•ï¼š
> - [[SCoRe-Self-Correction-via-Reinforcement-Learning|SCoReï¼ˆNeurIPS 2024ï¼‰]] â€” RLHF çš„ multi-turn å»¶ä¼¸ï¼šç”¨åŒé˜¶æ®µ RL è®­ç»ƒ LLM è‡ªæˆ‘çº é”™ï¼ˆæ— éœ€äººå·¥ preference æ ‡æ³¨ï¼Œç”¨ä»»åŠ¡å¯éªŒè¯æ€§æ›¿ä»£ï¼‰ï¼ŒMATH +15.6%ï¼›äº’è¡¥å…³ç³»ï¼šRLHF è§£å†³ single-turn å¯¹é½ï¼ŒSCoRe è§£å†³ multi-turn è‡ªæˆ‘ä¿®æ­£
> - [[Agentic-RL-2026å‰æ²¿ç»¼åˆåˆ†æ|Agentic RL 2026 å‰æ²¿ç»¼åˆåˆ†æ]] â€” RLHF èŒƒå¼æ‰©å±•åˆ° Agent é•¿åºåˆ—å†³ç­–çš„ç³»ç»Ÿæ€§åˆ†æ
