---
title: GRPO æ·±åº¦ç†è§£
brief: GRPOï¼ˆGroup Relative Policy Optimizationï¼‰æ˜¯ DeepSeekMath æå‡ºçš„æ— éœ€ç‹¬ç«‹ Critic çš„ RL ç®—æ³•ï¼Œé€šè¿‡ç»„å†…ç›¸å¯¹ä¼˜åŠ¿æ›¿ä»£ PPO çš„ä»·å€¼ä¼°è®¡ï¼Œå°†è®­ç»ƒæˆæœ¬é™ä½çº¦50%ï¼›æ ¸å¿ƒæ´å¯Ÿï¼šç”¨ç»„å†…å‡å€¼/æ–¹å·®æ ‡å‡†åŒ–æ›¿ä»£ä»·å€¼ç½‘ç»œï¼Œæ˜¯ DeepSeek-R1 æ¨ç†èƒ½åŠ›çªç ´çš„å…³é”®è®­ç»ƒæœºåˆ¶
type: concept
domain: ai/llm/rl/grpo
created: 2026-02-13
updated: 2026-02-22
tags:
  - ai/llm/rl/grpo
  - type/concept
status: complete
sources:
  - DeepSeekMath arXiv:2402.03300 (Sheng et al., 2024)
  - DeepSeek-R1 arXiv:2501.12948
  - "HuggingFace TRL GRPO Trainer: https://huggingface.co/docs/trl/grpo_trainer"
related:
  - "[[AI/3-LLM/RL/ç®—æ³•/PPO åŸç†]]"
  - "[[Projects/DeepSeek-R1-å­¦ä¹ ç¬”è®°]]"
  - "[[AI/3-LLM/RL/Fundamentals/RL & LLMs å…¥é—¨]]"
  - "[[AI/3-LLM/RL/å®è·µ/GRPO-TRLå®è·µ]]"
  - "[[AI/3-LLM/RL/å®è·µ/RLHF-å·¥ç¨‹å…¨æ ˆ]]"
---

# GRPO æ·±åº¦ç†è§£

> **Brief**ï¼šGRPOï¼ˆGroup Relative Policy Optimizationï¼‰æ˜¯ DeepSeekMath æå‡ºçš„ RL ç®—æ³•ï¼Œæ— éœ€å•ç‹¬ Critic ç½‘ç»œï¼Œé€šè¿‡åŒç»„ç”Ÿæˆç»“æœçš„ç›¸å¯¹å¯¹æ¯”è®¡ç®—ä¼˜åŠ¿å€¼ã€‚æ˜¯ DeepSeek-R1 æ¨ç†èƒ½åŠ›çš„æ ¸å¿ƒè®­ç»ƒæœºåˆ¶ï¼Œè®­ç»ƒæˆæœ¬æ¯” PPO ä½çº¦ 50%ã€‚
>
> æ¥æºï¼šDeepSeekMath, arXiv:2402.03300, Sec. 3

---

## 1. æ ¸å¿ƒåŠ¨æœºï¼šä¸ºä»€ä¹ˆä¸ç”¨ PPOï¼Ÿ

PPOï¼ˆProximal Policy Optimizationï¼‰æ˜¯ RLHF çš„æ ‡å‡†ç®—æ³•ï¼Œä½†åœ¨ LLM è®­ç»ƒä¸­æœ‰ä¸€ä¸ªæ ¹æœ¬æ€§ä»£ä»·ï¼š**éœ€è¦åŒæ—¶ç»´æŠ¤ 4 ä¸ªæ¨¡å‹**ï¼š
- $\pi_\theta$ï¼ˆç­–ç•¥æ¨¡å‹ï¼Œåœ¨è®­ç»ƒï¼‰
- $\pi_{\text{ref}}$ï¼ˆå‚è€ƒæ¨¡å‹ï¼Œå†»ç»“ï¼‰
- $V_\phi$ï¼ˆCritic/ä»·å€¼æ¨¡å‹ï¼Œé¢„æµ‹åŸºçº¿ï¼‰
- $r_\phi$ï¼ˆå¥–åŠ±æ¨¡å‹ï¼‰

**GRPO çš„æ ¸å¿ƒæ´å¯Ÿ**ï¼šåœ¨æ•°å­¦æ¨ç†è¿™ç±»ä»»åŠ¡ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨"ç»„å†…ç›¸å¯¹æ’å"ä»£æ›¿ä»·å€¼ç½‘ç»œå¯¹åŸºçº¿çš„ä¼°è®¡ã€‚

> æ¥æºï¼šDeepSeekMath arXiv:2402.03300, Sec. 3.1ï¼šWe propose GRPO... eliminates the need for an additional critic model.

---

## 2. ç®—æ³•æ­¥éª¤

### Step 1ï¼šåˆ†ç»„é‡‡æ ·ï¼ˆGroup Samplingï¼‰

å¯¹æ¯ä¸ªé—®é¢˜ $q$ï¼Œä»å½“å‰ç­–ç•¥ $\pi_\theta$ é‡‡æ · $G$ ä¸ªè¾“å‡ºï¼š

$$\{o_1, o_2, \ldots, o_G\} \sim \pi_{\theta_{\text{old}}}(\cdot \mid q)$$

$G$ é€šå¸¸å– 8~16ï¼ˆDeepSeekMath ç”¨ $G=8$ï¼‰ã€‚

> æ¥æºï¼šDeepSeekMath arXiv:2402.03300, Sec. 3.2

### Step 2ï¼šå¥–åŠ±æ‰“åˆ†

ç”¨å¥–åŠ±æ¨¡å‹ï¼ˆæˆ–è§„åˆ™å‡½æ•°ï¼‰å¯¹æ¯ä¸ªè¾“å‡ºæ‰“åˆ†ï¼š

$$\{r_1, r_2, \ldots, r_G\}$$

æ•°å­¦é¢˜ä¸­é€šå¸¸ç”¨è§„åˆ™å‡½æ•°ï¼šæ­£ç¡® $r_i = 1$ï¼Œé”™è¯¯ $r_i = 0$ã€‚

### Step 3ï¼šç»„å†…ç›¸å¯¹ä¼˜åŠ¿ï¼ˆGroup Relative Advantageï¼‰

è®¡ç®—ç»„å†…æ ‡å‡†åŒ–ä¼˜åŠ¿å€¼ï¼ˆè¿™æ˜¯ GRPO åŒºåˆ«äº PPO çš„æ ¸å¿ƒï¼‰ï¼š

$$\hat{A}_i = \frac{r_i - \text{mean}(\{r_j\}_{j=1}^G)}{\text{std}(\{r_j\}_{j=1}^G)}$$

**ç›´è§‰**ï¼šä¸é—®"è¿™ä¸ªå›ç­”ç»å¯¹ä¸Šå¥½ä¸å¥½"ï¼Œè€Œæ˜¯é—®"è¿™ä¸ªå›ç­”åœ¨åŒç»„é‡Œæ¯”å¹³å‡é«˜å¤šå°‘"ã€‚mean æ˜¯ç»„å†…åŸºçº¿ï¼Œstd æ˜¯å½’ä¸€åŒ–å› å­ã€‚

**æ•°å€¼ç¤ºä¾‹**ï¼š
$G=4$ï¼Œå¥–åŠ± $\{1, 0, 1, 0\}$ï¼Œåˆ™ mean=0.5, std=0.53
æ­£ç¡®è¾“å‡ºï¼š$\hat{A} = (1-0.5)/0.53 = +0.94$
é”™è¯¯è¾“å‡ºï¼š$\hat{A} = (0-0.5)/0.53 = -0.94$

> æ¥æºï¼šDeepSeekMath arXiv:2402.03300, Eq. (3)

### Step 4ï¼šç­–ç•¥æ›´æ–°ç›®æ ‡å‡½æ•°

$$\mathcal{L}_{\text{GRPO}}(\theta) = \mathbb{E}_{q, \{o_i\}} \left[ \frac{1}{G} \sum_{i=1}^{G} \frac{1}{|o_i|} \sum_{t=1}^{|o_i|} \min\left( \rho_{i,t}(\theta)\, \hat{A}_i,\ \text{clip}(\rho_{i,t}(\theta), 1{-}\epsilon, 1{+}\epsilon)\, \hat{A}_i \right) - \beta\, \mathbb{D}_{\text{KL}}[\pi_\theta \| \pi_{\text{ref}}] \right]$$

å…¶ä¸­ï¼š
- $\rho_{i,t}(\theta) = \frac{\pi_\theta(o_{i,t} \mid q, o_{i,<t})}{\pi_{\theta_{\text{old}}}(o_{i,t} \mid q, o_{i,<t})}$ï¼šæ¦‚ç‡æ¯”ï¼ˆimportance sampling ratioï¼‰
- $\text{clip}(\cdot, 1{-}\epsilon, 1{+}\epsilon)$ï¼šPPO-clipï¼Œé˜²æ­¢ç­–ç•¥æ­¥å­å¤ªå¤§ï¼ˆ$\epsilon=0.2$ï¼‰
- $\beta\, \mathbb{D}_{\text{KL}}[\pi_\theta \| \pi_{\text{ref}}]$ï¼šKL æ•£åº¦æƒ©ç½šï¼Œé˜²æ­¢åç¦»å‚è€ƒæ¨¡å‹ï¼ˆ$\beta=0.04$ï¼ŒDeepSeekMathï¼‰

**KL æ•£åº¦å±•å¼€**ï¼š

$$\mathbb{D}_{\text{KL}}[\pi_\theta \| \pi_{\text{ref}}] = \frac{\pi_\theta(o \mid q)}{\pi_{\text{ref}}(o \mid q)} - \log \frac{\pi_\theta(o \mid q)}{\pi_{\text{ref}}(o \mid q)} - 1$$

> æ¥æºï¼šDeepSeekMath arXiv:2402.03300, Eq. (4)-(5)

---

## 3. GRPO ç®—æ³•æµç¨‹å›¾

```mermaid
flowchart TD
    A["è¾“å…¥é—®é¢˜ q"] --> B["ä» Ï€_Î¸_old é‡‡æ · G ä¸ªè¾“å‡º"]
    B --> C["å¥–åŠ±æ¨¡å‹/è§„åˆ™æ‰“åˆ† râ‚...r_G"]
    C --> D["è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ Ã‚_i = (r_i - mean) / std"]
    D --> E["è®¡ç®—æ¦‚ç‡æ¯” Ï_i,t"]
    E --> F["PPO-clip ç›®æ ‡å‡½æ•° + KL æƒ©ç½š"]
    F --> G["æ›´æ–° Ï€_Î¸"]
    G -->|"ä¸‹ä¸€ä¸ª batch"| A

    style D fill:#f9f,stroke:#333,stroke-width:2px
    style F fill:#bbf,stroke:#333,stroke-width:2px
```

---

## 4. GRPO vs PPO å¯¹æ¯”

| ç»´åº¦ | PPO | GRPO |
|------|-----|------|
| ä»·å€¼ç½‘ç»œ | éœ€è¦ç‹¬ç«‹ Critic æ¨¡å‹ | âŒ æ— éœ€ï¼Œç”¨ç»„å†…ç»Ÿè®¡æ›¿ä»£ |
| æ˜¾å­˜å ç”¨ | 4ä¸ªæ¨¡å‹ï¼ˆActor+Critic+Ref+RMï¼‰ | 3ä¸ªæ¨¡å‹ï¼ˆActor+Ref+RMï¼‰ |
| è®­ç»ƒç¨³å®šæ€§ | Critic è®­ç»ƒå¯èƒ½ä¸ç¨³å®š | ç›¸å¯¹æ›´ç¨³å®š |
| é€‚ç”¨åœºæ™¯ | é€šç”¨ RL | æœ‰æ˜ç¡®å¯¹é”™çš„ä»»åŠ¡ï¼ˆæ•°å­¦/ä»£ç ï¼‰ |
| åŸºçº¿ä¼°è®¡ | ä»·å€¼ç½‘ç»œå­¦ä¹  | ç»„å†…å‡å€¼ï¼ˆæ›´ç®€å•ï¼Œä½†æ–¹å·®æ›´å¤§ï¼‰ |

> æ¥æºï¼šDeepSeekMath arXiv:2402.03300 Table 2ï¼šGRPO åœ¨æ•°å­¦æ¨ç†ä¸Šè¶…è¶Š PPOï¼ŒåŒæ—¶èŠ‚çœçº¦ 50% è®¡ç®—èµ„æºã€‚

---

## 5. å…³é”®è¶…å‚

| è¶…å‚ | æ¨èå€¼ | ä½œç”¨ |
|-----|--------|------|
| $G$ï¼ˆç»„å¤§å°ï¼‰ | 8~16 | è¶Šå¤§åŸºçº¿è¶Šå‡†ï¼Œä½†æ˜¾å­˜è¶Šè´µ |
| $\epsilon$ | 0.2 | PPO clip èŒƒå›´ï¼Œæ§åˆ¶æ¯æ­¥æ›´æ–°å¹…åº¦ |
| $\beta$ | 0.04ï¼ˆDeepSeekMathï¼‰| KL æƒ©ç½šå¼ºåº¦ï¼Œè¶Šå¤§è¶Šä¿å®ˆ |
| å­¦ä¹ ç‡ | 1e-6 ~ 1e-5 | RL é˜¶æ®µæ¯” SFT å°1-2ä¸ªæ•°é‡çº§ |

> æ¥æºï¼šDeepSeekMath arXiv:2402.03300, Sec. 4 (Experimental Setup)ï¼›DeepSeek-R1 arXiv:2501.12948 ç”¨äº†ç±»ä¼¼è¶…å‚èŒƒå›´

---

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **æ•°å­¦æ¨ç†è®­ç»ƒ**ï¼šæœ‰æ˜ç¡®åˆ¤é¢˜æ ‡å‡†ï¼ˆæ­£ç¡®/é”™è¯¯ï¼‰ï¼ŒGRPO æ˜¯é¦–é€‰ï¼ˆDeepSeek-R1 å°±æ˜¯è¿™ä¹ˆåšçš„ï¼‰
- **ä»£ç ç”Ÿæˆ RL**ï¼šå•å…ƒæµ‹è¯•é€šè¿‡/å¤±è´¥ = å¤©ç„¶å¥–åŠ±å‡½æ•°ï¼Œç›´æ¥å¥—ç”¨
- **å·¥å…·è°ƒç”¨ RL**ï¼šå·¥å…·è¿”å›ç»“æœæœ‰ç¡®å®šæ€§ â†’ ç”¨è§„åˆ™å‡½æ•°æ‰“åˆ†ï¼ŒGRPO è®­ç»ƒ

### å·¥ç¨‹å®ç°è¦ç‚¹ï¼ˆTRL æ¡†æ¶ï¼‰

```python
from trl import GRPOConfig, GRPOTrainer

config = GRPOConfig(
    num_generations=8,        # Gï¼Œç»„å¤§å°
    max_new_tokens=512,       # æœ€å¤§ç”Ÿæˆé•¿åº¦
    beta=0.04,                # KL æ•£åº¦æƒ©ç½šç³»æ•°
    epsilon=0.2,              # PPO clip èŒƒå›´
    learning_rate=1e-6,
)
```

> æ¥æºï¼šHuggingFace TRL GRPO Trainer æ–‡æ¡£ https://huggingface.co/docs/trl/grpo_trainer

### å¸¸è§å‘
- **å¥–åŠ±æ–¹å·®å¤ªå¤§**ï¼šç»„å†…å…¨å¯¹æˆ–å…¨é”™æ—¶ stdâ†’0ï¼Œä¼˜åŠ¿å€¼åˆ†æ¯ç‚¸æ‰ã€‚è§£å†³ï¼šåŠ å¹³æ»‘é¡¹æˆ–æ£€æŸ¥å¥–åŠ±è®¾è®¡
- **KL æ•£åº¦çˆ†ç‚¸**ï¼š$\beta$ å¤ªå°æ—¶ç­–ç•¥é£˜å¾—å¾ˆè¿œï¼Œç”Ÿæˆä¹±åºã€‚è§£å†³ï¼šç›‘æ§ KLï¼Œ$\beta$ è°ƒå¤§
- **ç»„å¤§å° G çš„æ˜¾å­˜**ï¼š$G=16$ æ„å‘³ç€ä¸€æ¬¡ç”Ÿæˆ 16 ä¸ªå›ç­”ï¼Œæ³¨æ„ OOMã€‚è§£å†³ï¼šgradient_checkpointing=True

### é¢è¯•é«˜é¢‘é—®æ³•

- Q: GRPO å’Œ PPO çš„æœ€å¤§åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
  A: GRPO å»æ‰äº†ç‹¬ç«‹çš„ Criticï¼ˆä»·å€¼ç½‘ç»œï¼‰ï¼Œç”¨åŒä¸€é—®é¢˜çš„å¤šä¸ªé‡‡æ ·ç»“æœç»„å†…å‡å€¼ä½œä¸ºåŸºçº¿ï¼Œä¼˜åŠ¿å€¼ $\hat{A}_i = (r_i - \text{mean})/\text{std}$ï¼ŒèŠ‚çœçº¦50%è®¡ç®—èµ„æºã€‚ä»£ä»·æ˜¯åªé€‚åˆæœ‰æ˜ç¡®å¯¹é”™çš„ä»»åŠ¡ã€‚

- Q: ä¸ºä»€ä¹ˆ GRPO çš„ä¼˜åŠ¿å€¼è¦é™¤ä»¥ std å½’ä¸€åŒ–ï¼Ÿ
  A: ä¸å½’ä¸€åŒ–çš„è¯ï¼Œä¸åŒé—®é¢˜çš„å¥–åŠ±å°ºåº¦å·®å¼‚å¾ˆå¤§ï¼ˆç®€å•é¢˜å…¨å¯¹/å…¨é”™ã€éš¾é¢˜æ··åˆï¼‰ï¼Œå¯¼è‡´æ¢¯åº¦ä¸ç¨³å®šã€‚å½’ä¸€åŒ–åç»Ÿä¸€é‡çº²ï¼Œä¸åŒæ‰¹æ¬¡å¯æ¯”ã€‚

---

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿ
GRPO æ˜¯"æç®€ä¸»ä¹‰ RL"çš„å…¸èŒƒï¼šPPO ç”¨å¤æ‚çš„ä»·å€¼ç½‘ç»œå­¦åŸºçº¿ï¼ŒGRPO è¯´"æˆ‘ç”¨ç»Ÿè®¡é‡ä»£æ›¿ç¥ç»ç½‘ç»œ"ï¼Œæ•ˆæœè¿˜æ›´å¥½ã€‚**è¿™è¯´æ˜ï¼šåœ¨ç‰¹å®šçº¦æŸä¸‹ï¼Œç®€å•çš„ç»Ÿè®¡æ¨æ–­å¾€å¾€èƒœè¿‡å¤æ‚çš„ç¥ç»ä¼°è®¡ã€‚**

å¯¹è€æ¿çš„å¯ç¤ºï¼šè®¾è®¡ RL å¥–åŠ±å‡½æ•°æ—¶ï¼Œä¼˜å…ˆè€ƒè™‘æœ‰æ˜ç¡®å¯¹é”™çš„ä»»åŠ¡ï¼ˆä»£ç /æ•°å­¦/å·¥å…·è°ƒç”¨ï¼‰ï¼Œè¿™æ—¶ GRPO æ¯” PPO æ›´çœèµ„æºï¼Œæ›´ç¨³å®šã€‚

### å±€é™ä¸æœªè§£é—®é¢˜
- **å¥–åŠ±ç¨€ç–é—®é¢˜**ï¼šå¼€æ”¾æ€§å¯¹è¯ä»»åŠ¡æ²¡æœ‰æ˜ç¡®å¯¹é”™ï¼ŒGRPO éš¾ä»¥ç›´æ¥ç”¨
- **ç»„å¤§å°çš„é€‰æ‹©**ï¼š$G$ å¤ªå°åŸºçº¿ä¼°è®¡æ–¹å·®å¤§ï¼Œ$G$ å¤ªå¤§æ˜¾å­˜è´µï¼Œæœ€ä¼˜ $G$ ä¸ä»»åŠ¡å¼ºç›¸å…³
- **é•¿é“¾æ¨ç†ä¸­çš„ä¿¡ç”¨åˆ†é…**ï¼šæ¯ä¸ª token å…±äº«åŒä¸€ä¸ªç»„çº§ä¼˜åŠ¿å€¼ï¼Œé•¿å›ç­”ä¸­æ—©æœŸ token çš„ä¿¡ç”¨åˆ†é…ä¸å‡†ç¡®ï¼ˆâ†’ è§ [[AI/3-LLM/RL/ç®—æ³•/Blockwise-Advantage-Estimation]] å¯¹æ­¤çš„æ”¹è¿›ï¼‰

### è„‘æš´æ‹“å±•
- å¦‚æœæŠŠ GRPO çš„"ç»„å†…ç›¸å¯¹"æ€è·¯ç”¨åˆ° Reward Model è®­ç»ƒä¸Šï¼Œæ˜¯å¦å¯ä»¥åš Pairwise GRPOï¼Ÿ
- GRPO + å¤šè½®å¯¹è¯ï¼šæ¯è½®ç»“æŸæ‰æœ‰å¥–åŠ±ï¼Œä¸­é—´ token æ€ä¹ˆåˆ†ä¿¡ç”¨ï¼Ÿâ†’ è§ [[AI/2-Agent/Agentic-RL/CM2-Checklist-Rewards-Multi-Turn-Tool-Use-RL]]
- æç«¯æƒ…å†µï¼š$G=2$ æ—¶ GRPO é€€åŒ–ä¸ºä»€ä¹ˆï¼Ÿï¼ˆæç¤ºï¼šä¸ DPO æœ‰è”ç³»ï¼‰

> ğŸ”— See also:
> - [[AI/3-LLM/RL/ç®—æ³•/PPO åŸç†]] â€” GRPO å»æ‰äº† PPO çš„ Criticï¼Œå¯¹æ¯”å­¦ä¹ ä¸¤è€…
> - [[AI/3-LLM/RL/ç®—æ³•/Blockwise-Advantage-Estimation]] â€” è§£å†³ GRPO é•¿åºåˆ—ä¿¡ç”¨åˆ†é…é—®é¢˜
> - [[AI/2-Agent/Agentic-RL/CM2-Checklist-Rewards-Multi-Turn-Tool-Use-RL]] â€” å¤šè½® RL çš„å¥–åŠ±è®¾è®¡
> - [[AI/3-LLM/RL/ç®—æ³•/QeRL-Quantization-Enhanced-RL]] â€” GRPO åœ¨é‡åŒ–ç¯å¢ƒä¸‹çš„ç¨³å®šæ€§
>
> ğŸ¤– **GRPO åœ¨ Agent åœºæ™¯çš„æ ¹æœ¬æ€§æ”¹é€ **ï¼ˆç†è®º â†’ åº”ç”¨çš„å®Œæ•´é“¾ï¼‰ï¼š
> - [[AI/2-Agent/Agentic-RL/GiGPO-Group-in-Group-Policy-Optimization|GiGPOï¼ˆNeurIPS 2025ï¼‰]] â€” GRPO åœ¨ Agent è®­ç»ƒçš„æ ¸å¿ƒé—®é¢˜ï¼šæ‰€æœ‰ step å…±äº« episode-level advantageã€‚GiGPO ç”¨ Anchor State Grouping å®ç° step-level credit assignmentï¼Œå†…å­˜ç­‰äº GRPO
> - [[AI/2-Agent/Agentic-RL/Tree-GRPO-Tree-Search-LLM-Agent-RL|Tree-GRPOï¼ˆICLR 2026ï¼‰]] â€” æŠŠ GRPO çš„çº¿æ€§ rollout æ›¿æ¢ä¸ºæ ‘æœç´¢ï¼Œå…±äº«å‰ç¼€èŠ‚çœ 1/4 é¢„ç®—ï¼ŒåŒå±‚ advantageï¼ˆintra-tree + inter-treeï¼‰
> - [[AI/2-Agent/Agentic-RL/Multi-Agent-RL-è®­ç»ƒä¸“é¢˜|Multi-Agent RL è®­ç»ƒä¸“é¢˜]] â€” GRPO çš„ grouping å‡è®¾åœ¨å¤š agent åœºæ™¯ break downï¼ˆä¸åŒ agent æœ‰ä¸åŒ prompt/roleï¼‰ï¼ŒMAGRPO/AT-GRPO çš„ä¸“é—¨ä¿®å¤æ–¹æ¡ˆ
> - [[AI/2-Agent/Agentic-RL/TSR-Trajectory-Search-Rollouts-Multi-Turn-RL|TSRï¼ˆarXiv:2602.11767ï¼ŒICML 2026ï¼‰]] â€” GRPO çš„"é‡‡æ · G æ¡ rollout"ç­–ç•¥å‡è®¾ rollout è´¨é‡å‡ç­‰ï¼›TSR æŒ‡å‡º rollout è´¨é‡æ˜¯ multi-turn RL çš„çœŸæ­£ç“¶é¢ˆâ€”â€”ç”¨è®­ç»ƒæ—¶æ ‘æœç´¢ä¿è¯æ¯ä¸ª rollout éƒ½åœ¨å½“å‰æœ€ä¼˜åŠ¨ä½œè·¯å¾„ä¸Šï¼Œ+15% æ•ˆæœ
> - [[AI/2-Agent/Agentic-RL/MIG-Step-Marginal-Information-Gain-Credit-Assignment|MIGï¼ˆarXiv:2602.01034ï¼‰]] â€” GRPO çš„ episode-level reward é—®é¢˜å»¶ä¼¸ï¼šç”¨ä¿¡æ¯è®ºé‡åŒ–æ¯æ­¥çš„è¾¹é™…ä¿¡æ¯å¢ç›Šä½œä¸º dense rewardï¼ŒMonotonic Watermark é˜²æ­¢"å…ˆé™åå‡"çš„ reward hacking
> - [[AI/2-Agent/Agentic-RL/SeeUPO-Sequence-Level-Agentic-RL-Convergence-Guarantees|SeeUPOï¼ˆarXiv:2602.06554ï¼‰]] âš ï¸ **é‡è¦ç†è®ºè¾¹ç•Œ**ï¼šæ­£å¼è¯æ˜ GRPO çš„ variance normalizationï¼ˆé™¤ä»¥ Ïƒï¼‰åœ¨ multi-turn contextual bandit åœºæ™¯ä¸­**ç ´åæ”¶æ•›æ€§**ï¼ˆä¸å¯èƒ½å®šç†ï¼‰ï¼›å•è½®ä»»åŠ¡ GRPO ä»æœ‰æ”¶æ•›ä¿è¯ï¼Œå¤šè½® Agent è®­ç»ƒéœ€æ¢ SeeUPOï¼ˆé€†åºæ›´æ–° + æ—  variance normï¼‰

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models](https://arxiv.org/abs/2402.03300) â€” GRPO åŸå§‹æå‡ºï¼ŒSec. 3 æ˜¯æ ¸å¿ƒï¼Œå¿…è¯» â­â­â­â­â­
- [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via RL](https://arxiv.org/abs/2501.12948) â€” GRPO å¤§è§„æ¨¡å®æˆ˜ï¼Œè¯æ˜æ¨ç†èƒ½åŠ›å¯ RL æ¿€å‘ â­â­â­â­â­

### æ·±åº¦è§£è¯»
- [HuggingFace TRL GRPO Trainer æ–‡æ¡£](https://huggingface.co/docs/trl/grpo_trainer) â€” å®˜æ–¹å®ç°æ–‡æ¡£ï¼Œå«å‚æ•°è¯¦è§£ â­â­â­â­
- [Understanding GRPO](https://huggingface.co/blog/putting-rl-back-in-rlhf) â€” HF Blogï¼šæŠŠ RL æ”¾å› RLHF â­â­â­â­

### å®è·µèµ„æº
- [verl GRPO è®­ç»ƒç¤ºä¾‹](https://verl.readthedocs.io/en/latest/examples/grpo.html) â€” å¤§è§„æ¨¡ GRPO è®­ç»ƒæ¡†æ¶ â­â­â­â­
- [OpenR1 é¡¹ç›®](https://github.com/huggingface/open-r1) â€” å¤ç° DeepSeek-R1 çš„å¼€æºå®ç°ï¼Œå« GRPO â­â­â­â­â­

### ä»£ç æ‰‹æ’•ï¼ˆç†è®º â†’ ä»£ç ï¼‰
- [[Projects/MA-RLHF/lc8-GRPO/lc8-01-GRPO-æ‰‹æ’•å®æ“|GRPO-æ‰‹æ’•å®æ“]] â€” **å¼ºçƒˆæ¨è**ï¼šä»é›¶æ‰‹å†™ GRPO è®­ç»ƒå¾ªç¯ï¼ˆadvantage è®¡ç®—/clip/group normalizationï¼‰ï¼ŒMA-RLHF é¡¹ç›®ä»£ç æ³¨è§£ â­â­â­â­â­
- [[Projects/MA-RLHF/lc8-GRPO/lc8-02-GRPO-å®Œæ•´Notebookå®ç°|GRPO å®Œæ•´ Notebook å®ç°]] â€” **ç«¯åˆ°ç«¯éªŒè¯**ï¼šç»„é‡‡æ · + advantage å½’ä¸€åŒ– + KL é¡¹å®Œæ•´ Notebookï¼ŒéªŒè¯ç†è®ºç»†èŠ‚
- [[Projects/MA-RLHF/lc8-GRPO/lc8-04-GRPO-KLæ•£åº¦ä¸‰ç§è¿‘ä¼¼|GRPO KL æ•£åº¦ä¸‰ç§è¿‘ä¼¼]] â€” k1/k2/k3 Schulman è¿‘ä¼¼å®ç°å¯¹æ¯”ï¼šç²¾åº¦ vs è®¡ç®—æˆæœ¬ tradeoff
- [[Projects/MA-RLHF/lc8-PPO/lc8-01-PPO-æ‰‹æ’•å®æ“|PPO-æ‰‹æ’•å®æ“]] â€” PPO actor-critic å®ç°å¯¹ç…§ï¼Œç†è§£ GRPO ä¸ºä»€ä¹ˆèƒ½å»æ‰ critic
- [[Projects/MA-RLHF/MA-RLHF-æ‰‹æ’•å®æ“-ç³»åˆ—ç´¢å¼•|MA-RLHF æ‰‹æ’•å®æ“ç³»åˆ—ç´¢å¼•]] â€” æ¶æ„/æ¨ç†/Infra/RL å…¨é“¾è·¯ä»£ç å®æ“æ€»ç´¢å¼•
