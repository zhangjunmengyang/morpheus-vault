---
brief: 对齐技术演进综述：RLHF → DPO → ORPO → KTO → SteerLM → Constitutional AI 完整谱系；每种方法的核心动机/损失函数/优缺点对比；interview/hot 必备，面试被问对齐方向可直接引用。
title: 对齐技术综述：RLHF → DPO → ORPO → KTO → SteerLM → Constitutional AI
date: 2026-02-13
rating: ★★★★★
tags:
  - ai/llm/rl
  - ai/alignment
  - type/survey
  - interview/hot
sources:
  - "DPO: arXiv:2305.18290"
  - "KTO: arXiv:2402.01306"
  - "ORPO: arXiv:2403.07691"
  - "Constitutional AI: arXiv:2212.08073"
  - "SteerLM: arXiv:2310.05344"
related:
  - "RLHF/DPO 2026 技术全景"
  - "[[AI/3-LLM/RL/实践/RLHF-工程全栈|RLHF 全链路]]"
  - "[[AI/3-LLM/RL/算法/GRPO 深度理解|GRPO 深度理解]]"
  - "[[AI/3-LLM/RL/实践/RARL-Reward-Modeling-Survey|RARL Reward Modeling Survey]]"
status: active
---

# 对齐技术综述

> 从 RLHF 到 DPO 再到无需偏好对的 KTO，对齐技术的演进主线是：**降低训练复杂度、减少人工标注需求、提升训练稳定性**。本文系统对比六种主流对齐方法。

## 1. 对齐的本质

Pre-trained LLM 是一个"预测下一个 token"的机器，它不天然懂得：
- **有用 (Helpful)**：回答应该准确、相关
- **无害 (Harmless)**：拒绝有害请求
- **诚实 (Honest)**：不编造事实

对齐 = 让模型的输出分布从 $p_{\text{pretrain}}$ 向人类偏好分布偏移，同时不能偏移太远（保留基础能力）。

## 2. RLHF：经典三阶段

[[AI/3-LLM/RL/实践/RLHF-工程全栈|RLHF 全链路]] 的标准流程：

```
Stage 1: SFT          Stage 2: RM Training    Stage 3: RL (PPO)
───────────────        ─────────────────        ──────────────────
Human demos            Human preferences        Policy optimization
     │                      │                        │
     ▼                      ▼                        ▼
π_SFT(y|x)            r_θ(x, y)               max E[r_θ(x,y)] - β·KL(π‖π_ref)
```

### 2.1 Reward Model

$$r_\theta(x, y_w) > r_\theta(x, y_l)$$

训练目标（Bradley-Terry model）：

$$\mathcal{L}_{RM} = -\mathbb{E}_{(x, y_w, y_l)} \left[ \log \sigma \left( r_\theta(x, y_w) - r_\theta(x, y_l) \right) \right]$$

### 2.2 PPO 优化

$$\max_\pi \mathbb{E}_{x \sim D, y \sim \pi} \left[ r_\theta(x, y) \right] - \beta \cdot D_{KL} \left[ \pi(y|x) \| \pi_{\text{ref}}(y|x) \right]$$

PPO 的 clipped objective 确保策略更新不会太大：

$$L^{CLIP} = \mathbb{E} \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]$$

**RLHF 的痛点**：
- 需要训练独立的 Reward Model（额外模型 + 额外数据）
- PPO 训练不稳定（reward hacking、mode collapse）
- 4 个模型同时在内存（policy, ref, reward, value）→ 显存炸裂
- 超参数极其敏感

## 3. DPO：去掉 Reward Model

[[AI/3-LLM/RL/实践/DPO-TRL实践]]（Direct Preference Optimization）的核心洞察：**Reward Model 可以用 policy 本身隐式表达**。

### 3.1 推导核心

从 RLHF 的最优解出发：

$$\pi^*(y|x) = \frac{1}{Z(x)} \pi_{\text{ref}}(y|x) \exp\left(\frac{1}{\beta} r(x, y)\right)$$

反解出隐式 reward：

$$r(x, y) = \beta \log \frac{\pi^*(y|x)}{\pi_{\text{ref}}(y|x)} + \beta \log Z(x)$$

代入 Bradley-Terry model，Z(x) 抵消，得到 DPO loss：

$$\mathcal{L}_{DPO} = -\mathbb{E}_{(x, y_w, y_l)} \left[ \log \sigma \left( \beta \log \frac{\pi_\theta(y_w|x)}{\pi_{\text{ref}}(y_w|x)} - \beta \log \frac{\pi_\theta(y_l|x)}{\pi_{\text{ref}}(y_l|x)} \right) \right]$$

### 3.2 DPO 的优势与局限

| 优势 | 局限 |
|------|------|
| 不需要 Reward Model | 需要 reference model（存内存） |
| 训练稳定，类似 SFT | 离线学习，不能探索新策略 |
| 只需偏好对数据 | 对数据质量极其敏感 |
| 实现简单（几十行代码） | β 调参依然重要 |

```python
# DPO Loss 核心实现
import torch
import torch.nn.functional as F

def dpo_loss(
    policy_chosen_logps: torch.Tensor,    # π_θ(y_w|x)
    policy_rejected_logps: torch.Tensor,   # π_θ(y_l|x)
    reference_chosen_logps: torch.Tensor,  # π_ref(y_w|x)
    reference_rejected_logps: torch.Tensor,# π_ref(y_l|x)
    beta: float = 0.1
) -> torch.Tensor:
    chosen_rewards = beta * (policy_chosen_logps - reference_chosen_logps)
    rejected_rewards = beta * (policy_rejected_logps - reference_rejected_logps)
    loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()
    return loss
```

## 4. ORPO：连 Reference Model 都不要

**ORPO**（Odds Ratio Preference Optimization, 2024）进一步简化——把 SFT 和偏好对齐合并为一步，不需要 reference model：

$$\mathcal{L}_{ORPO} = \underbrace{\mathcal{L}_{SFT}(y_w)}_{\text{NLL on chosen}} + \lambda \cdot \underbrace{\mathcal{L}_{OR}}_{\text{Odds Ratio penalty}}$$

其中 Odds Ratio 部分：

$$\mathcal{L}_{OR} = -\log \sigma \left( \log \frac{\text{odds}_\theta(y_w|x)}{\text{odds}_\theta(y_l|x)} \right)$$

$$\text{odds}_\theta(y|x) = \frac{p_\theta(y|x)}{1 - p_\theta(y|x)}$$

**优势**：只需一个模型、一次训练，同时完成 SFT + 对齐。
**实验结果**：在 Mistral-7B 上与 DPO+SFT 效果相当，但训练时间减半。

## 5. KTO：不需要偏好对

[[Projects/MA-RLHF/lc8-KTO/lc8-04-KTO-手撕实操]]（Kahneman-Tversky Optimization, 2024）的突破：**不需要成对的 (chosen, rejected) 数据，只需要 pointwise 的"好/坏"标签**。

灵感来自 Kahneman & Tversky 的前景理论：人对损失的敏感度大于对收益的敏感度。

$$\mathcal{L}_{KTO} = \mathbb{E}_{(x,y) \sim D_w} \left[ w(y) \cdot (1 - v_w) \right] + \mathbb{E}_{(x,y) \sim D_l} \left[ w(y) \cdot (1 - v_l) \right]$$

其中：
- $v_w = \sigma(\beta \cdot r_\theta(x, y_w) - z_{\text{ref}})$（好样本的隐式 value）
- $v_l = \sigma(z_{\text{ref}} - \beta \cdot r_\theta(x, y_l))$（坏样本的隐式 value，注意符号反转）
- $z_{\text{ref}}$ = KL reference point

**核心优势**：在真实场景中，获取"这个回复好不好"比"A 和 B 哪个更好"容易得多。thumbs up/down 数据比成对比较数据多几个数量级。

## 6. SteerLM：属性可控对齐

**SteerLM**（NVIDIA, 2023）让用户在推理时通过属性标签控制输出风格：

```
训练阶段:
1. 收集多属性标注: helpfulness=4, humor=2, toxicity=0, creativity=3
2. SFT 训练时将属性标签作为条件输入

推理阶段:
User: 写一个笑话
[helpfulness: 5, humor: 5, creativity: 4, verbosity: 2]
→ 模型生成高幽默、高创意的简洁回复
```

**优势**：单个模型支持多种风格，不需要为每种偏好训练不同模型。
**局限**：需要多维度标注数据，标注成本高。

## 7. Constitutional AI：自我对齐

Constitutional AI（Anthropic, 2022）核心思想：**用一组原则（Constitution）让模型自我批评和修正，减少对人工标注的依赖**。

### 7.1 两阶段流程

```
阶段 1: Supervised (Constitutional SFT)
─────────────────────────────────────────
Prompt → 模型生成初始回复
     → 模型根据原则自我批评 (Critique)
     → 模型修正回复 (Revision)
     → 用修正后的回复做 SFT

阶段 2: RL from AI Feedback (RLAIF)
─────────────────────────────────────────
对同一 prompt 的多个回复
     → AI 根据原则判断哪个更好（代替人类标注者）
     → 训练 Reward Model
     → PPO / DPO 优化
```

### 7.2 Constitution 示例

```
Principle 1: Choose the response that is less harmful or toxic.
Principle 2: Choose the response that is more honest and doesn't fabricate information.
Principle 3: Choose the response that is more helpful while being safe.
Principle 4: Choose the response that sounds most similar to what a careful, wise person would say.
...
```

**优势**：几乎不需要人工偏好标注，可以快速迭代原则。
**局限**：原则的质量直接决定对齐效果；AI 评判可能放大自身偏见。

## 8. 全景对比

| 方法 | 年份 | 需要 RM | 需要 Ref Model | 数据要求 | 训练复杂度 | 实战效果 |
|------|------|---------|---------------|---------|-----------|---------|
| **RLHF (PPO)** | 2020 | ✅ | ✅ | 偏好对 | ⭐⭐⭐⭐⭐ | 🏆 标杆 |
| **DPO** | 2023 | ❌ | ✅ | 偏好对 | ⭐⭐ | 🥇 接近 RLHF |
| **ORPO** | 2024 | ❌ | ❌ | 偏好对 | ⭐ | 🥈 高效 |
| **KTO** | 2024 | ❌ | ✅ | 单点好/坏 | ⭐⭐ | 🥇 数据友好 |
| **SteerLM** | 2023 | ❌ | ❌ | 属性标注 | ⭐⭐ | 🥈 可控性强 |
| **CAI (RLAIF)** | 2022 | ✅(AI) | ✅ | 原则 + 少量 | ⭐⭐⭐⭐ | 🥇 低人工成本 |

### 选型指南

- **有大量偏好对数据 + 充足算力** → RLHF (PPO) 或 [[AI/3-LLM/RL/算法/GRPO 深度理解|GRPO]]
- **有偏好对数据 + 算力有限** → DPO（最稳妥）
- **想合并 SFT 和对齐** → ORPO
- **只有 thumbs up/down 数据** → KTO
- **需要多风格可控** → SteerLM
- **缺少人工标注** → Constitutional AI / RLAIF
- **在线 RL 探索 + 数学/代码任务** → [[AI/3-LLM/RL/算法/GRPO 深度理解|GRPO]] / RLOO

## 9. 面试常考题

### Q1: DPO 相比 RLHF 的核心改进是什么？推导关键步骤？
**答**：DPO 证明了 RLHF 中的 Reward Model 可以被 policy 本身隐式表达。关键推导：从 RL 最优解 π*(y|x) ∝ π_ref(y|x)·exp(r(x,y)/β) 反解出 r(x,y) = β·log(π*(y|x)/π_ref(y|x)) + β·log Z(x)，代入 Bradley-Terry 偏好模型后 Z(x) 抵消，得到仅依赖 policy log-prob ratio 的 loss。核心优势：不需要独立 RM、训练稳定、实现简单；核心局限：离线学习无法探索、对数据质量敏感。

### Q2: KTO 和 DPO 的本质区别？KTO 的数据优势是什么？
**答**：DPO 需要成对偏好数据 (x, y_w, y_l)——同一 prompt 下两个回复比较；KTO 只需 pointwise 数据 (x, y, good/bad)——每个回复独立标注好坏。KTO 基于前景理论，对"损失"比"收益"赋予更高权重。数据优势：现实中 thumbs up/down 比成对比较容易获取几个数量级，且不需要为同一 prompt 生成多个回复。

### Q3: Constitutional AI 是如何减少人工标注的？
**答**：两步走：(1) Constitutional SFT——让模型根据预定义原则自我批评并修正有害回复，用修正结果做 SFT；(2) RLAIF——用 AI 而非人类根据原则判断偏好对，训练 RM 后做 RL。人工只需要定义原则（几十条），不需要标注数万条偏好。局限是 AI 评判可能放大偏见，且原则覆盖面有限。

### Q4: ORPO 为什么不需要 Reference Model？
**答**：ORPO 将 SFT loss 和偏好 loss 合并——SFT 部分用 chosen response 的 NLL loss 保证基础质量，偏好部分用 odds ratio 而非 log probability ratio 来区分 chosen/rejected。由于 odds ratio 本身就编码了"好坏差异"，不需要 reference model 作为基线。整个训练只需一个模型，一阶段完成。

### Q5: 实际训练中，DPO 的 β 参数如何选择？有什么影响？
**答**：β 控制与 reference model 的 KL 散度惩罚强度。β 大 → 更保守，输出接近 ref model，不容易过拟合但对齐效果弱；β 小 → 更激进，对齐效果强但可能 reward hacking / 偏离太远。实践中：β ∈ [0.05, 0.5]，通常 0.1 是好的起点。可以通过监控 chosen/rejected 的 reward margin 和 KL 散度来调整——如果 margin 很大但 KL 失控，说明 β 太小。

---

## See Also

**纵深阅读（对齐技术完整图谱）：**
- RLHF/DPO 2026 技术全景 — 本综述的深度版，含 SimPO/IPO/REBEL/ReMax/REINFORCE++ 等 2025-2026 新算法全覆盖；面试终极武器
- [[AI/3-LLM/RL/实践/RLHF-工程全栈|RLHF 全链路]] — RLHF 三阶段完整工程实操（数据/RM训练/PPO调参）

**GRPO 方向（从对齐到推理能力训练）：**
- [[AI/3-LLM/RL/算法/GRPO 深度理解|GRPO 深度理解]] — DeepSeek-R1 的训练算法，对齐技术向推理能力训练的演化；GRPO = RLHF 的 critic-free 简化 + group sampling

**Reward Modeling 扩展：**
- [[AI/3-LLM/RL/实践/RARL-Reward-Modeling-Survey|RARL Reward Modeling Survey]] — Reward Model 的全面综述；Constitutional AI 的 RLAIF 在 RM 设计中的延伸

**推荐阅读：**
1. [DPO 原文](https://arxiv.org/abs/2305.18290)（Rafailov et al., 2023）— 必读，推导干净优雅
2. [KTO 原文](https://arxiv.org/abs/2402.01306)（Ethayarajh et al., 2024）— 前景理论 × LLM 对齐的跨学科工作
3. [ORPO 原文](https://arxiv.org/abs/2403.07691)（Hong et al., 2024）— 最简洁的对齐方案
4. [Constitutional AI](https://arxiv.org/abs/2212.08073)（Bai et al., 2022）— RLAIF 的开山之作
