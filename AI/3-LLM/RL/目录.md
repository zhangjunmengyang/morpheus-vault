---
title: "å¼ºåŒ–å­¦ä¹  for LLM"
type: moc
domain: ai/llm/rl
tags:
  - ai/llm/rl
  - type/reference
---

# ğŸ¯ å¼ºåŒ–å­¦ä¹  for LLM

> LLM Post-Training çš„æ ¸å¿ƒæ–¹å‘ â€” ä» RLHF åˆ° GRPO å†åˆ° Agentic RL

## åŸºç¡€ç†è®º (Fundamentals)
- [[é©¬å°”ç§‘å¤«|é©¬å°”ç§‘å¤«]] â€” MDP åŸºç¡€
- [[è´å°”æ›¼æ–¹ç¨‹|è´å°”æ›¼æ–¹ç¨‹]] â€” ä»·å€¼å‡½æ•°
- [[ç­–ç•¥æ¢¯åº¦æ–¹æ³•|ç­–ç•¥æ¢¯åº¦æ–¹æ³•]] â€” PG æ—ç®—æ³•åŸºç¡€
- [[On-Policy vs Off-Policy|On-Policy vs Off-Policy]]
- [[KLæ•£åº¦|KLæ•£åº¦]] â€” æ­£åˆ™åŒ–æ ¸å¿ƒæ¦‚å¿µ
- [[MCTS|MCTS]] â€” è’™ç‰¹å¡æ´›æ ‘æœç´¢
- [[ä¸ºä»€ä¹ˆ PPO ä¼˜äº PG|ä¸ºä»€ä¹ˆ PPO ä¼˜äº PG]]
- [[PPL è®¡ç®— äº¤å‰ç†µæŸå¤±ä¸ ignore_index|PPL è®¡ç®—]]
- [[RL æ¦‚è§ˆ|RL æ¦‚è§ˆ]]
- [[RL & LLMs å…¥é—¨|RL & LLMs å…¥é—¨]] â€” HF Course
- [[HF Deep RL Course|HF Deep RL Course]]
- [[å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸç†|å¼ºåŒ–å­¦ä¹ çš„æ•°å­¦åŸç†]]
- [[RLVR-Edge-of-Competence|RLVR at the Edge of Competence]] â€” èƒ½åŠ›è¾¹ç•Œä¸Šçš„ RLVRï¼Œç ”ç©¶è®­ç»ƒä¿¡å·æœ‰æ•ˆåŒºé—´

## æ ¸å¿ƒç®—æ³•

### PPO
- [[PPO åŸç†|PPO åŸç†]] â€” æœ€ç»å…¸çš„ RLHF ç®—æ³•
- [[PPO-TRLå®è·µ|PPO-TRLå®è·µ]]
- [[PPO-verlå®è·µ|PPO-verlå®è·µ]]

### GRPO â­ï¼ˆé‡ç‚¹æ–¹å‘ï¼‰
- [[GRPO æ·±åº¦ç†è§£|GRPO æ·±åº¦ç†è§£]] â€” æ ¸å¿ƒåŸç†
- [[DeepSeek R1 å­¦ä¹ ç¬”è®°|DeepSeek R1 å­¦ä¹ ç¬”è®°]]
- [[DeepSeek-Math|DeepSeek-Math]] â€” æ•°å­¦æ¨ç†è®ºæ–‡
- [[Blockwise-Advantage-Estimation|Blockwise Advantage Estimation]] â€” GRPO credit assignment æ”¹è¿›
- [[TRL ä¸­å®ç° GRPO|TRL ä¸­å®ç° GRPO]]
- [[GRPO-TRLå®è·µ|GRPO-TRLå®è·µ]]
- [[GRPO-verlå®è·µ|GRPO-verlå®è·µ]]
- [[GRPO-Unslothå®è·µ|GRPO-Unslothå®è·µ]]
- [[GRPO-demo|GRPO-demo]]
- [[OpenR1|OpenR1]]
- [[iGRPO|iGRPO]] â€” è¿­ä»£å¼è‡ªåé¦ˆ GRPO (arXiv:2602.09000)
- [[ProGRPO-Probabilistic-Advantage-Reweighting|ProGRPO]] â€” Probabilistic Group Relative POï¼šåœ¨ advantage å±‚å¼•å…¥æ¦‚ç‡ç½®ä¿¡åº¦é‡åŠ æƒï¼Œ"æ‰¶å¼±æŠ‘å¼º"å¯¹æŠ— entropy collapseï¼›Pass@32 æ¯” GRPO +13.9%ï¼›â˜…â˜…â˜…â˜…ï¼ˆarXiv:2602.05281ï¼‰

### DPO
- [[DPO-TRLå®è·µ|DPO-TRLå®è·µ]]
- [[DPO-Unslothå®è·µ|DPO-Unslothå®è·µ]]

### DAPO
- [[DAPO-verlå®è·µ|DAPO-verlå®è·µ]]

### KTO
- [[KTO-TRLå®è·µ|KTO-TRLå®è·µ]]

### RLOO
- [[RLOO-TRLå®è·µ|RLOO-TRLå®è·µ]]

### Preference Optimizationï¼ˆåå¥½ä¼˜åŒ–å˜ä½“ï¼‰
- [[SimPO-Simple-Preference-Optimization-Reference-Free|SimPO]] â€” reference-freeï¼Œlength-normalized implicit rewardï¼ŒNeurIPS 2024ï¼ˆarXiv:2405.14734ï¼‰â˜…â˜…â˜…â˜…â˜†
- [[IPO-Identity-Preference-Optimization|IPO (Î¨PO)]] â€” DPO ç†è®ºä¿®æ­£ï¼šç»•è¿‡ Bradley-Terry å‡è®¾ï¼Œé€šç”¨ Î¨PO æ¡†æ¶ï¼ŒAISTATS 2024ï¼ŒDeepMindï¼ˆarXiv:2310.12036ï¼‰â˜…â˜…â˜…â˜…â˜†
- [[ORPO-Odds-Ratio-Preference-Optimization|ORPO]] â€” ä¸€é˜¶æ®µ alignmentï¼ˆSFT + preference åˆä¸€ï¼‰ï¼Œæ— éœ€ reference modelï¼Œodds ratio penaltyï¼ˆarXiv:2403.07691ï¼‰â˜…â˜…â˜…â˜†â˜†

### å…¶ä»–ç®—æ³• (Other-Algorithms)
- [[PRIME-Process-Reward-Implicit-MLE|PRIME]] â­ â€” Process Reinforcement through Implicit Rewardsï¼šåœ¨çº¿ PRM æ›´æ–°ï¼Œæ— éœ€ step-level æ ‡æ³¨ï¼Œtoken-level implicit process rewardï¼›Eurus-2-7B-PRIME ç”¨ 10% æ•°æ®è¶… Qwen2.5-Math-7B-Instructï¼›THUï¼ˆarXiv:2502.01456ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[REINFORCE-Plus-Plus-Global-Advantage-Normalization|REINFORCE++]] â€” å…¨å±€ batch advantage normalizationï¼Œcritic-freeï¼Œç†è®ºæœ‰æ•ˆæ— åï¼ˆarXiv:2501.03262ï¼‰â˜…â˜…â˜…â˜…â˜†
- [[ReMax-RL-Alignment-REINFORCE-Max-Baseline|ReMax]] â€” REINFORCE + greedy rollout baselineï¼Œæ¯” PPO çœ 46% å†…å­˜ï¼ŒICML 2024ï¼ˆarXiv:2310.10505ï¼‰â˜…â˜…â˜…â˜†â˜†
- [[REBEL-Regret-Based-RL-LLM-Alignment|REBEL]] â€” Regret-based RLï¼šå›å½’ relative reward = NPG ç­‰ä»·ï¼Œå¤„ç† intransitive preferenceï¼ˆarXiv:2404.16767ï¼‰â˜…â˜…â˜…â˜…â˜†
- [[DCPO è®ºæ–‡|DCPO]] â€” Dynamic Clipping
- [[Beyond Correctness è®ºæ–‡|Beyond Correctness]] â€” Process + Outcome Rewards
- [[GPG-verlå®è·µ|GPG]]
- [[OPO-verlå®è·µ|OPO]]
- [[SPIN-verlå®è·µ|SPIN]]
- [[SPPO-verlå®è·µ|SPPO]]
- [[CollabLLM-verlå®è·µ|CollabLLM]]
- [[OpenRS-Pairwise-Adaptive-Rubric|OpenRS]] â€” Pairwise Adaptive Rubricï¼Œnon-verifiable reward å¯¹é½ï¼Œè§£å†³ reward hackingï¼ˆarXiv:2602.14069ï¼‰
- [[GSPO-Unslothå®è·µ|GSPOï¼ˆUnslothå®è·µç‰ˆï¼‰]]
- [[GSPO-Group-Sequence-Policy-Optimization|GSPOï¼ˆQwen3å›¢é˜Ÿæ­£å¼ç‰ˆï¼‰]] â€” Alibaba/Qwen å›¢é˜Ÿï¼šåºåˆ—çº§ IS æ›¿ä»£ token çº§ ISï¼Œä»æ•°å­¦ä¸Šæ¶ˆè§£åºåˆ—å¥–åŠ±ä¸ token æ›´æ–°çš„å¯¹é½é”™è¯¯ï¼›Qwen3 RL post-training å®é™…åœ¨ç”¨ï¼›â˜…â˜…â˜…â˜…ï¼ˆarXiv:2507.18071ï¼‰
- [[MEL-Meta-Experience-Learning|MEL]] â€” Meta-Experience Learning
- [[CM2 â€” Checklist Rewardså¤šè½®Tool Use RL|CM2]] â€” Checklist Rewards å¤šè½® Tool Use RL
- [[SkillRL â€” é€’å½’æŠ€èƒ½å¢å¼ºçš„Agentæ¼”åŒ–|SkillRL]] â€” é€’å½’æŠ€èƒ½å¢å¼º Agent æ¼”åŒ–
- [[RLTF-RL-from-Text-Feedback|RLTF]] â€” RL from Text Feedbackï¼Œæ–‡æœ¬åé¦ˆå¥–åŠ±è®¾è®¡ï¼ˆarXiv:2602.02482ï¼‰
- [[HiPER-Hierarchical-Plan-Execute-RL-Credit-Assignment|HiPER]] â€” åˆ†å±‚ RL + æ˜¾å¼ Credit Assignmentï¼Œå¤šæ­¥ Agent é•¿ horizonï¼ˆarXiv:2602.16165ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[LACONIC-Length-Constrained-RL|LACONIC]] â€” Primal-Dual RL æ§åˆ¶ CoT è¾“å‡ºé•¿åº¦ï¼Œæ¨ç†æ•ˆç‡ï¼ˆarXiv:2602.14468ï¼‰â˜…â˜…â˜…
- [[E-SPL-Evolutionary-System-Prompt-Learning|E-SPL]] â€” RL æƒé‡æ›´æ–°ï¼ˆç¨‹åºæ€§çŸ¥è¯†ï¼‰+ è¿›åŒ–ç®—æ³• system prompt ä¼˜åŒ–ï¼ˆå£°æ˜æ€§çŸ¥è¯†ï¼‰è”åˆè®­ç»ƒï¼›AIME25 56.3â†’60.6%ï¼ˆarXiv:2602.14697ï¼‰â˜…â˜…â˜…â˜…
- [[GEPA-Reflective-Prompt-Evolution|GEPA]] â­ â€” çº¯ prompt è¿›åŒ–è¶…è¶Š GRPOï¼ˆ5/6ä»»åŠ¡ï¼‰ï¼Œrollout å‡å°‘ 35xï¼›E-SPL=GEPA+RLï¼›ICLR 2026 Oralï¼ŒUCB+Stanford+MITï¼ˆarXiv:2507.19457ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[Goldilocks-RL-Task-Difficulty-Curriculum|Goldilocks RL]] â€” Teacher æ¨¡å‹åœ¨çº¿é¢„æµ‹é¢˜ç›®éš¾åº¦ï¼Œé€‰ pâ‰ˆ0.5 çš„æ ·æœ¬è®­ç»ƒï¼Œé€ƒç¦» sparse reward ä½æ•ˆé™·é˜±ï¼›Apple+EPFLï¼ˆarXiv:2602.14868ï¼‰â˜…â˜…â˜…â˜…
- [[PACED-RL-Partition-Function-Difficulty-Scheduler|PACED-RL]] â­ â€” GFlowNet é…åˆ†å‡½æ•° Z_Ï† åŒç”¨ï¼šæ—¢åšå½’ä¸€åŒ–ã€åˆåšåœ¨çº¿éš¾åº¦è°ƒåº¦å™¨ï¼ˆé›¶é¢å¤–å¼€é”€ï¼‰ï¼›ä¸ Goldilocks ç‹¬ç«‹æ”¶æ•›åˆ°åŒä¸€è§„å¾‹ï¼ˆä¸­é—´éš¾åº¦æœ€ä¼˜ï¼‰ï¼›ICML 2026 æŠ•ç¨¿ï¼ˆarXiv:2602.12642ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[VAM-Verbalized-Action-Masking-Exploration|VAM]] â€” Within-state æ¢ç´¢å¡Œç¼©çš„è¯Šæ–­ä¸ä¿®å¤ï¼šè¯­è¨€åŒ– action masking å¼ºåˆ¶ group å†… rollout è¦†ç›–ä¸åŒ action åˆ†æ”¯ï¼›â˜…â˜…â˜…â˜†ï¼ˆarXiv:2602.16833ï¼Œå›½é™…è±¡æ£‹åœºæ™¯ï¼‰
- [[STAPO-Spurious-Token-Aware-Policy-Optimization|STAPO]] â€” 0.01% spurious tokens æºå¸¦è™šå‡æ¢¯åº¦æ˜¯ RL è®­ç»ƒå´©æºƒæ ¹æºï¼›mask æ‰å³å¯ç¨³å®šè®­ç»ƒï¼›æ¸…å+æ»´æ»´ï¼ˆarXiv:2602.15620ï¼‰â˜…â˜…â˜…â˜…
- [[Stable-Asynchrony-VCPO-Off-Policy-RL|Stable Asynchrony (VCPO)]] â€” å¼‚æ­¥ off-policy RL çš„æ–¹å·®çˆ†ç‚¸æ ¹å› ä¸ä¿®å¤ï¼šVariance-Controlled Policy Optimizationï¼Œè§£å†³ generation/training è§£è€¦åçš„ staleness é—®é¢˜ï¼›MIT HAN Labï¼ˆSong Hanï¼‰â˜…â˜…â˜…â˜…
- [[SAPO-Soft-Adaptive-Policy-Optimization|SAPO]] â€” Qwen å›¢é˜Ÿï¼ˆQwen3-VL åœ¨ç”¨ï¼‰ï¼šsechÂ² è½¯é—¨æ§æ›¿ä»£ç¡¬è£å‰ªï¼Œä¸å¯¹ç§°æ¸©åº¦å¤„ç†æ­£è´Ÿ advantageï¼›åŒæ­¥ RL åœºæ™¯ä¸‹æ¯” GRPO/GSPO æ›´ç¨³å®šï¼›GSPOâ†’SAPO æ”¹è¿›é“¾æ¡ï¼ˆarXiv:2511.20347ï¼‰â˜…â˜…â˜…â˜…
- [[RePO-Rephrasing-Policy-Optimization|RePO]] â€” Rephrasing Policy Optimizationï¼šOff-policy çŸ¥è¯†å˜æˆ On-policy å…¼å®¹è½¨è¿¹å†æ³¨å…¥è®­ç»ƒï¼Œè§£å†³ hard sample ä¸‰è§’å›°å¢ƒï¼ˆSFTé€€åŒ–/On-policyé‡‡ä¸åˆ°/Off-policyä¸ç¨³å®šï¼‰ï¼›â˜…â˜…â˜…ï¼ˆarXiv:2602.10819ï¼‰
- [[MASPO-Mass-Adaptive-Soft-Policy-Optimization|MASPO]] â€” ç»Ÿä¸€æ¢¯åº¦åˆ©ç”¨+æ¦‚ç‡è´¨é‡+ä¿¡å·å¯é æ€§çš„ GRPO ä¸‰ç»´æ”¹è¿›ï¼šè½¯è£å‰ªæ›¿ä»£ç¡¬è£å‰ª + æ¦‚ç‡è´¨é‡æ ¡æ­£ + reward ä¿¡å·å¯é æ€§åŠ æƒï¼›å¾®è½¯äºšç ”ï¼ˆarXiv:2602.17550ï¼‰â˜…â˜…â˜…â˜…
- [[OAPL-Off-Policy-RL-LLM-Reasoning|OAPL]] â­ â€” ç†è®ºæœ€å¹²å‡€çš„ off-policy RLï¼šKL-regularized RL closed-form è§£æ¨å¯¼ squared regression lossï¼Œæ— éœ€ ISï¼Œå®¹å¿ 400 æ­¥ policy lagï¼›Cornell+Databricks+Harvardï¼ˆarXiv:2602.19362ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[LAD-Learning-Advantage-Distribution|LAD]] â­ â€” æŠŠ"æœ€å¤§åŒ–æœŸæœ› advantage"é‡æ„ä¸º"åŒ¹é…ä¼˜åŠ¿è¯±å¯¼åˆ†å¸ƒ"ï¼ˆf-divergenceï¼‰ï¼šLemma 3.2 æ¨å¯¼é›¶ä»£ä»·ä»£ç† lossï¼Œè‡ªç„¶ä¿ç•™å¤šæ¨¡å¼è½¨è¿¹æ— éœ€ç†µæ­£åˆ™ï¼›AIME2024 +3.31ï¼Œå¤šæ ·æ€§ dist-4 +0.154ï¼›UW-Madisonï¼ˆarXiv:2602.20132ï¼‰â˜…â˜…â˜…â˜…â˜†
- [[RICOL-Retrospective-In-Context-Online-Learning|RICOL]] â­ â€” NeurIPS 2025ï¼ŒCMU+HKU+Stanfordï¼šTheorem 4.1 æ‰“é€š ICL=RL ç†è®ºç­‰ä»·æ€§â€”â€”ICL å‰å log-prob å·®æ­£æ¯”äº advantage functionï¼›critic-free sparse reward credit assignmentï¼›æ ·æœ¬æ•ˆç‡ PPO çš„ 3-5Ã—ï¼ˆarXiv:2602.17497ï¼‰â˜…â˜…â˜…â˜…
- [[DEEP-GRPO-Deep-Dense-Exploration-Pivot-Resampling|DEEP-GRPO]] â€” Root Saturation é—®é¢˜æ ¹æ²»ï¼šPivot-Driven Resampling ä¸“æ”»æ·±å±‚ error-prone statesï¼›å¯¹æ¯” TreeRL/AttnRL æ¢ç´¢å¯å‘å¼çš„ç¼ºé™·ï¼›ICML æŠ•ç¨¿ï¼Œ2602.14169ï¼ˆâ˜…â˜…â˜…â˜…â˜†ï¼‰
- [[IntroLLM-Introspective-Temperature-Policy-Hierarchical-RL|IntroLLMï¼ˆè‡ªçœæ¸©åº¦ç­–ç•¥ï¼‰]] â­ â€” GRPOä¸ƒç»´Diversityç»´åº¦æœ€ç»†ç²’åº¦å®ç°ï¼šç”¨éšçŠ¶æ€hâ‚œå­¦ä¹ temperature policyï¼ˆhierarchical RLï¼‰ï¼›Betaåˆ†å¸ƒæ··åˆåŠ¨ä½œç©ºé—´ï¼›é«˜æ¸©=æ¨ç†è½¬æŠ˜ç‚¹/ä½æ¸©=æ•°å€¼è®¡ç®—ï¼›ç›¾å«Phase3æ¿€æ´»æ¢é’ˆçš„é—´æ¥æ”¯æŒâ€”â€”åŒä¸€hâ‚œèƒ½å¦åŒºåˆ†æ³¨å…¥æ”»å‡»ï¼Ÿï¼ˆarXiv:2602.13035ï¼ŒICMLæŠ•ç¨¿ï¼‰â˜…â˜…â˜…â˜…
- [[VESPO-Variational-Sequence-Policy-Optimization|VESPO]] â­ â€” å˜åˆ†æ¨å¯¼é—­åˆå½¢å¼ soft kernel `Ï•(W)=W^Î±Â·exp(-Î»W)`ï¼Œç†è®ºä¸¥æ ¼è¶…è¶Šæ‰€æœ‰ heuristic clipï¼ˆGRPO/GSPO/SAPOï¼‰ï¼Œstaleness ratio 64Ã— å¼‚æ­¥è®­ç»ƒç¨³å®šï¼›â˜…â˜…â˜…â˜…â˜…ï¼ˆarXiv:2602.10693ï¼‰
- [[AT-RL-Anchor-Token-Reinforcement-Learning-Multimodal|AT-RL]] â€” å¤šæ¨¡æ€ RLVRï¼šä»… 15% token æœ‰å¼ºè§†è§‰-æ–‡æœ¬è€¦åˆï¼ˆ"è§†è§‰é”šç‚¹"ï¼‰ï¼Œå›¾èšç±»è¯†åˆ«å¹¶é€‰æ‹©æ€§å¼ºåŒ–ï¼›32B æ¨¡å‹ MathVista 80.2 è¶…è¶Š 72B-Instructï¼›ä»… 1.2% å¼€é”€ï¼ˆarXiv:2602.11455ï¼‰â˜…â˜…â˜…â˜…
- [[VPPO-Visually-Perceptive-Policy-Optimization|VPPOï¼ˆä¸Šæµ· AI Lab + SJTU + CUHKï¼‰]] â­ â€” é¦–ä¸ªå°† token-level è§†è§‰ä¾èµ–åº¦å¼•å…¥å¤šæ¨¡æ€ RLVRï¼šåŒæœºåˆ¶ï¼ˆè½¨è¿¹çº§ advantage reweighting å‹åˆ¶è¯­è¨€æ·å¾„ + token çº§ç¨€ç–æ¢¯åº¦æ©ç èšç„¦æ„ŸçŸ¥ pivotï¼‰ï¼›7B +19.2% / 32B +7.6%ï¼Œ8 benchmarkï¼›æ’ä»¶å¼å¯å åŠ  GRPO/DAPOï¼ˆarXiv:2510.09285ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[NoRD-Dr-GRPO-Reasoning-Free-VLA-Autonomous-Driving|NoRDï¼ˆApplied Intuition + UC Berkeleyï¼ŒCVPR 2026ï¼‰]] â€” **Dr. GRPO è·¨åŸŸå®è¯**ï¼šè‡ªåŠ¨é©¾é©¶ VLA é¦–æ¬¡éªŒè¯ GRPO difficulty bias çš„ç³»ç»Ÿæ€§å±å®³ï¼ˆå¼± SFT + std å½’ä¸€åŒ– â†’ ä¸­ç­‰éš¾åº¦æ ·æœ¬æ¢¯åº¦è¢«å‹åˆ¶ï¼‰ï¼›Dr. GRPO ä¿®å¤å PDM +11.68%ï¼ˆvs GRPO +0.67%ï¼‰ï¼›æ— æ¨ç†æ ‡æ³¨ + 80k æ•°æ®è¾¾ SOTA ç«äº‰åŠ›ï¼ˆarXiv:2602.21172ï¼‰â˜…â˜…â˜…â˜…â˜†

## è®­ç»ƒæ¡†æ¶ (Frameworks)
- [[AI/3-LLM/RL/Frameworks/Jet-RL-FP8-On-Policy-RL-Training|Jet-RL]] â€” NVIDIA+MIT HAN Labï¼šç»Ÿä¸€ FP8 on-policy RL è®­ç»ƒç²¾åº¦æµï¼Œè§£å†³ BF16-train/FP8-rollout åœ¨é•¿ rollout(>8K) æ—¶ç²¾åº¦å´©æºƒå’Œè®­ç»ƒå‘æ•£é—®é¢˜ï¼ˆarXiv:2601.14243ï¼‰â˜…â˜…â˜…â˜…
- [[AI/3-LLM/RL/Frameworks/QeRL-Quantization-Enhanced-RL|QeRL]] â­ â€” ICLR 2026ï¼ˆNVIDIA+MIT+HKU+THU+Song Hanï¼‰ï¼šé‡åŒ–å™ªå£°æ˜¯æœ‰ç›Šçš„â€”â€”4-bit é‡åŒ–+LoRA çš„ RL è®­ç»ƒä¸ä»… 1.5Ã— åŠ é€Ÿï¼Œåœ¨å¤šé¡¹åŸºå‡†ä¸Šè¿˜**è¶…è¶Š** 16-bit LoRAï¼›see-also: [[AI/3-LLM/RL/Frameworks/Jet-RL-FP8-On-Policy-RL-Training|Jet-RL]]ï¼ˆarXiv:2510.11696ï¼‰â˜…â˜…â˜…â˜…
- [[AI/3-LLM/RL/Frameworks/Slime-RL-Framework|Slime RL Framework]] â€” GLM-5 çš„å¼‚æ­¥ RL åŸºç¡€è®¾æ–½ï¼šè§£å†³ generation bottleneck >90%ï¼ŒAPRIL æ¡†æ¶ï¼ˆsee-also æŒ‡å‘æ·±åº¦ç‰ˆï¼‰

## æ‰‹æ’•å®æ“ï¼ˆCode Practiceï¼‰

> æ¥æºï¼š[MA-RLHF](https://github.com/dhcode-cpp/MA-RLHF) â€” é¢è¯•ä»£ç é¢˜æ ¸å¿ƒæ­¦å™¨åº“

- [[MA-RLHF-æ‰‹æ’•å®æ“-ç³»åˆ—ç´¢å¼•|ğŸ”¥ æ‰‹æ’•å®æ“æ€»ç´¢å¼•]] â­ â€” å…¨éƒ¨ 25 ç¯‡å¯¼èˆªï¼Œå«ä¸‰æ¡é¢è¯•é€Ÿé€šè·¯å¾„
- [[AI/3-LLM/RL/Fundamentals/RLåŸºç¡€ç®—æ³•æ‰‹æ’•å®æ“|RLåŸºç¡€ç®—æ³•æ‰‹æ’•]] â€” REINFORCE/A2C/PPO/DQN å®Œæ•´ PyTorch å®ç°
- [[AI/3-LLM/RL/PPO/PPO-æ‰‹æ’•å®æ“-MA-RLHF|PPO-æ‰‹æ’•]] â€” 4æ¨¡å‹æ¶æ„ï¼ˆActor/Critic/RM/Refï¼‰+ GAE + MA-PPO å¤šé€‚é…å™¨
- [[AI/3-LLM/RL/PPO/MA-RLHF-æ ¸å¿ƒä»£ç æ³¨è§£|MA-RLHF-ä»£ç æ³¨è§£]] â€” å®Œæ•´é¡¹ç›® SFTâ†’RMâ†’PPO å…¨æµæ°´çº¿é€è¡Œè§£æ
- [[AI/3-LLM/RL/GRPO/GRPO-æ‰‹æ’•å®æ“|GRPO-æ‰‹æ’•]] â€” Clipç‰ˆ/ç®€åŒ–ç‰ˆ loss + Group Relative Advantageï¼ˆå¯¹åº” GRPOæ·±åº¦ç†è§£è®ºæ–‡éƒ¨åˆ†ï¼‰
- [[AI/3-LLM/RL/DPO/DPO-æ‰‹æ’•å®æ“|DPO-æ‰‹æ’•]] â€” KL-constrained â†’ éšå¼reward â†’ Bradley-Terry â†’ DPO loss å®Œæ•´æ¨å¯¼
- [[AI/3-LLM/RL/KTO/KTO-æ‰‹æ’•å®æ“|KTO-æ‰‹æ’•]] â€” å‰æ™¯ç†è®º + å•æ ·æœ¬åå¥½ï¼ˆæ— éœ€ pair-wiseï¼‰
- [[AI/3-LLM/RL/PPO/PRM-O1-Search-æ‰‹æ’•å®æ“|PRM-O1-Search-æ‰‹æ’•]] â€” Process Reward + Beam Search + MCTS

## ç»¼è¿°ä¸æ·±åº¦ç¬”è®°
- [[RL-Signal-Granularity-Causal-Structure-Principle|ğŸ”¥ RL ä¿¡å·ç²’åº¦ä¸å› æœç»“æ„åŒ¹é…åŸåˆ™]] â­ â€” W å±‚å…ƒå‘½é¢˜ï¼šTree-GRPO/GiGPO/VPPO/Perception-R1 å››è·¯ç‹¬ç«‹å°è¯ï¼Œä¿¡å·ç²’åº¦åº”ä¸ä»»åŠ¡æœ€å°å› æœå•å…ƒå¯¹é½ï¼›é¢è¯•æ­¦å™¨ + è®­ç»ƒè¯Šæ–­å·¥å…· â˜…â˜…â˜…â˜…â˜…
- [[GRPO-Improvement-Panorama-2026|GRPO æ”¹è¿›å…¨æ™¯ 2026]] â­ â€” **ä¸ƒç»´æ¡†æ¶å…ƒåˆ†æ**ï¼ˆv2 æ›´æ–° 2026-02-21ï¼‰ï¼šDiversity/Token/Exploration/Sample/TrustRegion/Off-Policy/System ä¸ƒå±‚åˆ†ç±»ï¼ŒProGRPO+RePO è¡¥å…¥ Diversity ç»´åº¦ï¼›æ·±å±‚ç»Ÿä¸€è§†è§’ï¼šæ‰€æœ‰é—®é¢˜æŒ‡å‘åŒä¸€æ ¹å› ï¼ˆåºåˆ—çº§å¥–åŠ±è®­ç»ƒ token çº§å†³ç­–ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[RLRR-Reference-Guided-Alignment-Non-Verifiable|RLRR]] â­ â€” Reference-Guided RL Alignment for Non-Verifiable Domainsï¼šç”¨é«˜è´¨é‡ reference + RefEval judge ä¸ºå¯¹é½ä»»åŠ¡é€ è½¯ verifierï¼ŒDPO æ€§èƒ½æ¥è¿‘ä¸“è®­ ArmoRMï¼›ICLR 2026ï¼ˆYale+Meta+Scale AI+Salesforceï¼ŒarXiv:2602.16802ï¼‰â˜…â˜…â˜…â˜…â˜†
- [[REMuL-CoT-Faithfulness-Multi-Listener-RL|REMuL]] â­ â€” CoT Faithfulness via Multi-Listener RLï¼šå®šä¹‰å¯æ“ä½œçš„ faithfulnessï¼ˆæ¨ç†é“¾å¯è¢«å…¶ä»–æ¨¡å‹"ç»§ç»­æ‰§è¡Œ"åˆ°ç›¸åŒç»“è®ºï¼‰ï¼Œä¸¤é˜¶æ®µè®­ç»ƒï¼ˆGRPO faithfulness RL â†’ masked SFT correctnessï¼‰ï¼Œ**å”¯ä¸€åŒæ—¶æå‡ faithfulness å’Œ accuracy çš„æ–¹æ³•**ï¼›UNC+Ciscoï¼ˆarXiv:2602.16154ï¼ŒICMLæŠ•ç¨¿ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[RL-Training-Stability-2026-Unified-Analysis|RL è®­ç»ƒç¨³å®šæ€§ 2026 ç»Ÿä¸€åˆ†æ]] â­ â€” Scholar ç»¼åˆç¬”è®° v3ï¼šSTAPO/Goldilocks/VCPO/DEEP-GRPO/MASPO/DAPO/LACONIC å››ç»´æ‹“æ‰‘ï¼ˆToken/æ ·æœ¬/æ¢ç´¢/ç³»ç»Ÿï¼‰ï¼ŒæŒç»­æ›´æ–°ä¸­ï¼ˆ2026-02-20ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[MARS-Margin-Aware-Reward-Modeling-Self-Refinement|MARS]] â€” Fisher information è¯æ˜ä½ margin pair æä¾›æœ€å¤§è®­ç»ƒæ›²ç‡ï¼Œadaptive margin-aware æ•°æ®å¢å¼ºèšç„¦ decision boundary é™„è¿‘ï¼›ICMLæŠ•ç¨¿ï¼ˆarXiv:2602.17658ï¼‰â˜…â˜…â˜…â˜…
- [[Rationale-Consistency-GenRM-Deceptive-Alignment|Rationale Consistencyï¼ˆGenRM æ¬ºéª—æ€§å¯¹é½ï¼‰]] â­ â€” GenRM/LLM-as-Judge å­˜åœ¨"æ¬ºéª—æ€§å¯¹é½"ï¼šoutcome accuracy å®Œå…¨æ— æ³•åŒºåˆ†æ­£ç¡®æ¨ç† vs è¡¨é¢çŒœå¯¹ï¼›å¼•å…¥ RC æŒ‡æ ‡ + R_rationaleÃ—R_outcome ä¹˜æ³•é—¨æ§ï¼›RM-Bench SOTA 87.1%ï¼ŒRLHFåˆ›æ„å†™ä½œ+7%ï¼›Qwen Team+Fudan+Tsinghuaï¼ˆarXiv:2602.04649ï¼‰â˜…â˜…â˜…â˜…â˜…
- [[Likelihood-Based-Reward-Designs-CoT-RL|Likelihood-Based Rewardï¼ˆlog-probé€šç”¨rewardï¼‰]] â€” ç³»ç»Ÿæ¯”è¾ƒå…­ç§log-probè¡ç”Ÿrewardï¼Œå‘ç°log-probæ˜¯å”¯ä¸€åœ¨å¯éªŒè¯+ä¸å¯éªŒè¯åœºæ™¯ä¸‹éƒ½workçš„æ–¹æ³•ï¼›æ¶ˆé™¤"æ¯ä»»åŠ¡ä¸“ç”¨verifier"éœ€æ±‚ï¼›Meta FAIR+UvA+NYUï¼ˆarXiv:2602.03979ï¼‰â˜…â˜…â˜…â˜…â˜†
- [[Reward-Design-2026-Panorama|Reward Design 2026 å…¨æ™¯]] â­ â€” åŸåˆ›å…ƒåˆ†æï¼šå¯†åº¦ï¼ˆlog-probï¼‰/æ¨ç†è´¨é‡ï¼ˆRCä¹˜æ³•é—¨æ§ï¼‰/è¾¹ç•Œï¼ˆMARS Fisherï¼‰ä¸‰ç»´æ¡†æ¶ç»Ÿä¸€MARS+RC+Likelihoodä¸‰ç¯‡ï¼›4åœºæ™¯å†³ç­–æ ‘ï¼ˆæœ‰æ— verifierÃ—æ˜¯å¦éœ€è¦GenRMï¼‰ï¼›å·²çŸ¥3ä¸ªæœªè§£é—®é¢˜ï¼ˆâ˜…â˜…â˜…â˜…â˜…ï¼‰
- [[å¼ºåŒ–å­¦ä¹ ä¸RLHFåº”ç”¨-2026å…¨æ™¯|å¼ºåŒ–å­¦ä¹ ä¸RLHFåº”ç”¨ 2026 å…¨æ™¯]] â­ â€” é¢è¯•æ­¦å™¨ç‰ˆï¼Œ741è¡Œï¼Œç»å…¸RL(MDP/Bellman/PPO/GAE)â†’RLHF/DPO/GRPOå…¨é“¾è·¯ï¼Œä»åŸºç¡€åˆ°å‰æ²¿å®Œæ•´è¦†ç›– â˜…â˜…â˜…â˜…â˜…
- [[RLHF å…¨é“¾è·¯|RLHF å…¨é“¾è·¯]] â€” å®Œæ•´ RLHF ä¸‰é˜¶æ®µ
- [[RLHF-DPO-2026-æŠ€æœ¯å…¨æ™¯|RLHF/DPO 2026 æŠ€æœ¯å…¨æ™¯]] â€” é¢è¯•æ­¦å™¨ç‰ˆï¼Œ1147è¡Œï¼ŒRLHFâ†’RLAIFâ†’DPO å…¨é“¾è·¯ï¼ˆ2026-02-20ï¼‰
- [[å¯¹é½æŠ€æœ¯ç»¼è¿°|å¯¹é½æŠ€æœ¯ç»¼è¿°]] â€” RLHF â†’ DPO â†’ ORPO â†’ KTO â†’ SteerLM â†’ Constitutional AI
- [[RARL-Reward-Modeling-Survey|RARL Reward Modeling Survey]] â€” RL reasoning alignment ç»¼è¿°

## ç›¸å…³ MOC
- â†‘ ä¸Šçº§ï¼š[[AI/3-LLM/ç›®å½•]]
- â†’ äº¤å‰ï¼š[[AI/2-Agent/ç›®å½•]]ï¼ˆAgentic RLï¼‰
