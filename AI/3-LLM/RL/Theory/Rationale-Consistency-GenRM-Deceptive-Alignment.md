---
title: "Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models"
brief: GenRM/LLM-as-Judge å­˜åœ¨ã€Œæ¬ºéª—æ€§å¯¹é½ã€â€”â€”outcome accuracy å®Œå…¨æ— æ³•åŒºåˆ†æ­£ç¡®æ¨ç† vs è¡¨é¢çŒœå¯¹ï¼›å¼•å…¥ RC æŒ‡æ ‡ï¼ˆäººç±»åŸå­ç†ç”±å¹³å‡è½¯å¬å›ï¼‰+ R_final=R_rationaleÃ—R_outcome ä¹˜æ³•é—¨æ§å¼ºåˆ¶æ¨ç†-ç»“æœä¸€è‡´ï¼›MetaJudge åœ¨ RM-Bench SOTA 87.1%ï¼ŒRLHFåˆ›æ„å†™ä½œ+7%ï¼›o3 RCâ‰ˆ0.4ï¼Œo3-mini RCâ‰ˆ0.2
date: 2026-02-21
updated: 2026-02-22
tags:
  - ai/llm/rl
  - reward-model
  - generative-reward-model
  - llm-as-judge
  - deceptive-alignment
  - rlhf
  - grpo
  - evaluation
domain: ai/llm/rl/theory
arxiv: "2602.04649"
rating: â˜…â˜…â˜…â˜…â˜…
status: active
sources:
  - "[arXiv:2602.04649] Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models (Qwen Team + Fudan + Tsinghua, 2026)"
  - HelpSteer3-Atomic + CW-Atomic datasets
  - "RM-Bench: https://arxiv.org/abs/2410.16184"
related:
  - "[[AI/3-LLM/RL/Theory/MARS-Margin-Aware-Reward-Modeling-Self-Refinement|MARS]]"
  - "[[AI/3-LLM/RL/Other-Algorithms/RICOL-Retrospective-In-Context-Online-Learning|RICOL]]"
  - "[[AI/3-LLM/RL/Theory/RLRR-Reference-Guided-Alignment-Non-Verifiable|RLRR]]"
  - "[[AI/5-AI å®‰å…¨/AutoInject-RL-Prompt-Injection-Attack|AutoInject]]"
  - "[[AI/3-LLM/Evaluation/PERSIST-LLM-Personality-Stability-Benchmark|PERSIST]]"
---

# Rationale Consistencyï¼šå¥–åŠ±æ¨¡å‹çš„æ¬ºéª—æ€§å¯¹é½é—®é¢˜

**arXiv**: 2602.04649  
**æœºæ„**: Qwen Team (Alibaba) + Fudan University + Tsinghua University  
**ä½œè€…**: Binghai Wang, Yantao Liu, Yuxuan Liu, Tianyi Tang, Shenzhi Wang, et al. (Junyang Lin é¢†è¡”)  
**æäº¤**: 2026-02-07  
**è¯„åˆ†**: â˜…â˜…â˜…â˜…â˜…  
**ä¸€å¥è¯**: GenRM å’Œ LLM-as-Judge å­˜åœ¨"æ¬ºéª—æ€§å¯¹é½"â€”â€”ä»¥æ­£ç¡®ç†ç”±å¾—å‡ºæ­£ç¡®ç»“è®º vs ä»¥é”™è¯¯ç†ç”±å¾—å‡ºæ­£ç¡®ç»“è®ºï¼Œoutcome accuracy å®Œå…¨åŒºåˆ†ä¸äº†è¿™ä¸¤ç§æƒ…å†µï¼›å¼•å…¥ Rationale Consistency æŒ‡æ ‡ + æ··åˆå¥–åŠ±è®­ç»ƒï¼ŒRM-Bench SOTA 87.1%ï¼ŒRLHF åˆ›æ„å†™ä½œ +7%ã€‚

## See Also

- [[AI/3-LLM/RL/Theory/MARS-Margin-Aware-Reward-Modeling-Self-Refinement|MARS]] â€” margin-aware rewardï¼šå…³æ³¨åˆ¤æ–­è¾¹ç•Œå¤„çš„æ ·æœ¬ï¼›æœ¬æ–‡å…³æ³¨åˆ¤æ–­æ¨ç†è¿‡ç¨‹çš„è´¨é‡â€”â€”ä¸¤è€…éƒ½åœ¨é—®"reward ä¿¡å·æ˜¯å¦å¤Ÿå¥½"ï¼Œæ–¹å‘ä¸åŒï¼ˆMARS=è¾¹ç•Œè´¨é‡ï¼Œæœ¬æ–‡=æ¨ç†è´¨é‡ï¼‰
- [[AI/3-LLM/RL/Other-Algorithms/RICOL-Retrospective-In-Context-Online-Learning|RICOL]] â€” ç”¨ in-context learning æ”¹å–„ RL çš„ rewardï¼›æœ¬æ–‡ç›´æ¥æ”¹å˜ reward model çš„è®­ç»ƒç›®æ ‡â€”â€”åŒä¸ºæå‡ reward å¯é æ€§ï¼Œè·¯å¾„ä¸åŒ
- [[AI/3-LLM/RL/Theory/RLRR-Reference-Guided-Alignment-Non-Verifiable|RLRR]] â€” éå¯éªŒè¯ä»»åŠ¡çš„ reward modelingï¼›æœ¬æ–‡å…³æ³¨ GenRM çš„æ¨ç†è´¨é‡é—®é¢˜â€”â€”ä¸¤è€…å…±åŒæŒ‡å‘ï¼šscalar reward ä¿¡å·ä¸è¶³ä»¥å¯¹é½å¤æ‚ä»»åŠ¡
- [[AI/5-AI å®‰å…¨/AutoInject-RL-Prompt-Injection-Attack|AutoInject]] â€” reward çš„å¦ä¸€å¤±æ•ˆæ¨¡å¼ï¼šadversarial attack æ“çºµ rewardï¼›æœ¬æ–‡çš„"deceptive alignment"æ˜¯è®­ç»ƒç›®æ ‡è®¾è®¡ç¼ºé™·å¯¼è‡´çš„å¤±æ•ˆâ€”â€”ä¸¤è€…å…±åŒæ„æˆ reward å¯é æ€§çš„ç‹¬ç«‹å¨èƒå›¾è°±
- [[AI/3-LLM/Evaluation/PERSIST-LLM-Personality-Stability-Benchmark|PERSIST]] â€” è¡Œä¸ºä¸€è‡´æ€§åŸºå‡†ï¼šæœ¬æ–‡è¯æ˜ outcome-correct æ¨¡å‹æ¨ç†è¿‡ç¨‹å¯ä»¥å®Œå…¨ä¸åŒï¼ˆo3 vs o3-miniï¼‰ï¼›PERSIST è¯æ˜åŒä¸€æ¨¡å‹å¯¹åŒä¸€é—®é¢˜å›ç­”å¯ä»¥ä¸ä¸€è‡´â€”â€”ä¸¤è€…ä»ä¸åŒè§’åº¦æ­ç¤ºå½“å‰LLMç¼ºä¹"çœŸæ­£ç†è§£"çš„ç»“æ„æ€§é—®é¢˜

---

## æ ¸å¿ƒé—®é¢˜ï¼šæ¬ºéª—æ€§å¯¹é½

### è¿™ä¸ªé—®é¢˜å·²ç»å­˜åœ¨å¾ˆä¹…äº†

Reward model åœ¨ static benchmark ä¸Šè¡¨ç°å¥½ï¼Œä½†åœ¨ RLHF è®­ç»ƒè¿‡ç¨‹ä¸­æ³›åŒ–å¤±è´¥â€”â€”è¿™æ˜¯å·²çŸ¥é—®é¢˜ï¼ˆreward hackingï¼‰ã€‚

ä½†è¿™ç¯‡è®ºæ–‡æ­ç¤ºäº†ä¸€ä¸ªæ›´æ·±å±‚çš„é—®é¢˜ï¼š**å³ä½¿ outcome accuracy é«˜ï¼Œåˆ¤æ–­è¿‡ç¨‹ä¹Ÿå¯èƒ½æ˜¯é”™çš„ã€‚**

### o3 vs o3-mini çš„å…³é”®æ¡ˆä¾‹

é¢˜ç›®ï¼šè¯„ä¼°å“ªä¸ªå¹¿å‘Šæ›´å¥½ï¼ˆè¦æ±‚ï¼šåŒ…å«"Tips"å…³é”®è¯ + å­—æ•°é™åˆ¶ 100 å­—ç¬¦ä»¥å†…ï¼‰

äººç±»åˆ¤æ–­çš„çœŸå®åŸå› ï¼ˆground truth checklistï¼‰ï¼š
- R1: Response A ç¼ºå°‘äº§å“å "Tips"
- R2: Response A ä½¿ç”¨ hashtagï¼ˆå¹¿å‘Šä¸­ä¸æ°å½“ï¼‰
- R3: Response A è¶…è¿‡ 100 å­—ç¬¦
- R4: Response B ç¼ºå°‘"play in advance"æ¦‚å¿µ

| æ¨¡å‹ | Outcome | Rationale Consistency | å®é™…æ¨ç†æ–¹å¼ |
|------|---------|----------------------|------------|
| **o3-mini** | 100%ï¼ˆé€‰å¯¹äº†ï¼‰| **0%**ï¼ˆ4/4 å…¨é”™ï¼‰| çœ‹æ ¼å¼æ ‡ç­¾ã€è¡¨æƒ…ç¬¦å·ï¼Œæ²¡æœ‰æ•°å­—ç¬¦â€”â€”é è¡¨é¢çº¿ç´¢ |
| **o3** | 100%ï¼ˆé€‰å¯¹äº†ï¼‰| **75%**ï¼ˆ3/4 å¯¹ï¼‰| å®é™…æ•°å­—ç¬¦ï¼ˆéªŒè¯ R3ï¼‰ã€æ£€æŸ¥äº§å“åï¼ˆR1ï¼‰ã€ç†è§£ trade-offï¼ˆR4ï¼‰|

ä¸¤ä¸ªæ¨¡å‹éƒ½ç­”å¯¹äº†ï¼Œä½†æ¨ç†è·¯å¾„å®Œå…¨ä¸åŒã€‚**o3-mini æ˜¯åœ¨çŒœ**ï¼Œè€Œä¸”çŒœå¯¹äº†ã€‚Outcome accuracy æ— æ³•åŒºåˆ†è¿™ä¸¤ç§æƒ…å†µã€‚

---

## MetaJudge æ¡†æ¶

### æ ¸å¿ƒæ€è·¯ï¼šæŠŠäººç±»æ¨ç†åŸå­åŒ–ï¼Œç„¶åå¯¹é½

**æ­¥éª¤ä¸€ï¼šåŸå­åˆ†è§£ï¼ˆAtomic Decompositionï¼‰**

ç”¨ GPT-5 æŠŠäººç±»æ ‡æ³¨è€…çš„è‡ªç”±æ ¼å¼ç†ç”±åˆ†è§£ä¸º**äº’æ–¥çš„åŸå­å•å…ƒ**ï¼š
```
åŸå§‹ï¼šResponse A is worse because it's too long and misses the point
â†“ åˆ†è§£
R1: Response A exceeds the character limit (factual check)
R2: Response A fails to include product name (content check)
```

ä¿ç•™åŸåˆ™ï¼š
- ä¿ç•™æœ‰è¯æ®çš„å…·ä½“ç†ç”±ï¼Œè¿‡æ»¤ä¸»è§‚æ³›æ³›ä¹‹è¯
- æ¯ä¸ªåŸå­æ˜¯ç‹¬ç«‹çš„è¯­ä¹‰å•å…ƒï¼Œæ— å†—ä½™

**æ­¥éª¤äºŒï¼šè¯­ä¹‰åŒ¹é…ï¼ˆ1-to-1ï¼‰**

ç”¨ LLM åš strict one-to-one è¯­ä¹‰åŒ¹é…ï¼Œé˜²æ­¢æ¨¡å‹ç”¨ä¸€ä¸ªå®½æ³›ç†ç”±åŒ¹é…å¤šä¸ªäººç±»åŸå­ï¼š

```
S_total = max_Ï€ Î£_{(i,j)âˆˆÏ€} s_ij

å…¶ä¸­ Ï€ æ˜¯åŒ¹é…é›†ï¼Œè¦æ±‚ R_h å’Œ R_ai ä¸­æ¯ä¸ªç†ç”±æœ€å¤šå‡ºç°ä¸€æ¬¡
```

**æ­¥éª¤ä¸‰ï¼šRationale Consistency è®¡ç®—**

```
RC = (1/N) Î£_k S_total(k) / |R_h(k)|

= äººç±»åŸå­ç†ç”±è¢«æ¨¡å‹æˆåŠŸåŒ¹é…çš„æ¯”ä¾‹ï¼ˆå¹³å‡è½¯å¬å›ï¼‰
```

### å¯é æ€§éªŒè¯

- **è¯„ä¼°è€…ç¨³å®šæ€§**ï¼šQwen-Plus å’Œ DeepSeek-R1 ä½œä¸º MetaJudge çš„ RÂ² = 0.983ï¼ŒRMSE = 0.006ï¼ˆå‡ ä¹æ— å·®å¼‚ï¼‰
- **è·¨åŸŸæ³›åŒ–**ï¼šHelpSteer3-Atomic å’Œ CW-Atomicï¼ˆåˆ›æ„å†™ä½œï¼Œä¸åŒæ ‡æ³¨è€…ï¼‰çš„ Spearman Ï = 0.85

---

## ä¸»è¦å®éªŒå‘ç°

### 19 ä¸ª frontier æ¨¡å‹çš„ RC vs ç²¾åº¦åˆ†å¸ƒ

**ç»¿åŒºï¼ˆé«˜ RC æ¨¡å‹ï¼‰**ï¼šGPT-5ã€o3ã€Gemini 3 Pro  
**çº¢åŒºï¼ˆæ¬ºéª—æ€§å¯¹é½é™·é˜±ï¼‰**ï¼šo3-miniã€Gemini 3 Flash

å…³é”®å‘ç°ï¼š
1. **ç²¾åº¦æ­£åœ¨æ¥è¿‘é¥±å’Œ**ï¼šfrontier æ¨¡å‹ä¹‹é—´çš„ outcome accuracy å·®è·è¶Šæ¥è¶Šå°ï¼Œä½† RC ä»ç„¶é«˜åº¦åŒºåˆ†æ€§
2. **mini æ¨¡å‹ç³»ç»Ÿæ€§åœ°è½å…¥çº¢åŒº**ï¼šo3-mini vs o3ï¼ŒGemini 3 Flash vs Gemini 3 Proâ€”â€”smaller/faster ç‰ˆæœ¬æ™®éç”¨è¡¨é¢çº¿ç´¢æ›¿ä»£çœŸå®æ¨ç†
3. **å³ä½¿æœ€å¥½çš„æ¨¡å‹ RC ä¹Ÿåªæœ‰ ~0.4**ï¼šæœ‰å·¨å¤§æå‡ç©ºé—´

---

## è®­ç»ƒæ–¹æ³•ï¼šæ··åˆå¥–åŠ± GenRM

### ä¸‰ç§å¥–åŠ±ä¿¡å·

**Outcome Reward**ï¼ˆæ ‡å‡†äºŒå…ƒåˆ¤æ–­ï¼‰ï¼š
```
R_outcome = 1 if é¢„æµ‹ == äººç±»æ ‡ç­¾ else 0
```

**Rationale Reward**ï¼ˆæ–°è´¡çŒ®ï¼Œç”¨ Average Precisionï¼‰ï¼š
```
R_rationale = AP = Î£_k (P@k Ã— I(k)) / |R_h|

å…¶ä¸­ P@k æ˜¯ top-k çš„ç²¾ç¡®ç‡ï¼ŒI(k) æ˜¯æ˜¯å¦åœ¨æœ€ä¼˜åŒ¹é…é›†ä¸­
```

AP è€Œé F1 çš„åŸå› ï¼šAP å¼•å…¥äº†**è½¯æ’åçº¦æŸ**â€”â€”ä¸ä»…è¦æ±‚è¦†ç›–äººç±»ç†ç”±ï¼Œè¿˜é¼“åŠ±æŠŠæœ€æ ¸å¿ƒçš„ç†ç”±æ’åœ¨å‰é¢ï¼Œä¸º RL æä¾›æ›´å¹³æ»‘çš„æ¢¯åº¦ä¿¡å·ã€‚

**æ··åˆå¥–åŠ±ï¼ˆå…³é”®åˆ›æ–°ï¼‰**ï¼š
```
R_final = R_rationale Ã— R_outcome
```

ä¹˜æ³•å½¢å¼å®ç°äº†**é—¨æ§æœºåˆ¶**ï¼š
- åˆ¤æ–­é”™è¯¯ï¼ˆR_outcome = 0ï¼‰â†’ æ— è®ºæ¨ç†å¤šå¥½éƒ½å¾— 0 åˆ†
- åˆ¤æ–­æ­£ç¡®ä½†æ¨ç†é”™è¯¯ï¼ˆR_outcome = 1ï¼ŒR_rationale â‰ˆ 0ï¼‰â†’ å‡ ä¹å¾— 0 åˆ†
- **åˆ¤æ–­æ­£ç¡®ä¸”æ¨ç†æ­£ç¡®æ‰èƒ½å¾—é«˜åˆ†**

è¿™ä¸ªè®¾è®¡ç›´æ¥åˆ‡æ–­äº†"çŒœå¯¹ç­”æ¡ˆ"çš„æ·å¾„ã€‚

**ä¼˜åŒ–ç®—æ³•**ï¼šGRPOï¼ˆä¸ DeepSeek/Qwen ç³»åˆ—ä¿æŒä¸€è‡´ï¼‰

### è®­ç»ƒç»“æœ

| æŒ‡æ ‡ | Outcome-only baseline | æ··åˆå¥–åŠ±ï¼ˆæœ¬æ–‡ï¼‰| æå‡ |
|------|-----------------------|----------------|------|
| RM-Bench Overall | ~82% | **87.1%** | +5% |
| JudgeBench Overall | ~75% | **82.0%** | +7% |
| RC (rationale consistency) | 25% | **37%** | +12pp |
| Arena Hard v2 åˆ›æ„å†™ä½œ | baseline | +7% | RLHF ä¸‹æ¸¸æå‡ |

SOTA å¯¹æ¯”ï¼ˆRM-Benchï¼‰ï¼š
- Qwen3-30B-A3Bï¼ˆæœ¬æ–‡ï¼‰ï¼š**87.1%**ï¼ˆå…¨éƒ¨æ¨¡å‹ä¸­ç¬¬ä¸€ï¼‰
- RM-R1-Distilled-Qwen-32Bï¼š84.9%
- DeepSeek-R1ï¼ˆLLM-as-judgeï¼‰ï¼š75.8%

---

## æˆ‘çš„åˆ†æ

### è¿™ç¯‡è®ºæ–‡çœŸæ­£æœ‰ä»·å€¼çš„æ˜¯ä»€ä¹ˆ

**é—®é¢˜çš„æå‡ºæ¯”æ–¹æ³•æœ¬èº«æ›´é‡è¦**ã€‚

"Outcome accuracy æ˜¯ä¸å¤Ÿçš„"â€”â€”è¿™å¥è¯çœ‹èµ·æ¥ç®€å•ï¼Œä½†èƒŒåæœ‰æ·±åˆ»çš„è®¤è¯†è®ºå«ä¹‰ï¼š

æˆ‘ä»¬åœ¨ç”¨ reward model æ›¿ä»£äººç±»è¯„åˆ¤ï¼Œå‡è®¾ RM å­¦åˆ°äº†"äººç±»åˆ¤æ–­çš„é€»è¾‘"ã€‚ä½†å®é™…ä¸Šï¼ŒRM å¯èƒ½åªå­¦åˆ°äº†"äººç±»åˆ¤æ–­ç»“æœçš„åˆ†å¸ƒ"ã€‚è¿™ä¸¤è€…åœ¨è®­ç»ƒé›†ä¸Šéš¾ä»¥åŒºåˆ†ï¼Œä½†åœ¨ distribution shift ä¸‹ä¼šåˆ†é“æ‰¬é•³ã€‚

è¿™ç±»ä¼¼äºï¼šä¸€ä¸ªå­¦ç”Ÿåšå¯¹äº†æ•°å­¦é¢˜ï¼Œä½†ç”¨äº†é”™è¯¯çš„æ–¹æ³•ã€‚åœ¨è€ƒåŒç±»é¢˜æ—¶æ²¡æœ‰é—®é¢˜ï¼Œé‡åˆ°å˜ä½“é¢˜å°±æš´éœ²äº†ã€‚

### æ··åˆå¥–åŠ±çš„ç²¾å¦™è®¾è®¡

`R_final = R_rationale Ã— R_outcome` è¿™ä¸ªä¹˜æ³•æœ‰ä¸¤å±‚æ„æ€ï¼š

1. **å……åˆ†æ¡ä»¶**ï¼šåªæœ‰å½“æ¨ç†å’Œç»“è®ºéƒ½å¯¹ï¼Œæ‰ç»™æ»¡åˆ†
2. **å¿…è¦æ¡ä»¶çš„å±‚æ¬¡**ï¼šç»“è®ºæ­£ç¡®æ˜¯å¿…è¦æ¡ä»¶ï¼ˆoutcome = 0 ç›´æ¥å±è”½ï¼‰ï¼Œæ¨ç†æ­£ç¡®æ˜¯å……åˆ†æ¡ä»¶

è¿™æ¯” `R_final = Î± Ã— R_rationale + (1-Î±) Ã— R_outcome` çš„åŠ æ³•æ›´å¥½ï¼Œå› ä¸ºåŠ æ³•å…è®¸"æ¨ç†æå¥½"å¼¥è¡¥"ç»“è®ºé”™è¯¯"ï¼Œè€Œä¹˜æ³•ä¸å…è®¸ã€‚

AP è€Œé F1 çš„è®¾è®¡é€‰æ‹©ä¹Ÿå¾ˆè®²ç©¶ï¼šF1 æ˜¯æ— åºé›†åˆçš„åŒ¹é…ï¼ŒAP æ˜¯æœ‰åºåˆ—è¡¨çš„åŒ¹é…ã€‚è¦æ±‚æ¨¡å‹æŠŠæœ€é‡è¦çš„ç†ç”±æ’åœ¨å‰é¢ï¼Œè¿™å¯¹äº RLHF çš„ reward signal æ¥è¯´æ›´æœ‰ä»·å€¼â€”â€”å®ƒå‘Šè¯‰æ¨¡å‹"è¿™ä¸ªç†ç”±å¾ˆé‡è¦"ï¼Œè€Œä¸åªæ˜¯"è¿™ä¸ªç†ç”±å¯¹ä¸å¯¹"ã€‚

### å¯¹ RLHF å®è·µçš„å«ä¹‰

**ç›´æ¥å½±å“**ï¼šå¦‚æœä½ åœ¨ç”¨ LLM-as-Judge åš rewardï¼Œè€Œè¿™ä¸ª Judge æ˜¯ o3-mini ç±»çš„æ¨¡å‹ï¼Œé‚£ä¹ˆä½ çš„ reward signal å¯èƒ½æ­£åœ¨ä¼ é€’é”™è¯¯çš„æ¢¯åº¦â€”â€”ä¸æ˜¯"ä¸ºä»€ä¹ˆè¿™ä¸ªå›ç­”æ›´å¥½"ï¼Œè€Œæ˜¯"å“ªä¸ªå›ç­”åœ¨è¡¨é¢ä¸Šæ›´åƒå¥½å›ç­”"ã€‚

ç»“æœï¼šRL è®­ç»ƒå‡ºæ¥çš„æ¨¡å‹ä¼šå­¦åˆ°å¦‚ä½•åœ¨è¡¨é¢ä¸Š"çœ‹èµ·æ¥"æ›´å¥½ï¼Œè€Œä¸æ˜¯å¦‚ä½•çœŸæ­£åœ°æå‡è´¨é‡ã€‚è¿™æ˜¯ reward hacking çš„ä¸€ç§æ–°å½¢å¼ã€‚

**å®è·µå»ºè®®**ï¼š
1. ç”¨æ›´å¤§ã€æ›´å¼ºçš„æ¨¡å‹åš judgeï¼ˆo3 vs o3-mini çš„ RC å·®è·æ˜¯ 50%ï¼‰
2. æˆ–è€…ç”¨æœ¬æ–‡çš„æ–¹æ³•åœ¨ judge ä¸Šè®­ç»ƒ RC
3. æœ€ä½æˆæœ¬æ–¹æ¡ˆï¼šåœ¨ judge prompt ä¸­è¦æ±‚åˆ—ä¸¾å…·ä½“ã€å¯éªŒè¯çš„ç†ç”±ï¼Œå¼ºåˆ¶å®ƒåš factual check è€Œé style check

### ä¸å…¶ä»–è®ºæ–‡çš„å¯¹æ¯”è§†è§’

- **MARS**ï¼šå…³æ³¨ reward boundaryï¼ˆmarginï¼‰çš„è´¨é‡â€”â€”æ ·æœ¬åœ¨ BT loss çš„ Hessian ä¸­è´¡çŒ®å¤šå°‘æ›²ç‡
- **æœ¬æ–‡**ï¼šå…³æ³¨ reward reasoningï¼ˆrationaleï¼‰çš„è´¨é‡â€”â€”åˆ¤æ–­ç”¨çš„é€»è¾‘æ˜¯å¦å’Œäººç±»å¯¹é½
- **AutoInject**ï¼šreward å¯ä»¥è¢«æ“çºµï¼ˆadversarial suffixï¼‰â€”â€”æœ¬æ–‡çš„"deceptive alignment"æ˜¯å¦ä¸€ç§ reward å¤±æ•ˆï¼Œä½†æœºåˆ¶å®Œå…¨ä¸åŒï¼ˆä¸æ˜¯æ”»å‡»ï¼Œæ˜¯è®­ç»ƒç›®æ ‡è®¾è®¡ç¼ºé™·ï¼‰

ä¸‰è€…æ”¾åœ¨ä¸€èµ·ï¼šreward ä¿¡å·çš„è´¨é‡æ˜¯ RLHF çš„æ ¸å¿ƒè„†å¼±ç‚¹ï¼Œè‡³å°‘æœ‰ä¸‰ä¸ªç‹¬ç«‹çš„å¤±æ•ˆæ¨¡å¼ï¼ˆmarginè´¨é‡ã€æ¨ç†è´¨é‡ã€å¯¹æŠ—æ”»å‡»ï¼‰ã€‚

### å±€é™

1. **benchmark è¦†ç›–èŒƒå›´**ï¼šä¸»è¦åœ¨ general conversation + creative writingï¼Œå¯¹ math/code çš„ rationale consistency è¯„ä¼°è¾ƒå°‘
2. **è®­ç»ƒæ•°æ®ä¾èµ–**ï¼šéœ€è¦ HelpSteer3 é£æ ¼çš„ expert annotated rationaleï¼Œæ ‡æ³¨æˆæœ¬ä¸ä½
3. **MetaJudge æœ¬èº«çš„ bias**ï¼šåŸå­åˆ†è§£ç”¨ GPT-5 åšï¼Œå¦‚æœ GPT-5 çš„åˆ†è§£æœ‰ç³»ç»Ÿæ€§åå·®ï¼Œæ•´ä¸ª pipeline éƒ½ä¼šå—å½±å“
4. **RC ä¸Šé™**ï¼šå³ä½¿æœ€å¥½çš„æ¨¡å‹ RC åªæœ‰ ~0.4â€”â€”æ˜¯æ¨¡å‹ä¸Šé™è¿˜æ˜¯ MetaJudge æ¡†æ¶çš„ä¸Šé™ï¼Ÿå°šä¸æ¸…æ¥š

---

## å…³é”®å…¬å¼

**æ··åˆå¥–åŠ±ï¼ˆæ ¸å¿ƒè´¡çŒ®ï¼‰**ï¼š
```
R_final = R_rationale Ã— R_outcome
         â† æ¨ç†è´¨é‡ Ã— ç»“è®ºæ­£ç¡®æ€§ï¼ˆä¹˜æ³•é—¨æ§ï¼‰
```

**Rationale Rewardï¼ˆAPï¼‰**ï¼š
```
R_rationale = Î£_k [P@k Ã— I(k)] / |R_h|
ï¼ˆæ³¨é‡é¡ºåºçš„åŒ¹é…ï¼ŒæŠŠé‡è¦ç†ç”±æ’å‰é¢ï¼‰
```

**Rationale Consistencyï¼ˆè¯„ä¼°æŒ‡æ ‡ï¼‰**ï¼š
```
RC = (1/N) Î£_k [S_total(k) / |R_h(k)|]
   = å¹³å‡è½¯å¬å›ï¼ˆæ¨¡å‹è¦†ç›–äº†å¤šå°‘äººç±»åŸå­ç†ç”±ï¼‰
```

---

## å…³é”®æ•°å­—

```
æ¨¡å‹è¯„ä¼°ï¼ˆ19 ä¸ª frontier æ¨¡å‹ï¼‰ï¼š
  æœ€é«˜ RCï¼šGPT-5ã€o3 ç³»åˆ—ï¼ˆâ‰ˆ 0.4ï¼‰
  æœ€ä½ RCï¼šå°å‹æ¨ç†æ¨¡å‹ï¼ˆo3-miniç±»ï¼‰
  RC åŒºåˆ† o3 vs o3-mini çš„å·®è·ï¼š~50%ï¼ˆè€Œ outcome accuracy å‡ ä¹ç›¸åŒï¼‰

è®­ç»ƒç»“æœï¼ˆQwen3-30B-A3Bï¼‰ï¼š
  RM-Benchï¼š87.1%ï¼ˆSOTAï¼‰
  JudgeBenchï¼š82.0%ï¼ˆSOTAï¼‰
  vs outcome-onlyï¼š+5% å¹³å‡
  RC æå‡ï¼š25% â†’ 37%ï¼ˆ+12ppï¼Œé€†è½¬äº† outcome-only è®­ç»ƒçš„ RC ä¸‹é™è¶‹åŠ¿ï¼‰
  RLHF ä¸‹æ¸¸ï¼ˆArena Hard v2 Creative Writingï¼‰ï¼š+7%

MetaJudge å¯é æ€§ï¼š
  ä¸åŒ evaluator çš„ RÂ²ï¼š0.983
  è·¨åŸŸ Spearman Ïï¼š0.85
```

---

## Tags
#RewardModel #GenRM #LLMasJudge #DeceptiveAlignment #RationaleConsistency #GRPO #RLHF #MetaJudge #RM-Bench #JudgeBench #QwenTeam #æ¨ç†è¿‡ç¨‹å¯¹é½

---

> ğŸ“ **ç‰ˆæœ¬è¯´æ˜**ï¼š`AI/LLM/Evaluation/MetaJudge-Rationale-Consistency-GenRM-Deceptive-Alignment.md` ä¸º Scholar æ—©æœŸç®€åŒ–ç‰ˆï¼ˆdeprecatedï¼‰ï¼Œæœ¬æ–‡ä¸ºå®Œæ•´æ­£å¼ç‰ˆã€‚
