---
title: 模型并行策略：从 DP 到 5D 并行
brief: 大模型训练的并行策略全景——DP、ZeRO、TP、PP、SP、CP 的原理/通信量/适用场景对比，附 LLaMA 3 405B 的 4D 并行工业实例和面试高频题。
type: concept
domain: ai/llm/infra
created: 2026-02-13
updated: 2026-02-22
tags:
  - ai/llm/training
  - ai/distributed
  - ai/infrastructure
  - type/concept
  - interview/hot
status: complete
sources:
  - "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism — arXiv:1909.08053"
  - "GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism — arXiv:1811.06965"
  - "PipeDream: Generalized Pipeline Parallelism for DNN Training — arXiv:1806.03377"
  - "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models — arXiv:1910.02054"
  - The Llama 3 Herd of Models — arXiv:2407.21783
related:
  - "[[Megatron-LM|Megatron-LM]]"
  - "[[DeepSpeed|DeepSpeed]]"
  - "[[FSDP|FSDP]]"
---

# 模型并行策略：从 DP 到 5D 并行

> 大模型训练的核心工程挑战——如何把一个万亿参数的模型分布到数千张 GPU 上高效训练

## 1. 为什么需要并行？

以 LLaMA 3 405B 为例：
- 参数量：405B × 2 bytes (BF16) ≈ **810 GB**（仅权重）
- 优化器状态（Adam）：405B × 12 bytes ≈ **4.86 TB**
- 单张 H100 显存：80 GB

→ 单卡根本放不下，必须跨卡/跨节点分布。

## 2. 数据并行（Data Parallelism, DP）

### 原理

```
最朴素的并行：每张 GPU 持有完整模型副本，各自处理不同 mini-batch

GPU 0: Model copy → forward(batch_0) → backward → grad_0 ─┐
GPU 1: Model copy → forward(batch_1) → backward → grad_1 ─┤── AllReduce → 更新
GPU 2: Model copy → forward(batch_2) → backward → grad_2 ─┤
GPU 3: Model copy → forward(batch_3) → backward → grad_3 ─┘
```

### 通信开销

```
AllReduce 通信量 = 2 × model_size × (N-1)/N ≈ 2 × model_size
```

- 通信与参数量成正比，与数据量无关
- 适用于**模型能放入单卡**的场景

### PyTorch DDP 示例

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# 初始化
dist.init_process_group(backend="nccl")
model = MyModel().to(local_rank)
model = DDP(model, device_ids=[local_rank])

# 训练循环——与单卡代码几乎一致
for batch in dataloader:
    loss = model(batch)
    loss.backward()  # DDP 自动在 backward 中 overlap AllReduce
    optimizer.step()
```

**局限**：每张卡都存完整模型 + 优化器状态 → 显存浪费严重。

## 3. ZeRO（Zero Redundancy Optimizer）

DeepSpeed 团队提出，**消除数据并行中的冗余存储**：

> 来源：Rajbhandari et al., "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models" arXiv:1910.02054

```
                      每卡显存占用（以 7.5B 模型为例）
                      ──────────────────────────────
方式            参数(W)   梯度(G)   优化器状态(OS)   总计
────────────────────────────────────────────────────
DDP             15GB     15GB      60GB           ~90GB
ZeRO Stage 1   15GB     15GB      60/N GB        ↓
ZeRO Stage 2   15GB     15/N GB   60/N GB        ↓↓
ZeRO Stage 3   15/N GB  15/N GB   60/N GB        ~90/N GB
────────────────────────────────────────────────────
N = GPU 数量
```

### 三阶段策略

```
ZeRO-1: 分片优化器状态（OS）
  → 每张 GPU 只存 1/N 的 optimizer state
  → forward/backward 不变，只在 step 时通信

ZeRO-2: 分片优化器状态 + 梯度（OS + G）
  → 梯度 Reduce-Scatter 后只保留本片段
  → 通信量与 DDP 相同

ZeRO-3 (FSDP): 分片所有（OS + G + W）
  → forward 前 AllGather 收集参数，用完立即释放
  → backward 时再 AllGather，计算梯度后 Reduce-Scatter
  → 通信量 = 3 × model_size（比 DDP 多 50%）
```

### PyTorch FSDP 示例

```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import ShardingStrategy

model = MyLLM()
model = FSDP(
    model,
    sharding_strategy=ShardingStrategy.FULL_SHARD,  # ZeRO-3
    mixed_precision=MixedPrecision(
        param_dtype=torch.bfloat16,
        reduce_dtype=torch.float32,
    ),
    auto_wrap_policy=transformer_auto_wrap_policy,
)
```

## 4. 张量并行（Tensor Parallelism, TP）

[[Megatron-LM]] 提出——**将单个层的矩阵运算切分到多张 GPU**。

> 来源：Shoeybi et al., "Megatron-LM" arXiv:1909.08053, Sec. 3

### MLP 层的列切分/行切分

```
原始 MLP: Y = GeLU(XA) · B

TP 切分（2 卡为例）:
  A = [A₁ | A₂]  (列切分)
  B = [B₁]        (行切分)
      [B₂]

GPU 0: Y₁ = GeLU(X·A₁) · B₁
GPU 1: Y₂ = GeLU(X·A₂) · B₂

Y = AllReduce(Y₁ + Y₂)
```

### Self-Attention 的切分

```
多头注意力天然适合 TP——按 head 切分

GPU 0: heads 0-15  → Q₀K₀V₀ → Attention₀ → O₀ ─┐
GPU 1: heads 16-31 → Q₁K₁V₁ → Attention₁ → O₁ ─┤── AllReduce
```

### 通信开销

```
每个 Transformer Layer 需要:
  Forward:  2 次 AllReduce（MLP + Attention）
  Backward: 2 次 AllReduce

通信量 = O(batch_size × seq_len × hidden_dim)

关键：TP 的通信发生在层内（latency-sensitive），
     需要高带宽互联（NVLink > 600 GB/s）
     → TP 通常限制在单节点内（8卡）
```

## 5. 流水线并行（Pipeline Parallelism, PP）

**将模型按层分段，每段放在不同 GPU 上**：

```
Stage 0 (GPU 0): Layers 0-7
Stage 1 (GPU 1): Layers 8-15
Stage 2 (GPU 2): Layers 16-23
Stage 3 (GPU 3): Layers 24-31
```

### 朴素实现的问题——Bubble

```
朴素流水线：大量 GPU 空闲时间（bubble）

时间 →  t1    t2    t3    t4    t5    t6    t7    t8
GPU 0:  [F0]  [F1]  [F2]  [F3]  idle  idle  idle  [B3][B2][B1][B0]
GPU 1:  idle  [F0]  [F1]  [F2]  [F3]  idle  idle  idle ...
GPU 2:  idle  idle  [F0]  [F1]  [F2]  [F3]  idle  idle ...
GPU 3:  idle  idle  idle  [F0]  [F1]  [F2]  [F3]  [B3]...

Bubble ratio = (P-1) / (M+P-1)，P=stages, M=micro-batches
```

### 调度优化

> 来源：GPipe arXiv:1811.06965（micro-batch pipeline）；PipeDream arXiv:1806.03377（1F1B）；Megatron-LM v2 arXiv:2104.04473（interleaved 1F1B）

| 调度方案 | 核心思想 | Bubble 改善 |
|----------|---------|------------|
| 1F1B | 稳态阶段交替执行 F 和 B，及时释放激活值 | 显著 |
| Interleaved 1F1B | 每个 GPU 持有多个非连续 stage | 进一步减少，但增加通信 |
| Zero Bubble PP (2024) | 将 backward 拆为 B（输入梯度）和 W（权重梯度），精细调度 F/B/W | 几乎零 bubble |

### 通信开销

```
相邻 Stage 间: 点对点通信 (P2P Send/Recv)
通信量 = O(batch_size × seq_len × hidden_dim) per micro-batch
优势：通信量与模型大小无关，只与激活值大小有关
     → 适合跨节点（IB 网络即可）
```

## 6. 序列并行（Sequence Parallelism, SP）

### Megatron SP

解决 TP 中 **LayerNorm / Dropout 的激活值冗余**：

```
标准 TP: LayerNorm 和 Dropout 在每张卡上都对完整序列执行
  → 这些 op 的激活值没有被切分，造成冗余

Megatron SP: 在非 TP 区域沿序列维度切分
  → AllReduce 替换为 ReduceScatter + AllGather
  → 激活值内存减少到 1/TP_size
```

### Context Parallelism (CP)

Ring Attention 思想——**将超长序列切分到多张 GPU**：

```
序列长度 128K，4 卡 CP:
GPU 0: tokens 0-32K
GPU 1: tokens 32K-64K
GPU 2: tokens 64K-96K
GPU 3: tokens 96K-128K

通过 Ring 通信传递 KV，每卡计算完整 Attention
→ 解决 KV Cache 和激活值的显存瓶颈
```

## 7. 4D/5D 并行：工业实践

LLaMA 3 405B 训练使用 **16K H100 GPU** 的 4D 并行：

> 来源：Meta, "The Llama 3 Herd of Models" arXiv:2407.21783, Sec. 3.3

```
                    16,384 GPUs
                    ┌─────────────────────────┐
 Data Parallel:     │  DP = 128 组            │  跨节点
                    │  ┌──────────────────┐    │
 Pipeline Parallel: │  │  PP = 16 stages  │    │  跨节点
                    │  │  ┌────────────┐   │   │
 Tensor Parallel:   │  │  │ TP = 8     │   │   │  节点内 NVLink
                    │  │  │  (1 node)  │   │   │
 Context Parallel:  │  │  │ CP = ?     │   │   │  长序列时启用
                    │  │  └────────────┘   │   │
                    │  └──────────────────┘    │
                    └─────────────────────────┘

总 GPU = DP × PP × TP (× CP) = 128 × 16 × 8 = 16,384
```

## 8. 选型指南

```
场景                     推荐策略                    理由
──────────────────────────────────────────────────────────────
7B 模型 / 单节点 8卡      FSDP (ZeRO-3)              简单高效
13-70B / 多节点           FSDP + TP (节点内)          平衡显存和通信
70B-400B / 大规模         TP + PP + FSDP              3D 并行标配
超长序列 (128K+)          + Context Parallelism        切分 KV Cache
万卡训练                  4D/5D 并行 + 精细调度       Meta/Google 级别
推理 (非训练)             TP (节点内) + PP (跨节点)    最小化延迟
```

## 9. 面试高频题

### Q1: 数据并行和模型并行的本质区别？
**答**：数据并行是"相同模型、不同数据"——每张卡持有完整模型副本，处理不同 mini-batch，需要 AllReduce 同步梯度。模型并行是"不同模型片、相同数据"——将模型拆分到多卡，需要在前向/反向中传递中间激活值。数据并行的通信量与参数量成正比；模型并行（TP/PP）的通信量与激活值大小成正比。

### Q2: ZeRO Stage 3 和 TP 都能解决单卡放不下模型的问题，如何选择？
**答**：ZeRO-3 (FSDP) 在 forward 前 AllGather 参数、backward 后 Reduce-Scatter 梯度，通信量是 DDP 的 1.5 倍，但对网络带宽要求低（可跨节点）。TP 在每层内部做 AllReduce，通信频率高但每次量小，对延迟敏感，必须用 NVLink（节点内）。实践中：**节点内用 TP，跨节点用 FSDP/ZeRO-3**。

### Q3: Pipeline Parallelism 的 bubble 如何减少？
**答**：三个方向：(1) 增大 micro-batch 数量 M，bubble ratio = (P-1)/(M+P-1) → M 越大 bubble 越小；(2) 采用 Interleaved 1F1B 调度，每卡持有多个 stage 的虚拟 chunk；(3) Zero Bubble PP 将 backward 拆为 B（输入梯度）和 W（权重梯度），通过精细调度 F/B/W 消除几乎所有 bubble。

### Q4: 序列并行和上下文并行的区别？
**答**：Megatron Sequence Parallelism 是 TP 的补充——在 TP 不切分的区域（LayerNorm、Dropout）沿序列维度切分**激活值**，减少冗余显存，不改变计算逻辑。Context Parallelism 是独立维度——将超长序列真正切分到多卡，每卡只处理一段 token，通过 Ring Attention 传递 KV 完成全局 Attention 计算，解决的是**序列长度**带来的显存和计算瓶颈。

### Q5: 如何设计一个 70B 模型在 64 张 H100 上的训练并行策略？
**答**：8 台 8xH100 节点。推荐配置：节点内 TP=8（利用 NVLink 900 GB/s），跨节点 FSDP（ZeRO-3 或 ZeRO-2），不需要 PP（70B 用 TP=8+FSDP 即可）。实际 GPU 分配：64 / 8(TP) = 8 组 FSDP。若序列长度 ≤ 8K，这就够了；若 128K+，加 CP=2，此时 DP=4, TP=8, CP=2。关键是先算显存：70B × 18 bytes(BF16+Adam) / 64 ≈ 20 GB/卡的参数+优化器，加上激活值（取决于 seq_len 和 micro-batch），选择合适的 activation checkpointing 策略。

---

## 📚 推荐阅读

### 原始论文
- [Megatron-LM](https://arxiv.org/abs/1909.08053) — Tensor Parallelism 奠基之作
- [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism](https://arxiv.org/abs/1811.06965) — Pipeline Parallelism 经典设计
- [PipeDream: Generalized Pipeline Parallelism](https://arxiv.org/abs/1806.03377) — 1F1B schedule 的原始提出
- [ZeRO](https://arxiv.org/abs/1910.02054) — 数据并行显存优化三阶段

### 深度解读
- [大模型并行训练全景（知乎）](https://zhuanlan.zhihu.com/p/617087561) — 中文社区最全面的 DP/TP/PP/SP 对比 ⭐⭐⭐⭐
- [LLaMA 3 Training Infrastructure](https://arxiv.org/abs/2407.21783) — Meta 万卡 4D 并行的工程细节

### 实践资源
- [Megatron-Core GitHub](https://github.com/NVIDIA/Megatron-LM) — 3D 并行工业级实现
- [PyTorch Distributed Overview](https://pytorch.org/docs/stable/distributed.html) — 官方分布式训练文档

## 🔧 落地应用

### 直接可用场景
- **训练策略选型**：本文的选型指南可直接用于评估项目需要的 GPU 数量和并行配置
- **面试准备**：Q1-Q5 覆盖了分布式训练 90% 的面试问题

### 工程实现要点
- **通信量公式速记**：DP AllReduce = $2M$，FSDP = $3M$，TP 每层 = $4 \times B \times S \times H$（$M$=模型大小，$B$=batch，$S$=seq_len，$H$=hidden_dim）
- **Bubble ratio 公式**：$\text{bubble} = \frac{P-1}{M+P-1}$，其中 $P$=pipeline stages，$M$=micro-batches
- **黄金法则**：节点内 TP（NVLink），跨节点 FSDP/PP（InfiniBand），DP 最外层

### 面试高频问法
- Q: 给你 128 张 H100，训练一个 180B 模型，你怎么配置并行策略？
  A: 16 节点 × 8 GPU。节点内 TP=8（NVLink 900GB/s）；跨节点 PP=4（16/4=4 组 FSDP）；FSDP(ZeRO-2) DP=4。总 GPU = 8×4×4 = 128。验算显存：180B×2B(BF16)/8(TP)=45GB 参数/卡，加优化器分片后可控。若序列长，追加 CP。

## 💡 启发与思考

### So What？对老板意味着什么
- 这篇是"分布式训练选型的一张图"——所有并行策略的 trade-off（通信 vs 显存 vs 编程复杂度）都在这里
- 理解 4D/5D 并行是读懂 [[Megatron-LM]]、[[DeepSpeed]]、[[FSDP]] 的前提

### 未解问题与局限
- Zero Bubble PP 的论文结果很惊艳，但在异构集群（不同代 GPU）上的实际表现未知
- Context Parallelism 的最优分片粒度和 Ring Attention 的通信模式有理论最优解吗？

### 脑暴：如果往下延伸
- 如果 [[FlashAttention]] 继续优化到几乎零额外显存，CP 是否还有存在必要？
- 5D 并行 + 自动搜索最优配置（如 Alpa arXiv:2201.12023）能否让用户不再手动调 TP/PP/DP？

## 相关

> 🔗 See also: [[Megatron-LM|Megatron-LM]] — TP/PP 工业级实现
> 🔗 See also: [[DeepSpeed|DeepSpeed]] — ZeRO 系列显存优化
> 🔗 See also: [[FSDP|FSDP]] — PyTorch 原生 ZeRO-3

- [[FlashAttention]] — Attention 计算优化
- [[Transformer 位置编码]] — 序列并行相关
- [[小规模训练手册]] — 小模型训练实践
