---
title: æ¨¡å‹å¹¶è¡Œç­–ç•¥ï¼šä» DP åˆ° 5D å¹¶è¡Œ
brief: å¤§æ¨¡å‹è®­ç»ƒçš„å¹¶è¡Œç­–ç•¥å…¨æ™¯â€”â€”DPã€ZeROã€TPã€PPã€SPã€CP çš„åŸç†/é€šä¿¡é‡/é€‚ç”¨åœºæ™¯å¯¹æ¯”ï¼Œé™„ LLaMA 3 405B çš„ 4D å¹¶è¡Œå·¥ä¸šå®ä¾‹å’Œé¢è¯•é«˜é¢‘é¢˜ã€‚
type: concept
domain: ai/llm/infra
created: 2026-02-13
updated: 2026-02-22
tags:
  - ai/llm/training
  - ai/distributed
  - ai/infrastructure
  - type/concept
  - interview/hot
status: complete
sources:
  - "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism â€” arXiv:1909.08053"
  - "GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism â€” arXiv:1811.06965"
  - "PipeDream: Generalized Pipeline Parallelism for DNN Training â€” arXiv:1806.03377"
  - "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models â€” arXiv:1910.02054"
  - The Llama 3 Herd of Models â€” arXiv:2407.21783
related:
  - "[[AI/3-LLM/Infra/Megatron-LM|Megatron-LM]]"
  - "[[AI/3-LLM/Infra/DeepSpeed|DeepSpeed]]"
  - "[[AI/3-LLM/Infra/FSDP|FSDP]]"
---

# æ¨¡å‹å¹¶è¡Œç­–ç•¥ï¼šä» DP åˆ° 5D å¹¶è¡Œ

> å¤§æ¨¡å‹è®­ç»ƒçš„æ ¸å¿ƒå·¥ç¨‹æŒ‘æˆ˜â€”â€”å¦‚ä½•æŠŠä¸€ä¸ªä¸‡äº¿å‚æ•°çš„æ¨¡å‹åˆ†å¸ƒåˆ°æ•°åƒå¼  GPU ä¸Šé«˜æ•ˆè®­ç»ƒ

## 1. ä¸ºä»€ä¹ˆéœ€è¦å¹¶è¡Œï¼Ÿ

ä»¥ LLaMA 3 405B ä¸ºä¾‹ï¼š
- å‚æ•°é‡ï¼š405B Ã— 2 bytes (BF16) â‰ˆ **810 GB**ï¼ˆä»…æƒé‡ï¼‰
- ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆAdamï¼‰ï¼š405B Ã— 12 bytes â‰ˆ **4.86 TB**
- å•å¼  H100 æ˜¾å­˜ï¼š80 GB

â†’ å•å¡æ ¹æœ¬æ”¾ä¸ä¸‹ï¼Œå¿…é¡»è·¨å¡/è·¨èŠ‚ç‚¹åˆ†å¸ƒã€‚

## 2. æ•°æ®å¹¶è¡Œï¼ˆData Parallelism, DPï¼‰

### åŸç†

```
æœ€æœ´ç´ çš„å¹¶è¡Œï¼šæ¯å¼  GPU æŒæœ‰å®Œæ•´æ¨¡å‹å‰¯æœ¬ï¼Œå„è‡ªå¤„ç†ä¸åŒ mini-batch

GPU 0: Model copy â†’ forward(batch_0) â†’ backward â†’ grad_0 â”€â”
GPU 1: Model copy â†’ forward(batch_1) â†’ backward â†’ grad_1 â”€â”¤â”€â”€ AllReduce â†’ æ›´æ–°
GPU 2: Model copy â†’ forward(batch_2) â†’ backward â†’ grad_2 â”€â”¤
GPU 3: Model copy â†’ forward(batch_3) â†’ backward â†’ grad_3 â”€â”˜
```

### é€šä¿¡å¼€é”€

```
AllReduce é€šä¿¡é‡ = 2 Ã— model_size Ã— (N-1)/N â‰ˆ 2 Ã— model_size
```

- é€šä¿¡ä¸å‚æ•°é‡æˆæ­£æ¯”ï¼Œä¸æ•°æ®é‡æ— å…³
- é€‚ç”¨äº**æ¨¡å‹èƒ½æ”¾å…¥å•å¡**çš„åœºæ™¯

### PyTorch DDP ç¤ºä¾‹

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–
dist.init_process_group(backend="nccl")
model = MyModel().to(local_rank)
model = DDP(model, device_ids=[local_rank])

# è®­ç»ƒå¾ªç¯â€”â€”ä¸å•å¡ä»£ç å‡ ä¹ä¸€è‡´
for batch in dataloader:
    loss = model(batch)
    loss.backward()  # DDP è‡ªåŠ¨åœ¨ backward ä¸­ overlap AllReduce
    optimizer.step()
```

**å±€é™**ï¼šæ¯å¼ å¡éƒ½å­˜å®Œæ•´æ¨¡å‹ + ä¼˜åŒ–å™¨çŠ¶æ€ â†’ æ˜¾å­˜æµªè´¹ä¸¥é‡ã€‚

## 3. ZeROï¼ˆZero Redundancy Optimizerï¼‰

DeepSpeed å›¢é˜Ÿæå‡ºï¼Œ**æ¶ˆé™¤æ•°æ®å¹¶è¡Œä¸­çš„å†—ä½™å­˜å‚¨**ï¼š

> æ¥æºï¼šRajbhandari et al., "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models" arXiv:1910.02054

```
                      æ¯å¡æ˜¾å­˜å ç”¨ï¼ˆä»¥ 7.5B æ¨¡å‹ä¸ºä¾‹ï¼‰
                      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
æ–¹å¼            å‚æ•°(W)   æ¢¯åº¦(G)   ä¼˜åŒ–å™¨çŠ¶æ€(OS)   æ€»è®¡
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DDP             15GB     15GB      60GB           ~90GB
ZeRO Stage 1   15GB     15GB      60/N GB        â†“
ZeRO Stage 2   15GB     15/N GB   60/N GB        â†“â†“
ZeRO Stage 3   15/N GB  15/N GB   60/N GB        ~90/N GB
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
N = GPU æ•°é‡
```

### ä¸‰é˜¶æ®µç­–ç•¥

```
ZeRO-1: åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆOSï¼‰
  â†’ æ¯å¼  GPU åªå­˜ 1/N çš„ optimizer state
  â†’ forward/backward ä¸å˜ï¼Œåªåœ¨ step æ—¶é€šä¿¡

ZeRO-2: åˆ†ç‰‡ä¼˜åŒ–å™¨çŠ¶æ€ + æ¢¯åº¦ï¼ˆOS + Gï¼‰
  â†’ æ¢¯åº¦ Reduce-Scatter ååªä¿ç•™æœ¬ç‰‡æ®µ
  â†’ é€šä¿¡é‡ä¸ DDP ç›¸åŒ

ZeRO-3 (FSDP): åˆ†ç‰‡æ‰€æœ‰ï¼ˆOS + G + Wï¼‰
  â†’ forward å‰ AllGather æ”¶é›†å‚æ•°ï¼Œç”¨å®Œç«‹å³é‡Šæ”¾
  â†’ backward æ—¶å† AllGatherï¼Œè®¡ç®—æ¢¯åº¦å Reduce-Scatter
  â†’ é€šä¿¡é‡ = 3 Ã— model_sizeï¼ˆæ¯” DDP å¤š 50%ï¼‰
```

### PyTorch FSDP ç¤ºä¾‹

```python
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp import ShardingStrategy

model = MyLLM()
model = FSDP(
    model,
    sharding_strategy=ShardingStrategy.FULL_SHARD,  # ZeRO-3
    mixed_precision=MixedPrecision(
        param_dtype=torch.bfloat16,
        reduce_dtype=torch.float32,
    ),
    auto_wrap_policy=transformer_auto_wrap_policy,
)
```

## 4. å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelism, TPï¼‰

[[AI/3-LLM/Infra/Megatron-LM]] æå‡ºâ€”â€”**å°†å•ä¸ªå±‚çš„çŸ©é˜µè¿ç®—åˆ‡åˆ†åˆ°å¤šå¼  GPU**ã€‚

> æ¥æºï¼šShoeybi et al., "Megatron-LM" arXiv:1909.08053, Sec. 3

### MLP å±‚çš„åˆ—åˆ‡åˆ†/è¡Œåˆ‡åˆ†

```
åŸå§‹ MLP: Y = GeLU(XA) Â· B

TP åˆ‡åˆ†ï¼ˆ2 å¡ä¸ºä¾‹ï¼‰:
  A = [Aâ‚ | Aâ‚‚]  (åˆ—åˆ‡åˆ†)
  B = [Bâ‚]        (è¡Œåˆ‡åˆ†)
      [Bâ‚‚]

GPU 0: Yâ‚ = GeLU(XÂ·Aâ‚) Â· Bâ‚
GPU 1: Yâ‚‚ = GeLU(XÂ·Aâ‚‚) Â· Bâ‚‚

Y = AllReduce(Yâ‚ + Yâ‚‚)
```

### Self-Attention çš„åˆ‡åˆ†

```
å¤šå¤´æ³¨æ„åŠ›å¤©ç„¶é€‚åˆ TPâ€”â€”æŒ‰ head åˆ‡åˆ†

GPU 0: heads 0-15  â†’ Qâ‚€Kâ‚€Vâ‚€ â†’ Attentionâ‚€ â†’ Oâ‚€ â”€â”
GPU 1: heads 16-31 â†’ Qâ‚Kâ‚Vâ‚ â†’ Attentionâ‚ â†’ Oâ‚ â”€â”¤â”€â”€ AllReduce
```

### é€šä¿¡å¼€é”€

```
æ¯ä¸ª Transformer Layer éœ€è¦:
  Forward:  2 æ¬¡ AllReduceï¼ˆMLP + Attentionï¼‰
  Backward: 2 æ¬¡ AllReduce

é€šä¿¡é‡ = O(batch_size Ã— seq_len Ã— hidden_dim)

å…³é”®ï¼šTP çš„é€šä¿¡å‘ç”Ÿåœ¨å±‚å†…ï¼ˆlatency-sensitiveï¼‰ï¼Œ
     éœ€è¦é«˜å¸¦å®½äº’è”ï¼ˆNVLink > 600 GB/sï¼‰
     â†’ TP é€šå¸¸é™åˆ¶åœ¨å•èŠ‚ç‚¹å†…ï¼ˆ8å¡ï¼‰
```

## 5. æµæ°´çº¿å¹¶è¡Œï¼ˆPipeline Parallelism, PPï¼‰

**å°†æ¨¡å‹æŒ‰å±‚åˆ†æ®µï¼Œæ¯æ®µæ”¾åœ¨ä¸åŒ GPU ä¸Š**ï¼š

```
Stage 0 (GPU 0): Layers 0-7
Stage 1 (GPU 1): Layers 8-15
Stage 2 (GPU 2): Layers 16-23
Stage 3 (GPU 3): Layers 24-31
```

### æœ´ç´ å®ç°çš„é—®é¢˜â€”â€”Bubble

```
æœ´ç´ æµæ°´çº¿ï¼šå¤§é‡ GPU ç©ºé—²æ—¶é—´ï¼ˆbubbleï¼‰

æ—¶é—´ â†’  t1    t2    t3    t4    t5    t6    t7    t8
GPU 0:  [F0]  [F1]  [F2]  [F3]  idle  idle  idle  [B3][B2][B1][B0]
GPU 1:  idle  [F0]  [F1]  [F2]  [F3]  idle  idle  idle ...
GPU 2:  idle  idle  [F0]  [F1]  [F2]  [F3]  idle  idle ...
GPU 3:  idle  idle  idle  [F0]  [F1]  [F2]  [F3]  [B3]...

Bubble ratio = (P-1) / (M+P-1)ï¼ŒP=stages, M=micro-batches
```

### è°ƒåº¦ä¼˜åŒ–

> æ¥æºï¼šGPipe arXiv:1811.06965ï¼ˆmicro-batch pipelineï¼‰ï¼›PipeDream arXiv:1806.03377ï¼ˆ1F1Bï¼‰ï¼›Megatron-LM v2 arXiv:2104.04473ï¼ˆinterleaved 1F1Bï¼‰

| è°ƒåº¦æ–¹æ¡ˆ | æ ¸å¿ƒæ€æƒ³ | Bubble æ”¹å–„ |
|----------|---------|------------|
| 1F1B | ç¨³æ€é˜¶æ®µäº¤æ›¿æ‰§è¡Œ F å’Œ Bï¼ŒåŠæ—¶é‡Šæ”¾æ¿€æ´»å€¼ | æ˜¾è‘— |
| Interleaved 1F1B | æ¯ä¸ª GPU æŒæœ‰å¤šä¸ªéè¿ç»­ stage | è¿›ä¸€æ­¥å‡å°‘ï¼Œä½†å¢åŠ é€šä¿¡ |
| Zero Bubble PP (2024) | å°† backward æ‹†ä¸º Bï¼ˆè¾“å…¥æ¢¯åº¦ï¼‰å’Œ Wï¼ˆæƒé‡æ¢¯åº¦ï¼‰ï¼Œç²¾ç»†è°ƒåº¦ F/B/W | å‡ ä¹é›¶ bubble |

### é€šä¿¡å¼€é”€

```
ç›¸é‚» Stage é—´: ç‚¹å¯¹ç‚¹é€šä¿¡ (P2P Send/Recv)
é€šä¿¡é‡ = O(batch_size Ã— seq_len Ã— hidden_dim) per micro-batch
ä¼˜åŠ¿ï¼šé€šä¿¡é‡ä¸æ¨¡å‹å¤§å°æ— å…³ï¼Œåªä¸æ¿€æ´»å€¼å¤§å°æœ‰å…³
     â†’ é€‚åˆè·¨èŠ‚ç‚¹ï¼ˆIB ç½‘ç»œå³å¯ï¼‰
```

## 6. åºåˆ—å¹¶è¡Œï¼ˆSequence Parallelism, SPï¼‰

### Megatron SP

è§£å†³ TP ä¸­ **LayerNorm / Dropout çš„æ¿€æ´»å€¼å†—ä½™**ï¼š

```
æ ‡å‡† TP: LayerNorm å’Œ Dropout åœ¨æ¯å¼ å¡ä¸Šéƒ½å¯¹å®Œæ•´åºåˆ—æ‰§è¡Œ
  â†’ è¿™äº› op çš„æ¿€æ´»å€¼æ²¡æœ‰è¢«åˆ‡åˆ†ï¼Œé€ æˆå†—ä½™

Megatron SP: åœ¨é TP åŒºåŸŸæ²¿åºåˆ—ç»´åº¦åˆ‡åˆ†
  â†’ AllReduce æ›¿æ¢ä¸º ReduceScatter + AllGather
  â†’ æ¿€æ´»å€¼å†…å­˜å‡å°‘åˆ° 1/TP_size
```

### Context Parallelism (CP)

Ring Attention æ€æƒ³â€”â€”**å°†è¶…é•¿åºåˆ—åˆ‡åˆ†åˆ°å¤šå¼  GPU**ï¼š

```
åºåˆ—é•¿åº¦ 128Kï¼Œ4 å¡ CP:
GPU 0: tokens 0-32K
GPU 1: tokens 32K-64K
GPU 2: tokens 64K-96K
GPU 3: tokens 96K-128K

é€šè¿‡ Ring é€šä¿¡ä¼ é€’ KVï¼Œæ¯å¡è®¡ç®—å®Œæ•´ Attention
â†’ è§£å†³ KV Cache å’Œæ¿€æ´»å€¼çš„æ˜¾å­˜ç“¶é¢ˆ
```

## 7. 4D/5D å¹¶è¡Œï¼šå·¥ä¸šå®è·µ

LLaMA 3 405B è®­ç»ƒä½¿ç”¨ **16K H100 GPU** çš„ 4D å¹¶è¡Œï¼š

> æ¥æºï¼šMeta, "The Llama 3 Herd of Models" arXiv:2407.21783, Sec. 3.3

```
                    16,384 GPUs
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 Data Parallel:     â”‚  DP = 128 ç»„            â”‚  è·¨èŠ‚ç‚¹
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
 Pipeline Parallel: â”‚  â”‚  PP = 16 stages  â”‚    â”‚  è·¨èŠ‚ç‚¹
                    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
 Tensor Parallel:   â”‚  â”‚  â”‚ TP = 8     â”‚   â”‚   â”‚  èŠ‚ç‚¹å†… NVLink
                    â”‚  â”‚  â”‚  (1 node)  â”‚   â”‚   â”‚
 Context Parallel:  â”‚  â”‚  â”‚ CP = ?     â”‚   â”‚   â”‚  é•¿åºåˆ—æ—¶å¯ç”¨
                    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ€» GPU = DP Ã— PP Ã— TP (Ã— CP) = 128 Ã— 16 Ã— 8 = 16,384
```

## 8. é€‰å‹æŒ‡å—

```
åœºæ™¯                     æ¨èç­–ç•¥                    ç†ç”±
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
7B æ¨¡å‹ / å•èŠ‚ç‚¹ 8å¡      FSDP (ZeRO-3)              ç®€å•é«˜æ•ˆ
13-70B / å¤šèŠ‚ç‚¹           FSDP + TP (èŠ‚ç‚¹å†…)          å¹³è¡¡æ˜¾å­˜å’Œé€šä¿¡
70B-400B / å¤§è§„æ¨¡         TP + PP + FSDP              3D å¹¶è¡Œæ ‡é…
è¶…é•¿åºåˆ— (128K+)          + Context Parallelism        åˆ‡åˆ† KV Cache
ä¸‡å¡è®­ç»ƒ                  4D/5D å¹¶è¡Œ + ç²¾ç»†è°ƒåº¦       Meta/Google çº§åˆ«
æ¨ç† (éè®­ç»ƒ)             TP (èŠ‚ç‚¹å†…) + PP (è·¨èŠ‚ç‚¹)    æœ€å°åŒ–å»¶è¿Ÿ
```

## 9. é¢è¯•é«˜é¢‘é¢˜

### Q1: æ•°æ®å¹¶è¡Œå’Œæ¨¡å‹å¹¶è¡Œçš„æœ¬è´¨åŒºåˆ«ï¼Ÿ
**ç­”**ï¼šæ•°æ®å¹¶è¡Œæ˜¯"ç›¸åŒæ¨¡å‹ã€ä¸åŒæ•°æ®"â€”â€”æ¯å¼ å¡æŒæœ‰å®Œæ•´æ¨¡å‹å‰¯æœ¬ï¼Œå¤„ç†ä¸åŒ mini-batchï¼Œéœ€è¦ AllReduce åŒæ­¥æ¢¯åº¦ã€‚æ¨¡å‹å¹¶è¡Œæ˜¯"ä¸åŒæ¨¡å‹ç‰‡ã€ç›¸åŒæ•°æ®"â€”â€”å°†æ¨¡å‹æ‹†åˆ†åˆ°å¤šå¡ï¼Œéœ€è¦åœ¨å‰å‘/åå‘ä¸­ä¼ é€’ä¸­é—´æ¿€æ´»å€¼ã€‚æ•°æ®å¹¶è¡Œçš„é€šä¿¡é‡ä¸å‚æ•°é‡æˆæ­£æ¯”ï¼›æ¨¡å‹å¹¶è¡Œï¼ˆTP/PPï¼‰çš„é€šä¿¡é‡ä¸æ¿€æ´»å€¼å¤§å°æˆæ­£æ¯”ã€‚

### Q2: ZeRO Stage 3 å’Œ TP éƒ½èƒ½è§£å†³å•å¡æ”¾ä¸ä¸‹æ¨¡å‹çš„é—®é¢˜ï¼Œå¦‚ä½•é€‰æ‹©ï¼Ÿ
**ç­”**ï¼šZeRO-3 (FSDP) åœ¨ forward å‰ AllGather å‚æ•°ã€backward å Reduce-Scatter æ¢¯åº¦ï¼Œé€šä¿¡é‡æ˜¯ DDP çš„ 1.5 å€ï¼Œä½†å¯¹ç½‘ç»œå¸¦å®½è¦æ±‚ä½ï¼ˆå¯è·¨èŠ‚ç‚¹ï¼‰ã€‚TP åœ¨æ¯å±‚å†…éƒ¨åš AllReduceï¼Œé€šä¿¡é¢‘ç‡é«˜ä½†æ¯æ¬¡é‡å°ï¼Œå¯¹å»¶è¿Ÿæ•æ„Ÿï¼Œå¿…é¡»ç”¨ NVLinkï¼ˆèŠ‚ç‚¹å†…ï¼‰ã€‚å®è·µä¸­ï¼š**èŠ‚ç‚¹å†…ç”¨ TPï¼Œè·¨èŠ‚ç‚¹ç”¨ FSDP/ZeRO-3**ã€‚

### Q3: Pipeline Parallelism çš„ bubble å¦‚ä½•å‡å°‘ï¼Ÿ
**ç­”**ï¼šä¸‰ä¸ªæ–¹å‘ï¼š(1) å¢å¤§ micro-batch æ•°é‡ Mï¼Œbubble ratio = (P-1)/(M+P-1) â†’ M è¶Šå¤§ bubble è¶Šå°ï¼›(2) é‡‡ç”¨ Interleaved 1F1B è°ƒåº¦ï¼Œæ¯å¡æŒæœ‰å¤šä¸ª stage çš„è™šæ‹Ÿ chunkï¼›(3) Zero Bubble PP å°† backward æ‹†ä¸º Bï¼ˆè¾“å…¥æ¢¯åº¦ï¼‰å’Œ Wï¼ˆæƒé‡æ¢¯åº¦ï¼‰ï¼Œé€šè¿‡ç²¾ç»†è°ƒåº¦ F/B/W æ¶ˆé™¤å‡ ä¹æ‰€æœ‰ bubbleã€‚

### Q4: åºåˆ—å¹¶è¡Œå’Œä¸Šä¸‹æ–‡å¹¶è¡Œçš„åŒºåˆ«ï¼Ÿ
**ç­”**ï¼šMegatron Sequence Parallelism æ˜¯ TP çš„è¡¥å……â€”â€”åœ¨ TP ä¸åˆ‡åˆ†çš„åŒºåŸŸï¼ˆLayerNormã€Dropoutï¼‰æ²¿åºåˆ—ç»´åº¦åˆ‡åˆ†**æ¿€æ´»å€¼**ï¼Œå‡å°‘å†—ä½™æ˜¾å­˜ï¼Œä¸æ”¹å˜è®¡ç®—é€»è¾‘ã€‚Context Parallelism æ˜¯ç‹¬ç«‹ç»´åº¦â€”â€”å°†è¶…é•¿åºåˆ—çœŸæ­£åˆ‡åˆ†åˆ°å¤šå¡ï¼Œæ¯å¡åªå¤„ç†ä¸€æ®µ tokenï¼Œé€šè¿‡ Ring Attention ä¼ é€’ KV å®Œæˆå…¨å±€ Attention è®¡ç®—ï¼Œè§£å†³çš„æ˜¯**åºåˆ—é•¿åº¦**å¸¦æ¥çš„æ˜¾å­˜å’Œè®¡ç®—ç“¶é¢ˆã€‚

### Q5: å¦‚ä½•è®¾è®¡ä¸€ä¸ª 70B æ¨¡å‹åœ¨ 64 å¼  H100 ä¸Šçš„è®­ç»ƒå¹¶è¡Œç­–ç•¥ï¼Ÿ
**ç­”**ï¼š8 å° 8xH100 èŠ‚ç‚¹ã€‚æ¨èé…ç½®ï¼šèŠ‚ç‚¹å†… TP=8ï¼ˆåˆ©ç”¨ NVLink 900 GB/sï¼‰ï¼Œè·¨èŠ‚ç‚¹ FSDPï¼ˆZeRO-3 æˆ– ZeRO-2ï¼‰ï¼Œä¸éœ€è¦ PPï¼ˆ70B ç”¨ TP=8+FSDP å³å¯ï¼‰ã€‚å®é™… GPU åˆ†é…ï¼š64 / 8(TP) = 8 ç»„ FSDPã€‚è‹¥åºåˆ—é•¿åº¦ â‰¤ 8Kï¼Œè¿™å°±å¤Ÿäº†ï¼›è‹¥ 128K+ï¼ŒåŠ  CP=2ï¼Œæ­¤æ—¶ DP=4, TP=8, CP=2ã€‚å…³é”®æ˜¯å…ˆç®—æ˜¾å­˜ï¼š70B Ã— 18 bytes(BF16+Adam) / 64 â‰ˆ 20 GB/å¡çš„å‚æ•°+ä¼˜åŒ–å™¨ï¼ŒåŠ ä¸Šæ¿€æ´»å€¼ï¼ˆå–å†³äº seq_len å’Œ micro-batchï¼‰ï¼Œé€‰æ‹©åˆé€‚çš„ activation checkpointing ç­–ç•¥ã€‚

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Megatron-LM](https://arxiv.org/abs/1909.08053) â€” Tensor Parallelism å¥ åŸºä¹‹ä½œ
- [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism](https://arxiv.org/abs/1811.06965) â€” Pipeline Parallelism ç»å…¸è®¾è®¡
- [PipeDream: Generalized Pipeline Parallelism](https://arxiv.org/abs/1806.03377) â€” 1F1B schedule çš„åŸå§‹æå‡º
- [ZeRO](https://arxiv.org/abs/1910.02054) â€” æ•°æ®å¹¶è¡Œæ˜¾å­˜ä¼˜åŒ–ä¸‰é˜¶æ®µ

### æ·±åº¦è§£è¯»
- [å¤§æ¨¡å‹å¹¶è¡Œè®­ç»ƒå…¨æ™¯ï¼ˆçŸ¥ä¹ï¼‰](https://zhuanlan.zhihu.com/p/617087561) â€” ä¸­æ–‡ç¤¾åŒºæœ€å…¨é¢çš„ DP/TP/PP/SP å¯¹æ¯” â­â­â­â­
- [LLaMA 3 Training Infrastructure](https://arxiv.org/abs/2407.21783) â€” Meta ä¸‡å¡ 4D å¹¶è¡Œçš„å·¥ç¨‹ç»†èŠ‚

### å®è·µèµ„æº
- [Megatron-Core GitHub](https://github.com/NVIDIA/Megatron-LM) â€” 3D å¹¶è¡Œå·¥ä¸šçº§å®ç°
- [PyTorch Distributed Overview](https://pytorch.org/docs/stable/distributed.html) â€” å®˜æ–¹åˆ†å¸ƒå¼è®­ç»ƒæ–‡æ¡£

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **è®­ç»ƒç­–ç•¥é€‰å‹**ï¼šæœ¬æ–‡çš„é€‰å‹æŒ‡å—å¯ç›´æ¥ç”¨äºè¯„ä¼°é¡¹ç›®éœ€è¦çš„ GPU æ•°é‡å’Œå¹¶è¡Œé…ç½®
- **é¢è¯•å‡†å¤‡**ï¼šQ1-Q5 è¦†ç›–äº†åˆ†å¸ƒå¼è®­ç»ƒ 90% çš„é¢è¯•é—®é¢˜

### å·¥ç¨‹å®ç°è¦ç‚¹
- **é€šä¿¡é‡å…¬å¼é€Ÿè®°**ï¼šDP AllReduce = $2M$ï¼ŒFSDP = $3M$ï¼ŒTP æ¯å±‚ = $4 \times B \times S \times H$ï¼ˆ$M$=æ¨¡å‹å¤§å°ï¼Œ$B$=batchï¼Œ$S$=seq_lenï¼Œ$H$=hidden_dimï¼‰
- **Bubble ratio å…¬å¼**ï¼š$\text{bubble} = \frac{P-1}{M+P-1}$ï¼Œå…¶ä¸­ $P$=pipeline stagesï¼Œ$M$=micro-batches
- **é»„é‡‘æ³•åˆ™**ï¼šèŠ‚ç‚¹å†… TPï¼ˆNVLinkï¼‰ï¼Œè·¨èŠ‚ç‚¹ FSDP/PPï¼ˆInfiniBandï¼‰ï¼ŒDP æœ€å¤–å±‚

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: ç»™ä½  128 å¼  H100ï¼Œè®­ç»ƒä¸€ä¸ª 180B æ¨¡å‹ï¼Œä½ æ€ä¹ˆé…ç½®å¹¶è¡Œç­–ç•¥ï¼Ÿ
  A: 16 èŠ‚ç‚¹ Ã— 8 GPUã€‚èŠ‚ç‚¹å†… TP=8ï¼ˆNVLink 900GB/sï¼‰ï¼›è·¨èŠ‚ç‚¹ PP=4ï¼ˆ16/4=4 ç»„ FSDPï¼‰ï¼›FSDP(ZeRO-2) DP=4ã€‚æ€» GPU = 8Ã—4Ã—4 = 128ã€‚éªŒç®—æ˜¾å­˜ï¼š180BÃ—2B(BF16)/8(TP)=45GB å‚æ•°/å¡ï¼ŒåŠ ä¼˜åŒ–å™¨åˆ†ç‰‡åå¯æ§ã€‚è‹¥åºåˆ—é•¿ï¼Œè¿½åŠ  CPã€‚

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- è¿™ç¯‡æ˜¯"åˆ†å¸ƒå¼è®­ç»ƒé€‰å‹çš„ä¸€å¼ å›¾"â€”â€”æ‰€æœ‰å¹¶è¡Œç­–ç•¥çš„ trade-offï¼ˆé€šä¿¡ vs æ˜¾å­˜ vs ç¼–ç¨‹å¤æ‚åº¦ï¼‰éƒ½åœ¨è¿™é‡Œ
- ç†è§£ 4D/5D å¹¶è¡Œæ˜¯è¯»æ‡‚ [[AI/3-LLM/Infra/Megatron-LM]]ã€[[AI/3-LLM/Infra/DeepSpeed]]ã€[[AI/3-LLM/Infra/FSDP]] çš„å‰æ

### æœªè§£é—®é¢˜ä¸å±€é™
- Zero Bubble PP çš„è®ºæ–‡ç»“æœå¾ˆæƒŠè‰³ï¼Œä½†åœ¨å¼‚æ„é›†ç¾¤ï¼ˆä¸åŒä»£ GPUï¼‰ä¸Šçš„å®é™…è¡¨ç°æœªçŸ¥
- Context Parallelism çš„æœ€ä¼˜åˆ†ç‰‡ç²’åº¦å’Œ Ring Attention çš„é€šä¿¡æ¨¡å¼æœ‰ç†è®ºæœ€ä¼˜è§£å—ï¼Ÿ

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- å¦‚æœ [[AI/3-LLM/Architecture/FlashAttention]] ç»§ç»­ä¼˜åŒ–åˆ°å‡ ä¹é›¶é¢å¤–æ˜¾å­˜ï¼ŒCP æ˜¯å¦è¿˜æœ‰å­˜åœ¨å¿…è¦ï¼Ÿ
- 5D å¹¶è¡Œ + è‡ªåŠ¨æœç´¢æœ€ä¼˜é…ç½®ï¼ˆå¦‚ Alpa arXiv:2201.12023ï¼‰èƒ½å¦è®©ç”¨æˆ·ä¸å†æ‰‹åŠ¨è°ƒ TP/PP/DPï¼Ÿ

## ç›¸å…³

> ğŸ”— See also: [[AI/3-LLM/Infra/Megatron-LM|Megatron-LM]] â€” TP/PP å·¥ä¸šçº§å®ç°
> ğŸ”— See also: [[AI/3-LLM/Infra/DeepSpeed|DeepSpeed]] â€” ZeRO ç³»åˆ—æ˜¾å­˜ä¼˜åŒ–
> ğŸ”— See also: [[AI/3-LLM/Infra/FSDP|FSDP]] â€” PyTorch åŸç”Ÿ ZeRO-3

- [[AI/3-LLM/Architecture/FlashAttention]] â€” Attention è®¡ç®—ä¼˜åŒ–
- [[AI/3-LLM/Architecture/Transformer ä½ç½®ç¼–ç ]] â€” åºåˆ—å¹¶è¡Œç›¸å…³
- [[AI/3-LLM/Pretraining/å°è§„æ¨¡è®­ç»ƒæ‰‹å†Œ]] â€” å°æ¨¡å‹è®­ç»ƒå®è·µ
