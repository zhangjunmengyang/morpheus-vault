---
title: Megatron-LM
brief: NVIDIA çš„å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œæ ¸å¿ƒè´¡çŒ®æ˜¯é«˜æ•ˆ Tensor Parallelism åŠä¸ Pipeline/Data Parallelism çš„ 3D ç»„åˆï¼›è®­ç»ƒ 70B+ æ¨¡å‹æ—¶ TP+PP åŸºæœ¬æ˜¯å·¥ä¸šæ ‡é…ã€‚
type: concept
domain: ai/llm/infra
created: 2026-02-13
updated: 2026-02-22
tags:
  - ai/llm/infra
  - type/concept
  - interview/hot
status: complete
sources:
  - "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism â€” arXiv:1909.08053"
  - Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM â€” arXiv:2104.04473
  - https://github.com/NVIDIA/Megatron-LM
related:
  - "[[DeepSpeed|DeepSpeed]]"
  - "[[FSDP|FSDP]]"
  - "[[æ¨¡å‹å¹¶è¡Œç­–ç•¥|æ¨¡å‹å¹¶è¡Œç­–ç•¥]]"
---
# Megatron-LM

> NVIDIA å‡ºå“çš„å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹è®­ç»ƒæ¡†æ¶ï¼Œæ ¸å¿ƒå–ç‚¹æ˜¯**é«˜æ•ˆçš„æ¨¡å‹å¹¶è¡Œ**ã€‚

å®˜æ–¹ä»“åº“ï¼šhttps://github.com/NVIDIA/Megatron-LM

## ä¸ºä»€ä¹ˆéœ€è¦ Megatron-LM

å•å¡æ”¾ä¸ä¸‹å¤§æ¨¡å‹ï¼Œè¿™æ˜¯æœ€æœ´ç´ çš„åŠ¨æœºã€‚PyTorch åŸç”Ÿçš„ `DistributedDataParallel` åªè§£å†³äº†æ•°æ®å¹¶è¡Œï¼Œæ¨¡å‹æœ¬èº«å¾—å¡è¿›ä¸€å¼ å¡ã€‚å½“å‚æ•°é‡åˆ° 7B ä»¥ä¸Šï¼Œfp16 ä¸‹å…‰æƒé‡å°±è¦ 14GBï¼ŒåŠ ä¸Š optimizer statesã€activationsï¼Œä¸€å¼  80GB A100 éƒ½æ‰è¥Ÿè§è‚˜ã€‚

Megatron-LM çš„æ ¸å¿ƒè´¡çŒ®ï¼š**æŠŠ Tensor Parallelism åšåˆ°äº†æè‡´**ï¼Œå¹¶ä¸”å’Œ Pipeline Parallelismã€Data Parallelism æ— ç¼ç»„åˆã€‚

> æ¥æºï¼šShoeybi et al., "Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism" arXiv:1909.08053

## ä¸‰ç»´å¹¶è¡Œ (3D Parallelism)

```mermaid
graph TD
    subgraph DP["Data Parallelism (DP)"]
        subgraph R0["Replica 0"]
            S0R0["Stage 0<br/>TP=2"]
            S1R0["Stage 1<br/>TP=2"]
            S0R0 -->|PP| S1R0
        end
        subgraph R1["Replica 1"]
            S0R1["Stage 0<br/>TP=2"]
            S1R1["Stage 1<br/>TP=2"]
            S0R1 -->|PP| S1R1
        end
    end
    R0 -.->|AllReduce| R1
```
> å›¾ï¼šMegatron-LM 3D å¹¶è¡Œç¤ºæ„â€”â€”DP å±‚åŒ…è£¹ PPï¼ˆstage åˆ†å±‚ï¼‰ï¼Œæ¯ä¸ª stage å†…éƒ¨åš TP åˆ‡åˆ†

### Tensor Parallelism (TP)

å°†å•ä¸ª Transformer å±‚å†…çš„çŸ©é˜µè¿ç®—åˆ‡åˆ†åˆ°å¤šå¡ã€‚ä»¥ MLP ä¸ºä¾‹ï¼š

```python
# åŸå§‹: Y = GeLU(XA) * B
# TP åˆ‡åˆ†: A æŒ‰åˆ—åˆ‡, B æŒ‰è¡Œåˆ‡
# GPU 0: Y_0 = GeLU(X @ A_0) @ B_0
# GPU 1: Y_1 = GeLU(X @ A_1) @ B_1
# AllReduce: Y = Y_0 + Y_1
```

å…³é”®è®¾è®¡ï¼šæ¯ä¸ª TP ç»„å†…åªéœ€è¦ **2 æ¬¡ AllReduce**ï¼ˆforward ä¸€æ¬¡ï¼Œbackward ä¸€æ¬¡ï¼‰ï¼Œé€šä¿¡é‡å’Œå•å±‚å‚æ•°é‡æˆæ­£æ¯”ã€‚TP é€‚åˆæ”¾åœ¨ **NVLink è¿æ¥çš„åŒæœºå¡é—´**ï¼Œå› ä¸ºé€šä¿¡å¯†é›†ã€‚

> æ¥æºï¼šShoeybi et al. arXiv:1909.08053, Sec. 3 â€” MLP åˆ—åˆ‡åˆ†/è¡Œåˆ‡åˆ†çš„é€šä¿¡åˆ†æ

### Pipeline Parallelism (PP)

å°†æ¨¡å‹æŒ‰å±‚åˆ†æˆå¤šä¸ª stageï¼Œä¸åŒ stage æ”¾åœ¨ä¸åŒèŠ‚ç‚¹ã€‚æœ´ç´ çš„ PP ä¼šæœ‰ä¸¥é‡çš„ bubbleï¼ˆç©ºé—²æ—¶é—´ï¼‰ï¼ŒMegatron ç”¨äº† **1F1B schedule**ï¼ˆinterleavedï¼‰æ¥å‡å°‘ bubbleï¼š

```
# 4 micro-batches, 2 stages
# æœ´ç´ :  [F0 F1 F2 F3] [B3 B2 B1 B0]  -- å¤§é‡ bubble
# 1F1B:  [F0 F1] [F2 B0] [F3 B1] [B2 B3]  -- bubble æ˜¾è‘—å‡å°‘
```

PP é€‚åˆæ”¾åœ¨ **è·¨èŠ‚ç‚¹** åœºæ™¯ï¼Œå› ä¸ºåªéœ€è¦åœ¨ stage è¾¹ç•Œä¼ é€’ activationsï¼Œé€šä¿¡é‡ç›¸å¯¹å°ã€‚

> æ¥æºï¼šNarayanan et al. arXiv:2104.04473, Sec. 2.3 â€” Interleaved 1F1B schedule çš„ bubble åˆ†æ

### Data Parallelism (DP)

æœ€å¤–å±‚åŒ…ä¸€åœˆ DPã€‚Megatron åŒæ—¶æ”¯æŒä¼ ç»Ÿ DP å’Œ ZeRO-style çš„åˆ†å¸ƒå¼ä¼˜åŒ–å™¨ï¼ˆç±»ä¼¼ DeepSpeed ZeRO-1ï¼‰ã€‚

## æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | åŠŸèƒ½ |
|------|------|
| `megatron/core/transformer` | Transformer å±‚å®ç°ï¼Œå†…å»º TP æ”¯æŒ |
| `megatron/core/pipeline_parallel` | PP scheduleï¼ˆ1F1B, interleavedï¼‰ |
| `megatron/core/distributed` | åˆ†å¸ƒå¼é€šä¿¡ã€æ¢¯åº¦ AllReduce |
| `megatron/core/optimizer` | åˆ†å¸ƒå¼ä¼˜åŒ–å™¨ |
| `megatron/training` | è®­ç»ƒä¸»å¾ªç¯ã€checkpoint ç®¡ç† |

## Megatron-Core vs Megatron-LM

ä» v0.5 å¼€å§‹ï¼ŒNVIDIA æŠŠæ ¸å¿ƒå¹¶è¡Œé€»è¾‘æŠ½æˆäº† **Megatron-Core**ï¼ˆ`megatron/core/`ï¼‰ï¼Œå¯ä»¥ç‹¬ç«‹å®‰è£…ä½¿ç”¨ã€‚ä¸Šå±‚çš„ Megatron-LM æ˜¯åŸºäºå®ƒçš„å®Œæ•´è®­ç»ƒæ–¹æ¡ˆã€‚å¾ˆå¤šæ¡†æ¶ï¼ˆNeMoã€verlï¼‰ç›´æ¥ä¾èµ– Megatron-Coreã€‚

## ä¸ DeepSpeed çš„å¯¹æ¯”

| ç»´åº¦ | Megatron-LM | [[DeepSpeed]] |
|------|-------------|---------------|
| æ ¸å¿ƒä¼˜åŠ¿ | TP æè‡´ä¼˜åŒ– + PP è°ƒåº¦ | ZeRO ç³»åˆ—çœæ˜¾å­˜ï¼ŒAPI å‹å¥½ |
| TP æ”¯æŒ | åŸç”Ÿå†…å»º | éœ€é¢å¤–é…ç½® |
| ä¸Šæ‰‹é—¨æ§› | é«˜ | ä¸­ |
| å®è·µç»„åˆ | ç»å¸¸ä¸ DeepSpeed æ··ç”¨ï¼ˆMegatron-DeepSpeedï¼‰ | â€” |

æˆ‘çš„è§‚ç‚¹ï¼š**è®­ç»ƒ 70B+ æ¨¡å‹ï¼ŒMegatron çš„ TP + PP åŸºæœ¬æ˜¯æ ‡é…**ã€‚[[DeepSpeed]] ZeRO-3 è™½ç„¶ä¹Ÿèƒ½è·‘ï¼Œä½†é€šä¿¡æ•ˆç‡åœ¨å¤§è§„æ¨¡é›†ç¾¤ä¸Šä¸å¦‚ Megatron çš„æ‰‹åŠ¨å¹¶è¡Œã€‚å°æ¨¡å‹ï¼ˆ< 13Bï¼‰ç›´æ¥ç”¨ DeepSpeed ZeRO-2/3 æ›´çœå¿ƒã€‚

## è¸©å‘è®°å½•

1. **TP size å¿…é¡»æ•´é™¤ attention heads æ•°é‡**ï¼šæ¯”å¦‚ 32 heads åªèƒ½ç”¨ TP=1/2/4/8/16/32
2. **PP çš„ num_layers å¿…é¡»èƒ½è¢« PP size æ•´é™¤**ï¼šå¦åˆ™ç›´æ¥æŠ¥é”™
3. **æ··åˆç²¾åº¦**ï¼šMegatron é»˜è®¤ç”¨è‡ªå·±çš„ `Float16Module`ï¼Œå’Œ PyTorch AMP ä¸æ˜¯ä¸€å›äº‹
4. **Checkpoint æ ¼å¼**ï¼šMegatron çš„ checkpoint å’Œ HuggingFace æ ¼å¼ä¸é€šç”¨ï¼Œéœ€è¦è½¬æ¢è„šæœ¬

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) â€” å¥ åŸºä¹‹ä½œï¼ŒTP çš„åˆ—åˆ‡åˆ†/è¡Œåˆ‡åˆ†è®¾è®¡
- [Efficient Large-Scale Language Model Training on GPU Clusters](https://arxiv.org/abs/2104.04473) â€” Megatron-LM v2ï¼Œ3D å¹¶è¡Œ + interleaved 1F1B schedule

### æ·±åº¦è§£è¯»
- [Megatron-LM æºç è§£è¯»ç³»åˆ—ï¼ˆçŸ¥ä¹ï¼‰](https://zhuanlan.zhihu.com/p/622212228) â€” ä¸­æ–‡ç¤¾åŒºæœ€è¯¦ç»†çš„ä»£ç èµ°è¯» â­â­â­â­
- [NVIDIA Megatron-Core å®˜æ–¹æ–‡æ¡£](https://docs.nvidia.com/megatron-core/developer-guide/latest/index.html) â€” æ¨¡å—åŒ– API å‚è€ƒ

### å®è·µèµ„æº
- [NVIDIA/Megatron-LM GitHub](https://github.com/NVIDIA/Megatron-LM) â€” å®˜æ–¹ä»“åº“ï¼Œå«é¢„è®­ç»ƒè„šæœ¬å’Œæ¨¡å‹è½¬æ¢å·¥å…·
- [NeMo Framework](https://github.com/NVIDIA/NeMo) â€” åŸºäº Megatron-Core çš„ä¸Šå±‚è®­ç»ƒæ¡†æ¶

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **70B+ æ¨¡å‹é¢„è®­ç»ƒ**ï¼šMegatron TP+PP æ˜¯å¤§è§„æ¨¡é›†ç¾¤è®­ç»ƒçš„æ ‡é…æ–¹æ¡ˆ
- **RLHF/RL è®­ç»ƒåç«¯**ï¼š[[è®­ç»ƒåç«¯|verl]] å’Œ NeMo-Aligner ç›´æ¥è°ƒç”¨ Megatron-Core åšæ¨¡å‹å¹¶è¡Œ

### å·¥ç¨‹å®ç°è¦ç‚¹
- **TP size é€‰æ‹©**ï¼šå¿…é¡»æ•´é™¤ attention heads æ•°ï¼Œé€šå¸¸ TP=8ï¼ˆå•èŠ‚ç‚¹ NVLinkï¼‰
- **PP + 1F1B**ï¼šmicro-batch æ•°é‡ $M$ è¶Šå¤§ bubble ratio $\frac{P-1}{M+P-1}$ è¶Šå°ï¼Œå»ºè®® $M \geq 4P$
- **Checkpoint è½¬æ¢**ï¼šMegatron â†” HuggingFace æ ¼å¼éœ€è¦ä¸“ç”¨è„šæœ¬ï¼Œä¸å¯ç›´æ¥äº’é€š

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: Megatron-LM çš„ Tensor Parallelism æ˜¯æ€ä¹ˆåˆ‡åˆ† MLP å’Œ Attention çš„ï¼Ÿ
  A: MLP çš„ç¬¬ä¸€ä¸ªçº¿æ€§å±‚æŒ‰åˆ—åˆ‡åˆ†ï¼ˆ$A = [A_1 | A_2]$ï¼‰ï¼Œç¬¬äºŒä¸ªæŒ‰è¡Œåˆ‡åˆ†ï¼›Attention æŒ‰ head åˆ‡åˆ†ï¼Œå¤©ç„¶é€‚åˆ TPã€‚æ¯å±‚åªéœ€ 2 æ¬¡ AllReduceï¼ˆforward + backwardï¼‰ã€‚

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- Megatron-Core å·²æˆä¸º LLM è®­ç»ƒåŸºç¡€è®¾æ–½çš„"æ ‡å‡†ä»¶"â€”â€”verlã€NeMoã€OpenRLHF éƒ½ä¾èµ–å®ƒï¼›ç†è§£ TP/PP çš„é€šä¿¡æ¨¡å¼æ˜¯ä¼˜åŒ–è®­ç»ƒååçš„å‰æ
- è®­ç»ƒæˆæœ¬ä¼˜åŒ–çš„å…³é”®åœ¨äº **TP/PP/DP ä¸‰ç»´æ¯”ä¾‹çš„è°ƒä¼˜**ï¼Œè€Œéç®€å•å †å¡

### æœªè§£é—®é¢˜ä¸å±€é™
- Megatron çš„ PP schedule åœ¨å¼‚æ„ç¡¬ä»¶ï¼ˆä¸åŒä»£ GPU æ··éƒ¨ï¼‰ä¸Šè°ƒåº¦æ•ˆç‡æœªéªŒè¯
- FSDP2 + torch.compile æ­£åœ¨è¿½èµ¶ Megatron çš„æ€§èƒ½ï¼ŒPyTorch åŸç”Ÿæ–¹æ¡ˆæ˜¯å¦ä¼šå–ä»£ Megatron æ˜¯å¼€æ”¾é—®é¢˜

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- å¦‚æœæŠŠ [[æ¨¡å‹å¹¶è¡Œç­–ç•¥|æ¨¡å‹å¹¶è¡Œç­–ç•¥]] çš„ Zero Bubble PP å’Œ Megatron ç»“åˆï¼Œèƒ½å¦è¿›ä¸€æ­¥å‹ç¼©ä¸‡å¡è®­ç»ƒçš„ bubbleï¼Ÿ
- Megatron-Core ä½œä¸º"å¹¶è¡ŒåŸè¯­åº“"ï¼Œèƒ½å¦æ ‡å‡†åŒ–ä¸ºç±»ä¼¼ NCCL çš„è¡Œä¸šåŸºç¡€å±‚ï¼Ÿ

## ç›¸å…³

> ğŸ”— See also: [[DeepSpeed|DeepSpeed]] â€” ZeRO ç³»æ˜¾å­˜ä¼˜åŒ–ï¼Œä¸ Megatron TP/PP äº’è¡¥
> ğŸ”— See also: [[FSDP|FSDP]] â€” PyTorch åŸç”Ÿç«å“æ–¹æ¡ˆï¼ˆZeRO-3 ç­‰ä»·ï¼‰
> ğŸ”— See also: [[æ¨¡å‹å¹¶è¡Œç­–ç•¥|æ¨¡å‹å¹¶è¡Œç­–ç•¥]] â€” DP/TP/PP/SP/CP å…¨æ™¯å¯¹æ¯”

- [[åˆ†å¸ƒå¼è®­ç»ƒ|åˆ†å¸ƒå¼è®­ç»ƒ]] â€” å¹¶è¡Œç­–ç•¥å…¨æ™¯
- [[Ray|Ray]] â€” å¦ä¸€ä¸ªåˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶
- [[è®­ç»ƒåç«¯|verl è®­ç»ƒåç«¯]] â€” verl ä¸­ä½¿ç”¨ Megatron åç«¯
- [[HybridFlow|HybridFlow]] â€” verl çš„æ··åˆå¹¶è¡Œç¼–æ’
- [[verl æ¦‚è¿°|verl æ¦‚è¿°]]
- [[TRL æ¦‚è¿°|TRL æ¦‚è¿°]]
- [[OpenRLHF|OpenRLHF]]
