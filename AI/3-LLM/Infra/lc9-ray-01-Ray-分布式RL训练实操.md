---
title: "Ray åˆ†å¸ƒå¼ RL è®­ç»ƒç³»ç»Ÿå®æ“"
brief: "åŸºäºRayå®ç°Generator-Coordinator-Trainerä¸‰è§’æ¶æ„çš„åˆ†å¸ƒå¼RLè®­ç»ƒç³»ç»Ÿï¼šè®­æ¨åˆ†ç¦»è®¾è®¡ã€Remote Actoré€šä¿¡ã€GRPO rewardè®¡ç®—ã€ç»éªŒå›æ”¾ï¼Œæ˜¯ç†è§£OpenRLHF/verlç­‰æ¡†æ¶æ ¸å¿ƒè®¾è®¡çš„æ•™å­¦å®ç°ï¼Œæ¥æº MA-RLHF æ•™å­¦é¡¹ç›®ã€‚"
date: 2026-02-25
type: code-practice
source: "MA-RLHF (https://github.com/dhcode-cpp/MA-RLHF)"
tags: [code-practice, ray, distributed-rl, rlhf, grpo, training-infra]
related:
  - "[[AI/3-LLM/Infra/Ray-æ¨ç†ç³»ç»Ÿå®æ“|Ray-æ¨ç†ç³»ç»Ÿå®æ“]]"
  - "[[AI/3-LLM/RL/GRPO/GRPO-æ‰‹æ’•å®æ“|GRPO-æ‰‹æ’•å®æ“]]"
  - "[[AI/3-LLM/RL/PPO/PPO-æ‰‹æ’•å®æ“-MA-RLHF|PPO-æ‰‹æ’•å®æ“-MA-RLHF]]"
  - "[[AI/3-LLM/Infra/ZeRO-æ‰‹æ’•å®æ“|ZeRO-æ‰‹æ’•å®æ“]]"
---

# Ray åˆ†å¸ƒå¼ RL è®­ç»ƒç³»ç»Ÿæ‰‹æ’•å®æ“

> æ¥æºï¼šMA-RLHF (<https://github.com/dhcode-cpp/MA-RLHF>)  
> è·¯å¾„ï¼š`lecture/lc9_training/ray-train/`  
> å…¥åº“æ—¥æœŸï¼š2026-02-25  
> çŠ¶æ€ï¼šæ•™å­¦çº§ä»£ç ï¼ˆAI ç”Ÿæˆï¼Œè°ƒè¯•ä¸­ï¼‰ï¼Œç”¨äºç†è§£ OpenRLHF / verl ç­‰æ¡†æ¶çš„æ ¸å¿ƒè®¾è®¡

---

## ä¸€ã€ç³»ç»Ÿæ¶æ„æ¦‚è§ˆ

æœ¬é¡¹ç›®å®ç°äº†ä¸€ä¸ª **Generator-Coordinator-Trainer ä¸‰è§’æ¶æ„**çš„åˆ†å¸ƒå¼ RL è®­ç»ƒç³»ç»Ÿï¼Œæ ¸å¿ƒæ€è·¯æ˜¯**è®­æ¨åˆ†ç¦»**ï¼ˆTraining-Inference Separationï¼‰ï¼š

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Coordinator    â”‚
                    â”‚  (è°ƒåº¦ & ç›‘æ§)    â”‚
                    â””â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
           å¯åŠ¨/ç›‘æ§    â”‚           â”‚   å¯åŠ¨/ç›‘æ§
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Generator Actor â”‚â”€â”€â”€â”€â”€â”€â–¶   â”‚  Trainer Actor   â”‚
    â”‚  (rollout æ¨ç†)  â”‚  å‘é€    â”‚  (ç­–ç•¥æ›´æ–°)       â”‚
    â”‚  GPU:1 (vLLM)   â”‚  æ ·æœ¬    â”‚  GPU:0 (PyTorch) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â–²                             â”‚
              â”‚         å‚æ•°åŒæ­¥             â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   (pull-based æ‹‰å–)
```

**æ•°æ®æµ**ï¼š
1. **Coordinator** å¯åŠ¨è®­ç»ƒå¾ªç¯å’Œç”Ÿæˆå¾ªç¯
2. **Generator** ç”¨æœ¬åœ°æ¨¡å‹å‰¯æœ¬åš rollout æ¨ç†ï¼Œç”Ÿæˆ `(prompt, response)` å¯¹
3. Generator å°†æ ·æœ¬ **push** åˆ° Trainer çš„è®­ç»ƒé˜Ÿåˆ—
4. **Trainer** ä»é˜Ÿåˆ—é‡‡æ ·ï¼Œæ‰§è¡Œ PPO/SFT è®­ç»ƒæ­¥éª¤
5. Generator å®šæœŸä» Trainer **pull** æœ€æ–°å‚æ•°ï¼Œæ›´æ–°æœ¬åœ°æ¨¡å‹
6. å¾ªç¯å¾€å¤ï¼Œç›´åˆ°è¾¾åˆ°é¢„è®¾è¿è¡Œæ—¶é—´

**å…³é”®è®¾è®¡å†³ç­–**ï¼š
- å¤šä¸ª Generator å…±äº«ä¸€ä¸ª Trainerï¼ˆN:1 æ¶æ„ï¼‰
- Generator å’Œ Trainer å„å  0.5 GPUï¼ˆ`@ray.remote(num_gpus=0.5)`ï¼‰
- å‚æ•°åŒæ­¥æ˜¯ **pull-based**ï¼šGenerator ä¸»åŠ¨ä» Trainer æ‹‰å–ï¼Œè€Œé Trainer æ¨é€
- è®­ç»ƒé˜Ÿåˆ—æœ‰å®¹é‡ä¸Šé™ï¼ˆdeque maxlenï¼‰ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º

---

## äºŒã€æ ¸å¿ƒç»„ä»¶å®ç°

### 2.1 é…ç½®ï¼ˆConfigï¼‰

`config.py` å½“å‰ä¸ºç©ºæ–‡ä»¶ï¼Œä½†ä»£ç ä¸­å¼•ç”¨äº†ä¸‰ä¸ªé…ç½®å­—å…¸ï¼š

| é…ç½® | ç”¨é€” | å…³é”®å­—æ®µ |
|------|------|---------|
| `GENERATION_CONFIG` | ç”Ÿæˆå‚æ•° | `max_tokens`, `temperature`, `generation_batch_size`, `update_frequency` |
| `TRAIN_CONFIG` | è®­ç»ƒå‚æ•° | `learning_rate`, `batch_size`, `queue_size`, `grad_clip`, `clean_threshold`, `clean_keep` |
| `SYSTEM_CONFIG` | ç³»ç»Ÿå‚æ•° | `num_generators`, `train_interval`, `generate_interval`, `runtime_seconds`, `status_interval` |

> ğŸ’¡ å®é™…æ¡†æ¶ï¼ˆå¦‚ verlï¼‰é€šå¸¸ç”¨ Hydra/OmegaConf ç®¡ç†é…ç½®ï¼Œè¿™é‡Œç®€åŒ–ä¸º dictã€‚

### 2.2 æ¨¡å‹å®šä¹‰ï¼ˆModelsï¼‰

`models.py` å½“å‰ä¸ºç©ºï¼Œä½†ä»£ç ä¸­ä½¿ç”¨äº† `SharedLanguageModel` ç±»ï¼Œé¢„æœŸæ¥å£ï¼š

```python
class SharedLanguageModel(nn.Module):
    def forward(self, input_ids) -> logits       # è®­ç»ƒå‰å‘
    def generate(self, input_ids, max_length, temperature) -> token_ids  # æ¨ç†ç”Ÿæˆ
    def state_dict() / load_state_dict()          # å‚æ•°åŒæ­¥æ¥å£
```

> åœ¨çœŸå®ç³»ç»Ÿä¸­ï¼Œè¿™é‡Œä¼šæ¥å…¥ HuggingFace model + vLLM engineï¼Œæ¨¡å‹çš„ `forward` å’Œ `generate` åˆ†åˆ«ç”¨äºè®­ç»ƒå’Œæ¨ç†ã€‚

### 2.3 æ•°æ®å·¥å…·ï¼ˆDataUtilsï¼‰

ä¸¤ä¸ªæ ¸å¿ƒç±»ï¼š

**`DataProcessor`** â€” æ•°æ®å¤„ç†å™¨ï¼š

```python
@dataclass
class TrainingSample:
    prompt: str
    generated: str
    input_ids: torch.Tensor    # è¾“å…¥ token ids
    labels: torch.Tensor       # ç›®æ ‡ token idsï¼ˆå³ç§»ä¸€ä½ï¼‰
    generator_id: str          # æ¥è‡ªå“ªä¸ª Generator
    timestamp: float

class DataProcessor:
    def simulate_tokenize(self, text, max_length=50) -> List[int]
    def create_training_samples(self, prompts, generated_texts, generator_id) -> List[TrainingSample]
    def batch_samples(self, samples, batch_size)  # yield åˆ†æ‰¹
```

å…³é”®é€»è¾‘â€”â€”æ ·æœ¬æ„å»ºæ˜¯ç»å…¸ LM æ–¹å¼ï¼š
```python
combined = prompt + " " + generated
token_ids = tokenize(combined)
input_ids = token_ids[:-1]   # è¾“å…¥
labels    = token_ids[1:]    # ç›®æ ‡ï¼ˆå³ç§»ä¸€ä½ï¼‰
```

**`PromptManager`** â€” æç¤ºè¯ç®¡ç†å™¨ï¼šä»æ–‡ä»¶åŠ è½½ promptsï¼Œæ‰¾ä¸åˆ°æ–‡ä»¶æ—¶å›é€€åˆ° 20 æ¡ç¡¬ç¼–ç ç¤ºä¾‹ã€‚

### 2.4 Generator Actorï¼ˆrollout ç”Ÿæˆï¼‰

Generator æ˜¯ Ray Actorï¼Œè´Ÿè´£ç”¨å½“å‰ç­–ç•¥ç”Ÿæˆ rollout æ ·æœ¬ï¼š

```python
@ray.remote(num_gpus=0.5)
class GeneratorActor:
    def __init__(self, generator_id, trainer_actor, device_id=1):
        self.local_model = SharedLanguageModel().to(device)
        self.trainer_actor = trainer_actor  # æŒæœ‰ Trainer çš„å¼•ç”¨
        self._update_from_trainer()         # åˆå§‹åŒ–æ—¶æ‹‰å–å‚æ•°
```

**æ ¸å¿ƒæ–¹æ³• `generate_and_send`**ï¼š

```python
def generate_and_send(self, prompts, batch_size):
    for i in range(0, len(prompts), batch_size):
        batch_prompts = prompts[i:i+batch_size]

        # 1. ç”¨æœ¬åœ°æ¨¡å‹æ¨ç†ç”Ÿæˆ
        generated_texts = self.generate_with_vllm(batch_prompts)

        # 2. æ„é€ è®­ç»ƒæ ·æœ¬
        batch_samples = self.prepare_training_data(batch_prompts, generated_texts)

        # 3. æ¨é€åˆ° Trainer é˜Ÿåˆ—ï¼ˆåŒæ­¥ RPCï¼‰
        success = ray.get(
            self.trainer_actor.receive_generated_data.remote(batch_samples)
        )

        # 4. å®šæœŸä» Trainer æ‹‰å–æœ€æ–°å‚æ•°
        if (i // batch_size) % self.config["update_frequency"] == 0:
            self._update_from_trainer()
```

**å‚æ•°åŒæ­¥ï¼ˆpull-basedï¼‰**ï¼š

```python
def _update_from_trainer(self):
    params_info = ray.get(self.trainer_actor.get_current_params.remote())
    self.local_model.load_state_dict(params_info["params"])
    self.params_version = params_info["version"]
```

> ğŸ”‘ å‚æ•°é€šè¿‡ `state_dict()` åºåˆ—åŒ–ä¸º CPU tensor ä¼ è¾“ï¼ŒGenerator æ”¶åˆ°å `load_state_dict` + `.to(device)`ã€‚è¿™æ˜¯æœ€ç›´è§‚çš„åŒæ­¥æ–¹å¼ï¼Œä½†å¯¹å¤§æ¨¡å‹æ¥è¯´å¸¦å®½å¼€é”€å·¨å¤§â€”â€”å®é™…æ¡†æ¶ä¼šç”¨ NCCL broadcast æˆ–å…±äº«å†…å­˜ã€‚

### 2.5 Trainer Actorï¼ˆç­–ç•¥æ›´æ–°ï¼‰

Trainer æ˜¯å”¯ä¸€çš„è®­ç»ƒèŠ‚ç‚¹ï¼Œç»´æŠ¤æ¨¡å‹æƒé‡çš„ ground truthï¼š

```python
@ray.remote(num_gpus=0.5)
class TrainerActor:
    def __init__(self):
        self.model = SharedLanguageModel().to(device)
        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=lr)
        self.training_queue = deque(maxlen=config["queue_size"])  # æœ‰ç•Œé˜Ÿåˆ—
        self.params_version = 0   # ç‰ˆæœ¬å·ï¼Œæ¯æ¬¡è®­ç»ƒæ­¥ +1
```

**æ•°æ®æ¥æ”¶**â€”â€”Generator push è¿‡æ¥çš„æ ·æœ¬è¿›å…¥è®­ç»ƒé˜Ÿåˆ—ï¼š

```python
def receive_generated_data(self, batch_data):
    for data in batch_data:
        sample = TrainingSample(**data)
        self.training_queue.append(sample)   # deque è‡ªåŠ¨æ·˜æ±°æœ€æ—§æ ·æœ¬
    return True
```

**è®­ç»ƒæ­¥éª¤** `train_step`â€”â€”æ ¸å¿ƒè®­ç»ƒå¾ªç¯ï¼š

```python
def train_step(self, batch_size):
    if len(self.training_queue) < batch_size:
        return {"status": "waiting"}   # é˜Ÿåˆ—ä¸å¤Ÿï¼Œç­‰å¾…

    # éšæœºé‡‡æ ·ï¼ˆéé¡ºåºæ¶ˆè´¹ï¼ï¼‰
    indices = np.random.choice(len(self.training_queue), batch_size, replace=False)
    batch_samples = [self.training_queue[i] for i in indices]

    # padding åˆ°ç»Ÿä¸€é•¿åº¦
    max_len = max(t.size(0) for t in input_tensors)
    # ... pad with 0 for inputs, -100 for labels

    # æ ‡å‡†è®­ç»ƒæ­¥éª¤
    logits = self.model(input_tensor)
    loss = CrossEntropyLoss(logits.view(-1, V), labels.view(-1))
    loss.backward()
    clip_grad_norm_(self.model.parameters(), grad_clip)
    self.optimizer.step()

    self.params_version += 1  # ç‰ˆæœ¬å·é€’å¢
```

**é˜Ÿåˆ—æ¸…ç†**â€”â€”æ¯ 100 æ­¥è§¦å‘ä¸€æ¬¡ï¼š

```python
def _clean_queue(self):
    if len(self.training_queue) > clean_threshold:
        keep_indices = np.random.choice(current_size, clean_keep, replace=False)
        self.training_queue = deque([...keep...], maxlen=queue_size)
```

**å‚æ•°å¯¼å‡º**â€”â€”ä¾› Generator æ‹‰å–ï¼š

```python
def get_current_params(self):
    params_cpu = {k: v.cpu().clone() for k, v in self.model.state_dict().items()}
    return {"params": params_cpu, "version": self.params_version}
```

### 2.6 Coordinatorï¼ˆè°ƒåº¦å™¨ï¼‰

Coordinator æ˜¯ç³»ç»Ÿçš„"å¤§è„‘"ï¼Œè´Ÿè´£åˆ›å»º Actor å¹¶å¯åŠ¨å¾ªç¯ï¼š

```python
@ray.remote
class SystemCoordinator:
    def __init__(self, num_generators):
        # 1. åˆ›å»ºå”¯ä¸€çš„ Trainer
        self.trainer = TrainerActor.remote()

        # 2. åˆ›å»º N ä¸ª Generatorï¼Œæ¯ä¸ªæŒæœ‰ Trainer å¼•ç”¨
        self.generators = []
        for i in range(num_generators):
            gen = GeneratorActor.remote(
                generator_id=f"Generator-{i+1}",
                trainer_actor=self.trainer,
                device_id=i % 2   # è½®æµåˆ†é… GPU
            )
            self.generators.append(gen)
```

**è®­ç»ƒå¾ªç¯**â€”â€”å¯åŠ¨ä¸€ä¸ª Ray remote function ä½œä¸ºåå° workerï¼š

```python
def start_training_loop(self, interval):
    @ray.remote
    def training_worker(trainer, interval):
        while True:
            result = ray.get(trainer.train_step.remote())
            if result["status"] == "success" and result["step"] % 10 == 0:
                print(f"è®­ç»ƒæ­¥éª¤ {result['step']}: loss={result['loss']:.4f}")
            time.sleep(interval)

    self.training_future = training_worker.remote(self.trainer, interval)
```

**ç”Ÿæˆå¾ªç¯**â€”â€”æ¯ä¸ª Generator å„å¯åŠ¨ä¸€ä¸ªåå° workerï¼Œç‹¬ç«‹è½®è¯¢ promptsï¼š

```python
def start_generation_loop(self, prompts_file, interval):
    prompts = PromptManager.load_prompts(prompts_file)

    @ray.remote
    def generation_worker(generator, prompts, interval, worker_id):
        idx = 0
        while True:
            batch_prompts = prompts[idx:idx+2]
            result = ray.get(generator.generate_and_send.remote(batch_prompts))
            idx = (idx + 2) % len(prompts)
            time.sleep(interval + random_jitter)

    for i, gen in enumerate(self.generators):
        self.generation_futures.append(
            generation_worker.remote(gen, prompts, interval, i)
        )
```

**çŠ¶æ€ç›‘æ§**â€”â€”`get_system_status` èšåˆæ‰€æœ‰ Actor çŠ¶æ€ï¼›`stop_system` ä¼˜é›…åœæœºå¹¶æ‰“å°æœ€ç»ˆç»Ÿè®¡ã€‚

### 2.7 ä¸»æµç¨‹ï¼ˆMainï¼‰

`main.py` æ˜¯å…¥å£ï¼Œæµç¨‹éå¸¸æ¸…æ™°ï¼š

```python
def main():
    # 1. åˆå§‹åŒ– Rayï¼ˆå£°æ˜ 2 GPUï¼‰
    ray.init(num_gpus=2)

    # 2. åˆ›å»º Coordinatorï¼ˆä¼šè‡ªåŠ¨åˆ›å»º Trainer + Generatorsï¼‰
    coordinator = SystemCoordinator.remote(num_generators=N)

    # 3. å¯åŠ¨è®­ç»ƒå¾ªç¯
    ray.get(coordinator.start_training_loop.remote(interval=train_interval))

    # 4. å¯åŠ¨ç”Ÿæˆå¾ªç¯
    ray.get(coordinator.start_generation_loop.remote(prompts_file="prompts.txt"))

    # 5. è¿è¡ŒæŒ‡å®šæ—¶é—´ï¼Œå®šæœŸæ‰“å°çŠ¶æ€
    for i in range(runtime_seconds):
        if i % status_interval == 0:
            status = ray.get(coordinator.get_system_status.remote())
            print(f"æ­¥éª¤={status['trainer']['training_step']}, "
                  f"é˜Ÿåˆ—={status['trainer']['queue_size']}, "
                  f"loss={status['trainer']['avg_loss']:.4f}")
        time.sleep(1)

    # 6. åœæ­¢ & æ¸…ç†
    ray.get(coordinator.stop_system.remote())
    ray.shutdown()
```

> æ•´ä¸ªç³»ç»Ÿçš„å¯åŠ¨é¡ºåºï¼šRay init â†’ Coordinator â†’ Trainer â†’ Generators â†’ è®­ç»ƒå¾ªç¯ â†’ ç”Ÿæˆå¾ªç¯ â†’ ç›‘æ§å¾ªç¯ â†’ åœæœºã€‚

---

## ä¸‰ã€ä¸ verl / OpenRLHF æ¡†æ¶å¯¹æ¯”

| ç»´åº¦ | æœ¬é¡¹ç›®ï¼ˆray-train æ•™å­¦ç‰ˆï¼‰ | verl (APRIL) | OpenRLHF |
|------|--------------------------|-------------|----------|
| **æ¶æ„** | Generator-Coordinator-Trainer ä¸‰è§’ | Worker Groupï¼ˆActor/Critic/Ref/Rewardï¼‰ | Ray-based Actor å¤šè§’è‰² |
| **è®­æ¨åˆ†ç¦»** | âœ… Generator å’Œ Trainer åˆ†ç¦»åœ¨ä¸åŒ GPU | âœ… é€šè¿‡ WorkerGroup å®ç°ï¼Œæ”¯æŒ colocate/åˆ†ç¦» | âœ… vLLM æ¨ç† + è®­ç»ƒåˆ†ç¦» |
| **å‚æ•°åŒæ­¥** | Pull-basedï¼šGenerator ä¸»åŠ¨æ‹‰å– `state_dict` | NCCL broadcast / å…±äº«å†…å­˜ | NCCL broadcast |
| **ç”Ÿæˆå¼•æ“** | æ¨¡æ‹Ÿ vLLMï¼ˆå®é™…æ˜¯ PyTorch generateï¼‰ | çœŸæ­£é›†æˆ vLLM | çœŸæ­£é›†æˆ vLLM |
| **RL ç®—æ³•** | çº¯ SFT lossï¼ˆCrossEntropyï¼‰ | GRPO / PPO / REINFORCE++ | PPO / DPO / GRPO ç­‰ |
| **è®­ç»ƒé˜Ÿåˆ—** | æœ‰ç•Œ deque + éšæœºé‡‡æ · | åŒæ­¥æ‰¹å¤„ç†ï¼ˆæ— é˜Ÿåˆ—ï¼‰ | åŒæ­¥æ‰¹å¤„ç† |
| **å¼‚æ­¥æ€§** | åŠå¼‚æ­¥ï¼šGenerator å’Œ Trainer ç‹¬ç«‹å¾ªç¯ | é»˜è®¤åŒæ­¥ï¼ŒAPRIL æ”¯æŒå¼‚æ­¥ | åŒæ­¥ä¸ºä¸» |
| **è§„æ¨¡** | æ•™å­¦çº§ï¼ˆ1 Trainer + N Generatorï¼‰ | ç”Ÿäº§çº§ï¼ˆæ”¯æŒæ•°åƒ GPUï¼‰ | ç”Ÿäº§çº§ |

**æ ¸å¿ƒè®¾è®¡æ€è·¯çš„å…±æ€§**ï¼š

1. **Generator/Trainer è§£è€¦æ˜¯å¼‚æ­¥ RL çš„åŸºçŸ³**ï¼šæœ¬é¡¹ç›®å’Œ verl APRIL éƒ½å®ç°äº†æ¨ç†å’Œè®­ç»ƒçš„ç‹¬ç«‹è¿è¡Œã€‚Generator ä¸éœ€è¦ç­‰ Trainer è®­å®Œæ‰èƒ½ç»§ç»­ç”Ÿæˆï¼Œåä¹‹äº¦ç„¶ã€‚
2. **å‚æ•°ç‰ˆæœ¬ç®¡ç†**ï¼šæœ¬é¡¹ç›®ç”¨ `params_version` æ•´æ•°é€’å¢ï¼›verl ç”¨æ›´ç²¾ç»†çš„ç‰ˆæœ¬è¿½è¸ªæ¥å¤„ç† off-policy ä¿®æ­£ã€‚
3. **æ•°æ®æµæ–¹å‘ä¸€è‡´**ï¼šéƒ½æ˜¯ Generator â†’ (æ ·æœ¬) â†’ Trainer â†’ (å‚æ•°) â†’ Generator çš„é—­ç¯ã€‚

**å…³é”®å·®å¼‚**ï¼š

- æœ¬é¡¹ç›®çš„ `state_dict` ä¼ è¾“æ–¹å¼åœ¨å¤§æ¨¡å‹åœºæ™¯ä¸‹ä¸å¯è¡Œï¼ˆ7B æ¨¡å‹çº¦ 14GB å‚æ•°ï¼‰ï¼Œverl/OpenRLHF ç”¨ NCCL é›†åˆé€šä¿¡æˆ– Ray å¯¹è±¡å­˜å‚¨çš„é›¶æ‹·è´æœºåˆ¶
- æœ¬é¡¹ç›®æ²¡æœ‰ Reward Model / Reference Modelï¼Œç¼ºå°‘ PPO ä¸­çš„ advantage ä¼°è®¡
- æœ¬é¡¹ç›®çš„è®­ç»ƒé˜Ÿåˆ— + éšæœºé‡‡æ ·æ¨¡å¼æ›´åƒ off-policy replay bufferï¼Œè€Œéæ ‡å‡† on-policy PPO

---

## å››ã€å…³é”®æ´å¯Ÿ

### ä¸ºä»€ä¹ˆ Generator å’Œ Trainer è¦åˆ†ç¦»ï¼Ÿ

1. **ç¡¬ä»¶åˆ©ç”¨ç‡**ï¼šæ¨ç†æ˜¯ memory-boundï¼ˆå¤§ batchã€é•¿åºåˆ—ï¼‰ï¼Œè®­ç»ƒæ˜¯ compute-boundï¼ˆåå‘ä¼ æ’­ï¼‰ã€‚åˆ†ç¦»åå¯ä»¥é’ˆå¯¹å„è‡ªç‰¹ç‚¹åˆ†é…èµ„æºâ€”â€”æ¨ç†èŠ‚ç‚¹ç”¨ vLLM + PagedAttention æœ€å¤§åŒ–ååï¼Œè®­ç»ƒèŠ‚ç‚¹ç”¨ FSDP/DeepSpeed æœ€å¤§åŒ–è®¡ç®—æ•ˆç‡ã€‚

2. **å¼‚æ­¥æµæ°´çº¿**ï¼šå¦‚æœ Generator å’Œ Trainer ä¸²è¡Œæ‰§è¡Œï¼ˆå…ˆç”Ÿæˆä¸€æ‰¹ â†’ å†è®­ç»ƒä¸€æ­¥ â†’ å†ç”Ÿæˆï¼‰ï¼ŒGPU åˆ©ç”¨ç‡åªæœ‰ ~50%ã€‚åˆ†ç¦»åä¸¤è€…å¯ä»¥å¹¶è¡Œè¿è¡Œï¼Œå½¢æˆæµæ°´çº¿ã€‚

3. **å¼¹æ€§æ‰©å±•**ï¼šå¯ä»¥çµæ´»è°ƒæ•´ Generator å’Œ Trainer çš„æ•°é‡æ¯”ä¾‹ã€‚å¦‚æœç”Ÿæˆæ˜¯ç“¶é¢ˆï¼ˆé•¿åºåˆ—è§£ç ï¼‰ï¼Œå¤šåŠ  Generatorï¼›å¦‚æœè®­ç»ƒæ˜¯ç“¶é¢ˆï¼ˆå¤§æ¨¡å‹åå‘ä¼ æ’­ï¼‰ï¼Œå¤šåŠ è®­ç»ƒèŠ‚ç‚¹ã€‚

### å¼‚æ­¥ RL çš„ç¨³å®šæ€§é—®é¢˜æ€ä¹ˆå¤„ç†ï¼Ÿ

æœ¬é¡¹ç›®æš´éœ²äº†ä¸€ä¸ªé‡è¦é—®é¢˜ï¼š**Generator ç”¨çš„æ¨¡å‹å‚æ•°å¯èƒ½è½åäº Trainer çš„å½“å‰ç‰ˆæœ¬**ï¼ˆoff-policy gapï¼‰ã€‚

```
Generator ç”¨ v3 å‚æ•°ç”Ÿæˆ â†’ æ ·æœ¬è¿›å…¥é˜Ÿåˆ—
Trainer å·²ç»æ›´æ–°åˆ° v7 â†’ ç”¨ v7 çš„æ¨¡å‹å»è®­ç»ƒ v3 ç”Ÿæˆçš„æ ·æœ¬
```

è¿™å°±æ˜¯ç»å…¸çš„ **stale policy é—®é¢˜**ã€‚å¤„ç†æ–¹æ¡ˆï¼š

| æ–¹æ¡ˆ | åŸç† | ä»£è¡¨æ¡†æ¶ |
|------|------|---------|
| **é‡è¦æ€§é‡‡æ ·ä¿®æ­£** | ç”¨ `Ï€_new(a|s) / Ï€_old(a|s)` åŠ æƒï¼Œä¿®æ­£åˆ†å¸ƒåç§» | PPO çš„ clip ratio |
| **ä¸¢å¼ƒè¿‡æ—§æ ·æœ¬** | è®¾ç½®æœ€å¤§ staleness é˜ˆå€¼ï¼Œè¶…è¿‡å°±ä¸¢ | verl APRIL |
| **é¢‘ç¹åŒæ­¥** | å‡å° `update_frequency`ï¼Œæ¯ç”Ÿæˆä¸€ä¸ª batch å°±åŒæ­¥ | æœ¬é¡¹ç›®çš„ç®€å•æ–¹æ¡ˆ |
| **å®Œå…¨åŒæ­¥** | æ¯ä¸ª iteration å…ˆç”Ÿæˆã€åè®­ç»ƒï¼Œå¼ºåˆ¶ on-policy | verl é»˜è®¤æ¨¡å¼ |

æœ¬é¡¹ç›®çš„ `deque(maxlen=...)` æœ‰ç•Œé˜Ÿåˆ—æ˜¯ä¸€ç§éšå¼çš„"ä¸¢å¼ƒæ—§æ ·æœ¬"ç­–ç•¥â€”â€”å½“é˜Ÿåˆ—æ»¡æ—¶ï¼Œæœ€æ—©çš„æ ·æœ¬è¢«è‡ªåŠ¨æ·˜æ±°ã€‚ä½†æ›´ä¸¥æ ¼çš„åšæ³•åº”è¯¥æ£€æŸ¥ `params_version` å·®è·ã€‚

### æœ¬é¡¹ç›®çš„æ•™å­¦ä»·å€¼

è¿™ä¸ªé¡¹ç›®è™½ç„¶æ˜¯æ•™å­¦ä»£ç ï¼ˆæ¨¡æ‹Ÿ tokenizerã€ç©º config/modelsï¼‰ï¼Œä½†å®Œæ•´å±•ç¤ºäº†åˆ†å¸ƒå¼ RL è®­ç»ƒçš„**éª¨æ¶è®¾è®¡**ï¼š

1. âœ… Ray Actor æ¨¡å‹ï¼šæ¯ä¸ªè§’è‰²æ˜¯ç‹¬ç«‹çš„ Actorï¼Œé€šè¿‡ remote call é€šä¿¡
2. âœ… è®­æ¨åˆ†ç¦»ï¼šGenerator å’Œ Trainer åœ¨ä¸åŒ GPU ä¸Šç‹¬ç«‹è¿è¡Œ
3. âœ… å‚æ•°åŒæ­¥åè®®ï¼špull-basedï¼Œå¸¦ç‰ˆæœ¬å·
4. âœ… æœ‰ç•Œè®­ç»ƒé˜Ÿåˆ—ï¼šé˜²æ­¢å†…å­˜æº¢å‡º + éšå¼æ·˜æ±°æ—§æ ·æœ¬
5. âœ… Coordinator æ¨¡å¼ï¼šä¸­å¤®è°ƒåº¦å™¨ç®¡ç†ç”Ÿå‘½å‘¨æœŸ

ç†è§£äº†è¿™ä¸ªéª¨æ¶ï¼Œå†å»çœ‹ verl çš„ `WorkerGroup`ã€OpenRLHF çš„ `ActorModel` / `CriticModel`ï¼Œå°±èƒ½å¿«é€Ÿå®šä½æ¯ä¸ªç»„ä»¶å¯¹åº”çš„è§’è‰²ã€‚

---

## é™„ï¼šæ–‡ä»¶æ¸…å•

| æ–‡ä»¶ | è¡Œæ•° | èŒè´£ |
|------|------|------|
| `config.py` | 0 | é…ç½®å­—å…¸ï¼ˆå¾…å®ç°ï¼‰ |
| `models.py` | 0 | æ¨¡å‹å®šä¹‰ï¼ˆå¾…å®ç°ï¼‰ |
| `data_utils.py` | ~100 | æ•°æ®å¤„ç†ï¼štokenizeã€æ ·æœ¬æ„å»ºã€prompt ç®¡ç† |
| `generator_actor.py` | ~160 | Generator Actorï¼šrollout ç”Ÿæˆ + æ•°æ®æ¨é€ + å‚æ•°æ‹‰å– |
| `trainer_actor.py` | ~170 | Trainer Actorï¼šæ•°æ®æ¥æ”¶ + è®­ç»ƒ + å‚æ•°å¯¼å‡º |
| `coordinator.py` | ~180 | Coordinatorï¼šåˆ›å»º Actor + å¯åŠ¨å¾ªç¯ + ç›‘æ§ |
| `main.py` | ~80 | å…¥å£ï¼šRay init + å¯åŠ¨ + ç›‘æ§ + åœæœº |
