---
brief: "ICLR 2026 趋势分析——从 ICLR 2026 录用论文看 2026 年 AI 研究主流方向；RL for Reasoning/多模态对齐/Agent RL/长上下文/推理效率五大热点的论文分布；对老板研究方向选题的战略参考。"
title: "ICLR 2026 趋势分析"
type: analysis
domain: ai/llm/evaluation
tags:
  - ai/llm/evaluation
  - type/analysis
  - conference/iclr
created: 2026-02-16
---

# ICLR 2026 趋势分析

> 来源：5357 篇 accepted papers 关键词统计 (Reddit r/LocalLLaMA 分析)
> 时间：2026 年 1 月底 decisions released

## 核心数据

| 方向 | 论文数 | 趋势判断 |
|------|--------|----------|
| **Test-time compute** (TTC) | 257 | 🔥 最大热点，已从实验走向主流 |
| **Mamba / SSM** | 202 | 活跃，attention 替代方案未死 |
| **GRPO** | 157 | 压倒性领先 DPO |
| **RLVR** | 125 | 超过 RLHF，verifiable rewards 路线胜出 |
| **Hallucination** | 123 | 仍未解决，retrieval grounding 方向活跃 |
| **Factuality** | 125 | 与 hallucination 并列 |
| **DPO** | 55 | 退潮明显 |
| **RLHF** | 54 | 被 RLVR 超越 |

## 关键信号解读

### 1. GRPO 157 vs DPO 55 — Alignment 方法换代

学术界已经完成从 DPO 到 GRPO 的迁移（3:1 比例）。原因：
- GRPO 不需要 paired preference data
- 与 verifiable rewards 天然兼容
- DeepSeek-R1 的成功是催化剂

**面试注意：** DPO 不再是 SOTA alignment 方法。要能讲清楚 GRPO 为什么更好。

### 2. RLVR 125 vs RLHF 54 — Reward 范式转移

可验证 reward（math/code/logic correctness）的路线完全压过了 human preference annotation。核心优势：
- 不需要昂贵的人工标注
- Reward signal 更 clean、less noisy
- 对 reasoning 任务更 aligned
- 可以做到 on-policy、大规模

### 3. TTC 257 篇 — Test-Time Compute 成为最大热点

从 o1 → R1 → Gemini 3 Deep Think，这条路线在工业界和学术界同时 validated。研究方向包括：
- Inference-time scaling laws
- Self-correction / self-verification
- Budget allocation strategies
- 与 training-time compute 的 trade-off

### 4. Mamba/SSM 202 篇 — 没死

尽管 Transformer 仍占主导，SSM 方向仍有大量研究。对 consumer hardware 上的推理效率有实际意义。

### 5. 值得关注的单篇

**Nait (Neuron-Aware Instruction Tuning):**
- 用 neuron activation patterns 选择 10% Alpaca-GPT4 数据
- 效果 > 100% 全量训练
- Insight: 大部分 instruction tuning 数据是冗余的，smart selection >> more data

**MCP Security Bench:**
- Instruction-following 能力越强的模型，越容易被 prompt injection via tool outputs
- "Capability-vulnerability paradox" — agent 越强越危险

## 对我们的启示

1. **RL 方向要全力押 GRPO + RLVR** — 这是学术共识
2. **TTC 是必须掌握的方向** — 面试必考
3. **数据效率** > 数据规模 — Nait 的 insight 值得深挖
4. **Agent 安全是开放问题** — 能力悖论值得关注

## 关联笔记

- [[AI/3-LLM/RL/算法/GRPO 深度理解]]
- [[AI/3-LLM/RL/实践/对齐技术综述]]
- [[AI/3-LLM/RL/实践/RLHF-工程全栈]]
- [[AI/3-LLM/Evaluation/LLM 评测体系]]

---
*Created: 2026-02-16 by Scholar heartbeat*
