---
brief: "多模态大模型 2026 技术全景（面试武器版）——CLIP/BLIP/LLaVA/Qwen-VL/InternVL/Gemini 的架构演化；视觉 token 压缩/跨模态对齐/多模态 RL 的最新进展；面试武器版，覆盖 VLM 系统设计题。"
title: 多模态大模型 2026 技术全景（面试武器版）
tags: [multimodal, vision-language, vlm, interview-prep]
date: 2026-02-20
---

# 多模态大模型 2026 技术全景（面试武器版）

> **定位**：帮你在多模态方向面试中展示深度理解。2026 年，多模态已从"加分项"变成模型的标配能力——不懂多模态，等于不懂当代大模型。

---

## 1. 核心问题：如何让 LLM "看见"？

### 面试官会问：「LLM 本质上是 text-in text-out，你怎么让它理解图像？」

#### 1.1 问题的本质

LLM 的输入是 token sequence，每个 token 是离散的整数 ID。图像是连续的像素矩阵，维度为 $H \times W \times 3$。核心挑战就是**如何把视觉信号转化为 LLM 能理解的 token 表示**。

这本质上是一个 **跨模态对齐（Cross-modal Alignment）** 问题：

1. **表示层对齐**：把图像编码为与文本 embedding 维度相同的向量序列
2. **语义层对齐**：让视觉向量和文本向量在同一语义空间中，使得"一只猫"的文本 embedding 和猫图片的视觉 embedding 彼此接近
3. **任务层对齐**：让模型在 instruction following 场景下能正确使用视觉信息回答问题

#### 1.2 三代解决方案的演进

| 代次 | 方法 | 代表 | 核心思路 |
|------|------|------|----------|
| **第一代** | Caption Pipeline | VideoChat-Text | 用 BLIP/OFA 先把图像转为文字描述，再喂给 LLM。信息损失严重 |
| **第二代** | Learnable Interface | LLaVA / InternVL | 用预训练视觉编码器提取特征，通过可学习的 Projector（MLP / Q-Former）映射到 LLM 的 embedding 空间 |
| **第三代** | Native Multimodal | GPT-4o / Gemini 2.5 | 从预训练阶段就联合训练文本和视觉 token，不存在"桥接"瓶颈 |

#### 1.3 关键面试考点

**面试答题模板**：
> "让 LLM 看见图像，核心是解决跨模态表示对齐。当前主流方案分两条路线：一是 modular 路线，用冻结的视觉编码器（如 ViT）提取特征，通过 MLP 或 Cross-Attention 投射到 LLM 的 embedding 空间；二是 native 路线，在预训练阶段就将图像 patch 和文本 token 统一 tokenize 后联合训练。前者灵活可控、训练成本低；后者对齐更深入，但需要海量多模态数据和算力。2026 年的趋势是两者融合——先用大规模多模态预训练建立 native 能力，再用模块化编码器增强特定视觉任务。"

---

## 2. 架构范式深度解析

### 面试官会问：「多模态模型有哪几种架构？各有什么优劣？」

#### 2.1 三大融合范式

```
┌─────────────────────────────────────────────────────────┐
│                  多模态融合架构图谱                       │
├─────────────┬──────────────────┬────────────────────────┤
│  Early Fusion│  Cross-Attention │  Late Fusion           │
│  ┌───┐ ┌───┐│  ┌───┐   ┌───┐  │  ┌───┐       ┌───┐    │
│  │Img│+│Txt││  │Img│──→│LLM│  │  │Img│       │Txt│    │
│  └─┬─┘ └─┬─┘│  │Enc│ ↗ │+CA│  │  │Enc│       │Enc│    │
│    └──┬──┘  │  └───┘   └───┘  │  └─┬─┘       └─┬─┘    │
│    ┌──▼──┐  │                  │    └─────┬─────┘      │
│    │ LLM │  │                  │       ┌──▼──┐         │
│    └─────┘  │                  │       │Merge│         │
└─────────────┴──────────────────┴────────────────────────┘
```

##### (1) Early Fusion（输入拼接）

**原理**：把视觉 token 和文本 token 直接拼接为一个序列，一起送入 Transformer。

**代表模型**：
- **LLaVA 系列**：Vision Encoder → MLP Projector → 与 text tokens 拼接 → LLM
- **InternVL 系列**：ViT-MLP-LLM 范式，pixel unshuffle 后拼接
- **Fuyu**：直接把图像 patch 线性投影为 token，无需独立视觉编码器

**优点**：
- 实现简单，只需修改 embedding 层
- 视觉和文本 token 在所有 Transformer 层都有充分交互
- Self-attention 可以自由学习跨模态关系

**缺点**：
- 视觉 token 数量巨大（一张 1024×1024 图像可产生 4096+ tokens），占用大量 context window
- 计算成本 $O((N_v + N_t)^2)$，随视觉 token 数量二次增长
- 长文档 + 多图场景下 context 容易爆炸

##### (2) Cross-Attention Fusion（交叉注意力）

**原理**：在 LLM 的某些层插入 Cross-Attention 模块，让文本 token 作为 Query，视觉特征作为 Key/Value。

**代表模型**：
- **Flamingo / OpenFlamingo**：在冻结的 LLM 层间插入 Gated Cross-Attention Dense (GATED XATTN-DENSE)
- **Qwen-VL（早期版本）**：Cross-Attention + Resampler 压缩视觉 token
- **NVLM-X**：专门用 Gated Cross-Attention 处理 tile tokens
- **ViCA（2025）**：视觉 token 完全绕过 self-attention 和 FFN，仅通过稀疏 cross-attention 在关键层交互

**优点**：
- 视觉 token 不占用 LLM 的 context window，文本容量不受影响
- 计算效率更高：Cross-Attention 是 $O(N_t \times N_v)$，不是 $(N_t + N_v)^2$
- 可以灵活控制在哪些层引入视觉信息

**缺点**：
- 需要修改 LLM 架构，无法直接复用现有 LLM 权重
- Cross-Attention 层的设计（层数、位置、gating）需要精心调优
- 视觉和文本 token 的交互不如 Early Fusion 充分

##### (3) Late Fusion（后期融合）

**原理**：各模态独立编码到高层表示后，在最后阶段合并。

**代表模型**：
- **CLIP（对比学习版）**：视觉和文本各有独立 encoder，通过 cosine similarity 对齐
- **某些 Ensemble 方案**：视觉模型和语言模型独立推理，最终投票

**优点**：模块高度解耦，各模态可独立优化
**缺点**：跨模态交互最少，难以处理需要精细视觉-语言推理的任务

#### 2.2 Visual Tokenizer vs Cross-Attention：核心技术抉择

这是 2026 年面试最高频的架构对比题。

| 维度 | Visual Tokenizer（拼接路线） | Cross-Attention（注入路线） |
|------|------|------|
| **代表** | LLaVA, InternVL, Fuyu | Flamingo, ViCA, NVLM-X |
| **视觉信息注入方式** | 视觉 token 与文本 token 拼接后共享 self-attention | 视觉特征作为 KV 通过 cross-attention 注入 |
| **Context 占用** | 高（视觉 token 占用主序列） | 低（视觉不进主序列） |
| **跨模态交互深度** | 深（所有层都交互） | 可控（选择性层交互） |
| **LLM 修改程度** | 仅改 embedding 层 | 需插入 cross-attention 层 |
| **主流趋势** | ✅ 2025-2026 年主流 | 在特定场景有复苏趋势 |

**为什么 Visual Tokenizer 成为主流？**
- 实现最简单：只需一个 MLP 把视觉特征映射到 LLM 维度
- 复用现有 LLM 权重：不需要修改 Transformer 架构
- 社区验证充分：LLaVA → InternVL → Qwen-VL 一脉相承
- Visual Token 压缩技术（pixel unshuffle、adaptive pooling）已大幅缓解 token 数量问题

**Cross-Attention 的反击**：
- ViCA（2025 年 2 月）证明视觉 token 完全绕过 self-attention 和 FFN、仅用稀疏 cross-attention 就能达到同等效果，且推理更快
- 当图像数量或分辨率极高时，cross-attention 的 context 优势变得关键

#### 2.3 Hybrid Fusion（混合融合）—— 新趋势

**NVLM-H** 是混合融合的代表：
- 低分辨率的 thumbnail token → 进入 self-attention（Early Fusion）
- 高分辨率的 tile token → 通过 Gated Cross-Attention 注入

这种设计同时获得了全局语义理解（thumbnail）和细节感知（tiles）的优势。

#### 2.4 面试答题模板

> "多模态架构有三大范式：Early Fusion 把视觉和文本 token 拼接共享 self-attention，交互充分但 context 开销大；Cross-Attention 将视觉作为 KV 注入特定层，效率高但需改 LLM 架构；Late Fusion 各模态独立处理后合并，交互最少。当前主流是 Early Fusion 路线的 Visual Tokenizer 方案——LLaVA、InternVL、Qwen-VL 都采用 ViT-MLP/Projector-LLM 三段式架构。但 2025 年底 ViCA 等工作证明稀疏 Cross-Attention 在效率上有显著优势，未来可能出现 hybrid 融合成为主流。"

---

## 3. 视觉编码器对比

### 面试官会问：「不同的 Vision Encoder 有什么区别？为什么 SigLIP 逐渐取代 CLIP？」

#### 3.1 主流视觉编码器一览

| 编码器 | 开发者 | 预训练方法 | 参数量 | 特点 |
|--------|--------|-----------|--------|------|
| **ViT-L/14 (CLIP)** | OpenAI | 对比学习 (ITC) | 304M | 开山之作，text-image 对齐能力强 |
| **EVA-CLIP** | BAAI | CLIP + MAE 蒸馏 | 1B+ | 更强的视觉表示，曾是开源主力 |
| **SigLIP** | Google | Sigmoid Loss 对比学习 | 400M-878M | 去掉 softmax 归一化，batch 效率更高 |
| **SigLIP2** | Google | 多任务多阶段 | 400M-878M | 区域感知，多分辨率，2025 SOTA |
| **InternViT-6B** | OpenGVLab | 对比 + 生成式训练 | 6B | 最大的开源视觉编码器 |
| **InternViT-300M** | OpenGVLab | 蒸馏自 6B | 300M | 轻量版，InternVL 3.5 可选 |
| **AIMv2** | Apple | Multimodal Autoregressive | 多尺寸 | 自回归预训练，同时生成 patch 和 text token |
| **DINOv2** | Meta | Self-supervised (无标签) | 1B+ | 纯视觉自监督，不依赖文本对齐 |

#### 3.2 CLIP vs SigLIP：为什么后者赢了？

**CLIP 的局限**：
- 使用 softmax 归一化的 InfoNCE Loss，要求 batch 内做全局归一化
- Batch size 必须极大才能提供足够负样本
- 训练需要跨 GPU 同步 similarity matrix，通信开销大

**SigLIP 的改进**：
- 使用 **Sigmoid Loss**：每个 image-text pair 独立计算二分类损失
- 不需要全局归一化，batch 内不同 pair 互不影响
- 可以用更大的 batch size（Google 用了 32K）而不增加通信开销
- 在相同训练量下，SigLIP 的 zero-shot 性能全面超过 CLIP

**SigLIP2（2025）的进一步进化**：
- 多任务训练：对比学习 + 自蒸馏 + 图像描述生成 + 区域感知任务
- 多分辨率支持：NaViT（Native Resolution ViT）技术
- 更强的区域级理解，适合细粒度视觉任务

**面试考点**：LLaVA-OneVision 用 SigLIP 作为视觉编码器，InternVL 用自研的 InternViT。两者各有优势，SigLIP 社区更通用、InternViT 在中文场景下表现更优。

#### 3.3 分辨率处理策略——决定视觉能力的关键

这是面试中很容易被追问的深度问题。不同模型对高分辨率图像的处理差异巨大：

##### (1) 固定分辨率（Fixed Resolution）

- 所有图像 resize 到固定尺寸（如 224×224 或 336×336）
- 编码为固定数量的 token（如 256 个）
- **优点**：简单高效
- **缺点**：丢失细节信息，小字体 OCR 能力差
- **代表**：早期 LLaVA、Gemma 3（resize 到 896×896 → 256 tokens）

##### (2) Dynamic Resolution / Any-Resolution

- 将图像按原始宽高比分割为多个 tiles（每个 tile 独立编码）
- 保留一个低分辨率 thumbnail 作为全局语义
- **优点**：保留原始分辨率信息，OCR 和细节理解强
- **缺点**：token 数量不确定，可能很大
- **代表**：
  - **InternVL 3.0/3.5**：Dynamic Resolution + pixel unshuffle（token 数降为 1/4）
  - **Qwen-VL 系列**：Dynamic Resolution + 自适应分辨率编码
  - **LLaVA-OneVision**：AnyRes 策略，最大视觉 token 数跨场景平衡

##### (3) Variable Visual Position Encoding (V2PE)

- InternVL 3.0 引入，根据图像复杂度和宽高比自适应分配位置编码
- 让模型在处理不同分辨率图像时有更灵活的空间感知
- 解决了传统绝对位置编码在多分辨率下的泛化问题

##### (4) Native Resolution Processing

- **Pixtral（Mistral）**：原生分辨率处理，不做 resize 或 tiling
- 图像 patch 直接按原始空间位置编码
- 最大程度保留原始信息

#### 3.4 面试答题模板

> "视觉编码器的选择从 CLIP 时代演进到了 SigLIP 时代。SigLIP 用 Sigmoid Loss 替代 softmax-based InfoNCE，消除了全局归一化的需求，训练效率更高且性能更好。分辨率处理方面，Dynamic Resolution 已成为主流——InternVL 用 dynamic tiles + pixel unshuffle，Qwen-VL 用自适应分辨率编码，LLaVA-OneVision 用 AnyRes。核心思路都是在 token 数量可控的前提下尽可能保留原始分辨率信息。InternVL 3.0 的 V2PE 是一个值得关注的创新——它让位置编码能自适应不同分辨率。"

---

## 4. 训练范式：三阶段流水线

### 面试官会问：「多模态模型的训练分几个阶段？每个阶段在做什么？」

#### 4.1 经典三阶段训练

```
Stage 1: Alignment          Stage 2: Instruction         Stage 3: Preference
(模态对齐预训练)               Tuning (指令微调)            Optimization (偏好优化)
                                                          
┌─────────────┐             ┌─────────────┐              ┌─────────────┐
│ Image-Text  │             │ Multimodal  │              │ RLHF / DPO  │
│ Pairs       │──→ MLP ──→ │ Instruction │──→ Full ──→  │ / RLAIF     │
│ (大规模弱标注)│   Projector │ Data        │   Model     │             │
└─────────────┘    Only     └─────────────┘   Finetune   └─────────────┘

冻结: ViT ✅ LLM ✅          冻结: ViT ✅/❌ LLM ❌        冻结: 通常全开放
训练: Projector only         训练: LLM + Projector        训练: 全模型 or LoRA
数据: ~1M pairs              数据: ~1-5M instructions      数据: ~100K preferences
目标: 对齐视觉-文本空间       目标: 学会遵循多模态指令       目标: 提升回答质量和安全性
```

##### Stage 1: Pre-training / Alignment（模态对齐）

**目标**：让 Projector 学会把视觉编码器的输出映射到 LLM 的输入空间。

**典型设置**：
- **冻结**：Vision Encoder + LLM 都冻结
- **训练**：只训练 Projector（MLP / Q-Former / Resampler）
- **数据**：大规模 image-caption pairs（LAION、CC3M、SBU 等），通常 1-5M 对
- **Loss**：标准的 language modeling loss（预测 caption 的下一个 token）
- **耗时**：相对短，因为只训练很少的参数

**直觉理解**：这一步就像给一个只会说话的人装上一副"眼镜"——先不改变他的思维方式（LLM 冻结），只是让他能通过眼镜看到外面的世界。

##### Stage 2: Instruction Tuning（指令微调）

**目标**：让模型学会根据指令使用视觉信息完成各种任务。

**典型设置**：
- **冻结**：Vision Encoder 可冻结或解冻（取决于方案）
- **训练**：LLM 全参数微调 + Projector
- **数据**：高质量多模态指令数据（VQA、OCR、图表理解、推理、对话等），通常 1-5M 条
- **Loss**：Instruction following 的 language modeling loss
- **关键数据集**：LLaVA-Instruct、ShareGPT4V、ALLaVA、Cambrian-1 Data Mix

**InternVL 的细化**：InternVL 把这一步进一步拆为：
1. **Vision-Language Contrastive Training**：对比学习对齐 ViT 和 LLM
2. **Vision-Language Generative Training**：生成式训练建立 caption 能力
3. **Supervised Fine-tuning**：指令微调

##### Stage 3: Preference Optimization（偏好优化）

**目标**：让模型的输出更符合人类偏好——更准确、更安全、减少幻觉。

**三种主流方法**：

| 方法 | 原理 | 优点 | 缺点 |
|------|------|------|------|
| **RLHF** | 训练 Reward Model → PPO 优化策略 | 效果好，工业验证充分 | 训练不稳定，需要 4 个模型 |
| **DPO** | 直接用偏好对数据优化，无需 Reward Model | 简单稳定，只需 1 个模型 | 对数据质量要求高 |
| **RLAIF** | 用 AI（如 GPT-4V）生成偏好判断替代人类 | 可规模化 | 依赖 judge 模型的质量 |

**多模态 DPO 的特殊性**：
- 需要构建多模态偏好数据：同一张图 + 同一个问题，生成好/坏两个回答
- 常见幻觉类型：描述图中不存在的物体、错误的空间关系、OCR 错误
- LLaVA-RLHF、RLHF-V 等工作专门针对视觉幻觉做偏好优化

#### 4.2 2025-2026 年训练范式的新趋势

##### (1) Native Multimodal Pre-training

GPT-4o 和 Gemini 2.5 的做法：**从头开始就联合训练所有模态**，不区分 Stage 1 和 Stage 2。

- 所有模态的 token（文本、图像 patch、音频帧）被统一编码到同一 vocabulary
- 训练目标是 unified next-token prediction
- 数据规模：万亿级 multimodal tokens

这种方式打破了传统三阶段的限制，但需要极其庞大的计算资源。

##### (2) Test-Time Training / Thinking

Qwen3-VL 和 InternVL 3.5 引入了 **Thinking 变体**：
- 推理时模型会先生成一段内部推理链（类似 Chain-of-Thought）
- 再基于推理链输出最终答案
- 这相当于在推理阶段引入了额外的"训练信号"

##### (3) Mixed-Task Progressive Training

- 渐进式引入不同难度和类型的任务
- 从简单的 caption 到复杂的多轮推理
- 避免灾难性遗忘

#### 4.3 面试答题模板

> "多模态模型的经典训练分三个阶段：第一阶段是 Alignment，冻结 ViT 和 LLM，只训练中间的 Projector，用大规模 image-caption 数据对齐视觉和语言空间；第二阶段是 Instruction Tuning，解冻 LLM 做全参数微调，用高质量多模态指令数据让模型学会遵循指令完成视觉任务；第三阶段是偏好优化，用 DPO 或 RLHF 减少幻觉、提升回答质量。2026 年的趋势是 native multimodal pre-training——GPT-4o、Gemini 从头联合训练所有模态，不再有显式的对齐阶段。但对于开源社区和资源有限的团队，三阶段范式仍然是最实用的路径。"

---

## 5. 主流模型对比

### 面试官会问：「GPT-4o、Gemini、Claude、Qwen-VL 在多模态能力上有什么差异？」

#### 5.1 2026 年 Q1 主流模型全景

| 模型 | 厂商 | 架构类型 | 视觉编码器 | LLM 骨干 | Context | 特色能力 |
|------|------|---------|-----------|---------|---------|---------|
| **GPT-5 / GPT-4o** | OpenAI | Native Multimodal | 多分辨率 ViT | Dense Transformer | 128K-400K | 原生音视觉，强推理 |
| **Gemini 2.5 Pro** | Google | Native Multimodal | 内置 ViT | Dense / MoE | 2M | 超长上下文，多模态 |
| **Gemini 3 Pro** | Google | Native Multimodal | 改进 ViT | Dense | 2M | 最新一代，架构优化 |
| **Claude Opus 4.5** | Anthropic | Constitutional AI + Vision | 未公开 | Dense Transformer | 200K+ | 安全性+长文本分析 |
| **Qwen3-VL-235B** | 阿里 | Native Multimodal MoE | 自研编码器 | MoE (22B active) | 256K→1M | 32语言OCR，视频理解 |
| **InternVL 3.5** | OpenGVLab | ViT-MLP-LLM | InternViT-6B/300M | Qwen3/GPT-OSS | 长上下文 | 开源 SOTA，V2PE |
| **LLaVA-OneVision 1.5** | 社区 | ViT-MLP-LLM | SigLIP2 + 区域编码器 | Qwen2 系列 | 32K-128K | 开源标杆，跨场景迁移 |
| **GLM-4.6V** | Z.ai | GLM Transformer | 内置编码器 | GLM | 128K | 原生工具调用，UI理解 |
| **DeepSeek-V3.2** | DeepSeek | MoE + MLA + DSA | 多模态编码器 | 671B total / 37B active | 128K | 编码推理极强，高效 |
| **Llama 4 Maverick** | Meta | Dense / MoE Hybrid | 开源编码器 | 17B active / 400B MoE | 1M | Meta 首个原生多模态开源 |

#### 5.2 闭源三巨头深度对比

##### GPT-5 / GPT-4o（OpenAI）

**架构特点**：
- **Native Multimodal**：GPT-4o 是 OpenAI 第一个原生多模态模型，可以接受文本、音频、图像、视频的任意组合作为输入，生成文本、音频、图像的任意组合输出
- Dense Transformer 架构，不使用 MoE
- 多分辨率视觉编码：Vision encoder 在多个分辨率级别处理图像
- 原生音频能力：可以直接理解和生成语音，无需 ASR/TTS pipeline

**性能亮点**：
- MMMU（多模态理解）：GPT-5.1 达到 84.2%，领先所有模型
- AIME 2025（数学推理）：GPT-5.2 达到 100%
- ARC-AGI-2（抽象推理）：52.9%
- 视觉理解在图表、图形、技术文档方面表现突出

**局限**：
- 推理速度较慢（26 秒 latency 在 Roboflow 测试中）
- API 成本最高（$5/M tokens input）
- 闭源，不可微调（仅支持有限 fine-tuning）

##### Gemini 2.5 Pro / 3 Pro（Google）

**架构特点**：
- **Native Multimodal**：从预训练就联合处理文本、图像、视频、音频
- Hierarchical Tokenization：每种模态有优化的 tokenization 策略
- Cross-modal Attention：跨模态注意力层将文本概念锚定到视觉内容
- MoE（Flash 版本）：稀疏专家混合架构，推理速度快 2x
- **超长上下文**：最高 2M tokens，可处理整本书或数小时视频

**性能亮点**：
- MMMU：70.2%（benchmark 评分）
- 多模态推理综合性能最强
- Flash 版本延迟仅 8.39 秒，性价比极高
- 原生代码执行：可以实时生成和调试代码
- 100+ 语言支持

**局限**：
- Pro 版本延迟仍较高（~16 秒）
- 部分视觉细节理解不如 GPT-5

##### Claude Opus 4.5 / Sonnet 4.6（Anthropic）

**架构特点**：
- Constitutional AI 框架下的多模态架构
- 视觉-语言深度整合（具体架构未公开）
- 超长上下文：200K+ tokens，近乎完美的上下文回忆能力
- 强安全机制：区分有害请求和合理边缘情况

**性能亮点**：
- SWE-bench Verified：80.9%（代码生成 SOTA）
- 技术文档分析能力突出：研究论文、复杂公式、专业符号
- 长文本分析：可以在单次会话中分析大规模文档、代码库
- MMMU：约 77.8%（视觉不是最强项，但实用性强）

**局限**：
- 视觉理解不是最强项（MMMU 落后于 GPT-5 和 Gemini）
- API 成本最高之一（Opus $15/M tokens）
- 图像生成能力有限

#### 5.3 开源四强深度对比

##### Qwen3-VL（阿里巴巴）

**关键创新**：
- **Native Multimodal MoE**：235B 总参数，仅 22B 激活，效率极高
- **32 语言 OCR**：包括希腊语、希伯来语、印地语等小语种
- **视觉 Agent 能力**：可操作 PC/手机界面，识别 UI 元素并执行任务
- **超长上下文**：原生 256K，可扩展到 1M tokens
- **视频理解**：支持小时级视频的秒级索引

**为什么 Qwen3-VL 是开源王者**：
> 在 MMLU、AIME25、LiveBench1125 等文本 benchmark 上达到 DeepSeek-V3、Claude Opus 4 级别；在多模态 benchmark（通用问答、2D/3D grounding、视频理解、OCR、文档理解）上全面对标 Gemini 2.5 Pro 和 GPT-5。

##### InternVL 3.0 / 3.5（OpenGVLab）

**关键创新**：
- **ViT-MLP-LLM 范式**：坚持三段式架构，持续迭代
- **InternViT-6B**：最大的开源视觉编码器，6B 参数
- **V2PE（Variable Visual Position Encoding）**：自适应位置编码
- **Pixel Unshuffle**：将视觉 token 数量降为 1/4
- **InternVL 3.5**：语言模型初始化自 Qwen3 和 GPT-OSS

**训练流程（三阶段）**：
1. Vision-Language Contrastive Training
2. Vision-Language Generative Training
3. Supervised Fine-tuning

##### LLaVA-OneVision 1.5（社区）

**关键创新**：
- **完全开源框架**：数据、代码、训练流程全部公开
- **SigLIP2 + 区域感知编码器**：比原始 SigLIP 有更强的细粒度理解
- **AnyRes 策略**：跨场景（图像、视频、多图）统一分辨率处理
- **跨场景能力迁移**：不同场景的视觉 token 数量设计为相近，促进能力共享

##### GLM-4.6V（Z.ai）

**关键创新**：
- **原生多模态工具调用**：图像可直接作为工具参数，无需转文本
- **前端复制**：UI 截图 → HTML/CSS/JS，像素级精度
- **128K 上下文**：支持 200 页 PPT 或小时级视频
- 双版本：106B（云端）+ 9B Flash（本地）

#### 5.4 面试对比速查表

| 场景 | 最优选择 | 原因 |
|------|---------|------|
| 通用多模态推理 | Gemini 2.5 Pro | 最强综合能力 + 超长上下文 |
| 数学/科学推理 | GPT-5.2 | AIME 100%，GPQA 最强 |
| 代码生成 | Claude Opus 4.5 | SWE-bench 80.9% |
| 多语言 OCR | Qwen3-VL-235B | 32 语言支持，亚洲语言最优 |
| 开源自部署 | InternVL 3.5 / Qwen3-VL | 开源 SOTA，支持微调 |
| 视频理解 | Gemini 2.5 Pro / Qwen3-VL | 2M/1M 上下文，原生视频 |
| 安全敏感场景 | Claude Opus 4.5 | Constitutional AI，安全性最高 |
| 低成本高性能 | Gemini 2.5 Flash | $1.25/M tokens，MoE 架构高效 |

---

## 6. 视频理解

### 面试官会问：「视频理解和图像理解有什么本质区别？Video-LLM 怎么做？」

#### 6.1 核心挑战：时序维度的引入

图像理解只需处理空间信息（spatial），视频理解还要处理**时间信息（temporal）**。核心挑战：

1. **Token 爆炸**：1 帧 = 几百个 visual tokens，1 分钟 30fps 视频 = 1800 帧 × 几百 tokens = 数十万 tokens
2. **时序建模**：动作识别、事件因果、状态变化等需要跨帧推理
3. **长视频处理**：小时级视频的信息压缩和关键帧提取
4. **多模态同步**：视频的视觉流、音频流、字幕流需要对齐

#### 6.2 Video-LLM 架构分类

##### (1) Frame Sampling + Image Encoder（帧采样路线）

**最主流的方案**，将视频退化为图像序列处理：

```
Video → 采样 K 帧 → 每帧用 ViT 编码 → 拼接所有帧的 token → LLM
```

- **优点**：可以直接复用图像 VLM，实现简单
- **缺点**：帧间时序关系完全依赖 LLM 的 self-attention 隐式学习
- **代表**：LLaVA-Video、VideoChat、MiniGPT4-Video

**采样策略至关重要**：
- Uniform Sampling：均匀采样，简单但可能错过关键帧
- Key Frame Extraction：基于视觉变化度的关键帧提取
- Hierarchical Sampling：多层级采样（密集采样关键段 + 稀疏采样背景段）

##### (2) Temporal Encoder（时序编码器路线）

在 ViT 和 LLM 之间插入专门的时序编码模块：

```
Video → ViT (per frame) → Temporal Encoder → LLM
```

- **STORM（NVIDIA）**：Mamba-Based Temporal Projector，用状态空间模型编码时序动态
- **VideoLoom**：拼接 frame-pooled、pixel-level、temporal-position tokens，使用 ⟨Temp⟩ 和 ⟨Seg⟩ 特殊 token
- **UFVideo**：multi-scale feature fusion，融合不同时间尺度的特征

**STORM 架构详解**：
```
ViT (frozen) → Mamba Temporal Projector → Visual Token Reduction → LLM
                    ↑
              Spatiotemporal Token Reduction (STORM)
              - 用 Mamba 的选择性状态空间模型捕获时序
              - Token 数量大幅压缩（可处理更长视频）
              - 保留时序动态信息
```

##### (3) 多编码器融合路线

使用多个专业视觉编码器，各司其职：

```
Video → CLIP Encoder (语义) ──┐
     → DINOv2 (空间细节) ────┼→ Fusion Module → LLM
     → Temporal Encoder (动态) ┘
```

- 不同编码器擅长不同方面：CLIP 擅长语义对齐，DINOv2 擅长空间细节，专门的时序编码器捕获动态
- 融合策略：concatenation、cross-attention、MoE routing

##### (4) 超长视频处理策略

| 策略 | 原理 | 代表 |
|------|------|------|
| **Token Compression** | 对帧级 token 做池化/压缩 | HiCo, ToMe, STORM |
| **Memory Cache** | 维护全局记忆，滑动窗口处理 | Memory-augmented Video-LLMs |
| **Hierarchical Processing** | 先粗粒度扫描，再细粒度聚焦 | VideoAgent |
| **超长上下文 LLM** | 直接利用 LLM 的长上下文能力 | Gemini 2.5 (2M), Qwen3-VL (1M) |

**Qwen3-VL 的视频方案**：
- 原生 256K 上下文（可扩展到 1M）
- 秒级视频索引：每一秒都有精确的 temporal token
- 可以处理小时级视频并回答细节问题

**Gemini 2.5 Pro 的视频方案**：
- 2M token 上下文窗口
- 原生视频理解（不是拆帧处理）
- 可以直接分析长达数小时的视频

#### 6.3 Video Understanding Benchmark

| Benchmark | 考察重点 | SOTA 水平 (2026) |
|-----------|---------|-----------------|
| **VideoMME** | 综合视频理解 | Gemini 2.5 Pro 领先 |
| **MVBench** | 多维度视频理解 | Qwen3-VL 和 InternVL 竞争 |
| **LongVideoBench** | 长视频理解 | 超长上下文模型占优 |
| **EgoSchema** | 第一人称视频理解 | 具身智能方向的关键 benchmark |

#### 6.4 面试答题模板

> "视频理解的核心挑战是时序建模和 token 爆炸。当前有三种主流方案：一是帧采样 + 图像编码器，简单但丢失时序信息，靠 LLM 隐式学习时序；二是引入专门的 Temporal Encoder，如 NVIDIA 的 STORM 用 Mamba 做时序编码并压缩 token；三是多编码器融合，用 CLIP、DINOv2、时序编码器各司其职。长视频处理方面，2026 年的趋势是利用超长上下文 LLM（Gemini 的 2M、Qwen3-VL 的 1M）直接处理更多帧，同时用 hierarchical token compression 控制开销。"

---

## 7. 语音 + 多模态

### 面试官会问：「GPT-4o 的语音能力和传统 ASR+TTS pipeline 有什么区别？端到端语音 LLM 是怎么实现的？」

#### 7.1 两种范式的对比

##### (1) Cascaded Pipeline（级联管线）

```
语音输入 → ASR (Whisper) → 文本 → LLM 推理 → 文本 → TTS → 语音输出
```

**问题**：
- **延迟高**：3-5 个模块串联，每个都有推理延迟
- **信息丢失**：语音中的情感、语调、停顿等非文本信息在 ASR 阶段全部丢失
- **错误累积**：ASR 的错误直接影响 LLM 的理解
- **无法实时打断**：必须等语音说完、转写完才能处理

##### (2) Native / End-to-End Audio（原生音频）

```
语音输入 → [Audio Tokens] → Multimodal LLM → [Audio Tokens] → 语音输出
```

**优势**：
- **低延迟**：端到端处理，延迟可达 sub-200ms
- **信息保留**：语调、情感、说话风格都编码在 audio tokens 中
- **实时交互**：支持打断、overlapping speech
- **副语言理解**：可以理解笑声、叹气、犹豫等非语言信号

#### 7.2 GPT-4o 的原生音频能力

GPT-4o 是第一个商用的原生多模态模型，其 System Card 明确声明：

> "GPT-4o is an autoregressive omni model, which accepts as input any combination of text, audio, image, and video and generates any combination of text, audio, and image outputs."

**关键技术推测**：
- Audio 被编码为离散的 audio tokens（类似 EnCodec / SoundStream 的 neural audio codec）
- Audio tokens 和文本 tokens 在同一个 Transformer 中联合处理
- 可以生成带有情感和风格的语音输出
- OpenAI 后续推出了 `gpt-realtime` 和 `gpt-audio` 系列模型

**OpenAI 的音频模型演进**：
- `gpt-4o-mini-transcribe`：优化的 ASR 模型，在 Common Voice、FLEURS 上达到更低 WER
- `gpt-4o-audio`：原生音频理解和生成
- `gpt-realtime`：实时语音对话模型

#### 7.3 Gemini 的音频能力

- Gemini 2.5 Pro/Flash 原生支持音频理解
- 可以分析音频文件中的语音内容、音乐、环境声
- 结合视频时可以同步理解视觉和听觉信息
- 2M 上下文窗口可以处理长达数小时的音频

#### 7.4 端到端语音 LLM 的技术路线

##### Audio Tokenization

将连续语音波形转化为离散 token 序列：

| 方法 | 代表 | 原理 |
|------|------|------|
| **Neural Audio Codec** | EnCodec, SoundStream | RVQ（残差向量量化）编码音频为多层级离散码 |
| **Semantic Token** | HuBERT, w2v-BERT | 自监督学习提取语音的语义 token |
| **Hybrid** | TWIST, SpiRit-LM | 结合语义 token 和声学 token |

##### 架构设计

**Speech-to-Speech LLM 典型架构**：
```
Audio Input → Audio Encoder → [Semantic + Acoustic Tokens]
                                        ↓
                              Multimodal LLM Backbone
                                        ↓
                          [Generated Semantic + Acoustic Tokens]
                                        ↓
                              Audio Decoder / Vocoder → Speech Output
```

**代表模型**：
- **JoyVoice**：情感语音生成
- **EMOVA**：多模态情感语音理解和生成
- **Step-Audio**：语音直接集成到 LLM 骨干
- **A2-LLM**：端到端对话音频头像大模型

##### 2025-2026 关键趋势

1. **Speech-to-Speech 延迟 < 200ms**：实时对话成为可能
2. **情感理解和生成**：不仅理解语义，还能感知和表达情感
3. **多说话人处理**：混合语音中识别和生成不同说话人
4. **Audio-Visual 联合**：同时理解画面中的说话人和语音内容

#### 7.5 面试答题模板

> "传统语音交互是 ASR→LLM→TTS 的级联管线，延迟高且丢失副语言信息。GPT-4o 开创了原生音频路线——将语音编码为离散 audio tokens，与文本 tokens 在同一 Transformer 中联合处理，实现端到端的语音理解和生成。关键技术包括 Neural Audio Codec（如 EnCodec）将连续波形量化为离散码、Semantic Tokens 捕获语义信息。2025-2026 年的前沿是 sub-200ms 延迟的 speech-to-speech 模型、情感语音理解和生成、以及 audio-visual 联合建模。"

---

## 8. 2026 前沿趋势

### 面试官会问：「多模态的未来方向是什么？你怎么看原生多模态和世界模型？」

#### 8.1 原生多模态（Native Multimodal）

**定义**：从预训练第一步就联合处理所有模态，而非后期"拼接"。

**为什么重要**：
- 传统 modular 路线（ViT + MLP + LLM）有"桥接瓶颈"——Projector 的表达能力有限，无法完全传递视觉信息
- Native 路线消除了桥接瓶颈，视觉和语言从一开始就在同一表示空间中学习
- 2025 年底，native multimodal 已成为 frontier model 的标配（GPT-4o、Gemini 2.5、Gemini 3）

**技术实现**：
- **Unified Tokenizer**：文本用 BPE/SentencePiece，图像用 ViT patch embedding 或 VQ-VAE，音频用 neural codec
- **Unified Pre-training**：所有模态的 token 统一做 next-token prediction
- **Unified Architecture**：同一个 Transformer 处理所有模态，无需模态特定模块

**开源进展**：
- Qwen3-VL 被标注为 "Naive Multimodal"（原生多模态）架构
- Llama 4 Maverick 是 Meta 首个原生多模态开源模型
- 趋势：开源模型正从 modular 向 native 转型

#### 8.2 Any-to-Any 模型

**定义**：可以接受任意模态组合作为输入，输出任意模态组合。

```
┌──────────────────────────────────────────┐
│              Any-to-Any Model            │
│                                          │
│  Input:  Text + Image + Audio + Video    │
│           ↓ ↓ ↓ ↓                        │
│         Unified Transformer              │
│           ↓ ↓ ↓ ↓                        │
│  Output: Text + Image + Audio + Video    │
└──────────────────────────────────────────┘
```

**当前进展**：
- **GPT-4o**：Any-to-Any 的先驱（text/image/audio in → text/image/audio out）
- **Gemini 2.5/3**：同样支持多模态输入输出
- **开源 Any-to-Any**：Chameleon (Meta)、Emu3、NExT-GPT 等尝试

**技术挑战**：
- 不同模态的 token 分布差异巨大，联合训练不稳定
- 生成质量（尤其图像/音频生成）不如专门的生成模型
- 训练数据中模态配对的不均衡

#### 8.3 世界模型（World Models）

**定义**：不仅能描述当前画面，还能预测未来会发生什么——建立对物理世界的内部模型。

**关键能力**：
- **物理直觉**：理解重力、碰撞、流体等物理规律
- **因果推理**：如果推倒多米诺骨牌，后面会发生什么
- **时空预测**：给定当前帧，生成未来帧序列

**代表方向**：
- **Sora / DALL-E Video**（OpenAI）：通过视频生成展示世界理解
- **Genie 2**（DeepMind）：交互式世界模型，支持操控
- **学术方向**：将多模态 LLM 与 physics simulator 结合

**为什么面试要关注**：
> 世界模型是 AGI 的重要组件。如果一个模型不仅能看懂图像，还能理解物理规律并预测未来，那就离"真正的理解"更近了一步。这也是为什么 Sora 的发布引起了如此大的关注——它暗示了 LLM 可能正在构建某种世界的内部表示。

#### 8.4 具身智能（Embodied Intelligence）

**定义**：让 AI 在物理世界中感知、推理和行动。

**多模态模型在具身智能中的角色**：

```
感知（多模态输入）→ 推理（LLM/VLM）→ 行动（机器人控制）
  Camera + Lidar       VLA Model          Motor Commands
  Microphone           (Vision-Language-   Gripper Control
  Touch Sensor          Action Model)      Navigation
```

**Vision-Language-Action (VLA) 模型**：
- 在 VLM 基础上增加 action output head
- 可以将自然语言指令转化为机器人动作序列
- 代表：RT-2 (Google)、Octo、OpenVLA

**2026 关键进展**：
- Gartner 预测 2026 年底 40% 企业应用将嵌入 AI Agent（2025 年不到 5%）
- Microsoft 提出"具身交互"是 AI 学习的关键方向——通过在环境中移动和操作来学习
- 仿真环境生成训练数据成为可行路径
- 多模态 LLM 作为机器人的"大脑"，提供推理和规划能力

#### 8.5 其他前沿趋势

##### (1) 多模态 Agent

- VLM + 工具调用 = 视觉 Agent
- 可以操作 GUI（点击按钮、填写表单）
- 代表：GLM-4.6V 的原生工具调用、Qwen3-VL 的视觉 Agent 能力

##### (2) 多模态 MoE

- 不同模态激活不同的专家网络
- 效率极高：Qwen3-VL-235B 总参数 235B 但仅 22B 激活
- DeepSeek-V3.2：671B 总参数仅 37B 激活

##### (3) 光学压缩（Contexts Optical Compression）

- DeepSeek-OCR 提出：将文本渲染为图像，用视觉编码器压缩
- 可压缩 20x 同时保持 97% OCR 精度
- 反向思维：不是让 LLM 看图，而是让 LLM 通过"看"来压缩文本

##### (4) 多模态 RAG

- 检索增强生成从纯文本扩展到多模态
- 可以检索相关图像、表格、图表来辅助回答
- 企业文档理解的核心能力

#### 8.6 面试答题模板

> "2026 年多模态的四大趋势：一是 Native Multimodal，从预训练就联合所有模态，消除桥接瓶颈；二是 Any-to-Any，支持任意模态组合的输入输出；三是 World Model，从视觉描述走向物理理解和未来预测；四是 Embodied Intelligence，将 VLM 扩展为 VLA（Vision-Language-Action），赋予机器人感知、推理和行动的能力。这些趋势的共同方向是——多模态不再是 LLM 的附加能力，而是成为 AI 理解和交互世界的基础。"

---

## 9. 面试高频题 12 道 + 深度参考答案

### Q1: 多模态大模型的三种主流架构是什么？各有什么优缺点？

**参考答案**：

三种主流架构是 Early Fusion、Cross-Attention Fusion 和 Late Fusion。

**Early Fusion**（代表：LLaVA、InternVL、Qwen-VL）将视觉 token 和文本 token 拼接为单一序列送入 Transformer。优点是实现简单、跨模态交互充分（所有层都有 self-attention）；缺点是视觉 token 占用大量 context window，计算复杂度为 $O((N_v + N_t)^2)$。

**Cross-Attention Fusion**（代表：Flamingo、ViCA）在 LLM 层中插入 cross-attention 模块，文本 token 作 Query，视觉特征作 Key/Value。优点是视觉不占 context、效率高；缺点是需要修改 LLM 架构、交互不如 Early Fusion 充分。

**Late Fusion** 各模态独立编码后合并高层表示。最简单但交互最少，不适合需要精细推理的任务。

当前主流是 Early Fusion 路线，但 hybrid 方案（如 NVLM-H 混合使用 self-attention 和 cross-attention）是未来趋势。

---

### Q2: SigLIP 相比 CLIP 有什么改进？为什么成为新的主流选择？

**参考答案**：

CLIP 使用 softmax 归一化的 InfoNCE Loss，需要在 batch 内做全局归一化来计算对比损失。这意味着 batch size 越大效果越好，但也带来巨大的跨 GPU 通信开销。

SigLIP 的核心改进是将对比损失替换为 **Sigmoid Loss**——每个 image-text pair 独立做二分类，判断是否匹配。这消除了全局归一化的需求，不同 pair 之间互不影响，因此可以使用更大的 batch size（Google 用了 32K）而不增加通信成本。在相同训练量下，SigLIP 的 zero-shot 性能全面超过 CLIP。

SigLIP2 进一步引入了多任务训练（对比学习 + 自蒸馏 + caption 生成 + 区域感知）和 NaViT（多分辨率支持），大幅提升了细粒度视觉理解。LLaVA-OneVision 用 SigLIP 作为视觉编码器就是看中了这些优势。

---

### Q3: Pixel Unshuffle 是什么？为什么 InternVL 要用它？

**参考答案**：

Pixel Unshuffle 是一种空间到通道的转换操作：将空间维度上相邻的像素重排到通道维度。例如，对 ViT 输出的 $H \times W \times C$ 特征图，2x Pixel Unshuffle 会得到 $\frac{H}{2} \times \frac{W}{2} \times 4C$ 的特征图。

InternVL 用它的核心目的是**减少视觉 token 数量到 1/4**。因为在 Early Fusion 架构中，视觉 token 直接占用 LLM 的 context window。一张高分辨率图像动辄产生数千个 token，如果不压缩，多图或长文档场景下 context 会爆炸。

Pixel Unshuffle 的优势在于它是无损信息变换（不丢失信息，只改变维度分配），配合后续的 MLP Projector 可以在不丢关键视觉信息的前提下大幅降低 token 数量。这是 InternVL 系列能在保持性能的同时处理更多图像的关键技术。

---

### Q4: 多模态模型的幻觉问题有哪些类型？怎么缓解？

**参考答案**：

多模态幻觉的主要类型：

1. **物体幻觉**：描述图中不存在的物体（如"图中有一只猫"但实际没有）
2. **属性幻觉**：物体存在但属性描述错误（如颜色、大小、形状错误）
3. **关系幻觉**：物体间空间关系描述错误（如"A 在 B 左边"但实际在右边）
4. **数量幻觉**：计数错误
5. **OCR 幻觉**：文字识别错误或编造不存在的文字

**缓解方法**：
- **训练阶段**：DPO/RLHF 针对幻觉数据做偏好优化（如 LLaVA-RLHF、RLHF-V）
- **数据阶段**：使用高质量、多样化的训练数据，减少 caption 数据中的错误
- **推理阶段**：引入 visual grounding（让模型指出证据区域）、chain-of-thought（先分析图像内容再回答）
- **架构层面**：提升分辨率处理能力（Dynamic Resolution），让模型能看清细节
- **后处理**：用额外模型检查输出与图像的一致性

---

### Q5: 解释 Dynamic Resolution 技术及其在 VLM 中的重要性。

**参考答案**：

Dynamic Resolution 是让模型能处理不同分辨率和宽高比图像的技术，而不是粗暴地 resize 到固定尺寸。

**核心做法**：将图像按原始宽高比分割为多个固定大小的 tiles（如 448×448），每个 tile 独立通过 ViT 编码，同时保留一个 resize 到低分辨率的 thumbnail 作为全局语义表示。

**为什么重要**：
- 固定分辨率会严重损害 OCR 能力（小字体被 resize 后无法识别）
- 不同宽高比的图像强制 resize 会引入变形
- 文档、表格、PPT 等场景对分辨率要求极高

**各模型的实现**：
- InternVL：Dynamic Resolution + Pixel Unshuffle（token 数降 1/4）+ V2PE（自适应位置编码）
- Qwen-VL：自适应分辨率编码，根据图像内容动态调整
- LLaVA-OneVision：AnyRes 策略，控制不同场景的最大 token 数相近
- Pixtral（Mistral）：原生分辨率处理，完全不做 resize

---

### Q6: 为什么视频理解比图像理解难得多？Video-LLM 的关键挑战是什么？

**参考答案**：

视频比图像多了**时间维度**，带来三个核心挑战：

1. **Token 爆炸**：1 秒 30fps 视频 = 30 帧 × 数百 tokens/帧 = 数千 tokens。1 分钟就是数十万 tokens，远超大多数 LLM 的 context window。

2. **时序建模**：动作识别（"正在跑"vs"站着"）、事件因果（"球掉下来因为被踢了"）、状态变化（"水杯从满变空"）都需要跨帧推理。单独看每一帧无法获得这些信息。

3. **信息冗余**：视频中相邻帧高度相似，需要高效的压缩策略。

**解决方案的演进**：
- 基础方案：Uniform Frame Sampling（可能错过关键帧）
- 进阶方案：Temporal Encoder（如 STORM 的 Mamba-based projector）显式编码时序
- 前沿方案：超长上下文（Gemini 2M / Qwen3-VL 1M）+ Hierarchical Token Compression

---

### Q7: 原生多模态（Native Multimodal）和模块化多模态（Modular Multimodal）有什么本质区别？

**参考答案**：

**模块化多模态**：分别预训练视觉编码器和 LLM，然后通过 Projector（MLP / Q-Former）连接。典型架构是"冻结 ViT + 可训练 Projector + 冻结/微调 LLM"。存在"桥接瓶颈"——Projector 的有限表达能力限制了视觉信息向 LLM 的传递。

**原生多模态**：从预训练第一步就将所有模态的 token 统一处理。图像 patch、文本 token、音频帧被编码为同一 vocabulary 空间中的 token，用统一的 next-token prediction 目标训练。不存在独立的"视觉编码器"和"语言模型"的概念边界。

**本质区别**：
- 模块化 = 先独立学习再对齐（类比：先分别学英语和中文，再学翻译）
- 原生 = 从一开始就联合学习（类比：在双语环境中同时学两种语言）

**各自优势**：
- 模块化：训练成本低、灵活可替换、开源生态成熟
- 原生：跨模态理解更深、没有瓶颈、性能天花板更高

---

### Q8: MoE（Mixture of Experts）在多模态模型中有什么特殊应用？

**参考答案**：

MoE 在多模态模型中有两个层面的应用：

**模型效率层面**：
- Qwen3-VL-235B 总参数 235B 但每次推理仅激活 22B
- DeepSeek-V3.2 总参数 671B 但仅激活 37B
- Gemini Flash 系列用 MoE 实现 2x 推理加速
- 大幅降低推理成本的同时保持大模型的知识容量

**多模态特定层面**：
- 不同模态可以路由到不同的专家网络
- 视觉 token 可能激活"视觉专家"，文本 token 激活"语言专家"
- 跨模态 token 可能同时激活多个专家，实现融合推理
- 这种设计让模型在不同模态上都保持专业化，同时支持跨模态协作

**为什么 MoE 在多模态中特别重要**：
多模态模型需要处理的信息量远大于纯文本模型（一张图 = 数百 tokens），MoE 让模型在保持大容量的同时控制每次推理的计算开销。

---

### Q9: 解释 Vision-Language-Action (VLA) 模型，它和传统 VLM 有什么区别？

**参考答案**：

VLM（Vision-Language Model）处理视觉和文本的理解与生成：输入图像+文本，输出文本。

VLA（Vision-Language-Action Model）在 VLM 基础上增加了**动作输出**：输入图像+文本指令，输出机器人控制指令（如关节角度、末端执行器位置、导航路径）。

**典型架构**：
```
Camera Image + "Pick up the red cup" → VLA Model → [gripper_x, gripper_y, gripper_z, open/close]
```

**代表模型**：RT-2 (Google)、Octo、OpenVLA

**关键挑战**：
- 训练数据稀缺：机器人操作数据远少于 image-caption 数据
- 动作空间连续化：机器人控制量是连续值，不是离散 token
- 实时性要求：机器人控制需要低延迟
- 泛化性：在模拟环境训练的模型需要迁移到真实环境

VLA 是多模态走向具身智能的关键一步，也是 2026 年最受关注的研究方向之一。

---

### Q10: 如何评估多模态模型的性能？有哪些关键 benchmark？

**参考答案**：

多模态评估是个多维度问题，主要 benchmark 包括：

| Benchmark | 考察维度 | 说明 |
|-----------|---------|------|
| **MMMU** | 多学科多模态理解 | 大学水平的多学科题目+图表，综合评估视觉理解+知识推理 |
| **MMBench** | 多维度视觉理解 | 系统化评估感知、推理、知识等能力 |
| **TextVQA / OCRBench** | OCR 能力 | 评估图像中文字识别和理解 |
| **DocVQA** | 文档理解 | 评估对文档布局、表格、图表的理解 |
| **ChartQA** | 图表理解 | 评估对统计图表的数据读取和推理 |
| **VideoMME / MVBench** | 视频理解 | 评估时序推理和视频分析 |
| **POPE** | 幻觉检测 | 专门评估物体幻觉程度 |
| **MathVista** | 数学视觉推理 | 数学图形+推理 |
| **RealWorldQA** | 真实场景理解 | 评估在真实照片上的理解能力 |

**评估的陷阱**：
- 大多数视频 benchmark 存在 modality bias（仅看字幕就能回答大部分问题）
- 静态 benchmark 容易被 data contamination 污染
- 需要综合多个 benchmark 才能全面评估
- 实际应用中的性能与 benchmark 分数可能有显著差异

---

### Q11: DeepSeek-OCR 的"光学压缩"（Contexts Optical Compression）是什么思路？

**参考答案**：

这是一个非常巧妙的反向思维：传统方向是让 LLM 理解图像中的文字（图→文），DeepSeek-OCR 反过来——**将文字渲染为图像，用视觉编码器压缩后再用语言模型解码**（文→图→文）。

**为什么这有意义**：
- LLM 处理长文本的计算开销是 $O(n^2)$（attention），n 是 token 数
- 但一张包含大量文字的图像，用视觉编码器只需要少量 visual tokens 就能表示
- 例如一页文档可能有 2000 个文本 token，但渲染为图像后只需 200 个 visual tokens

**技术实现**：
- DeepEncoder（轻量视觉编码器）将高分辨率文档图像编码为紧凑 visual tokens
- DeepSeek-3B-MoE-A570M 解码器将 visual tokens 解码回文字
- 压缩比可达 20x，在 10x 以下压缩比时保持 97% OCR 精度
- 在 A100-40G 上达到近 2500 tokens/s 的解码速度

**面试亮点**：这体现了"不同模态可以互相利用"的思想——视觉编码器天然具有空间压缩能力，可以用来压缩文本信息。

---

### Q12: 2026 年，如果你要从零设计一个多模态模型，你会怎么选择架构？

**参考答案**：

这要根据目标和资源分情况讨论：

**如果资源充足（大厂级别）**：
- **Native Multimodal Pre-training**：统一 tokenizer + 统一 next-token prediction
- 参考 GPT-4o / Gemini 的路线
- 所有模态从第一步就联合训练
- 使用 MoE 架构控制推理成本（如 Qwen3-VL 的 235B/22B 方案）

**如果资源有限（创业/学术级别）**：
- **ViT-MLP-LLM 三段式**（InternVL / LLaVA 路线）
- 视觉编码器选 SigLIP2（最新最强的开源选择）
- LLM 骨干选 Qwen3 或 Llama 4（开源 SOTA）
- Projector 用 2 层 MLP（简单有效）
- Dynamic Resolution + Pixel Unshuffle 处理分辨率
- 三阶段训练：Alignment → Instruction Tuning → DPO

**通用建议**：
1. 优先选择 Dynamic Resolution（而非固定分辨率）
2. 考虑 V2PE 或类似的自适应位置编码
3. 训练数据质量 > 数量
4. DPO 对减少幻觉效果显著，一定要做
5. 如果需要视频能力，考虑加入 Temporal Encoder（而非纯帧采样）

---

## 附录 A：关键术语速查

| 术语 | 全称 | 含义 |
|------|------|------|
| VLM | Vision-Language Model | 视觉-语言模型 |
| MLLM | Multimodal Large Language Model | 多模态大语言模型 |
| VLA | Vision-Language-Action | 视觉-语言-动作模型 |
| ViT | Vision Transformer | 视觉 Transformer |
| MoE | Mixture of Experts | 混合专家架构 |
| ITC | Image-Text Contrastive | 图文对比学习 |
| ITM | Image-Text Matching | 图文匹配 |
| Q-Former | Querying Transformer | BLIP-2 的跨模态桥接模块 |
| DPO | Direct Preference Optimization | 直接偏好优化 |
| RLHF | Reinforcement Learning from Human Feedback | 基于人类反馈的强化学习 |
| RVQ | Residual Vector Quantization | 残差向量量化（音频编码） |
| V2PE | Variable Visual Position Encoding | 可变视觉位置编码 |
| NaViT | Native Resolution ViT | 原生分辨率 ViT |

## 附录 B：推荐阅读

1. **Survey 类**
   - "A Survey of Multimodal Models on Language and Vision" (Dec 2025) — 统一建模视角
   - "Efficient Multimodal Large Language Models: A Survey" (Dec 2025, Springer) — 效率优化
   - "Vision Language Models: A Survey of 26K Papers" (Oct 2025, arXiv) — 26K 论文趋势分析

2. **架构类**
   - InternVL 3.0/3.5 Technical Report — ViT-MLP-LLM 范式的最佳实践
   - LLaVA-OneVision 1.5 Paper — 开源 VLM 框架设计
   - ViCA (Feb 2025) — Vision-Only Cross-Attention 的效率方案
   - Sebastian Raschka "Understanding Multimodal LLMs" — 最好的入门综述

3. **视频理解**
   - STORM (NVIDIA) — Mamba-based Temporal Projector
   - VideoLoom — Multi-scale Feature Fusion

4. **前沿方向**
   - GPT-4o System Card — 原生多模态架构参考
   - DeepSeek-OCR Paper — 光学压缩的创新思路
   - AIMv2 (Apple) — 自回归预训练视觉编码器

---

> **最后的面试建议**：多模态面试不是背概念，而是展示你对**为什么这样设计**的理解。每回答一个架构选择，都要说出 trade-off：选 Early Fusion 是因为简单且充分交互，代价是 context 占用大；选 Dynamic Resolution 是因为保留细节，代价是 token 数不确定。能说出 trade-off 的候选人，才是真正理解技术的人。

---

## See Also

- [[AI/3-LLM/MLLM/目录|MLLM MOC]] — 多模态大模型知识全图谱
- [[MLLM 概述|MLLM 概述]] — MLLM 基础概念版
- [[R-4B 论文|R-4B]] — MLLM + RL 的代表论文：Auto-Thinking 能力
- [[AI/3-LLM/目录|LLM MOC]] — 多模态 LLM 的 LLM 基础
