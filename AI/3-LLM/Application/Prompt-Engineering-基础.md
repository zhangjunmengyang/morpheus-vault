---
brief: "Prompt Engineering 基础——零样本/少样本/角色提示/输出格式控制等基础技巧；理解 LLM 对 prompt 敏感的内在原因（token 概率/上下文窗口）；是高级 Prompt 技巧的前置知识。"
title: "Prompt Engineering"
type: concept
domain: ai/llm/prompt-engineering
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - ai/llm/prompt-engineering
  - type/concept
---
# Prompt Engineering

goodcase：[提示工程指南 | Prompt Engineering Guide](https%3A%2F%2Fwww.promptingguide.ai%2Fzh)

# Zero-Shot Prompting

指令调整已被证明可以改善零样本学习[Wei等人（2022）(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fpdf%2F2109.01652.pdf)。指令调整本质上是在通过指令描述的数据集上微调模型的概念。此外，[RLHF(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F1706.03741)（来自人类反馈的强化学习）已被采用以扩展指令调整，其中模型被调整以更好地适应人类偏好。这一最新发展推动了像ChatGPT这样的模型。我们将在接下来的章节中讨论所有这些方法和方法。

当零样本不起作用时，建议在提示中提供演示或示例，这就引出了少样本提示。

# Few-Shot Prompting

虽然大型语言模型展示了惊人的零样本能力，但在使用零样本设置时，它们在更复杂的任务上仍然表现不佳。少样本提示可以作为一种技术，以启用上下文学习，我们在提示中提供演示以引导模型实现更好的性能。演示作为后续示例的条件，我们希望模型生成响应。

根据 [Touvron et al. 2023(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fpdf%2F2302.13971.pdf) 等人的在 2023 年的论文，当模型规模足够大时，小样本提示特性开始出现 [(Kaplan et al., 2020)(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2001.08361)。

# Chain-of-Thought Prompting

在 [Wei等人（2022）(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2201.11903) 中引入的链式思考（CoT）提示通过中间推理步骤实现了复杂的推理能力。您可以将其与少样本提示相结合，以获得更好的结果，以便在回答之前进行推理的更复杂的任务。

**自动思维链（Auto-CoT）**

当使用思维链提示时，这个过程需要手工制作有效且多样化的例子。这种手动工作可能会导致次优解决方案。[Zhang et al. （2022）(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2210.03493) 提出了一种消除人工的方法，即利用 LLMs “让我们一步一步地思考” 提示来生成一个接一个的推理链。这种自动过程仍然可能在生成的链中出现错误。为了减轻错误的影响，演示的多样性很重要。这项工作提出了Auto-CoT，它对具有多样性的问题进行采样，并生成推理链来构建演示。

Auto-CoT 主要由两个阶段组成：

- 阶段1：**问题聚类**：将给定问题划分为几个聚类
- 阶段2：**演示抽样**：从每组数组中选择一个具有代表性的问题，并使用带有简单启发式的 Zero-Shot-CoT 生成其推理链
# Meta Prompting

元提示是一种高级提示技术，它侧重于任务和问题的结构和句法层面，而非其具体的内容细节。元提示的目标是构建一种更抽象、更结构化的方式与大型语言模型 (LLM) 进行交互，强调信息的形式和模式，而非传统的以内容为中心的方法。

根据[Zhang 等人（2024）（在新标签页中打开）](https%3A%2F%2Farxiv.org%2Fabs%2F2311.11482)，元提示的关键特征可以概括如下：

1. **结构导向**：优先考虑问题和解决方案的格式和模式，而不是具体内容。
1. **以语法为中心**：使用语法作为预期响应或解决方案的指导模板。
1. **抽象例子**：以抽象的例子作为框架，说明问题和解决方案的结构，而不关注具体的细节。
1. **多功能**：适用于各个领域，能够对各种问题提供结构化的响应。
1. **分类方法**：借鉴类型理论，强调提示中组成部分的分类和逻辑排列。
元提示和少量提示的不同之处在于，元提示更侧重于结构导向的方法，而不是少量提示强调的内容驱动的方法。

# Self-Consistency

也许在提示工程中更高级的技术之一是自我一致性。由 [Wang等人（2022）(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fpdf%2F2203.11171.pdf) 提出，自我一致性旨在“替换链式思维提示中使用的天真贪婪解码方法”。其想法是通过少样本 CoT 采样多个不同的推理路径，并使用生成结果选择最一致的答案。这有助于提高 CoT 提示在涉及算术和常识推理的任务中的性能。

# Generated Knowledge Prompting

LLM 继续得到改进，其中一种流行的技术是能够融合知识或信息，以帮助模型做出更准确的预测。

使用类似的思路，模型是否也可以在做出预测之前用于生成知识呢？这就是 [Liu 等人 2022(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fpdf%2F2110.08387.pdf) 的论文所尝试的——生成知识以作为提示的一部分。特别是，这对于常识推理等任务有多大帮助？

简直是废话论文。。。结论就是有知识比没有知识强。。。

# Prompt Chaining

提示链可以用于不同的场景，这些场景可能涉及多个操作或转换。例如，LLM 的一个常见用途是根据大型文本文档回答问题。想要更好阅读大文本文档，可以设计两个不同的提示，第一个提示负责提取相关引文以回答问题，第二个提示则以引文和原始文档为输入来回答给定的问题。换句话说，可以创建两个不同的提示来执行根据文档回答问题的任务。

# Tree of Thoughts (ToT)

对于需要探索或预判战略的复杂任务来说，传统或简单的提示技巧是不够的。最近，[Yao et el. (2023)(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2305.10601) 提出了思维树（Tree of Thoughts，ToT）框架，该框架基于思维链提示进行了总结，引导语言模型探索把思维作为中间步骤来解决通用问题。

ToT 维护着一棵思维树，思维由连贯的语言序列表示，这个序列就是解决问题的中间步骤。使用这种方法，LM 能够自己对严谨推理过程的中间思维进行评估。LM 将生成及评估思维的能力与搜索算法（如广度优先搜索和深度优先搜索）相结合，在系统性探索思维的时候可以向前验证和回溯。

# Retrieval Augmented Generation (RAG)

通用语言模型通过微调就可以完成几类常见任务，比如分析情绪和识别命名实体。这些任务不需要额外的背景知识就可以完成。

要完成更复杂和知识密集型的任务，可以基于语言模型构建一个系统，访问外部知识源来做到。这样的实现与事实更加一性，生成的答案更可靠，还有助于缓解“幻觉”问题。

# Automatic Prompt Engineer (APE)

[Zhou等人，（2022）(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2211.01910) 提出了自动提示工程师 （APE），这是一个用于自动指令生成和选择的框架。指令生成问题被构建为自然语言合成问题，使用 LLMs 作为黑盒优化问题的解决方案来生成和搜索候选解。

第一步涉及一个大型语言模型（作为推理模型），该模型接收输出演示以生成任务的指令候选项。这些候选解将指导搜索过程。使用目标模型执行指令，然后根据计算的评估分数选择最合适的指令。

APE 发现了一个比人工设计的“让我们一步一步地思考”提示更好的零样本 CoT 提示 （[Kojima 等人，2022(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2205.11916)）。

APE 相关论文：

- [Prompt-OIRL(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2309.06553) - 使用离线逆强化学习来生成与查询相关的提示。
- [OPRO(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2309.03409) - 引入使用 LLMs 优化提示的思想：让 LLMs “深呼吸”提高数学问题的表现。
- [AutoPrompt(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2010.15980) - 提出了一种基于梯度引导搜索的方法，用于自动创建各种任务的提示。
- [Prefix Tuning(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2101.00190) - 是一种轻量级的 fine-tuning 替代方案，为 NLG 任务添加可训练的连续前缀。
- [Prompt Tuning(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2104.08691) - 提出了一种通过反向传播学习软提示的机制。
# Active-Prompt

思维链（CoT）方法依赖于一组固定的人工注释范例。问题在于，这些范例可能不是不同任务的最有效示例。为了解决这个问题，[Diao 等人（2023）(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fpdf%2F2302.12246.pdf)最近提出了一种新的提示方法，称为 Active-Prompt，以适应 LLMs 到不同的任务特定示例提示（用人类设计的 CoT 推理进行注释）。

下面是该方法的说明。第一步是使用或不使用少量 CoT 示例查询 LLM。对一组训练问题生成 *k* 个可能的答案。基于 *k* 个答案计算不确定度度量（使用不一致性）。选择最不确定的问题由人类进行注释。然后使用新的注释范例来推断每个问题。

# Directional Stimulus Prompting

[Li 等人，（2023）(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2302.11520)提出了一种新的提示技术，以更好地指导 LLM 生成所需的摘要。

训练了一个可调节的策略 LM 来生成刺激/提示。越来越多地使用RL来优化 LLM。

下图显示了方向性刺激提示与标准提示的比较。策略 LM 可以很小，并且可以优化以生成指导黑盒冻结 LLM 的提示。

# PAL (Program-Aided Language Models)

[Gao 等人（2022）(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2211.10435)提出了一种使用 LLMs 读取自然语言问题并生成程序作为中间推理步骤的方法。被称为程序辅助语言模型（PAL），它与思维链提示不同，因为它不是使用自由形式文本来获得解决方案，而是将解决步骤卸载到类似 Python 解释器的编程运行时中。

# ReAct

从 [Yao 等人，2022(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2210.03629) 引入了一个框架，其中 LLMs 以交错的方式生成 *推理轨迹* 和 *任务特定操作* 。

生成推理轨迹使模型能够诱导、跟踪和更新操作计划，甚至处理异常情况。操作步骤允许与外部源（如知识库或环境）进行交互并且收集信息。

ReAct 框架允许 LLMs 与外部工具交互来获取额外信息，从而给出更可靠和实际的回应。

结果表明，ReAct 可以在语言和决策任务上的表现要高于几个最先进水准要求的的基线。ReAct 还提高了 LLMs 的人类可解释性和可信度。总的来说，作者发现了将 ReAct 和链式思考 (CoT) 结合使用的最好方法是在推理过程同时使用内部知识和获取到的外部信息。

论文首先对 ReAct 在知识密集型推理任务如问答 (HotPotQA) 和事实验证 ([Fever(opens in a new tab)](https%3A%2F%2Ffever.ai%2Fresources.html)) 上进行了评估。PaLM-540B 作为提示的基本模型。

通过在 HotPotQA 和 Fever 上使用不同提示方法得到的提示的表现结果说明了 ReAct 表现结果通常优于 Act (只涉及操作)。

我们还可以观察到 ReAct 在 Fever 上的表现优于 CoT，而在 HotpotQA 上落后于 CoT。

误差分析:

- **CoT 存在事实幻觉的问题**
- **ReAct 的结构性约束降低了它在制定推理步骤方面的灵活性**
- **ReAct 在很大程度上依赖于它正在检索的信息;非信息性搜索结果阻碍了模型推理，并导致难以恢复和重新形成思想**
结合并支持在 ReAct 和链式思考+自我一致性之间切换的提示方法通常优于所有其他提示方法。

知识密集型适合 COT 效果更好，事实验证型 ReAct** **效果更好。

# Reflexion

自我反思是一个通过语言反馈来强化基于语言的智能体的框架。根据 [Shinn et al. (2023)(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fpdf%2F2303.11366.pdf)，“自我反思是一种‘口头’强化的新范例，它将策略参数化为智能体的记忆编码与 LLM 的参数选择配对。”

在高层次上，自我反思将来自环境的反馈（自由形式的语言或者标量）转换为语言反馈，也被称作 **self-reflection**，为下一轮中 LLM 智能体提供上下文。这有助于智能体快速有效地从之前的错误中学习，进而提升许多高级任务的性能。

自我反思由三个不同的模型组成：

- **参与者（Actor）**：根据状态观测量生成文本和动作。参与者在环境中采取行动并接受观察结果，从而形成轨迹。[链式思考（CoT）(opens in a new tab)](https%3A%2F%2Fwww.promptingguide.ai%2Ftechniques%2Fcot) 和 [ReAct(opens in a new tab)](https%3A%2F%2Fwww.promptingguide.ai%2Ftechniques%2Freact) 被用作参与者模型。此外，还添加了记忆组件为智能体提供额外的上下文信息。
- **评估者（Evaluator）**：对参与者的输出进行评价。具体来说，它将生成的轨迹（也被称作短期记忆）作为输入并输出奖励分数。根据人物的不同，使用不同的奖励函数（决策任务使用LLM和基于规则的启发式奖励）。
- **自我反思（Self-Reflection）**：生成语言强化线索来帮助参与者实现自我完善。这个角色由大语言模型承担，能够为未来的试验提供宝贵的反馈。自我反思模型利用奖励信号、当前轨迹和其持久记忆生成具体且相关的反馈，并存储在记忆组件中。智能体利用这些经验（存储在长期记忆中）来快速改进决策。
总的来说，自我反思的关键步骤是a)定义任务，b)生成轨迹，c)评估，d)执行自我反思，e)生成下一条轨迹。下图展示了自我反思的智能体学习迭代优化其行为来解决决策、编程和推理等各种人物的例子。自我反思（Refelxion）通过引入自我评估、自我反思和记忆组件来拓展 ReAct 框架。

**自我反思最适合以下情况**

1. **智能体需要从尝试和错误中学习**：自我反思旨在通过反思过去的错误并将这些知识纳入未来的决策来帮助智能体提高表现。这非常适合智能体需要通过反复试验来学习的任务，例如决策、推理和编程。
1. **传统的强化学习方法失效**：传统的强化学习（RL）方法通常需要大量的训练数据和昂贵的模型微调。自我反思提供了一种轻量级替代方案，不需要微调底层语言模型，从而使其在数据和计算资源方面更加高效。
1. **需要细致入微的反馈**：自我反思利用语言反馈，这比传统强化学习中使用的标量奖励更加细致和具体。这让智能体能够更好地了解自己的错误，并在后续的试验中做出更有针对性的改进。
1. **可解释性和直接记忆很重要**：与传统的强化学习方法相比，自我反思提供了一种更可解释、更直接的情景记忆形式。智能体的自我反思存储在其记忆组件中，让分析和理解其学习过程变得更加简单。
自我反思在以下任务中是有效的：

- **序列决策**：自我反思提高了智能体在 AlfWorld 任务中的表现，涉及在各种环境中导航并完成多步目标。
- **推理**：自我反思提高了 HotPotQA 上智能体的性能，HotPotQA 是一个需要对多个文档进行推理的问答数据集。
- **编程**：自我反思的智能体在 HumanEval 和 MBPP 等基准测试上编写出了更好的代码，在某些情况下实现 SOTA 结果。
以下是自我反思的一些限制：

- **依赖自我评估能力**：反思依赖于智能体准确评估其表现并产生有用反思的能力。这可能是具有挑战性的，尤其是对于复杂的任务，但随着模型功能的不断改进，预计自我反思会随着时间的推移而变得更好。
- **长期记忆限制**：自我反思使用最大容量的滑动窗口，但对于更复杂的任务，使用向量嵌入或 SQL 数据库等高级结构可能会更有利。
- **代码生成限制**：测试驱动开发在指定准确的输入输出映射方面存在限制（例如，受硬件影响的非确定性生成器函数和函数输出）。
# Multimodal CoT Prompting

最近，[Zhang等人（2023）(opens in a new tab)](https%3A%2F%2Farxiv.org%2Fabs%2F2302.00923)提出了一种多模态思维链提示方法。传统的思维链提示方法侧重于语言模态。相比之下，多模态思维链提示将文本和视觉融入到一个两阶段框架中。第一步涉及基于多模态信息的理性生成。接下来是第二阶段的答案推断，它利用生成的理性信息。

---

## See Also

- [[AI/3-LLM/Prompt-Engineering/Prompt Engineering 高级|高级 Prompt 技巧]] — 进阶篇
- [[Prompt-Engineering-2026实战全景|Prompt Engineering 2026 实战全景]] — 2026 年完整全景版（2784行）
- [[GRPO-Improvement-Panorama-2026|GRPO 2026 全景]] — 自动 PE 方向：E-SPL/GEPA 用 RL 进化 prompt
- [[AI/3-LLM/目录|LLM MOC]] — 大语言模型知识全图谱
