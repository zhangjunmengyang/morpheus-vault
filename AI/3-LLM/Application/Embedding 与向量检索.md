---
title: Embedding ä¸å‘é‡æ£€ç´¢ï¼šä»æ¨¡å‹é€‰å‹åˆ°å·¥ç¨‹è½åœ°
brief: æ–‡æœ¬ Embedding ä» Word2Vec åˆ°æŒ‡ä»¤æ„ŸçŸ¥æ¨¡å‹çš„æ¼”è¿›ã€2024-2025 ä¸»æµæ¨¡å‹é€‰å‹ï¼ˆBGE-M3/E5-Mistral/GTE-Qwen2/Cohere/OpenAIï¼‰ã€ANN ç®—æ³•æ·±åº¦è§£æï¼ˆHNSW/IVF/PQï¼‰ã€å‘é‡æ•°æ®åº“å¯¹æ¯”ï¼ˆMilvus/Qdrant/Chroma/pgvectorï¼‰å’Œç»´åº¦ä¼˜åŒ–ã€‚RAG çš„åŸºçŸ³ç»„ä»¶ã€‚
date: 2026-02-13
updated: 2026-02-22
tags:
  - ai/llm/application
  - ai/embedding
  - ai/vector-search
  - ai/rag
  - type/practice
  - interview/hot
status: complete
sources:
  - OpenAI. 'text-embedding-ada-002 / text-embedding-3-large' API Documentation
  - "Xiao et al. 'C-Pack: Packaged Resources To Advance General Chinese Embedding (BGE)' arXiv:2309.07597"
  - Wang et al. 'Text Embeddings by Weakly-Supervised Contrastive Pre-training (E5)' arXiv:2212.03533
  - "Chen et al. 'BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity' arXiv:2402.03216"
  - Johnson et al. 'Billion-scale similarity search with GPUs (FAISS)' arXiv:1702.08734
related:
  - "[[RAG åŸç†ä¸æ¶æ„]]"
  - "[[æ£€ç´¢ç­–ç•¥]]"
  - "[[RAG-2026-æŠ€æœ¯å…¨æ™¯|RAG 2026 å…¨æ™¯]]"
---

# Embedding ä¸å‘é‡æ£€ç´¢ï¼šä»æ¨¡å‹é€‰å‹åˆ°å·¥ç¨‹è½åœ°

> å‘é‡æ£€ç´¢æ˜¯ RAG çš„åŸºçŸ³â€”â€”é€‰å¯¹ Embedding æ¨¡å‹ã€ç”¨å¯¹ ANN ç®—æ³•ã€é…å¥½å‘é‡æ•°æ®åº“ï¼Œå†³å®šäº†æ£€ç´¢è´¨é‡çš„ä¸Šé™

## 1. Embedding æ¨¡å‹åŸºç¡€

### ä»€ä¹ˆæ˜¯æ–‡æœ¬ Embeddingï¼Ÿ

```
æ–‡æœ¬ â†’ Embedding æ¨¡å‹ â†’ å›ºå®šç»´åº¦çš„ç¨ å¯†å‘é‡ âˆˆ R^d

"æœºå™¨å­¦ä¹ å¾ˆæœ‰è¶£" â†’ [0.023, -0.156, 0.089, ..., 0.045]  (d=1024)

æ ¸å¿ƒæ€§è´¨: è¯­ä¹‰ç›¸è¿‘çš„æ–‡æœ¬ â†’ å‘é‡ç©ºé—´ä¸­è·ç¦»è¿‘
  sim("æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ") > sim("æœºå™¨å­¦ä¹ ", "ç…®é¥­æŠ€å·§")
```

### è®­ç»ƒèŒƒå¼æ¼”è¿›

```
ç¬¬ä¸€ä»£: Word2Vec / GloVe â†’ è¯çº§åˆ«ï¼Œæ— ä¸Šä¸‹æ–‡
ç¬¬äºŒä»£: BERT â†’ å¥çº§åˆ« (CLS token æˆ– mean pooling)
ç¬¬ä¸‰ä»£: å¯¹æ¯”å­¦ä¹  â†’ SimCSE, E5, BGE
ç¬¬å››ä»£: æŒ‡ä»¤æ„ŸçŸ¥ â†’ Instructor, E5-Mistral, BGE-M3 (2024+)

å…³é”®è½¬æŠ˜: ç¬¬ä¸‰ä»£å¼€å§‹ä½¿ç”¨å¯¹æ¯”å­¦ä¹  (contrastive learning)
  â†’ æ­£ä¾‹å¯¹: (query, relevant_doc)
  â†’ è´Ÿä¾‹: in-batch negatives + hard negatives
  â†’ Loss: InfoNCE / triplet loss
```

## 2. ä¸»æµ Embedding æ¨¡å‹é€‰å‹

### 2024-2025 ä¸»æµæ¨¡å‹å¯¹æ¯”

```
æ¨¡å‹              ç»´åº¦    æœ€å¤§é•¿åº¦  è¯­è¨€     ç‰¹ç‚¹                   MTEB æ’å
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BGE-M3            1024    8192     å¤šè¯­è¨€   å¤šç²’åº¦æ£€ç´¢(dense+sparse+colbert) é¡¶çº§
BGE-large-zh-v1.5 1024    512      ä¸­æ–‡     ä¸­æ–‡æœ€å¼ºå¼€æº              ä¸­æ–‡ç¬¬ä¸€
E5-Mistral-7B     4096    32K      å¤šè¯­è¨€   åŸºäº LLM çš„ embedding     é¡¶çº§
GTE-Qwen2-7B      3584    32K      å¤šè¯­è¨€   Qwen2 backbone            é¡¶çº§
Cohere embed-v3   1024    512      å¤šè¯­è¨€   å•†ä¸š APIï¼Œå‹ç¼©æ”¯æŒ        å•†ä¸šç¬¬ä¸€
OpenAI text-3-large 3072  8191     å¤šè¯­è¨€   å•†ä¸š APIï¼Œå¹¿æ³›ä½¿ç”¨        å•†ä¸šé¡¶çº§
Jina-embeddings-v3 1024   8192     å¤šè¯­è¨€   æ”¯æŒ Matryoshka ç»´åº¦      å¼€æºä¼˜ç§€
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### é€‰å‹å†³ç­–æ ‘

```
åœºæ™¯åˆ¤æ–­:
  â”œâ”€ çº¯ä¸­æ–‡ â†’ BGE-large-zh-v1.5 (æ€§èƒ½æœ€ä¼˜)
  â”œâ”€ ä¸­è‹±æ··åˆ â†’ BGE-M3 (å¤šè¯­è¨€+å¤šç²’åº¦)
  â”œâ”€ éœ€è¦è¶…é•¿æ–‡æ¡£ â†’ E5-Mistral-7B / GTE-Qwen2 (32K ä¸Šä¸‹æ–‡)
  â”œâ”€ è¿½æ±‚æè‡´æ•ˆæœ â†’ GTE-Qwen2-7B (æœ€å¼ºå¼€æº)
  â”œâ”€ èµ„æºå—é™ â†’ BGE-small-zh / all-MiniLM (å°æ¨¡å‹)
  â””â”€ å•†ä¸š API â†’ Cohere embed-v3 / OpenAI text-3-large
```

### ä½¿ç”¨ç¤ºä¾‹

```python
# BGE-M3 ä½¿ç”¨ç¤ºä¾‹
from FlagEmbedding import BGEM3FlagModel

model = BGEM3FlagModel("BAAI/bge-m3", use_fp16=True)

queries = ["ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"]
documents = [
    "RAG æ˜¯æ£€ç´¢å¢å¼ºç”Ÿæˆï¼Œç»“åˆæ£€ç´¢å’Œç”Ÿæˆ",
    "ä»Šå¤©å¤©æ°”å¾ˆå¥½",
    "Retrieval-Augmented Generation combines search with LLMs",
]

# ç¼–ç ï¼ˆåŒæ—¶ç”Ÿæˆ dense + sparse + colbert è¡¨ç¤ºï¼‰
q_embeddings = model.encode(queries, return_dense=True, return_sparse=True)
d_embeddings = model.encode(documents, return_dense=True, return_sparse=True)

# Dense æ£€ç´¢
dense_scores = q_embeddings["dense_vecs"] @ d_embeddings["dense_vecs"].T
print(dense_scores)  
# [[0.85, 0.12, 0.78]]  â†’ è¯­ä¹‰æ£€ç´¢

# Sparse æ£€ç´¢ (ç±»ä¼¼ BM25 çš„ç¨€ç–åŒ¹é…)
# BGE-M3 çš„ä¼˜åŠ¿: dense + sparse æ··åˆæ£€ç´¢ï¼Œæ•ˆæœæœ€ä½³
```

```python
# Sentence-Transformers é€šç”¨æ–¹å¼
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("BAAI/bge-large-zh-v1.5")
embeddings = model.encode(["æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ ", "åšé¥­"])

# è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
sim_matrix = cosine_similarity(embeddings)
```

## 3. è·ç¦»åº¦é‡

```
åº¦é‡              å…¬å¼                          é€‚ç”¨åœºæ™¯
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ä½™å¼¦ç›¸ä¼¼åº¦        cos(a,b) = aÂ·b/(|a||b|)       æœ€å¸¸ç”¨ï¼Œå½’ä¸€åŒ–å‘é‡
ç‚¹ç§¯ (Inner Product) aÂ·b                         å·²å½’ä¸€åŒ–æ—¶ = ä½™å¼¦
æ¬§æ°è·ç¦» (L2)      âˆšÎ£(aáµ¢-báµ¢)Â²                  å¯¹ç»å¯¹è·ç¦»æ•æ„Ÿ
æ›¼å“ˆé¡¿è·ç¦» (L1)    Î£|aáµ¢-báµ¢|                     é«˜ç»´ç©ºé—´æ›´é²æ£’
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

å®è·µå»ºè®®:
  - å¤§å¤šæ•° Embedding æ¨¡å‹è¾“å‡ºå·²å½’ä¸€åŒ– â†’ ç”¨ä½™å¼¦æˆ–ç‚¹ç§¯
  - æœªå½’ä¸€åŒ– â†’ ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆè‡ªåŠ¨å½’ä¸€åŒ–ï¼‰
  - å‘é‡æ•°æ®åº“ä¸­ï¼Œå½’ä¸€åŒ–åç”¨ Inner Product æœ€å¿«
```

## 4. ANN ç®—æ³•æ·±åº¦è§£æ

ç²¾ç¡®æœ€è¿‘é‚»æœç´¢ O(NÂ·d) åœ¨ç™¾ä¸‡çº§æ•°æ®ä¸Šå¤ªæ…¢ â†’ éœ€è¦ **è¿‘ä¼¼æœ€è¿‘é‚» (ANN)** ç®—æ³•ã€‚

### 4.1 HNSWï¼ˆHierarchical Navigable Small Worldï¼‰

```
æ ¸å¿ƒæ€æƒ³: å¤šå±‚è·³è¡¨ + è´ªå¿ƒæœç´¢

å±‚ 2 (ç¨€ç–):    A â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ D â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ G
                â”‚                              â”‚
å±‚ 1 (ä¸­ç­‰):    A â”€â”€â”€â”€ C â”€â”€â”€â”€ D â”€â”€â”€â”€ F â”€â”€â”€â”€ G
                â”‚      â”‚      â”‚      â”‚      â”‚
å±‚ 0 (ç¨ å¯†):    A  B  C  D  E  F  G  H  I  J

æœç´¢è¿‡ç¨‹:
  1. ä»æœ€é«˜å±‚çš„å…¥å£ç‚¹å¼€å§‹
  2. è´ªå¿ƒæ‰¾åˆ°å½“å‰å±‚æœ€è¿‘çš„èŠ‚ç‚¹
  3. ä¸‹åˆ°ä¸‹ä¸€å±‚ï¼Œç»§ç»­è´ªå¿ƒæœç´¢
  4. åœ¨ç¬¬ 0 å±‚åšç²¾ç¡®çš„ beam search

å…³é”®å‚æ•°:
  M:       æ¯å±‚æœ€å¤§è¿æ¥æ•° (é»˜è®¤ 16)
  ef_construction: æ„å»ºæ—¶çš„ beam width (é»˜è®¤ 200)
  ef_search:       æœç´¢æ—¶çš„ beam width (é»˜è®¤ 100)
```

```
HNSW æ€§èƒ½ç‰¹å¾:
  æ„å»ºæ—¶é—´:  O(N Â· log(N) Â· M)
  æœç´¢æ—¶é—´:  O(log(N) Â· ef)
  å†…å­˜å ç”¨:  O(N Â· M Â· d)  â€” è¾ƒé«˜ï¼
  
  ä¼˜ç‚¹: æœç´¢é€Ÿåº¦å¿«ã€å¬å›ç‡é«˜ (>99%)
  ç¼ºç‚¹: å†…å­˜å ç”¨å¤§ï¼ˆç´¢å¼•éœ€å…¨éƒ¨åœ¨å†…å­˜ä¸­ï¼‰
  é€‚ç”¨: ç™¾ä¸‡åˆ°åƒä¸‡çº§æ•°æ®ï¼Œæœ‰å……è¶³å†…å­˜
```

### 4.2 IVFï¼ˆInverted File Indexï¼‰

```
æ ¸å¿ƒæ€æƒ³: å…ˆèšç±»ï¼Œæœç´¢æ—¶åªæŸ¥ç›¸å…³èšç±»

æ„å»º:
  1. å¯¹æ‰€æœ‰å‘é‡åš K-Means èšç±» (nlist ä¸ªèšç±»ä¸­å¿ƒ)
  2. æ¯ä¸ªå‘é‡å½’å…¥æœ€è¿‘çš„èšç±»

æœç´¢:
  1. æ‰¾åˆ° query æœ€è¿‘çš„ nprobe ä¸ªèšç±»
  2. åªåœ¨è¿™äº›èšç±»å†…åšç²¾ç¡®æœç´¢

           â”Œâ”€ cluster_0: [v1, v5, v8, ...]
  query â†’ â”œâ”€ cluster_1: [v2, v3, v9, ...]  â† æœç´¢è¿™äº›
           â”œâ”€ cluster_2: [v4, v7, ...]      â† æœç´¢è¿™äº›
           â””â”€ cluster_3: [v6, v10, ...]

å…³é”®å‚æ•°:
  nlist:  èšç±»æ•°é‡ (æ¨è âˆšN åˆ° 4âˆšN)
  nprobe: æœç´¢æ—¶æŸ¥è¯¢çš„èšç±»æ•° (æ¨è nlist çš„ 5-10%)
```

### 4.3 PQï¼ˆProduct Quantizationï¼‰

```
æ ¸å¿ƒæ€æƒ³: å°†é«˜ç»´å‘é‡åˆ‡åˆ†+é‡åŒ–ï¼Œæå¤§å‹ç¼©å†…å­˜

åŸå§‹å‘é‡ d=128:
  [0.12, -0.34, ..., 0.56]  â†’ 128 Ã— 4 bytes = 512 bytes

PQ å‹ç¼© (m=8 å­ç©ºé—´, nbits=8):
  å­ç©ºé—´1: [0.12,-0.34,...] â†’ èšç±»ID: 42   (1 byte)
  å­ç©ºé—´2: [0.08, 0.91,...] â†’ èšç±»ID: 137  (1 byte)
  ...
  å­ç©ºé—´8: [0.56,-0.11,...] â†’ èšç±»ID: 89   (1 byte)
  â†’ æ€»å…±åªéœ€ 8 bytes (å‹ç¼© 64 å€!)

å¸¸ç”¨ç»„åˆ:
  IVF + PQ: å…ˆç²—ç­›èšç±»ï¼Œå†ç”¨é‡åŒ–å‘é‡ç²¾æ’
  â†’ åäº¿çº§æ•°æ®çš„æ ‡å‡†æ–¹æ¡ˆ
```

### ç®—æ³•å¯¹æ¯”

```
ç®—æ³•      å¬å›ç‡   æœç´¢é€Ÿåº¦   å†…å­˜å ç”¨    æ„å»ºé€Ÿåº¦   é€‚ç”¨è§„æ¨¡
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Flat      100%    æœ€æ…¢       æœ€å¤§       æ— éœ€æ„å»º   <100K
HNSW      >99%    æœ€å¿«       å¤§(å†…å­˜)    ä¸­        100K-10M
IVF       95-99%  å¿«         ä¸­         å¿«        1M-100M
IVF+PQ    90-98%  å¿«         æœ€å°       å¿«        100M-10B
ScaNN     >99%    å¾ˆå¿«       ä¸­         å¿«        Google æ¨è
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

## 5. å‘é‡æ•°æ®åº“å¯¹æ¯”

### ä¸»æµæ–¹æ¡ˆï¼ˆ2025ï¼‰

```
æ•°æ®åº“        è¯­è¨€    éƒ¨ç½²æ¨¡å¼        ANN å¼•æ“       ç‰¹è‰²åŠŸèƒ½
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Milvus        Go/C++  åˆ†å¸ƒå¼/äº‘åŸç”Ÿ    HNSW/IVF/DiskANN  æœ€å…¨é¢,GPUåŠ é€Ÿ
Qdrant        Rust    å•æœº/åˆ†å¸ƒå¼      HNSW             Payloadè¿‡æ»¤,é€Ÿåº¦å¿«
Chroma        Python  åµŒå…¥å¼/CS       HNSW (hnswlib)   æœ€æ˜“ä¸Šæ‰‹,RAGé¦–é€‰
Weaviate      Go      åˆ†å¸ƒå¼          HNSW             æ¨¡å—åŒ–,GraphQL API
Pinecone      -       å…¨æ‰˜ç®¡ SaaS     ä¸“æœ‰             é›¶è¿ç»´,ä¼ä¸šçº§
Faiss         C++     Library         å…¨éƒ¨             Metaå‡ºå“,æœ€çµæ´»
pgvector      C       PG æ‰©å±•         IVFFlat/HNSW     PostgreSQL ç”Ÿæ€
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

### é€‰å‹å»ºè®®

```
åœºæ™¯                            æ¨è
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
å¿«é€ŸåŸå‹/ä¸ªäººé¡¹ç›®                Chroma (pip install å³ç”¨)
RAG ç”Ÿäº§ç¯å¢ƒ (ç™¾ä¸‡çº§)            Qdrant æˆ– Milvus Lite
å¤§è§„æ¨¡ç”Ÿäº§ (äº¿çº§)                Milvus (åˆ†å¸ƒå¼) æˆ– Pinecone
å·²æœ‰ PostgreSQL                 pgvector (å°‘åŠ ä¸€ä¸ªç»„ä»¶)
éœ€è¦æè‡´çµæ´»æ€§                   Faiss (Library, è‡ªå·±ç®¡ç†)
ä¼ä¸šçº§ + é›¶è¿ç»´                  Pinecone / Zilliz Cloud
```

### Qdrant å¿«é€Ÿç¤ºä¾‹

```python
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct

# 1. è¿æ¥
client = QdrantClient(":memory:")  # æˆ– url="http://localhost:6333"

# 2. åˆ›å»º collection
client.create_collection(
    collection_name="docs",
    vectors_config=VectorParams(
        size=1024,                 # BGE-M3 ç»´åº¦
        distance=Distance.COSINE,
    ),
)

# 3. æ’å…¥å‘é‡
points = [
    PointStruct(
        id=i,
        vector=embeddings[i].tolist(),
        payload={"text": documents[i], "source": "wiki"},
    )
    for i in range(len(documents))
]
client.upsert(collection_name="docs", points=points)

# 4. æœç´¢
results = client.search(
    collection_name="docs",
    query_vector=query_embedding.tolist(),
    limit=5,
    query_filter={  # å…ƒæ•°æ®è¿‡æ»¤
        "must": [{"key": "source", "match": {"value": "wiki"}}]
    },
)
```

## 6. ç»´åº¦é€‰æ‹©ä¸ä¼˜åŒ–

```
ç»´åº¦çš„å½±å“:
  å°ç»´åº¦ (256-512):  å­˜å‚¨çœã€æœç´¢å¿«ã€ä½†è¡¨è¾¾èƒ½åŠ›å¼±
  ä¸­ç»´åº¦ (768-1024): å¹³è¡¡é€‰æ‹©
  å¤§ç»´åº¦ (2048-4096): è¡¨è¾¾åŠ›å¼ºã€ä½†å­˜å‚¨å’Œæœç´¢æˆæœ¬é«˜

Matryoshka Representation Learning (MRL):
  â†’ è®­ç»ƒæ—¶è®©å‰ k ç»´ä¹Ÿæœ‰æ„ä¹‰
  â†’ å¯ä»¥æˆªæ–­åˆ°ä»»æ„ç»´åº¦ä½¿ç”¨
  â†’ å¦‚ OpenAI text-3-large: 3072 ç»´å¯æˆªåˆ° 256 ç»´

# MRL æˆªæ–­ç¤ºä¾‹
embedding = model.encode("text")        # [3072]
embedding_small = embedding[:256]       # [256], ä»ç„¶æœ‰æ•ˆ!
embedding_small /= np.linalg.norm(embedding_small)  # é‡æ–°å½’ä¸€åŒ–

å­˜å‚¨ä¼°ç®—:
  100 ä¸‡æ–‡æ¡£ Ã— 1024 ç»´ Ã— 4 bytes = ~4 GB
  100 ä¸‡æ–‡æ¡£ Ã— 256 ç»´ Ã— 4 bytes = ~1 GB
  â†’ 4 å€å·®è·ï¼Œåœ¨äº¿çº§æ•°æ®æ—¶å¾ˆå…³é”®
```

## 7. é¢è¯•é«˜é¢‘é¢˜

### Q1: ä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨ BERT çš„ CLS token åšå¥å­ embeddingï¼Ÿ
**ç­”**ï¼šBERT çš„ CLS token æ˜¯ä¸º NSP ä»»åŠ¡è®­ç»ƒçš„ï¼Œæ²¡æœ‰è¢«ä¼˜åŒ–æ¥è¡¨ç¤ºå¥å­çº§è¯­ä¹‰ç›¸ä¼¼åº¦ã€‚ç›´æ¥ä½¿ç”¨ CLS token åšä½™å¼¦ç›¸ä¼¼åº¦ï¼Œæ•ˆæœç”šè‡³ä¸å¦‚ GloVe å¹³å‡ã€‚éœ€è¦é¢å¤–çš„å¯¹æ¯”å­¦ä¹ è®­ç»ƒï¼ˆå¦‚ SimCSEã€BGEï¼‰æ¥è®©æ¨¡å‹å­¦ä¼šå°†è¯­ä¹‰ç›¸è¿‘çš„å¥å­æ˜ å°„åˆ°å‘é‡ç©ºé—´ä¸­çš„é‚»è¿‘ä½ç½®ã€‚mean poolingï¼ˆå¯¹æ‰€æœ‰ token å–å¹³å‡ï¼‰é€šå¸¸æ¯” CLS token æ•ˆæœå¥½ã€‚

### Q2: HNSW å’Œ IVF å„è‡ªé€‚ç”¨ä»€ä¹ˆåœºæ™¯ï¼Ÿ
**ç­”**ï¼šHNSW é€‚åˆ**é«˜å¬å›ç‡ã€ä½å»¶è¿Ÿ**çš„åœºæ™¯ï¼ˆç™¾ä¸‡åˆ°åƒä¸‡çº§ï¼‰ï¼Œä»£ä»·æ˜¯å†…å­˜å ç”¨å¤§ï¼ˆç´¢å¼•éœ€å…¨éƒ¨åœ¨å†…å­˜ä¸­ï¼‰ã€‚IVF é€‚åˆ**å¤§è§„æ¨¡æ•°æ®**ï¼ˆåƒä¸‡åˆ°åäº¿çº§ï¼‰ï¼Œé…åˆ PQ é‡åŒ–å¯ä»¥æå¤§å‹ç¼©å†…å­˜ï¼Œä½†å¬å›ç‡ç•¥ä½ã€‚å®é™…é€‰æ‹©ï¼šå†…å­˜å……è¶³ + æ•°æ®é‡ < 1000 ä¸‡ â†’ HNSWï¼›æ•°æ®é‡ > 1000 ä¸‡ or å†…å­˜å—é™ â†’ IVF+PQã€‚å¤šæ•°å‘é‡æ•°æ®åº“ï¼ˆMilvusã€Qdrantï¼‰é»˜è®¤ä½¿ç”¨ HNSWã€‚

### Q3: æ··åˆæ£€ç´¢ï¼ˆDense + Sparseï¼‰ä¸ºä»€ä¹ˆæ¯”çº¯ Dense æ•ˆæœå¥½ï¼Ÿ
**ç­”**ï¼šDense retrieval æ“…é•¿è¯­ä¹‰åŒ¹é…ï¼ˆ"æ±½è½¦" åŒ¹é… "è½¿è½¦"ï¼‰ï¼Œä½†å¯¹ç²¾ç¡®å…³é”®è¯åŒ¹é…å·®ï¼ˆå¦‚å‹å· "RTX 4090"ï¼‰ã€‚Sparse retrievalï¼ˆBM25ï¼‰æ“…é•¿ç²¾ç¡®åŒ¹é…ä½†ä¸ç†è§£è¯­ä¹‰ã€‚æ··åˆæ£€ç´¢ç»“åˆä¸¤è€…ä¼˜åŠ¿ï¼š(1) Dense æ‰¾åˆ°è¯­ä¹‰ç›¸å…³çš„æ–‡æ¡£ï¼›(2) Sparse æ‰¾åˆ°å…³é”®è¯ç²¾ç¡®åŒ¹é…çš„æ–‡æ¡£ï¼›(3) é€šè¿‡ RRF (Reciprocal Rank Fusion) æˆ–åŠ æƒèåˆåˆå¹¶æ’åã€‚BGE-M3 åŒæ—¶è¾“å‡º dense + sparse + colbert ä¸‰ç§è¡¨ç¤ºï¼Œæ˜¯æ··åˆæ£€ç´¢çš„æœ€ä½³é€‰æ‹©ã€‚

### Q4: Embedding ç»´åº¦å¦‚ä½•é€‰æ‹©ï¼Ÿæ˜¯å¦è¶Šå¤§è¶Šå¥½ï¼Ÿ
**ç­”**ï¼šä¸æ˜¯è¶Šå¤§è¶Šå¥½ï¼Œå­˜åœ¨è¾¹é™…é€’å‡æ•ˆåº”ã€‚768-1024 ç»´æ˜¯å¤šæ•°åœºæ™¯çš„ç”œç‚¹ã€‚ç»´åº¦å¢å¤§å¸¦æ¥ï¼š(1) å­˜å‚¨æˆæœ¬çº¿æ€§å¢é•¿ï¼ˆäº¿çº§æ•°æ®æ—¶å·®åˆ«æ˜¾è‘—ï¼‰ï¼›(2) æœç´¢å»¶è¿Ÿå¢åŠ ï¼ˆè·ç¦»è®¡ç®— O(d)ï¼‰ï¼›(3) é«˜ç»´ç©ºé—´çš„"ç»´åº¦ç¾éš¾"â€”â€”è·ç¦»åŒºåˆ†åº¦ä¸‹é™ã€‚å®è·µå»ºè®®ï¼šå…ˆç”¨æ”¯æŒ Matryoshka çš„æ¨¡å‹ï¼ˆå¦‚ Jina v3ã€OpenAI text-3ï¼‰ï¼Œç”¨å¤§ç»´åº¦è¯„ä¼°æ•ˆæœåŸºçº¿ï¼Œå†é€æ­¥æˆªæ–­åˆ°å°ç»´åº¦æ‰¾åˆ°æ•ˆæœ-æ•ˆç‡çš„å¹³è¡¡ç‚¹ã€‚

### Q5: å¦‚ä½•è¯„ä¼° Embedding + æ£€ç´¢ç³»ç»Ÿçš„æ•ˆæœï¼Ÿ
**ç­”**ï¼šå¤šå±‚æ¬¡è¯„ä¼°ï¼š(1) **Embedding è´¨é‡**â€”â€”ç”¨ MTEB benchmark çš„ retrieval å­é›†ï¼ˆNDCG@10ã€MRR@10ï¼‰ï¼›(2) **æ£€ç´¢å¬å›**â€”â€”åœ¨è‡ªå·±çš„æ•°æ®ä¸Šæ ‡æ³¨ query-doc ç›¸å…³å¯¹ï¼Œè®¡ç®— Recall@Kï¼ˆK=5/10/20ï¼‰ï¼›(3) **ç«¯åˆ°ç«¯æ•ˆæœ**â€”â€”RAG åœºæ™¯ä¸­ï¼Œæµ‹é‡æœ€ç»ˆç”Ÿæˆç­”æ¡ˆçš„å‡†ç¡®ç‡ï¼Œå› ä¸ºæ£€ç´¢åˆ°çš„æ–‡æ¡£è¿˜è¦ç»è¿‡ LLM ç†è§£å’Œç”Ÿæˆï¼›(4) **çº¿ä¸ŠæŒ‡æ ‡**â€”â€”ç”¨æˆ·æ»¡æ„åº¦ã€ç‚¹å‡»ç‡ã€é—®é¢˜è§£å†³ç‡ã€‚å¸¸è§å‘ï¼šMTEB åˆ†æ•°é«˜ä¸ä»£è¡¨ä½ çš„åœºæ™¯å¥½â€”â€”é¢†åŸŸç‰¹å®šæ•°æ®çš„åˆ†å¸ƒå¯èƒ½ä¸ benchmark å·®å¼‚å¤§ã€‚

---

---

## ğŸ“š æ¨èé˜…è¯»

### åŸå§‹è®ºæ–‡
- [C-Pack: Packaged Resources To Advance General Chinese Embedding (BGE)](https://arxiv.org/abs/2309.07597) â€” Xiao et al. 2023ï¼Œä¸­æ–‡ Embedding çš„æ ‡æ†
- [Text Embeddings by Weakly-Supervised Contrastive Pre-training (E5)](https://arxiv.org/abs/2212.03533) â€” Wang et al. 2022ï¼Œå¯¹æ¯”å­¦ä¹  Embedding çš„é‡Œç¨‹ç¢‘
- [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity](https://arxiv.org/abs/2402.03216) â€” ä¸‰æ¨¡æ€æ£€ç´¢ï¼ˆDense+Sparse+ColBERTï¼‰
- [Billion-scale similarity search with GPUs (FAISS)](https://arxiv.org/abs/1702.08734) â€” Johnson et al. 2017ï¼ŒANN æœç´¢çš„åŸºç¡€è®¾æ–½

### å®è·µèµ„æº
- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) â€” Embedding æ¨¡å‹æ•ˆæœæ’è¡Œæ¦œ â­â­â­â­â­
- [Sentence-Transformers](https://www.sbert.net/) â€” Embedding æ¨¡å‹çš„æ ‡å‡†ä½¿ç”¨æ¥å£
- [FAISS GitHub](https://github.com/facebookresearch/faiss) â€” Meta å‡ºå“ï¼Œå‘é‡æœç´¢çš„åŸºç¡€åº“

## ğŸ”§ è½åœ°åº”ç”¨

### ç›´æ¥å¯ç”¨åœºæ™¯
- **RAG æ£€ç´¢åŸºåº§**ï¼šEmbedding + å‘é‡æ•°æ®åº“æ˜¯ RAG ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶
- **è¯­ä¹‰æœç´¢å¼•æ“**ï¼šæ›¿ä»£å…³é”®è¯æœç´¢ï¼Œæ”¯æŒ"æ„æ€ç›¸è¿‘"çš„æ¨¡ç³ŠæŸ¥è¯¢
- **ç›¸ä¼¼åº¦æ¨è**ï¼šæ–‡æ¡£/å•†å“/ç”¨æˆ·ç”»åƒçš„å‘é‡åŒ– + è¿‘é‚»æœç´¢

### å·¥ç¨‹å®ç°è¦ç‚¹
- **æ¨¡å‹é€‰å‹é€ŸæŸ¥**ï¼šä¸­æ–‡ â†’ BGE-M3 / BGE-large-zhï¼›è‹±æ–‡ â†’ BGE-large-enï¼›é•¿æ–‡æ¡£ â†’ E5-Mistral-7B / GTE-Qwen2ï¼›API â†’ Cohere embed-v3
- **å‘é‡æ•°æ®åº“é€ŸæŸ¥**ï¼šåŸå‹ â†’ Chromaï¼›ç”Ÿäº§ <10M â†’ Qdrant/pgvectorï¼›>10M â†’ Milvus
- **å†…å­˜ä¼˜åŒ–**ï¼šMatryoshka æˆªæ–­ï¼ˆ3072â†’256 ç»´ï¼‰å¯å‡ 12 å€å­˜å‚¨ï¼Œæ•ˆæœæŸå¤± <5%

### é¢è¯•é«˜é¢‘é—®æ³•
- Q: ä¸ºä»€ä¹ˆä¸ç›´æ¥ç”¨ BERT CLS tokenï¼Ÿ
  A: CLS æ˜¯ä¸º NSP è®­ç»ƒçš„ï¼Œæœªä¼˜åŒ–å¥å­ç›¸ä¼¼åº¦ï¼›éœ€è¦å¯¹æ¯”å­¦ä¹ è®­ç»ƒï¼ˆSimCSE/BGEï¼‰æ‰èƒ½ç”¨äºæ£€ç´¢
- Q: HNSW å’Œ IVF æ€ä¹ˆé€‰ï¼Ÿ
  A: å†…å­˜å……è¶³ + <10M â†’ HNSWï¼›å¤§è§„æ¨¡/å†…å­˜å—é™ â†’ IVF+PQ

## ğŸ’¡ å¯å‘ä¸æ€è€ƒ

### So Whatï¼Ÿå¯¹è€æ¿æ„å‘³ç€ä»€ä¹ˆ
- Embedding æ¨¡å‹é€‰å‹ç›´æ¥å†³å®š RAG æ£€ç´¢è´¨é‡ä¸Šé™â€”â€”é€‰é”™æ¨¡å‹ï¼Œåé¢ä¼˜åŒ–éƒ½ç™½è´¹
- BGE-M3 çš„ Dense+Sparse+ColBERT ä¸‰æ¨¡æ€æ£€ç´¢æ˜¯å½“å‰æœ€å¼ºå¼€æºæ–¹æ¡ˆ

### æœªè§£é—®é¢˜ä¸å±€é™
- é¢†åŸŸé€‚åº”æ€§ï¼šé€šç”¨æ¨¡å‹åœ¨ä¸“ä¸šé¢†åŸŸï¼ˆç”Ÿç‰©åŒ»è¯/æ³•å¾‹ï¼‰å¯èƒ½è¡¨ç°ä¸ä½³ï¼Œéœ€è¦ Fine-tune
- é«˜ç»´ç©ºé—´çš„"ç»´åº¦ç¾éš¾"ï¼šç»´åº¦è¶Šé«˜ï¼Œå‘é‡é—´è·ç¦»åŒºåˆ†åº¦è¶Šä½

### è„‘æš´ï¼šå¦‚æœå¾€ä¸‹å»¶ä¼¸
- ç»“åˆ [[æ£€ç´¢ç­–ç•¥|æ£€ç´¢ç­–ç•¥]] çš„ Hybrid Searchï¼ŒBGE-M3 åŒæ—¶è¾“å‡º Dense+Sparse æ˜¯å¤©ç„¶çš„æ··åˆæ£€ç´¢æ–¹æ¡ˆ
- Matryoshka Embedding è®©"æ•ˆæœ-æˆæœ¬"å¯ä»¥è¿ç»­è°ƒèŠ‚â€”â€”è¿™å¯¹å¤§è§„æ¨¡ç³»ç»Ÿæ„ä¹‰é‡å¤§

> ğŸ”— See also: [[æ£€ç´¢ç­–ç•¥]] â€” æ£€ç´¢ç­–ç•¥æ˜¯ Embedding é€‰å‹çš„ä¸‹æ¸¸åº”ç”¨
> ğŸ”— See also: [[RAG-2026-æŠ€æœ¯å…¨æ™¯|RAG 2026 å…¨æ™¯]] â€” Embedding åœ¨å®Œæ•´ RAG æ¶æ„ä¸­çš„ä½ç½®
> ğŸ”— See also: [[RAG åŸç†ä¸æ¶æ„]] â€” RAG åŸºç¡€æ¶æ„
