---
title: "RL 思路探索"
category: "学习实践"
tags: [Agent, GRPO, LLM, Multi-Agent, SQL]
created: "2026-02-13"
updated: "2026-02-13"
---

# RL 思路探索

与 gemini 聊天

我们现在有个商业分析 Agent 场景。主要有几个 Agent：

plan、数据分析 agent（有取数 sql 工具、有 writer 写作工具）、summary 总结

分析场景有个很难的点，第一事实性数据幻觉容忍度低，比如 writer 对 取数工具取数后的表进行分析，解决排名第二的说成是排名第一的，也就是没有忠于表数据。

另外就是分析的偏好问题，更关注哪些点需要很多的业务知识，比如取数顺序路径，比如写作的风格，洞察性分析等，非常依赖分析的深度。

在类似这种乃至更泛化的通用分析场景下，如何利用强化学习来提升多 Agent系统能力呢？你能提出哪些好的 idea？

Idea 1: 基于“事实校验器”的奖励机制，解决数据忠实度问题 (RL from Verification)

我觉得第一个 idea 不错，我们展开讨论一下，就对于“表格和文本”结论对齐这个目标，如何一步步试试整个的 pipeline 呢，比如采用 GRPO 的方法来进行训练，另外我们已经有一个已有模型来合成一些数据了，结合工程化的手段看下怎么进行方案落地。还有个输入就是，我对事实类错误有几种观察。- 常识性或逻辑矛盾（“Knowledge Contradiction”，如违反常理或逻辑错误）

- 数值或数据计算错误（“Statistics Contradiction”，仅限于数据相关的错误）
- 主体名词错误（“Entity Contradiction”，如人名、地名、机构等主体识别错误）
- 关系排序错误（“Relation Contradiction”，仅限于比较或排序相关的错误）
- 时间顺序错误（“Temporal Contradiction”，即事件时序或时间关系错误）
- 未遵循用户指令（“Instruction Inconsistency”，如未按要求完成任务）
- 信息遗漏或取舍错误（“Incompleteness”，如有重要内容遗漏或取舍不当）
- 事实夸大或不当描述（“Overclaim”，如用词过度或对事实进行夸大）
也许可以以此来设计奖励或者一些其他的研究步骤。这个目标是很严谨且难度很高的，仔细深度思考，给出可落地的，具体的方案。

# 思考 1

## **表格-数据对**

- 正样本：数据合成
- **输入:** 业务场景下的真实数据表（或表结构Schema）。
- **过程:** 使用现有的合成模型（longcat），针对每个表格生成一个高质量、完全忠于事实、且分析风格良好的分析文本。
- **人工校验:** 这一步需要投入人力进行严格校验和润色
- 负样本**：程序化地注入负样本（错误类型）**LLM 篡改