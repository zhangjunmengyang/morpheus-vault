---
title: "模型评估"
category: "AI"
tags: [MAE, PPO, RAG, ViT, 矩阵]
created: "2026-02-13"
updated: "2026-02-13"
---

# 模型评估

## 基本概念

### 分类和回归的常见评估指标

**分类模型常用评估方法：**

[表格内容，请参考原文档]

指标

描述

Accuracy

准确率

Precision

精准度/查准率

Recall

召回率/查全率

P-R曲线

查准率为纵轴，查全率为横轴，作图

F1

F1值

Confusion Matrix

混淆矩阵

ROC

ROC曲线

AUC

ROC曲线下的面积

**回归模型常用评估方法：**

[表格内容，请参考原文档]

指标

描述

Mean Square Error (MSE, RMSE)

平均方差

Absolute Error (MAE, RAE)

绝对误差

R-Squared

R平方值

### **误差、偏差和方差**

- 误差（error）：一般地，我们把学习器的实际预测输出与样本的真是输出之间的差异称为“误差”。
- Error = Bias + Variance + Noise，Error反映的是整个模型的准确度。
**Noise**

噪声：描述了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。

**Bias**

- Bias衡量模型拟合训练数据的能力（训练数据不一定是整个 training dataset，而是只用于训练它的那一部分数据，例如：mini-batch），Bias反映的是模型在样本上的输出与真实值之间的误差，即模型本身的精准度。
- Bias 越小，拟合能力越高（可能产生overfitting）；反之，拟合能力越低（可能产生underfitting）。
- 偏差越大，越偏离真实数据，如下图第二行所示。
**Variance：**

- 方差公式：
- Variance描述的是预测值的变化范围，离散程度，也就是离其期望值的距离。方差越大，数据的分布越分散，模型的稳定程度越差。
- Variance反映的是模型每一次输出结果与模型输出期望之间的误差，即模型的稳定性。
- Variance越小，模型的泛化的能力越高；反之，模型的泛化的能力越低。
- 如果模型在训练集上拟合效果比较优秀，但是在测试集上拟合效果比较差劣，则方差较大，说明模型的稳定程度较差，出现这种现象可能是由于模型对训练集过拟合造成的。 如下图右列所示。
![image](assets/OQYQdKY21oMwjAxVXCMc2fW6nOS.png)

### 经验误差和泛化误差

经验误差（empirical error）：也叫训练误差（training error），模型在训练集上的误差。

泛化误差（generalization error）：模型在新样本集（测试集）上的误差称为“泛化误差”。

### 混淆矩阵（Confusion Matrix）

![image](assets/BSpGdgWeHoexeexExAnca42fnZf.png)

### 评估指标

1. 正确率（accuracy） 正确率是我们最常见的评价指标，accuracy = (TP+TN)/(ALL)，正确率是被分对的样本数在所有样本数中的占比，通常来说，正确率越高，分类器越好。
1. 错误率（error rate) 错误率则与正确率相反，描述被分类器错分的比例，error rate = (FP+FN)/(ALL)，对某一个实例来说，分对与分错是互斥事件，所以accuracy =1 - error rate。
1. 灵敏度（sensitivity） sensitivity = TP/P，表示的是所有正例中被分对的比例，衡量了分类器对正例的识别能力。
1. 特异性（specificity) specificity = TN/N，表示的是所有负例中被分对的比例，衡量了分类器对负例的识别能力。
1. 精度（precision） precision=TP/(TP+FP)，精度是精确性的度量，表示被分为正例的示例中实际为正例的比例。
1. 召回率（recall） 召回率是覆盖面的度量，度量有多个正例被分为正例，recall=TP/(TP+FN)=TP/P=sensitivity，可以看到召回率与灵敏度是一样的。
1. F1-score：精度和召回率反映了分类器分类性能的两个方面。如果综合考虑查准率与查全率，可以得到新的评价指标F1-score，也称为综合分类率：
1. micro-F1 & macro-F1：为了综合多个类别的分类情况，评测系统整体性能，经常采用的还有微平均F1（micro-averaging）和宏平均F1（macro-averaging ）两种指标。
1. 宏平均F1的计算方法先对每个类别单独计算F1值，再取这些F1值的算术平均值作为全局指标。
1. 微平均F1的计算方法是先累加计算各个类别的a、b、c、d的值，再由这些值求出F1值。
1. 由两种平均F1的计算方式不难看出，宏平均F1平等对待每一个类别，所以它的值主要受到稀有类别的影响，而微平均F1平等每一个样本，所以它的值受到常见类别的影响比较大。
1. F Beta score：[fbeta_score](https%3A%2F%2Fscikit-learn.org%2Fstable%2Fmodules%2Fgenerated%2Fsklearn.metrics.fbeta_score.html)
等同于：

![image](assets/MoScd4huSowVmkxNb45cWtuWnBf.png)

The `beta` parameter represents the ratio of recall importance to precision importance. `beta  1` gives more weight to recall, while `beta < 1` favors precision. For example, `beta = 2` makes recall twice as important as precision, while `beta = 0.5` does the opposite. Asymptotically, `beta -> +inf` considers only recall, and `beta -> 0` only precision.

1. 其他评价指标
1. 计算速度：分类器训练和预测需要的时间； 
1. 鲁棒性：处理缺失值和异常值的能力； 
1. 可扩展性：处理大数据集的能力； 
1. 可解释性：分类器的预测标准的可理解性，像决策树产生的规则就是很容易理解的，而神经网络的一堆参数就不好理解，我们只好把它看成一个黑盒子。
### ROC、AUC

ROC曲线是（Receiver Operating Characteristic Curve，受试者工作特征曲线）的简称，是以灵敏度（真阳性率）为纵坐标，以1减去特异性（假阳性率）为横坐标绘制的性能评价曲线。可以将不同模型对同一数据集的ROC曲线绘制在同一笛卡尔坐标系中，ROC曲线越靠近左上角，说明其对应模型越可靠。也可以通过ROC曲线下面的面积（Area Under Curve, AUC）来评价模型，AUC越大，模型越可靠。


![image](assets/BIHkdMkS7oIBAjxx7lncligfn5c.png)

在ROC曲线中，以FPR为x轴，TPR为y轴
