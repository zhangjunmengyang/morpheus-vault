# 小文件问题

增加 batch 大小: 这种方法很容易理解，batch 越大，从外部接收的 event 就越多，内存积累的数据也就越多，那么输出的文件数也就会变少，比如上边的时间从10s增加为100s，那么一个小时的文件数量就会减少到1152个。但别高兴太早，实时业务能等那么久吗，本来人家10s看到结果更新一次，现在要等快两分钟，是人都会骂娘。所以这种方法适用的场景是消息实时到达，但不想挤压在一起处理，因为挤压在一起处理的话，批处理任务在干等，这时就可以采用这种方法。

Coalesce大法好: 文章开头讲了，小文件的基数是 batch_number * partition_number，而第一种方法是减少 batch_number，那么这种方法就是减少 partition_number 了，这个 api 不细说，就是减少初始的分区个数。看过 spark 源码的童鞋都知道，对于窄依赖，一个子 RDD 的 partition 规则继承父 RDD，对于宽依赖(就是那些个叉叉叉ByKey操作)，如果没有特殊指定分区个数，也继承自父 rdd。那么初始的 SourceDstream 是几个 partiion，最终的输出就是几个 partition。所以 Coalesce 大法的好处就是，可以在最终要输出的时候，来减少一把 partition 个数。但是这个方法的缺点也很明显，本来是32个线程在写256M数据，现在可能变成了4个线程在写256M数据，而没有写完成这256M数据，这个 batch 是不算结束的。那么一个 batch 的处理时延必定增长，batch 挤压会逐渐增大。

Spark Streaming 外部来处理: 我们既然把数据输出到 hdfs，那么说明肯定是要用 Hive 或者 Spark Sql 这样的“sql on hadoop”系统类进一步进行数据分析，而这些表一般都是按照半小时或者一小时、一天，这样来分区的(注意不要和 Spark Streaming 的分区混淆，这里的分区，是用来做分区裁剪优化的)，那么我们可以考虑在 Spark Streaming 外再启动定时的批处理任务来合并 Spark Streaming 产生的小文件。这种方法不是很直接，但是却比较有用，“性价比”较高，唯一要注意的是，批处理的合并任务在时间切割上要把握好，搞不好就可能会去合并一个还在写入的 Spark Streaming 小文件。

自己调用 foreach 去 append: Spark Streaming 提供的 foreach 这个 outout 类 api （一种 Action 操作），可以让我们自定义输出计算结果的方法。那么我们其实也可以利用这个特性，那就是每个 batch 在要写文件时，并不是去生成一个新的文件流，而是把之前的文件打开。考虑这种方法的可行性，首先，HDFS 上的文件不支持修改，但是很多都支持追加，那么每个 batch 的每个 partition 就对应一个输出文件，每次都去追加这个 partition 对应的输出文件，这样也可以实现减少文件数量的目的。这种方法要注意的就是不能无限制的追加，当判断一个文件已经达到某一个阈值时，就要产生一个新的文件进行追加了。所以大概就是一直32个文件。

Hive & Spark 小文件常用参数

Hive

Spark

参数设置

解释

参数设置

解释

map合并

mapred.max.split.size mapred.min.split.size.per.node mapred.min.split.size.per.rack

mapred.max.split.size控制切分时的数据分片大小mapred.min.split.size.per.node和mapred.min.split.size.per.rack控制了切分后，在dn和机架内的合并阈值。三个参数的值可以根据情况自由调整具体的参数含义可以看[【专题】-hive&spark常用参数详解](https%3A%2F%2Fkm.sankuai.com%2Fpage%2F124233475)

set spark.hadoop.hive.exec.orc.split.strategy=ETL;spark.hadoopRDD.targetBytesInPartitionspark.hadoop.mapreduce.input.fileinputformat.split.maxsizespark.hadoop.mapreduce.input.fileinputformat.split.minsize

首先需要把切分策略设置成ETL模式，其余三个参数都可以控制合并后的文件大小。具体的参数解释可以参考[【专题】-hive&spark常用参数详解](https%3A%2F%2Fkm.sankuai.com%2Fpage%2F124233475)

reduce合并

hive.exec.reducers.bytes.per.reducer

增大该参数值，可以增大每个reducer处理的数据量，进而减少reduce task的数量，最终减少了输出文件数量。task_num=input_size/hive.exec.reducers.bytes.per.reducer注意：一个hive任务会拆分成多个MR任务，该参数会对多个MR任务的reduce阶段作用，所以并不是一个很好的减少小文件的方式（有可能会导致同任务的其他MR job执行缓慢）

set spark.sql.adaptive.enabled=true;spark.sql.adaptive.shuffle.targetPostShuffleInputSize

打开自适应开关，在最后一个stage中增加每个task的处理量，进而减少task数量，最终减少小文件数量。task_num=shuffle_read_size/spark.sql.adaptive.shuffle.targetPostShuffleInputSize

写入后合并

set hive.merge.mapfiles=true;set hive.merge.mapredfiles=true;hive.merge.size.per.taskhive.merge.smallfiles.avgsize

首先打开map端和reduce端的结果文件合并开关如果结果文件的平均大小小于hive.merge.mapfiles设置的值，则额外启动一轮job进行小文件的合并，合并后的期望文件大小由max(hive.merge.size.per.task, hive.merge.smallfiles.avgsize)来决定注意：如果对mapjoin的结果进行合并，一定要设置hive.merge.mapredfiles=true

spark.sql.mergeSmallFileSizespark.sql.targetBytesInPartitionWhenMerge

spark.sql.mergeSmallFileSize和hive参数hive.merge.smallfiles.avgsize作用相同，是小文件合并的阈值,最终的文件大小由max(spark.sql.mergeSmallFileSize, spark.sql.targetBytesInPartitionWhenMerge , spark.hadoopRDD.targetBytesInPartition )来决定注意：由于新起了一个stage做文件合并，所以参数spark.hadoopRDD.targetBytesInPartition也是其作用的
