---
title: "实时数据准确性"
category: "工程"
tags: [AI安全, Doris, Flink, Hive, Kafka]
created: "2026-02-13"
updated: "2026-02-13"
---

# 实时数据准确性

## 一、背景

近几年实时属于也逐渐步入精细化运营时代，很多情况下不准确的数据加工、不规范的生产流程带来的是后续高昂的解释成本以及对数据一致性、数据质量的严重破坏。

大数据实时计算因其灵活性强、效率高、处理能力强大等优势正在一步步深度参与到生产系统中，而不局限在分析场景。我们将参与生产的数据应用称为“数据服务”，“数据服务”场景对实时数据的质量要求更高，数据延迟、数据错误等质量问题可能会直接影响用户的体验甚至造成公司资损。

目前实时链路的特点：

1. **实时任务SQL化**：超过90%的实时任务使用NAU FlinkSQL开发。SQL化开发使得实时RD能更聚焦于业务逻辑，相比JavaAPI也更容易保障代码质量。偶尔涉及到除了每一条数据下发逻辑，比如有特殊需求的关联处理，一些字段的复杂加工现在还是在流里面加工。
1. **数据落表**：我们现在并不是所有中间数据都会落表，即使落也基本是 Hive（Doris 不是很适合明细查询），主要用于数据验证或者排查，通常都是需要的时候自己落表，主要还是最后一层才落 Doris/Hive，对外提供数据服务。
优选：Fact层及以上实时任务输出数据都会写入Doris中。一是出于后续加工的目的，可以在Doris中使用D2D对数据进行后续计算，二是出于质量保障的目的，通过对Doris中数据的查询和监控来验证流数据的准确性。

## 二、实时数据常见问题

面对的难题：

- 首先是**预防困难**：影响实时数据准确性的因素是多方面的，包括代码逻辑、业务理解这种个人因素，也包括集群环境以及上游数据异常等外部因素。在事前预防上难以做到面面俱到，以代码逻辑举例，实时数据处理并非像离线数据处理一样“所见即所得”，实时数据产出结果可能并不符合实时RD的预期，常面临数据不幂等、数据乱序等问题，这些问题非常隐蔽、难以发现。
- 其次是**发现困难**：一是由于实时数据在持续不间断的变化，难以对数据设置一个静态的、准确的参照物去比较，常常只能设置一个区间范围，容易漏报、误报。二是异常发现的时效性要求高，许多场景当异常发生到发现时长超过半小时就会造成用户的明显感知。三是部分场景对数据准确性要求极为严苛，单条明细数据的异常就会造成数据case，例如优选物流业务下的运输送达率，中心仓分拣完成率等指标，当一条数据异常时则不能到达100%([5])，如何发现千万分之一的数据错误是一项具有挑战性的任务。
发现/预防：DQC 监控、监控报警、战役稳定性保障（压测、扩容）、标准化建设

- 最后是**修复困难**：在数据修复上，实时数据也并非像离线数据一样存在“数据重刷”的通用解决方案。实时数据的交付物可能同时包括Kafka/Doris/Cellar等，在数据异常时重刷数据有时是个成本很高、时效很慢的方式，需要因地制宜的在不同场景下选择合适的修数方案。
## 三、准确性治理 SOP

### 3.1、预防

#### 3.1.1、生产消费乱序

常见问题需要进行沉淀，并且在开发时有意识进行治理。

全链路导致数据乱序风险类型总结：

![image](assets/UNdpdumi0oGzH1x7LeycofuOnpg.png)

#### 3.1.2、测试规范

上线前至少进行数据预览、重复数据、维度对比、枚举值分布等常规数据探查方式的验证，同时确定好验收方式，确保数据是符合预期的，单任务才具备上线标准。

对于下游有重要 SLA 作业的情况，强规范通常是需要进行多结点测试的，确保不会影响下游。

同时根据具体业务场景，会涉及到 SLA 就绪时间治理、一致性治理、表字段安全标注（数据安全）、（数据质量）等。

### 3.2、发现

已经经过预防的流程，以及规范生产以后，还可能存在潜在风险，需要尽可能在问题出现之前进行**问题发现**

- 
- 
### 3.3、修复

数据异常修复的常见场景

特殊订单数据修复会导致补贴数据膨胀的问题：

这个问题后续详细整理一下思考：

1. https://km.sankuai.com/collabpage/2454489728
1. https://km.sankuai.com/collabpage/2367431984