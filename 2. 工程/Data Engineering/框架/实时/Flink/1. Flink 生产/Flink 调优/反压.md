---
title: "反压"
category: "工程"
tags: [Checkpoint, Flink, Kafka, SQL, Spark]
created: "2026-02-13"
updated: "2026-02-13"
---

# 反压

### 8.1、反压机制

反压表示下游算子处理消息速率跟不上上游算子生产消息速率，反压机制：

1. 每个TM维护共享Network BufferPool，初始化时从堆外内存申请。
1. 每个Task创建自己的Local BufferPool，并可以从Network BufferPool申请额外内存。
1. 下游任务的Input Channel写满，向Local BufferPool申请buffer空间。
1. Local BufferPool也满了，向Network BufferPool申请buffer空间。
1. 当下游算子把能用的buffer空间都用完了，下游算子socket层关闭读取。
1. netty不能从socket的Receiver Buffer读消息，接收端socket的Receiver Buffer打满，TCP流控会让上游socket不下发数据。
1. 发送端socket的Sender Buffer打满，上游netty停止往socket写数据。
1. netty buffer写满后数据会在上游ResultSubPartition堆积，ResultSubPartition打满后，会向Local BufferPool申请buffer空间。
1. Local BufferPool也满了，向Network BufferPool申请buffer空间。
1. 上游把能用的buffer空间用完，Record Writer停止输出，上游不再生产数据。
1. 压力从下游往上游传输，一直传播到source，从而降低从外部组件中读数据的速率。
问题：TCP多路复用通道被占满，一个subtask反压，会导致其他正常的subtask接收不到数据。

Credit机制：

- 1.5版本后引入Credit反压策略，每次上游给下游发数据时，把buffer数据和堆积的数据量发给下游。
- 下游接收数据并向上游反馈Credit值，Credit值表示目前下游可以接收上游的buffer数量。
- 上游下一次最多只给下游发送Credit值的buffer数据，保证下游可以承受，且不会在TCP这一层消息堆积而影响其他subtask通信。
### 8.2、反压影响

- cp不会越过普通数据，数据处理阻塞会导致Barrier流经整个数据管道的时间变长，从而导致cp时间变长。
- 数据延迟。
反压问题

### **反压问题**

一般大数据框架反压策略对比：

- storm：超过水位线通知 zk，zk 通知生产者降低速度
- spark：速率控制器，用 PID 算法根据消息数量，处理时间等计算速率
- flink：反压设计天然优势，因为纯流式计算。利用网络传输和动态限流，数据在算子之间进行转换时，**放入的是分布式阻塞队列，**阻塞队列满了自然限速，实现逐级反压（特别像开车堵车）
**定位反压**： OK 0-0.1 、LOW 0.1-0.5、HIGH 0.5-1

直接表现是上游 kafka 出现堆积，虽然对延时要求不高的数据不那么怕，但是会有并发症：

1. 数据大规模堆积，暂时不被处理的数据都会写进状态中
1. 状态变得很大
1. checkpoint 也很困难，导致超时
1. CP 没做好，保证不了数据一致性
**反压原因**：

- 资源、并行度
- 数据倾斜
- GC：不合理的 TM 垃圾回收参数，通过 printGCdetail 看日志
- 代码本身：算子使用的问题
**反压核心指标（重要！！）**

- 发送端缓冲池使用率 **outPoolUsage**：发送使用率很低需要注意，说明数据发的很慢，可能下游 or 当前导致
- 接收端缓冲池使用率 **inPoolUasge**：和outp 一起看，如果收的快，发的慢，很有可能是
- 处理节点缓冲池使用率 **floatingBuffersUsage**：如果过低，表示处理能力可能跟不上了
- 数据输入方缓冲池使用率 **exclusiveBuffersUsage**：如果过高表示很多数据都在等着，处理速度跟不上
### 一、定位

在定位Flink作业中哪个算子背压高时，为了更清晰精准地定位，建议关闭chain链参数：env.disableOperatorChaining()

当UI界面切换到某个Task的BackPressure页面时，才会对这个Task触发反压检测，需要等待一段时间，才可以看到算子的背压状态。BackPressure界面会周期性地对Task线程栈信息采样，通过线程被阻塞在请求Buffer的频率来判断节点是否处于反压状态。

- Ratio<=0.1时，Status=OK；
- 0.1<Ratio<=0.5时，Status=LOW；
- Ratio>0.5时，Status=HIGH。
可以在Flink UI界面，从Sink算子开始向上游逐个Task排查，通过Task的反压状态可以大致锁定导致作业背压高的瓶颈算子，但是导致作业反压具体是哪个算子的定位，需要进一步通过Metric监控进行进一步的判断：

1.  当前Task处理慢导致，比如该算子有大量的与外部系统（接口调用，Tair访问等）交互的操作，导致该Task的处理能力不足，比如该Task的处理能力为2000qps，但是上游Task和下游Task的处理能力均为8000qps，此时如果上游Task下发数据速度为4000qps时，则该Task成为任务的瓶颈算子，对于这种问题，给瓶颈算子加并发是一种解决方案，但是更推荐采用异步IO、Guava Cache等手段来减少接口调用等待、Tair频繁访问对整个作业的性能影响。
1. 当前Task的上游算子有炸裂操作，比如上游算子是1条数据进，100条数据出，数据膨胀系数为100，这种任务应该考虑对炸裂算子下游的Task均增加并发，炸裂算子上游的Task则不需要增加。
### 二、解决

#### 2.1、Sink 反压

这个一般体现在 sink 的上一个算子反压，因为 sink 消费不动，导致数据积压在sink 端。我们主要考虑Sink算子的下游系统瓶颈问题，由于Sink端是Kafka，所以我们考虑**扩大Sink kafka topic的分区数**，1 -> 16

#### 2.2、Tranform 反压

将Sink kafka topic的分区数由1扩展到16之后，发现Map算子对应的Task背压已经不存在了，再向上游追，发现Correlate(invocation=[richSkuInfoV1Func(CAST($cor0.base_sku_id):VARCHAR(2147483647) CHARACTER SET "UTF-16LE")]成为了瓶颈算子：

在Apache Flink中，`CORRELATE`算子用于将表函数应用于表中的每一行。这种操作可以将一行输入扩展成多行输出，类似于SQL中的LATERAL TABLE FUNCTION。通过使用`CORRELATE`算子，你可以执行诸如 *展开json数组* 或者 *对字符串进行分词并展开* 这样的任务。

造成该算子成为瓶颈的主要原因是，该算子中有 频繁访问Tair的操作：对于每条数据都去访问Tair，组装sku的属性信息。此时观察该算子的每个并发的处理能力大约为520条/秒。

对该算子访问Tair的UDF进行改造，加一层Guava Cache，此时观察该算子的每个并发的处理能力大约为2800条/秒。

需要注意的是，Guava Cache是性能与准确性的一个折中，如果Tair维表更新很不频繁，尤其对于天级离线同步到Tair的维表，并且可以容忍一定的数据准确性的场景，强烈推荐使用Guava Cache加速任务的处理性能。另外，如果Task中有访问外部接口的操作时，使用异步IO可以显著加速该算子的处理性能，这里不再赘述。

#### 2.3、Source 端反压

这里说的Source算子是瓶颈算子的情况主要是指，上游Source kafka topic的各个分区数据量严重不均匀的情况，例如，早期数据采集侧采用 表名hash的方式 将mysql binlog数据采集到 kafka topic 的不同分区中，这就导致即使kafka topic有6个分区，但是下游使用的时候，往往只使用1个分区中的数据。

现在采集侧已经升级成基于表中id字段进行采集分区。但是对于历史的存量topic，如果发现有类似的现象，可以使用id进行hash采集数据，避免各个分区数据量不均衡的问题，结合我们在资源优化配置中将Source端算子的并行度设置为Kafka source topic分区数一致可基本。另外，一些非binlog的topic也会存在分区间数据量不平衡的问题，比如说，券/促销使用的某个topic，在写入数据时，可能根据促销id或者券id进行hash存到不同的topic中，如果使用某种券/促销活动的交易订单非常多，也会导致分区间数据量的不均衡，这些情况在开发中根据具体的业务场景注意合理设计。

**我们的经验是，对于与外部存储有交互或者访问外部接口的任务，瓶颈算子往往就是访问外部存储的算子，或者是访问外部接口的算子。对于非上述两种情况的作业还存在背压高的算子，往往是由于数据倾斜导致的，此时，加资源带来的收益微乎其微，对于此种情况，就需要针对任务进行数据倾斜的优化。**
