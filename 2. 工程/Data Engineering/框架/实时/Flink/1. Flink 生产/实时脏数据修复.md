---
title: "实时脏数据修复"
category: "工程"
tags: [Doris, Flink, Hive, Kafka, SQL]
created: "2026-02-13"
updated: "2026-02-13"
---

# 实时脏数据修复

脏数据分为几种情况：多数据、少数据、错误数据

涉及到的修复场景例如：数据表、流、KV 存储

修复：原子导入

场景

脏数据类型

解决方案

数据表

多

修复上游逻辑，刷数 / 回拨 Kafka，作业中添加临时逻辑过滤脏数据（比如时间小于今天），原子导入，导入前清空数据

doris 可以， k2hive 不好搞（可以用新表刷）

数据表

少

同上， doris 但需要注意延迟数据积压导致的乱序。

涉及到 hive 就比较麻烦（重复数据，没法删除分区）

数据表

错

修复同上，搞一个.fix任务，可以自己写自己

流

多

生产加过滤逻辑，但脏数据还在原 kafka 中，需要新建一个 kafka 然后落到下游，先启动一个旁路分支，同时写到下游。

流

少

- 当天：比如某个时段有缺口，启动旁路作业在对应时间点，补齐数据
- T - 1：用离线数据回刷
流

错

生产加过滤逻辑，但脏数据还在原 kafka 中，需要新建一个 kafka 然后落到下游，先启动一个旁路分支，同时写到下游。

追上数据以后停止原作业。

依赖下游的数据去重（如 Doris）

KV 

多

没办法，一般也不需要处理，等待过期

KV 

少

直接写入

KV 

错

覆盖

**流修复注意事项：状态过期时间、状态制作情况、对下游任务的影响**

生产链路有脏数据：

1. 从运维中心看异常任务，避免大量报警干扰（**报警淹没问题**）
1. 检查报脏数据异常的任务，如果不确定脏数据根因，**先加脏数据过滤逻辑并上线**，可接上次消费，不从状态恢复。这一步主要是为了快速屏蔽脏数据异常，继续生产。可能丢数据
1. 定位脏数据来源：
1. 报异常前有上线操作，通过回滚操作恢复正常生产
1. 组内没有上线操作，则顺着链路找到产生脏数据的任务，修复产生脏数据的逻辑。如果有维护链路文档，可以从文档找源头，否则从实时平台或datamap上按血缘找
1. 回溯数据：**务必一次搞定，避免数据重复增长**
1. 检查要回溯数据的下游链路，是否存在多流关联，状态保留时长多大（state，cellar，squirrel）
1. 如果状态保留时长<2*(当前时间-故障发生时间)，需要调大任务状态保留时长，避免回溯时长超过状态时长再次丢数。异常数据源消费点设为**最新**（已经加了脏数据过滤逻辑的也可以设置成接上次），其他数据源消费点设为**故障发生时间前5分钟**，启动任务
1. 如果状态时长足够大（半天以上），可以不动任务，等上游回溯
1. 没有状态的任务可以不动，等上游回溯
1. 如果是关联实时维表且维表实时数据源有异常数据，关联维表的数据源消费点设为**故障发生时间前5分钟**，启动任务
1. 源头任务复制一个任务用于回溯，扩并发，数据源消费点设为**故障发生时间前5分钟**，启动任务
1. rt任务可直接复制
1. nau任务需要新建和上线审批，sql直接从原始任务copy