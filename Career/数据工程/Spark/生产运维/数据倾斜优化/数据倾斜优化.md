---
title: "数据倾斜优化"
type: concept
domain: engineering/spark/生产运维/数据倾斜优化
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - engineering/spark/生产运维/数据倾斜优化
  - type/concept
---
# 数据倾斜优化

## 一、产生原因

select a.id, b.name from a left join b on a.id = b.id；

spark处理这个sql会对a和b的id字段做hash，把hash后处于相同分区的record发到同一台机器上进行join。

如果某一个 key 特别多，就会导致都被 hash 到同一台机器上，导致其执行缓慢，成为性能瓶颈。

引起倾斜的操作：

- 所有 join 的操作，除了 Doris 这种 colocate join，不涉及 shuffle 的，只要有 shuffle，就可能有倾斜
- 聚合函数：group by、distribute by ,窗口函数partition by
## 二、发现问题

如果作业是在运行过程中的话可以直接查看下面status=running的task的shuffle write size/records、shuffle read size/records数据明细，和其他task做比较，如果显著高于其他task可视为存在倾斜

![image](X63ndxw2romZOWxl9V7c3r8cnsc.png)

对于已经完成的，可以看 summary 统计

如果 max 明显比其他分位数高出很多，可以视为倾斜

![image](SzpTd7UWKoyA6wxrOypcLH6knl4.png)

### 解决：

Spark 数据倾斜的几种场景以及对应的解决方案，包括避免数据源倾斜，调整并行度，使用自定义 Partitioner，使用 Map 侧 Join 代替 Reduce 侧 Join（内存表合并），给倾斜 Key 加上随机前缀等。

什么是数据倾斜 对 Spark/Hadoop 这样的大数据系统来讲，数据量大并不可怕，可怕的是数据倾斜。数据倾斜指的是，并行处理的数据集中，某一部分（如 Spark 或 Kafka 的一个 Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈（木桶效应）。

数据倾斜是如何造成的 在 Spark 中，同一个 Stage 的不同 Partition 可以并行处理，而具有依赖关系的不同 Stage 之间是串行处理的。假设某个 Spark Job 分为 Stage 0和 Stage 1两个 Stage，且 Stage 1依赖于 Stage 0，那 Stage 0完全处理结束之前不会处理Stage 1。而 Stage 0可能包含 N 个 Task，这 N 个 Task 可以并行进行。如果其中 N-1个 Task 都在10秒内完成，而另外一个 Task 却耗时1分钟，那该 Stage 的总时间至少为1分钟。换句话说，一个 Stage 所耗费的时间，主要由最慢的那个 Task 决定。由于同一个 Stage 内的所有 Task 执行相同的计算，在排除不同计算节点计算能力差异的前提下，不同 Task 之间耗时的差异主要由该 Task 所处理的数据量决定。

具体解决方案 ：

1. 调整并行度分散同一个 Task 的不同 Key: Spark 在做 Shuffle 时，默认使用 HashPartitioner对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的 Key 对应的数据被分配到了同一个 Task 上，造成该 Task 所处理的数据远大于其它 Task，从而造成数据倾斜。如果调整 Shuffle 时的并行度，使得原本被分配到同一 Task 的不同 Key 发配到不同 Task 上处理，则可降低原 Task 所需处理的数据量，从而缓解数据倾斜问题造成的短板效应。图中左边绿色框表示 kv 样式的数据，key 可以理解成 name。可以看到 Task0 分配了许多的 key，调整并行度，多了几个 Task，那么每个 Task 处理的数据量就分散了。
1. 自定义Partitioner: 使用自定义的 Partitioner（默认为 HashPartitioner），将原本被分配到同一个 Task 的不同 Key 分配到不同 Task，可以拿上图继续想象一下，通过自定义 Partitioner 可以把原本分到 Task0 的 Key 分到 Task1，那么 Task0 的要处理的数据量就少了。
1. 将 Reduce side（侧） Join 转变为 Map side（侧） Join: 通过 Spark 的 Broadcast 机制，将 Reduce 侧 Join 转化为 Map 侧 Join，避免 Shuffle 从而完全消除 Shuffle 带来的数据倾斜。可以看到 RDD2 被加载到内存中了。
1. 为 skew 的 key 增加随机前/后缀: 为数据量特别大的 Key 增加随机前/后缀，使得原来 Key 相同的数据变为 Key 不相同的数据，从而使倾斜的数据集分散到不同的 Task 中，彻底解决数据倾斜问题。Join 另一则的数据中，与倾斜 Key 对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜 Key 如何加前缀，都能与之正常 Join。
1. 大表随机添加 N 种随机前缀，小表扩大 N 倍: 如果出现数据倾斜的 Key 比较多，上一种方法将这些大量的倾斜 Key 分拆出来，意义不大（很难一个 Key 一个 Key 都加上后缀）。此时更适合直接对存在数据倾斜的数据集全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集作笛卡尔乘积（即将数据量扩大 N 倍），可以看到 RDD2 扩大了 N 倍了，再和加完前缀的大数据做笛卡尔积。
### 输入倾斜

有些文件存储数据多，有些文件存储数据少，split时会按照文件进行切片，一个文件对应一个map task，数据量少的task计算快，数据量多的task计算慢。

1. **指定ETL策略**：修改参数`spark.hadoop.hive.exec.orc.split.strategy=ETL;`，对于小的尤其有数据倾斜的表，建议使用ETL策略。
1. **小文件合并**：`spark.hadoopRDD.targetBytesInPartition=134217728`，设置map端文件切分大小为128MB。
### shuffle倾斜

1. **（过滤少数导致倾斜的key）过滤异常 key**：如果发现导致倾斜的key就少数几个，且对计算本身影响不大，则可以将导致数据倾斜的少数key过滤掉，适用场景不多。
1. **（局部聚合+全局聚合）两阶段聚合**：将相同的key附加随机前缀，让原本被一个task处理的数据分散到多个task上做局部聚合。接着去除随机前缀，再次进行全局聚合，得到最终结果。仅适用于聚合类shuffle操作，不适用join shuffle。
1. **（消除 shuffle）大表和小表join，将reduce join改为map join**：reduce join是将相同key的数据汇聚到一个shuffle read task中进行join，而map join是将小表先拉取到内存，再将小表广播到大表中进行hash join。避免了shuffle阶段，从根本上消除了数据倾斜，spark sql添加参数`/*+ MAPJOIN(右表别名) */`。不适用于大表join大表，因为广播大表会引发OOM。
1. **（打散 key）大表join大表，增加随机前缀、扩容RDD并一分为二**：将倾斜严重表中导致数据倾斜的key拆分出来，并加上n以内的随机前缀。再对另一张表做同样的拆分，将导致倾斜的key对应的数据单独形成一份，并将该部分数据的key值加上1~n的前缀，即数据复制n倍，保证加上随机前缀的key能正常join，其他不会导致倾斜的数据不做处理。在join时会变成加上随机前缀的数据与膨胀n倍的数据关联，剩下两个均匀数据集进行关联，加上随机前缀后的key会分散到多个task中join，最后将两部分join结果union all合并。该方式会对数据集扩容，增加集群资源消耗。
## 5、数据倾斜的危害？

整体耗时大，不能发挥分布式系统的并行计算优势。（木桶效应，分布式系统的计算能力由耗时最长的任务决定）

部分机器处理的数据量过大，可能导致内存不足，任务失败。

## 6、具体怎样算数据倾斜？

所有并行节点平均执行时长avg，最大执行时长max：

`(max/avg > 2 && max > 30min) || max/avg > 10`
