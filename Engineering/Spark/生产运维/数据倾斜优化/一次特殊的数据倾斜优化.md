---
title: "一次特殊的数据倾斜优化"
type: concept
domain: engineering/spark/生产运维/数据倾斜优化
created: "2026-02-13"
updated: "2026-02-13"
tags:
  - engineering/spark/生产运维/数据倾斜优化
  - type/concept
---
# 一次特殊的数据倾斜优化

> 目标价需求，下游对有序性要求非常高

## 背景

这是一个比较特殊的数据倾斜场景。常规的倾斜优化手段（加盐打散、AQE skew join）都不好用，因为 **下游对数据的有序性有严格要求**——目标价需求中，同一个商品的价格变更必须按时间顺序处理，一旦乱序就会导致价格状态不一致。

作业链路大概是：

```
Kafka（商品价格变更事件）
    ↓
Spark Streaming / 批处理
    ↓
按商品 ID Join 维表 + 聚合计算
    ↓
写入下游存储（有序性要求）
```

问题：某些头部商品（爆品）的价格变更事件量是长尾商品的 **1000 倍以上**，导致对应分区的 Task 执行时间远超其他 Task。

## 为什么常规方案不行

### 加盐打散

```sql
-- 常规做法：在 key 后面拼随机数
SELECT concat(product_id, '_', floor(rand() * 10)) as salted_key, ...
```

**不行**。加盐后同一个商品的数据被分散到多个分区，无法保证同一商品的事件在同一个 Task 中按序处理。下游需要再做一次合并+排序，等于把问题后移了。

### AQE Skew Join

```properties
spark.sql.adaptive.skewJoin.enabled=true
```

AQE 的 skew join 会自动拆分倾斜分区，让多个 Task 并行处理同一个 key 的数据。但拆分后的多个 Task 各自产出部分结果，**无法保证全局有序性**。

### Broadcast Join

维表不大的话可以 Broadcast 避免 Shuffle，但这只解决 Join 阶段的倾斜。后续的聚合计算仍然倾斜。

## 实际方案：两阶段处理 + 分区内排序

### 第一阶段：预聚合（容忍乱序）

先做一轮不要求有序的预处理——过滤无效数据、补全维度信息、数据清洗。这一步可以放心用加盐打散：

```scala
val cleanedDF = rawDF
  // 加盐打散，解决 Join 倾斜
  .withColumn("salted_key", concat(col("product_id"), lit("_"), (rand() * 10).cast("int")))
  .join(broadcast(dimDF), col("product_id") === dimDF("id"))
  // 数据清洗
  .filter(col("price") > 0)
  .filter(col("event_time").isNotNull)
  // 去掉盐
  .drop("salted_key")
```

### 第二阶段：分区内有序写入

关键在于控制最终写入时的分区策略和排序：

```scala
val resultDF = cleanedDF
  // 按商品 ID 分区，保证同一商品在同一分区
  .repartition(col("product_id"))
  // 分区内按时间排序
  .sortWithinPartitions(col("product_id"), col("event_time"))

// 写入时保持分区内有序
resultDF.write
  .mode("overwrite")
  .option("maxRecordsPerFile", 500000) // 控制文件大小
  .parquet("output_path")
```

**核心思路**：`repartition(col("product_id"))` 确保同一商品在同一分区，`sortWithinPartitions` 确保分区内按时间排序。下游读取时只要按分区顺序消费，就能保证有序性。

### 但 repartition 不是又倾斜了？

是的。所以需要对热点商品做特殊处理：

```scala
// 识别热点商品
val hotProducts = rawDF
  .groupBy("product_id")
  .count()
  .filter(col("count") > threshold)
  .select("product_id")
  .collect()
  .map(_.getString(0))
  .toSet

val hotProductsBV = spark.sparkContext.broadcast(hotProducts)

// 热点商品单独处理
val hotDF = cleanedDF.filter(row => hotProductsBV.value.contains(row.getAs[String]("product_id")))
val normalDF = cleanedDF.filter(row => !hotProductsBV.value.contains(row.getAs[String]("product_id")))

// 正常商品：常规分区
val normalResult = normalDF
  .repartition(500, col("product_id"))
  .sortWithinPartitions("product_id", "event_time")

// 热点商品：按时间窗口二次拆分
val hotResult = hotDF
  .withColumn("time_bucket", (unix_timestamp(col("event_time")) / 3600).cast("long"))
  .repartition(col("product_id"), col("time_bucket"))
  .sortWithinPartitions("product_id", "event_time")
  .drop("time_bucket")
```

热点商品按小时级时间窗口拆分，每个窗口内保证有序。下游消费时按 `(product_id, time_bucket)` 顺序消费即可。这是在 **并行度** 和 **有序性** 之间的折中。

## 效果

| 指标 | 优化前 | 优化后 |
|------|--------|--------|
| 最长 Task 耗时 | 45 min | 8 min |
| 总作业耗时 | 52 min | 15 min |
| 数据有序性 | 保证 | 保证（窗口内） |

## 经验总结

1. **有序性要求让很多常规优化手段失效**，需要在方案设计层面做 trade-off
2. **两阶段处理**（无序预处理 + 有序写入）是一个通用模式
3. **热点分离**：识别 hot key，单独处理，避免一刀切
4. **时间窗口拆分**：在需要有序的场景，按时间窗口做分片是一个好的折中
5. 不要试图在一个 Stage 里同时解决倾斜和有序性问题

## 相关

- [[数据倾斜优化]]
- [[Engineering/Spark/SQL/Spark Shuffle|Spark Shuffle]]
- [[Engineering/Spark/SQL/Spark SQL|Spark SQL]]
- [[Engineering/Spark/生产运维/调优/Spark 调优|Spark 调优]]
- [[Engineering/Spark/SQL/Spark AQE + DDP|Spark AQE + DDP]]
- [[Engineering/Spark/存储/RDD/Spark Partitioner|Spark Partitioner]]
- [[Engineering/Spark/生产运维/ETL 性能优化|ETL 性能优化]]
